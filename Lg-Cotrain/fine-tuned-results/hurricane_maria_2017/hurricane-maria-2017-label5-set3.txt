Running with 5 label/class set 3

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-12 15:01:21 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-12 15:01:21 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_maria_2017
Using devices: cuda, cuda
2026-02-12 15:01:21 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 15:01:21 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 15:01:21 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 15:01:21 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 1.6282981065938907e-05
Weight Decay: 1.2845477442268009e-05
Batch Size: 32
No. Epochs: 13
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-12 15:01:22 - INFO - Learning Rate: 1.6282981065938907e-05
Weight Decay: 1.2845477442268009e-05
Batch Size: 32
No. Epochs: 13
Epoch Patience: 6
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 15:01:23 - INFO - Generating initial weights
Time taken for Epoch 1:20.50 - F1: 0.0596
2026-02-12 15:01:47 - INFO - Time taken for Epoch 1:20.50 - F1: 0.0596
Time taken for Epoch 2:20.22 - F1: 0.0808
2026-02-12 15:02:07 - INFO - Time taken for Epoch 2:20.22 - F1: 0.0808
Time taken for Epoch 3:20.25 - F1: 0.0917
2026-02-12 15:02:28 - INFO - Time taken for Epoch 3:20.25 - F1: 0.0917
Time taken for Epoch 4:20.26 - F1: 0.1072
2026-02-12 15:02:48 - INFO - Time taken for Epoch 4:20.26 - F1: 0.1072
Time taken for Epoch 5:20.29 - F1: 0.1067
2026-02-12 15:03:08 - INFO - Time taken for Epoch 5:20.29 - F1: 0.1067
Time taken for Epoch 6:20.32 - F1: 0.1193
2026-02-12 15:03:28 - INFO - Time taken for Epoch 6:20.32 - F1: 0.1193
Time taken for Epoch 7:20.32 - F1: 0.1231
2026-02-12 15:03:49 - INFO - Time taken for Epoch 7:20.32 - F1: 0.1231
Time taken for Epoch 8:20.33 - F1: 0.1252
2026-02-12 15:04:09 - INFO - Time taken for Epoch 8:20.33 - F1: 0.1252
Time taken for Epoch 9:20.38 - F1: 0.1416
2026-02-12 15:04:29 - INFO - Time taken for Epoch 9:20.38 - F1: 0.1416
Time taken for Epoch 10:20.33 - F1: 0.1486
2026-02-12 15:04:50 - INFO - Time taken for Epoch 10:20.33 - F1: 0.1486
Time taken for Epoch 11:20.38 - F1: 0.1522
2026-02-12 15:05:10 - INFO - Time taken for Epoch 11:20.38 - F1: 0.1522
Time taken for Epoch 12:20.35 - F1: 0.1512
2026-02-12 15:05:31 - INFO - Time taken for Epoch 12:20.35 - F1: 0.1512
Time taken for Epoch 13:20.32 - F1: 0.1535
2026-02-12 15:05:51 - INFO - Time taken for Epoch 13:20.32 - F1: 0.1535
Best F1:0.1535 - Best Epoch:13
2026-02-12 15:05:51 - INFO - Best F1:0.1535 - Best Epoch:13
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 15:05:52 - INFO - Starting co-training
Time taken for Epoch 1: 35.67s - F1: 0.32581341
2026-02-12 15:06:28 - INFO - Time taken for Epoch 1: 35.67s - F1: 0.32581341
Time taken for Epoch 2: 36.74s - F1: 0.47434301
2026-02-12 15:07:05 - INFO - Time taken for Epoch 2: 36.74s - F1: 0.47434301
Time taken for Epoch 3: 36.83s - F1: 0.52196637
2026-02-12 15:07:42 - INFO - Time taken for Epoch 3: 36.83s - F1: 0.52196637
Time taken for Epoch 4: 36.84s - F1: 0.55185530
2026-02-12 15:08:19 - INFO - Time taken for Epoch 4: 36.84s - F1: 0.55185530
Time taken for Epoch 5: 36.87s - F1: 0.60650578
2026-02-12 15:08:56 - INFO - Time taken for Epoch 5: 36.87s - F1: 0.60650578
Time taken for Epoch 6: 37.00s - F1: 0.61389065
2026-02-12 15:09:33 - INFO - Time taken for Epoch 6: 37.00s - F1: 0.61389065
Time taken for Epoch 7: 36.88s - F1: 0.59961229
2026-02-12 15:10:09 - INFO - Time taken for Epoch 7: 36.88s - F1: 0.59961229
Time taken for Epoch 8: 35.74s - F1: 0.62325815
2026-02-12 15:10:45 - INFO - Time taken for Epoch 8: 35.74s - F1: 0.62325815
Time taken for Epoch 9: 36.91s - F1: 0.64493046
2026-02-12 15:11:22 - INFO - Time taken for Epoch 9: 36.91s - F1: 0.64493046
Time taken for Epoch 10: 37.15s - F1: 0.63905379
2026-02-12 15:11:59 - INFO - Time taken for Epoch 10: 37.15s - F1: 0.63905379
Time taken for Epoch 11: 35.71s - F1: 0.65640522
2026-02-12 15:12:35 - INFO - Time taken for Epoch 11: 35.71s - F1: 0.65640522
Time taken for Epoch 12: 36.86s - F1: 0.64028086
2026-02-12 15:13:12 - INFO - Time taken for Epoch 12: 36.86s - F1: 0.64028086
Time taken for Epoch 13: 35.73s - F1: 0.63920477
2026-02-12 15:13:48 - INFO - Time taken for Epoch 13: 35.73s - F1: 0.63920477
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 15:13:51 - INFO - Fine-tuning models
Time taken for Epoch 1:2.82 - F1: 0.6689
2026-02-12 15:13:54 - INFO - Time taken for Epoch 1:2.82 - F1: 0.6689
Time taken for Epoch 2:3.90 - F1: 0.6589
2026-02-12 15:13:58 - INFO - Time taken for Epoch 2:3.90 - F1: 0.6589
Time taken for Epoch 3:2.82 - F1: 0.6615
2026-02-12 15:14:00 - INFO - Time taken for Epoch 3:2.82 - F1: 0.6615
Time taken for Epoch 4:2.82 - F1: 0.6592
2026-02-12 15:14:03 - INFO - Time taken for Epoch 4:2.82 - F1: 0.6592
Time taken for Epoch 5:2.82 - F1: 0.6568
2026-02-12 15:14:06 - INFO - Time taken for Epoch 5:2.82 - F1: 0.6568
Time taken for Epoch 6:2.82 - F1: 0.6453
2026-02-12 15:14:09 - INFO - Time taken for Epoch 6:2.82 - F1: 0.6453
Time taken for Epoch 7:2.83 - F1: 0.6468
2026-02-12 15:14:12 - INFO - Time taken for Epoch 7:2.83 - F1: 0.6468
Time taken for Epoch 8:2.83 - F1: 0.6488
2026-02-12 15:14:15 - INFO - Time taken for Epoch 8:2.83 - F1: 0.6488
Time taken for Epoch 9:2.83 - F1: 0.6505
2026-02-12 15:14:17 - INFO - Time taken for Epoch 9:2.83 - F1: 0.6505
Time taken for Epoch 10:2.82 - F1: 0.6524
2026-02-12 15:14:20 - INFO - Time taken for Epoch 10:2.82 - F1: 0.6524
Time taken for Epoch 11:2.82 - F1: 0.6536
2026-02-12 15:14:23 - INFO - Time taken for Epoch 11:2.82 - F1: 0.6536
Performance not improving for 10 consecutive epochs.
2026-02-12 15:14:23 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6689 - Best Epoch:0
2026-02-12 15:14:23 - INFO - Best F1:0.6689 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6723, Test ECE: 0.0177
2026-02-12 15:14:31 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6723, Test ECE: 0.0177
All results: {'f1_macro': 0.6722594221084504, 'ece': np.float64(0.017721765993702926)}
2026-02-12 15:14:31 - INFO - All results: {'f1_macro': 0.6722594221084504, 'ece': np.float64(0.017721765993702926)}

Total time taken: 790.64 seconds
2026-02-12 15:14:31 - INFO - 
Total time taken: 790.64 seconds
2026-02-12 15:14:32 - INFO - Trial 0 finished with value: 0.6722594221084504 and parameters: {'learning_rate': 1.6282981065938907e-05, 'weight_decay': 1.2845477442268009e-05, 'batch_size': 32, 'co_train_epochs': 13, 'epoch_patience': 6}. Best is trial 0 with value: 0.6722594221084504.
Using devices: cuda, cuda
2026-02-12 15:14:32 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 15:14:32 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 15:14:32 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 15:14:32 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 4.3691926416070904e-05
Weight Decay: 9.923466932895234e-05
Batch Size: 32
No. Epochs: 15
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-12 15:14:32 - INFO - Learning Rate: 4.3691926416070904e-05
Weight Decay: 9.923466932895234e-05
Batch Size: 32
No. Epochs: 15
Epoch Patience: 6
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 15:14:33 - INFO - Generating initial weights
Time taken for Epoch 1:20.40 - F1: 0.0762
2026-02-12 15:14:57 - INFO - Time taken for Epoch 1:20.40 - F1: 0.0762
Time taken for Epoch 2:20.34 - F1: 0.1240
2026-02-12 15:15:17 - INFO - Time taken for Epoch 2:20.34 - F1: 0.1240
Time taken for Epoch 3:20.34 - F1: 0.1004
2026-02-12 15:15:38 - INFO - Time taken for Epoch 3:20.34 - F1: 0.1004
Time taken for Epoch 4:20.29 - F1: 0.1138
2026-02-12 15:15:58 - INFO - Time taken for Epoch 4:20.29 - F1: 0.1138
Time taken for Epoch 5:20.39 - F1: 0.1475
2026-02-12 15:16:18 - INFO - Time taken for Epoch 5:20.39 - F1: 0.1475
Time taken for Epoch 6:20.31 - F1: 0.1879
2026-02-12 15:16:39 - INFO - Time taken for Epoch 6:20.31 - F1: 0.1879
Time taken for Epoch 7:20.31 - F1: 0.2180
2026-02-12 15:16:59 - INFO - Time taken for Epoch 7:20.31 - F1: 0.2180
Time taken for Epoch 8:20.36 - F1: 0.2490
2026-02-12 15:17:19 - INFO - Time taken for Epoch 8:20.36 - F1: 0.2490
Time taken for Epoch 9:20.37 - F1: 0.2509
2026-02-12 15:17:40 - INFO - Time taken for Epoch 9:20.37 - F1: 0.2509
Time taken for Epoch 10:20.38 - F1: 0.2676
2026-02-12 15:18:00 - INFO - Time taken for Epoch 10:20.38 - F1: 0.2676
Time taken for Epoch 11:20.39 - F1: 0.2664
2026-02-12 15:18:20 - INFO - Time taken for Epoch 11:20.39 - F1: 0.2664
Time taken for Epoch 12:20.35 - F1: 0.2690
2026-02-12 15:18:41 - INFO - Time taken for Epoch 12:20.35 - F1: 0.2690
Time taken for Epoch 13:20.37 - F1: 0.2760
2026-02-12 15:19:01 - INFO - Time taken for Epoch 13:20.37 - F1: 0.2760
Time taken for Epoch 14:20.39 - F1: 0.2777
2026-02-12 15:19:22 - INFO - Time taken for Epoch 14:20.39 - F1: 0.2777
Time taken for Epoch 15:20.35 - F1: 0.2830
2026-02-12 15:19:42 - INFO - Time taken for Epoch 15:20.35 - F1: 0.2830
Best F1:0.2830 - Best Epoch:15
2026-02-12 15:19:42 - INFO - Best F1:0.2830 - Best Epoch:15
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 15:19:43 - INFO - Starting co-training
Time taken for Epoch 1: 35.66s - F1: 0.53726294
2026-02-12 15:20:19 - INFO - Time taken for Epoch 1: 35.66s - F1: 0.53726294
Time taken for Epoch 2: 36.88s - F1: 0.59016978
2026-02-12 15:20:56 - INFO - Time taken for Epoch 2: 36.88s - F1: 0.59016978
Time taken for Epoch 3: 36.93s - F1: 0.62298927
2026-02-12 15:21:33 - INFO - Time taken for Epoch 3: 36.93s - F1: 0.62298927
Time taken for Epoch 4: 36.85s - F1: 0.62851202
2026-02-12 15:22:10 - INFO - Time taken for Epoch 4: 36.85s - F1: 0.62851202
Time taken for Epoch 5: 36.96s - F1: 0.63056135
2026-02-12 15:22:47 - INFO - Time taken for Epoch 5: 36.96s - F1: 0.63056135
Time taken for Epoch 6: 36.93s - F1: 0.64482698
2026-02-12 15:23:24 - INFO - Time taken for Epoch 6: 36.93s - F1: 0.64482698
Time taken for Epoch 7: 36.92s - F1: 0.63591850
2026-02-12 15:24:01 - INFO - Time taken for Epoch 7: 36.92s - F1: 0.63591850
Time taken for Epoch 8: 35.70s - F1: 0.64686381
2026-02-12 15:24:36 - INFO - Time taken for Epoch 8: 35.70s - F1: 0.64686381
Time taken for Epoch 9: 36.94s - F1: 0.64541741
2026-02-12 15:25:13 - INFO - Time taken for Epoch 9: 36.94s - F1: 0.64541741
Time taken for Epoch 10: 35.73s - F1: 0.63713264
2026-02-12 15:25:49 - INFO - Time taken for Epoch 10: 35.73s - F1: 0.63713264
Time taken for Epoch 11: 35.75s - F1: 0.64697537
2026-02-12 15:26:25 - INFO - Time taken for Epoch 11: 35.75s - F1: 0.64697537
Time taken for Epoch 12: 36.94s - F1: 0.66290838
2026-02-12 15:27:02 - INFO - Time taken for Epoch 12: 36.94s - F1: 0.66290838
Time taken for Epoch 13: 36.97s - F1: 0.65985277
2026-02-12 15:27:39 - INFO - Time taken for Epoch 13: 36.97s - F1: 0.65985277
Time taken for Epoch 14: 35.73s - F1: 0.64099297
2026-02-12 15:28:14 - INFO - Time taken for Epoch 14: 35.73s - F1: 0.64099297
Time taken for Epoch 15: 35.77s - F1: 0.66358683
2026-02-12 15:28:50 - INFO - Time taken for Epoch 15: 35.77s - F1: 0.66358683
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 15:28:54 - INFO - Fine-tuning models
Time taken for Epoch 1:2.82 - F1: 0.6641
2026-02-12 15:28:58 - INFO - Time taken for Epoch 1:2.82 - F1: 0.6641
Time taken for Epoch 2:3.92 - F1: 0.6536
2026-02-12 15:29:01 - INFO - Time taken for Epoch 2:3.92 - F1: 0.6536
Time taken for Epoch 3:2.82 - F1: 0.6554
2026-02-12 15:29:04 - INFO - Time taken for Epoch 3:2.82 - F1: 0.6554
Time taken for Epoch 4:2.82 - F1: 0.6535
2026-02-12 15:29:07 - INFO - Time taken for Epoch 4:2.82 - F1: 0.6535
Time taken for Epoch 5:2.81 - F1: 0.6512
2026-02-12 15:29:10 - INFO - Time taken for Epoch 5:2.81 - F1: 0.6512
Time taken for Epoch 6:2.82 - F1: 0.6497
2026-02-12 15:29:13 - INFO - Time taken for Epoch 6:2.82 - F1: 0.6497
Time taken for Epoch 7:2.81 - F1: 0.6396
2026-02-12 15:29:16 - INFO - Time taken for Epoch 7:2.81 - F1: 0.6396
Time taken for Epoch 8:2.81 - F1: 0.6347
2026-02-12 15:29:18 - INFO - Time taken for Epoch 8:2.81 - F1: 0.6347
Time taken for Epoch 9:2.82 - F1: 0.6320
2026-02-12 15:29:21 - INFO - Time taken for Epoch 9:2.82 - F1: 0.6320
Time taken for Epoch 10:2.82 - F1: 0.6202
2026-02-12 15:29:24 - INFO - Time taken for Epoch 10:2.82 - F1: 0.6202
Time taken for Epoch 11:2.83 - F1: 0.6209
2026-02-12 15:29:27 - INFO - Time taken for Epoch 11:2.83 - F1: 0.6209
Performance not improving for 10 consecutive epochs.
2026-02-12 15:29:27 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6641 - Best Epoch:0
2026-02-12 15:29:27 - INFO - Best F1:0.6641 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6580, Test ECE: 0.0202
2026-02-12 15:29:35 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6580, Test ECE: 0.0202
All results: {'f1_macro': 0.6580055153864778, 'ece': np.float64(0.020181901825284492)}
2026-02-12 15:29:35 - INFO - All results: {'f1_macro': 0.6580055153864778, 'ece': np.float64(0.020181901825284492)}

Total time taken: 903.46 seconds
2026-02-12 15:29:35 - INFO - 
Total time taken: 903.46 seconds
2026-02-12 15:29:35 - INFO - Trial 1 finished with value: 0.6580055153864778 and parameters: {'learning_rate': 4.3691926416070904e-05, 'weight_decay': 9.923466932895234e-05, 'batch_size': 32, 'co_train_epochs': 15, 'epoch_patience': 6}. Best is trial 0 with value: 0.6722594221084504.
Using devices: cuda, cuda
2026-02-12 15:29:35 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 15:29:35 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 15:29:35 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 15:29:35 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.00043935099061766144
Weight Decay: 0.0016819023765707337
Batch Size: 8
No. Epochs: 18
Epoch Patience: 8
 Accumulation Steps: 8
2026-02-12 15:29:36 - INFO - Learning Rate: 0.00043935099061766144
Weight Decay: 0.0016819023765707337
Batch Size: 8
No. Epochs: 18
Epoch Patience: 8
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 15:29:37 - INFO - Generating initial weights
Time taken for Epoch 1:22.78 - F1: 0.0189
2026-02-12 15:30:03 - INFO - Time taken for Epoch 1:22.78 - F1: 0.0189
Time taken for Epoch 2:22.68 - F1: 0.0695
2026-02-12 15:30:26 - INFO - Time taken for Epoch 2:22.68 - F1: 0.0695
Time taken for Epoch 3:22.72 - F1: 0.0757
2026-02-12 15:30:49 - INFO - Time taken for Epoch 3:22.72 - F1: 0.0757
Time taken for Epoch 4:22.73 - F1: 0.1340
2026-02-12 15:31:11 - INFO - Time taken for Epoch 4:22.73 - F1: 0.1340
Time taken for Epoch 5:22.73 - F1: 0.2469
2026-02-12 15:31:34 - INFO - Time taken for Epoch 5:22.73 - F1: 0.2469
Time taken for Epoch 6:22.73 - F1: 0.2819
2026-02-12 15:31:57 - INFO - Time taken for Epoch 6:22.73 - F1: 0.2819
Time taken for Epoch 7:22.87 - F1: 0.3232
2026-02-12 15:32:20 - INFO - Time taken for Epoch 7:22.87 - F1: 0.3232
Time taken for Epoch 8:22.75 - F1: 0.3118
2026-02-12 15:32:42 - INFO - Time taken for Epoch 8:22.75 - F1: 0.3118
Time taken for Epoch 9:22.80 - F1: 0.3355
2026-02-12 15:33:05 - INFO - Time taken for Epoch 9:22.80 - F1: 0.3355
Time taken for Epoch 10:22.80 - F1: 0.3217
2026-02-12 15:33:28 - INFO - Time taken for Epoch 10:22.80 - F1: 0.3217
Time taken for Epoch 11:22.78 - F1: 0.3381
2026-02-12 15:33:51 - INFO - Time taken for Epoch 11:22.78 - F1: 0.3381
Time taken for Epoch 12:22.77 - F1: 0.3649
2026-02-12 15:34:13 - INFO - Time taken for Epoch 12:22.77 - F1: 0.3649
Time taken for Epoch 13:22.80 - F1: 0.3544
2026-02-12 15:34:36 - INFO - Time taken for Epoch 13:22.80 - F1: 0.3544
Time taken for Epoch 14:22.76 - F1: 0.3379
2026-02-12 15:34:59 - INFO - Time taken for Epoch 14:22.76 - F1: 0.3379
Time taken for Epoch 15:22.75 - F1: 0.3425
2026-02-12 15:35:22 - INFO - Time taken for Epoch 15:22.75 - F1: 0.3425
Time taken for Epoch 16:22.74 - F1: 0.3505
2026-02-12 15:35:45 - INFO - Time taken for Epoch 16:22.74 - F1: 0.3505
Time taken for Epoch 17:22.75 - F1: 0.3315
2026-02-12 15:36:07 - INFO - Time taken for Epoch 17:22.75 - F1: 0.3315
Time taken for Epoch 18:22.78 - F1: 0.3140
2026-02-12 15:36:30 - INFO - Time taken for Epoch 18:22.78 - F1: 0.3140
Best F1:0.3649 - Best Epoch:12
2026-02-12 15:36:30 - INFO - Best F1:0.3649 - Best Epoch:12
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 15:36:31 - INFO - Starting co-training
Time taken for Epoch 1: 27.82s - F1: 0.04755179
2026-02-12 15:37:00 - INFO - Time taken for Epoch 1: 27.82s - F1: 0.04755179
Time taken for Epoch 2: 28.88s - F1: 0.04755179
2026-02-12 15:37:29 - INFO - Time taken for Epoch 2: 28.88s - F1: 0.04755179
Time taken for Epoch 3: 27.88s - F1: 0.04755179
2026-02-12 15:37:56 - INFO - Time taken for Epoch 3: 27.88s - F1: 0.04755179
Time taken for Epoch 4: 27.79s - F1: 0.04755179
2026-02-12 15:38:24 - INFO - Time taken for Epoch 4: 27.79s - F1: 0.04755179
Time taken for Epoch 5: 27.78s - F1: 0.04755179
2026-02-12 15:38:52 - INFO - Time taken for Epoch 5: 27.78s - F1: 0.04755179
Time taken for Epoch 6: 27.88s - F1: 0.04755179
2026-02-12 15:39:20 - INFO - Time taken for Epoch 6: 27.88s - F1: 0.04755179
Time taken for Epoch 7: 27.72s - F1: 0.04755179
2026-02-12 15:39:48 - INFO - Time taken for Epoch 7: 27.72s - F1: 0.04755179
Time taken for Epoch 8: 27.81s - F1: 0.04755179
2026-02-12 15:40:15 - INFO - Time taken for Epoch 8: 27.81s - F1: 0.04755179
Time taken for Epoch 9: 27.80s - F1: 0.04755179
2026-02-12 15:40:43 - INFO - Time taken for Epoch 9: 27.80s - F1: 0.04755179
Performance not improving for 8 consecutive epochs.
Performance not improving for 8 consecutive epochs.
2026-02-12 15:40:43 - INFO - Performance not improving for 8 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 15:40:46 - INFO - Fine-tuning models
Time taken for Epoch 1:3.11 - F1: 0.0476
2026-02-12 15:40:49 - INFO - Time taken for Epoch 1:3.11 - F1: 0.0476
Time taken for Epoch 2:4.11 - F1: 0.0189
2026-02-12 15:40:53 - INFO - Time taken for Epoch 2:4.11 - F1: 0.0189
Time taken for Epoch 3:3.09 - F1: 0.0189
2026-02-12 15:40:56 - INFO - Time taken for Epoch 3:3.09 - F1: 0.0189
Time taken for Epoch 4:3.09 - F1: 0.0189
2026-02-12 15:40:59 - INFO - Time taken for Epoch 4:3.09 - F1: 0.0189
Time taken for Epoch 5:3.09 - F1: 0.0189
2026-02-12 15:41:03 - INFO - Time taken for Epoch 5:3.09 - F1: 0.0189
Time taken for Epoch 6:3.08 - F1: 0.0189
2026-02-12 15:41:06 - INFO - Time taken for Epoch 6:3.08 - F1: 0.0189
Time taken for Epoch 7:3.09 - F1: 0.0189
2026-02-12 15:41:09 - INFO - Time taken for Epoch 7:3.09 - F1: 0.0189
Time taken for Epoch 8:3.09 - F1: 0.0189
2026-02-12 15:41:12 - INFO - Time taken for Epoch 8:3.09 - F1: 0.0189
Time taken for Epoch 9:3.09 - F1: 0.0189
2026-02-12 15:41:15 - INFO - Time taken for Epoch 9:3.09 - F1: 0.0189
Time taken for Epoch 10:3.09 - F1: 0.0189
2026-02-12 15:41:18 - INFO - Time taken for Epoch 10:3.09 - F1: 0.0189
Time taken for Epoch 11:3.09 - F1: 0.0189
2026-02-12 15:41:21 - INFO - Time taken for Epoch 11:3.09 - F1: 0.0189
Performance not improving for 10 consecutive epochs.
2026-02-12 15:41:21 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:0
2026-02-12 15:41:21 - INFO - Best F1:0.0476 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.1188
2026-02-12 15:41:30 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.1188
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.11880915400141984)}
2026-02-12 15:41:30 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.11880915400141984)}

Total time taken: 714.77 seconds
2026-02-12 15:41:30 - INFO - 
Total time taken: 714.77 seconds
2026-02-12 15:41:30 - INFO - Trial 2 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.00043935099061766144, 'weight_decay': 0.0016819023765707337, 'batch_size': 8, 'co_train_epochs': 18, 'epoch_patience': 8}. Best is trial 0 with value: 0.6722594221084504.
Using devices: cuda, cuda
2026-02-12 15:41:30 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 15:41:30 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 15:41:30 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 15:41:30 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 3.369592985100064e-05
Weight Decay: 0.0031920142783368214
Batch Size: 64
No. Epochs: 14
Epoch Patience: 4
 Accumulation Steps: 1
2026-02-12 15:41:30 - INFO - Learning Rate: 3.369592985100064e-05
Weight Decay: 0.0031920142783368214
Batch Size: 64
No. Epochs: 14
Epoch Patience: 4
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 15:41:31 - INFO - Generating initial weights
Time taken for Epoch 1:19.28 - F1: 0.0739
2026-02-12 15:41:54 - INFO - Time taken for Epoch 1:19.28 - F1: 0.0739
Time taken for Epoch 2:19.23 - F1: 0.1203
2026-02-12 15:42:14 - INFO - Time taken for Epoch 2:19.23 - F1: 0.1203
Time taken for Epoch 3:19.24 - F1: 0.1270
2026-02-12 15:42:33 - INFO - Time taken for Epoch 3:19.24 - F1: 0.1270
Time taken for Epoch 4:19.23 - F1: 0.1194
2026-02-12 15:42:52 - INFO - Time taken for Epoch 4:19.23 - F1: 0.1194
Time taken for Epoch 5:19.27 - F1: 0.1339
2026-02-12 15:43:11 - INFO - Time taken for Epoch 5:19.27 - F1: 0.1339
Time taken for Epoch 6:19.28 - F1: 0.1544
2026-02-12 15:43:31 - INFO - Time taken for Epoch 6:19.28 - F1: 0.1544
Time taken for Epoch 7:19.23 - F1: 0.1687
2026-02-12 15:43:50 - INFO - Time taken for Epoch 7:19.23 - F1: 0.1687
Time taken for Epoch 8:19.30 - F1: 0.1934
2026-02-12 15:44:09 - INFO - Time taken for Epoch 8:19.30 - F1: 0.1934
Time taken for Epoch 9:19.26 - F1: 0.2131
2026-02-12 15:44:28 - INFO - Time taken for Epoch 9:19.26 - F1: 0.2131
Time taken for Epoch 10:19.33 - F1: 0.2255
2026-02-12 15:44:48 - INFO - Time taken for Epoch 10:19.33 - F1: 0.2255
Time taken for Epoch 11:19.28 - F1: 0.2251
2026-02-12 15:45:07 - INFO - Time taken for Epoch 11:19.28 - F1: 0.2251
Time taken for Epoch 12:19.24 - F1: 0.2416
2026-02-12 15:45:26 - INFO - Time taken for Epoch 12:19.24 - F1: 0.2416
Time taken for Epoch 13:19.25 - F1: 0.2360
2026-02-12 15:45:45 - INFO - Time taken for Epoch 13:19.25 - F1: 0.2360
Time taken for Epoch 14:19.28 - F1: 0.2402
2026-02-12 15:46:05 - INFO - Time taken for Epoch 14:19.28 - F1: 0.2402
Best F1:0.2416 - Best Epoch:12
2026-02-12 15:46:05 - INFO - Best F1:0.2416 - Best Epoch:12
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 15:46:06 - INFO - Starting co-training
Time taken for Epoch 1: 46.54s - F1: 0.50593205
2026-02-12 15:46:53 - INFO - Time taken for Epoch 1: 46.54s - F1: 0.50593205
Time taken for Epoch 2: 47.85s - F1: 0.63227795
2026-02-12 15:47:41 - INFO - Time taken for Epoch 2: 47.85s - F1: 0.63227795
Time taken for Epoch 3: 47.97s - F1: 0.63771448
2026-02-12 15:48:29 - INFO - Time taken for Epoch 3: 47.97s - F1: 0.63771448
Time taken for Epoch 4: 48.01s - F1: 0.62798793
2026-02-12 15:49:17 - INFO - Time taken for Epoch 4: 48.01s - F1: 0.62798793
Time taken for Epoch 5: 46.71s - F1: 0.64245238
2026-02-12 15:50:04 - INFO - Time taken for Epoch 5: 46.71s - F1: 0.64245238
Time taken for Epoch 6: 48.01s - F1: 0.62800015
2026-02-12 15:50:52 - INFO - Time taken for Epoch 6: 48.01s - F1: 0.62800015
Time taken for Epoch 7: 46.72s - F1: 0.66570180
2026-02-12 15:51:38 - INFO - Time taken for Epoch 7: 46.72s - F1: 0.66570180
Time taken for Epoch 8: 48.00s - F1: 0.65097554
2026-02-12 15:52:26 - INFO - Time taken for Epoch 8: 48.00s - F1: 0.65097554
Time taken for Epoch 9: 46.71s - F1: 0.64254508
2026-02-12 15:53:13 - INFO - Time taken for Epoch 9: 46.71s - F1: 0.64254508
Time taken for Epoch 10: 46.75s - F1: 0.64935114
2026-02-12 15:54:00 - INFO - Time taken for Epoch 10: 46.75s - F1: 0.64935114
Time taken for Epoch 11: 46.75s - F1: 0.64291412
2026-02-12 15:54:46 - INFO - Time taken for Epoch 11: 46.75s - F1: 0.64291412
Performance not improving for 4 consecutive epochs.
Performance not improving for 4 consecutive epochs.
2026-02-12 15:54:46 - INFO - Performance not improving for 4 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 15:54:49 - INFO - Fine-tuning models
Time taken for Epoch 1:2.68 - F1: 0.6656
2026-02-12 15:54:52 - INFO - Time taken for Epoch 1:2.68 - F1: 0.6656
Time taken for Epoch 2:3.93 - F1: 0.6613
2026-02-12 15:54:56 - INFO - Time taken for Epoch 2:3.93 - F1: 0.6613
Time taken for Epoch 3:2.68 - F1: 0.6729
2026-02-12 15:54:59 - INFO - Time taken for Epoch 3:2.68 - F1: 0.6729
Time taken for Epoch 4:4.04 - F1: 0.6840
2026-02-12 15:55:03 - INFO - Time taken for Epoch 4:4.04 - F1: 0.6840
Time taken for Epoch 5:4.02 - F1: 0.6743
2026-02-12 15:55:07 - INFO - Time taken for Epoch 5:4.02 - F1: 0.6743
Time taken for Epoch 6:2.68 - F1: 0.6737
2026-02-12 15:55:10 - INFO - Time taken for Epoch 6:2.68 - F1: 0.6737
Time taken for Epoch 7:2.67 - F1: 0.6706
2026-02-12 15:55:12 - INFO - Time taken for Epoch 7:2.67 - F1: 0.6706
Time taken for Epoch 8:2.68 - F1: 0.6728
2026-02-12 15:55:15 - INFO - Time taken for Epoch 8:2.68 - F1: 0.6728
Time taken for Epoch 9:2.67 - F1: 0.6751
2026-02-12 15:55:18 - INFO - Time taken for Epoch 9:2.67 - F1: 0.6751
Time taken for Epoch 10:2.67 - F1: 0.6783
2026-02-12 15:55:20 - INFO - Time taken for Epoch 10:2.67 - F1: 0.6783
Time taken for Epoch 11:2.67 - F1: 0.6782
2026-02-12 15:55:23 - INFO - Time taken for Epoch 11:2.67 - F1: 0.6782
Time taken for Epoch 12:2.67 - F1: 0.6707
2026-02-12 15:55:26 - INFO - Time taken for Epoch 12:2.67 - F1: 0.6707
Time taken for Epoch 13:2.67 - F1: 0.6694
2026-02-12 15:55:28 - INFO - Time taken for Epoch 13:2.67 - F1: 0.6694
Time taken for Epoch 14:2.67 - F1: 0.6694
2026-02-12 15:55:31 - INFO - Time taken for Epoch 14:2.67 - F1: 0.6694
Performance not improving for 10 consecutive epochs.
2026-02-12 15:55:31 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6840 - Best Epoch:3
2026-02-12 15:55:31 - INFO - Best F1:0.6840 - Best Epoch:3
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6744, Test ECE: 0.0399
2026-02-12 15:55:39 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6744, Test ECE: 0.0399
All results: {'f1_macro': 0.674431911817966, 'ece': np.float64(0.039884870946820666)}
2026-02-12 15:55:39 - INFO - All results: {'f1_macro': 0.674431911817966, 'ece': np.float64(0.039884870946820666)}

Total time taken: 849.29 seconds
2026-02-12 15:55:39 - INFO - 
Total time taken: 849.29 seconds
2026-02-12 15:55:39 - INFO - Trial 3 finished with value: 0.674431911817966 and parameters: {'learning_rate': 3.369592985100064e-05, 'weight_decay': 0.0031920142783368214, 'batch_size': 64, 'co_train_epochs': 14, 'epoch_patience': 4}. Best is trial 3 with value: 0.674431911817966.
Using devices: cuda, cuda
2026-02-12 15:55:39 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 15:55:39 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 15:55:39 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 15:55:39 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 3.9205870955943774e-05
Weight Decay: 0.004075316568407703
Batch Size: 64
No. Epochs: 18
Epoch Patience: 5
 Accumulation Steps: 1
2026-02-12 15:55:40 - INFO - Learning Rate: 3.9205870955943774e-05
Weight Decay: 0.004075316568407703
Batch Size: 64
No. Epochs: 18
Epoch Patience: 5
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 15:55:41 - INFO - Generating initial weights
Time taken for Epoch 1:19.28 - F1: 0.0781
2026-02-12 15:56:04 - INFO - Time taken for Epoch 1:19.28 - F1: 0.0781
Time taken for Epoch 2:19.25 - F1: 0.1305
2026-02-12 15:56:23 - INFO - Time taken for Epoch 2:19.25 - F1: 0.1305
Time taken for Epoch 3:19.27 - F1: 0.1041
2026-02-12 15:56:42 - INFO - Time taken for Epoch 3:19.27 - F1: 0.1041
Time taken for Epoch 4:19.28 - F1: 0.1143
2026-02-12 15:57:01 - INFO - Time taken for Epoch 4:19.28 - F1: 0.1143
Time taken for Epoch 5:19.29 - F1: 0.1235
2026-02-12 15:57:21 - INFO - Time taken for Epoch 5:19.29 - F1: 0.1235
Time taken for Epoch 6:19.32 - F1: 0.1647
2026-02-12 15:57:40 - INFO - Time taken for Epoch 6:19.32 - F1: 0.1647
Time taken for Epoch 7:19.34 - F1: 0.1927
2026-02-12 15:57:59 - INFO - Time taken for Epoch 7:19.34 - F1: 0.1927
Time taken for Epoch 8:19.33 - F1: 0.2205
2026-02-12 15:58:19 - INFO - Time taken for Epoch 8:19.33 - F1: 0.2205
Time taken for Epoch 9:19.30 - F1: 0.2459
2026-02-12 15:58:38 - INFO - Time taken for Epoch 9:19.30 - F1: 0.2459
Time taken for Epoch 10:19.31 - F1: 0.2523
2026-02-12 15:58:57 - INFO - Time taken for Epoch 10:19.31 - F1: 0.2523
Time taken for Epoch 11:19.32 - F1: 0.2602
2026-02-12 15:59:17 - INFO - Time taken for Epoch 11:19.32 - F1: 0.2602
Time taken for Epoch 12:19.31 - F1: 0.2695
2026-02-12 15:59:36 - INFO - Time taken for Epoch 12:19.31 - F1: 0.2695
Time taken for Epoch 13:19.30 - F1: 0.2680
2026-02-12 15:59:55 - INFO - Time taken for Epoch 13:19.30 - F1: 0.2680
Time taken for Epoch 14:19.34 - F1: 0.2750
2026-02-12 16:00:15 - INFO - Time taken for Epoch 14:19.34 - F1: 0.2750
Time taken for Epoch 15:19.32 - F1: 0.2790
2026-02-12 16:00:34 - INFO - Time taken for Epoch 15:19.32 - F1: 0.2790
Time taken for Epoch 16:19.35 - F1: 0.2888
2026-02-12 16:00:53 - INFO - Time taken for Epoch 16:19.35 - F1: 0.2888
Time taken for Epoch 17:19.32 - F1: 0.3031
2026-02-12 16:01:13 - INFO - Time taken for Epoch 17:19.32 - F1: 0.3031
Time taken for Epoch 18:19.31 - F1: 0.3080
2026-02-12 16:01:32 - INFO - Time taken for Epoch 18:19.31 - F1: 0.3080
Best F1:0.3080 - Best Epoch:18
2026-02-12 16:01:32 - INFO - Best F1:0.3080 - Best Epoch:18
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:01:33 - INFO - Starting co-training
Time taken for Epoch 1: 46.65s - F1: 0.58523141
2026-02-12 16:02:20 - INFO - Time taken for Epoch 1: 46.65s - F1: 0.58523141
Time taken for Epoch 2: 47.88s - F1: 0.63768149
2026-02-12 16:03:08 - INFO - Time taken for Epoch 2: 47.88s - F1: 0.63768149
Time taken for Epoch 3: 47.99s - F1: 0.64751168
2026-02-12 16:03:56 - INFO - Time taken for Epoch 3: 47.99s - F1: 0.64751168
Time taken for Epoch 4: 47.98s - F1: 0.66542248
2026-02-12 16:04:44 - INFO - Time taken for Epoch 4: 47.98s - F1: 0.66542248
Time taken for Epoch 5: 47.98s - F1: 0.66087565
2026-02-12 16:05:32 - INFO - Time taken for Epoch 5: 47.98s - F1: 0.66087565
Time taken for Epoch 6: 46.73s - F1: 0.63371358
2026-02-12 16:06:19 - INFO - Time taken for Epoch 6: 46.73s - F1: 0.63371358
Time taken for Epoch 7: 46.72s - F1: 0.64263641
2026-02-12 16:07:06 - INFO - Time taken for Epoch 7: 46.72s - F1: 0.64263641
Time taken for Epoch 8: 46.75s - F1: 0.64437349
2026-02-12 16:07:52 - INFO - Time taken for Epoch 8: 46.75s - F1: 0.64437349
Time taken for Epoch 9: 46.75s - F1: 0.65637575
2026-02-12 16:08:39 - INFO - Time taken for Epoch 9: 46.75s - F1: 0.65637575
Performance not improving for 5 consecutive epochs.
Performance not improving for 5 consecutive epochs.
2026-02-12 16:08:39 - INFO - Performance not improving for 5 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:08:42 - INFO - Fine-tuning models
Time taken for Epoch 1:2.69 - F1: 0.6639
2026-02-12 16:08:45 - INFO - Time taken for Epoch 1:2.69 - F1: 0.6639
Time taken for Epoch 2:3.70 - F1: 0.6661
2026-02-12 16:08:49 - INFO - Time taken for Epoch 2:3.70 - F1: 0.6661
Time taken for Epoch 3:3.79 - F1: 0.6646
2026-02-12 16:08:52 - INFO - Time taken for Epoch 3:3.79 - F1: 0.6646
Time taken for Epoch 4:2.67 - F1: 0.6541
2026-02-12 16:08:55 - INFO - Time taken for Epoch 4:2.67 - F1: 0.6541
Time taken for Epoch 5:2.68 - F1: 0.6530
2026-02-12 16:08:58 - INFO - Time taken for Epoch 5:2.68 - F1: 0.6530
Time taken for Epoch 6:2.68 - F1: 0.6367
2026-02-12 16:09:01 - INFO - Time taken for Epoch 6:2.68 - F1: 0.6367
Time taken for Epoch 7:2.68 - F1: 0.6356
2026-02-12 16:09:03 - INFO - Time taken for Epoch 7:2.68 - F1: 0.6356
Time taken for Epoch 8:2.67 - F1: 0.6431
2026-02-12 16:09:06 - INFO - Time taken for Epoch 8:2.67 - F1: 0.6431
Time taken for Epoch 9:2.68 - F1: 0.6471
2026-02-12 16:09:09 - INFO - Time taken for Epoch 9:2.68 - F1: 0.6471
Time taken for Epoch 10:2.68 - F1: 0.6601
2026-02-12 16:09:11 - INFO - Time taken for Epoch 10:2.68 - F1: 0.6601
Time taken for Epoch 11:2.68 - F1: 0.6665
2026-02-12 16:09:14 - INFO - Time taken for Epoch 11:2.68 - F1: 0.6665
Time taken for Epoch 12:3.78 - F1: 0.6624
2026-02-12 16:09:18 - INFO - Time taken for Epoch 12:3.78 - F1: 0.6624
Time taken for Epoch 13:2.67 - F1: 0.6644
2026-02-12 16:09:20 - INFO - Time taken for Epoch 13:2.67 - F1: 0.6644
Time taken for Epoch 14:2.67 - F1: 0.6730
2026-02-12 16:09:23 - INFO - Time taken for Epoch 14:2.67 - F1: 0.6730
Time taken for Epoch 15:3.77 - F1: 0.6722
2026-02-12 16:09:27 - INFO - Time taken for Epoch 15:3.77 - F1: 0.6722
Time taken for Epoch 16:2.68 - F1: 0.6753
2026-02-12 16:09:29 - INFO - Time taken for Epoch 16:2.68 - F1: 0.6753
Time taken for Epoch 17:3.82 - F1: 0.6740
2026-02-12 16:09:33 - INFO - Time taken for Epoch 17:3.82 - F1: 0.6740
Time taken for Epoch 18:2.67 - F1: 0.6694
2026-02-12 16:09:36 - INFO - Time taken for Epoch 18:2.67 - F1: 0.6694
Time taken for Epoch 19:2.67 - F1: 0.6717
2026-02-12 16:09:39 - INFO - Time taken for Epoch 19:2.67 - F1: 0.6717
Time taken for Epoch 20:2.68 - F1: 0.6714
2026-02-12 16:09:41 - INFO - Time taken for Epoch 20:2.68 - F1: 0.6714
Time taken for Epoch 21:2.67 - F1: 0.6797
2026-02-12 16:09:44 - INFO - Time taken for Epoch 21:2.67 - F1: 0.6797
Time taken for Epoch 22:3.83 - F1: 0.6797
2026-02-12 16:09:48 - INFO - Time taken for Epoch 22:3.83 - F1: 0.6797
Time taken for Epoch 23:2.84 - F1: 0.6824
2026-02-12 16:09:51 - INFO - Time taken for Epoch 23:2.84 - F1: 0.6824
Time taken for Epoch 24:3.77 - F1: 0.6835
2026-02-12 16:09:54 - INFO - Time taken for Epoch 24:3.77 - F1: 0.6835
Time taken for Epoch 25:3.78 - F1: 0.6836
2026-02-12 16:09:58 - INFO - Time taken for Epoch 25:3.78 - F1: 0.6836
Time taken for Epoch 26:3.78 - F1: 0.6836
2026-02-12 16:10:02 - INFO - Time taken for Epoch 26:3.78 - F1: 0.6836
Time taken for Epoch 27:2.66 - F1: 0.6837
2026-02-12 16:10:05 - INFO - Time taken for Epoch 27:2.66 - F1: 0.6837
Time taken for Epoch 28:3.78 - F1: 0.6867
2026-02-12 16:10:08 - INFO - Time taken for Epoch 28:3.78 - F1: 0.6867
Time taken for Epoch 29:3.78 - F1: 0.6878
2026-02-12 16:10:12 - INFO - Time taken for Epoch 29:3.78 - F1: 0.6878
Time taken for Epoch 30:3.78 - F1: 0.6899
2026-02-12 16:10:16 - INFO - Time taken for Epoch 30:3.78 - F1: 0.6899
Time taken for Epoch 31:3.79 - F1: 0.6907
2026-02-12 16:10:20 - INFO - Time taken for Epoch 31:3.79 - F1: 0.6907
Time taken for Epoch 32:3.77 - F1: 0.6907
2026-02-12 16:10:24 - INFO - Time taken for Epoch 32:3.77 - F1: 0.6907
Time taken for Epoch 33:2.66 - F1: 0.6887
2026-02-12 16:10:26 - INFO - Time taken for Epoch 33:2.66 - F1: 0.6887
Time taken for Epoch 34:2.67 - F1: 0.6887
2026-02-12 16:10:29 - INFO - Time taken for Epoch 34:2.67 - F1: 0.6887
Time taken for Epoch 35:2.66 - F1: 0.6887
2026-02-12 16:10:32 - INFO - Time taken for Epoch 35:2.66 - F1: 0.6887
Time taken for Epoch 36:2.66 - F1: 0.6887
2026-02-12 16:10:34 - INFO - Time taken for Epoch 36:2.66 - F1: 0.6887
Time taken for Epoch 37:2.67 - F1: 0.6877
2026-02-12 16:10:37 - INFO - Time taken for Epoch 37:2.67 - F1: 0.6877
Time taken for Epoch 38:2.66 - F1: 0.6877
2026-02-12 16:10:40 - INFO - Time taken for Epoch 38:2.66 - F1: 0.6877
Time taken for Epoch 39:2.66 - F1: 0.6877
2026-02-12 16:10:42 - INFO - Time taken for Epoch 39:2.66 - F1: 0.6877
Time taken for Epoch 40:2.66 - F1: 0.6877
2026-02-12 16:10:45 - INFO - Time taken for Epoch 40:2.66 - F1: 0.6877
Time taken for Epoch 41:2.66 - F1: 0.6877
2026-02-12 16:10:48 - INFO - Time taken for Epoch 41:2.66 - F1: 0.6877
Performance not improving for 10 consecutive epochs.
2026-02-12 16:10:48 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6907 - Best Epoch:30
2026-02-12 16:10:48 - INFO - Best F1:0.6907 - Best Epoch:30
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6779, Test ECE: 0.0331
2026-02-12 16:10:55 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6779, Test ECE: 0.0331
All results: {'f1_macro': 0.677944471493021, 'ece': np.float64(0.03305770355520897)}
2026-02-12 16:10:55 - INFO - All results: {'f1_macro': 0.677944471493021, 'ece': np.float64(0.03305770355520897)}

Total time taken: 916.13 seconds
2026-02-12 16:10:55 - INFO - 
Total time taken: 916.13 seconds
2026-02-12 16:10:55 - INFO - Trial 4 finished with value: 0.677944471493021 and parameters: {'learning_rate': 3.9205870955943774e-05, 'weight_decay': 0.004075316568407703, 'batch_size': 64, 'co_train_epochs': 18, 'epoch_patience': 5}. Best is trial 4 with value: 0.677944471493021.
Using devices: cuda, cuda
2026-02-12 16:10:55 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:10:55 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:10:55 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:10:55 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 2.4493465650483942e-05
Weight Decay: 1.4735613660400188e-05
Batch Size: 32
No. Epochs: 16
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-12 16:10:56 - INFO - Learning Rate: 2.4493465650483942e-05
Weight Decay: 1.4735613660400188e-05
Batch Size: 32
No. Epochs: 16
Epoch Patience: 10
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:10:57 - INFO - Generating initial weights
Time taken for Epoch 1:20.30 - F1: 0.0658
2026-02-12 16:11:21 - INFO - Time taken for Epoch 1:20.30 - F1: 0.0658
Time taken for Epoch 2:20.20 - F1: 0.0893
2026-02-12 16:11:41 - INFO - Time taken for Epoch 2:20.20 - F1: 0.0893
Time taken for Epoch 3:20.23 - F1: 0.1200
2026-02-12 16:12:01 - INFO - Time taken for Epoch 3:20.23 - F1: 0.1200
Time taken for Epoch 4:20.25 - F1: 0.1293
2026-02-12 16:12:22 - INFO - Time taken for Epoch 4:20.25 - F1: 0.1293
Time taken for Epoch 5:20.29 - F1: 0.1431
2026-02-12 16:12:42 - INFO - Time taken for Epoch 5:20.29 - F1: 0.1431
Time taken for Epoch 6:20.32 - F1: 0.1579
2026-02-12 16:13:02 - INFO - Time taken for Epoch 6:20.32 - F1: 0.1579
Time taken for Epoch 7:20.33 - F1: 0.1633
2026-02-12 16:13:23 - INFO - Time taken for Epoch 7:20.33 - F1: 0.1633
Time taken for Epoch 8:20.37 - F1: 0.1727
2026-02-12 16:13:43 - INFO - Time taken for Epoch 8:20.37 - F1: 0.1727
Time taken for Epoch 9:20.39 - F1: 0.1831
2026-02-12 16:14:03 - INFO - Time taken for Epoch 9:20.39 - F1: 0.1831
Time taken for Epoch 10:20.39 - F1: 0.2041
2026-02-12 16:14:24 - INFO - Time taken for Epoch 10:20.39 - F1: 0.2041
Time taken for Epoch 11:20.36 - F1: 0.2121
2026-02-12 16:14:44 - INFO - Time taken for Epoch 11:20.36 - F1: 0.2121
Time taken for Epoch 12:20.40 - F1: 0.2271
2026-02-12 16:15:04 - INFO - Time taken for Epoch 12:20.40 - F1: 0.2271
Time taken for Epoch 13:20.42 - F1: 0.2350
2026-02-12 16:15:25 - INFO - Time taken for Epoch 13:20.42 - F1: 0.2350
Time taken for Epoch 14:20.37 - F1: 0.2383
2026-02-12 16:15:45 - INFO - Time taken for Epoch 14:20.37 - F1: 0.2383
Time taken for Epoch 15:20.38 - F1: 0.2386
2026-02-12 16:16:06 - INFO - Time taken for Epoch 15:20.38 - F1: 0.2386
Time taken for Epoch 16:20.34 - F1: 0.2404
2026-02-12 16:16:26 - INFO - Time taken for Epoch 16:20.34 - F1: 0.2404
Best F1:0.2404 - Best Epoch:16
2026-02-12 16:16:26 - INFO - Best F1:0.2404 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:16:27 - INFO - Starting co-training
Time taken for Epoch 1: 35.61s - F1: 0.48174542
2026-02-12 16:17:03 - INFO - Time taken for Epoch 1: 35.61s - F1: 0.48174542
Time taken for Epoch 2: 36.71s - F1: 0.51280737
2026-02-12 16:17:40 - INFO - Time taken for Epoch 2: 36.71s - F1: 0.51280737
Time taken for Epoch 3: 36.84s - F1: 0.59224956
2026-02-12 16:18:17 - INFO - Time taken for Epoch 3: 36.84s - F1: 0.59224956
Time taken for Epoch 4: 36.86s - F1: 0.63037121
2026-02-12 16:18:54 - INFO - Time taken for Epoch 4: 36.86s - F1: 0.63037121
Time taken for Epoch 5: 36.83s - F1: 0.63486616
2026-02-12 16:19:30 - INFO - Time taken for Epoch 5: 36.83s - F1: 0.63486616
Time taken for Epoch 6: 36.86s - F1: 0.63450808
2026-02-12 16:20:07 - INFO - Time taken for Epoch 6: 36.86s - F1: 0.63450808
Time taken for Epoch 7: 35.72s - F1: 0.64158194
2026-02-12 16:20:43 - INFO - Time taken for Epoch 7: 35.72s - F1: 0.64158194
Time taken for Epoch 8: 36.84s - F1: 0.65197011
2026-02-12 16:21:20 - INFO - Time taken for Epoch 8: 36.84s - F1: 0.65197011
Time taken for Epoch 9: 36.84s - F1: 0.65781923
2026-02-12 16:21:57 - INFO - Time taken for Epoch 9: 36.84s - F1: 0.65781923
Time taken for Epoch 10: 36.82s - F1: 0.65606894
2026-02-12 16:22:33 - INFO - Time taken for Epoch 10: 36.82s - F1: 0.65606894
Time taken for Epoch 11: 35.71s - F1: 0.65902141
2026-02-12 16:23:09 - INFO - Time taken for Epoch 11: 35.71s - F1: 0.65902141
Time taken for Epoch 12: 36.87s - F1: 0.65261393
2026-02-12 16:23:46 - INFO - Time taken for Epoch 12: 36.87s - F1: 0.65261393
Time taken for Epoch 13: 35.72s - F1: 0.64413965
2026-02-12 16:24:22 - INFO - Time taken for Epoch 13: 35.72s - F1: 0.64413965
Time taken for Epoch 14: 35.74s - F1: 0.64704836
2026-02-12 16:24:57 - INFO - Time taken for Epoch 14: 35.74s - F1: 0.64704836
Time taken for Epoch 15: 35.74s - F1: 0.63524112
2026-02-12 16:25:33 - INFO - Time taken for Epoch 15: 35.74s - F1: 0.63524112
Time taken for Epoch 16: 35.85s - F1: 0.64330783
2026-02-12 16:26:09 - INFO - Time taken for Epoch 16: 35.85s - F1: 0.64330783
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:26:12 - INFO - Fine-tuning models
Time taken for Epoch 1:2.82 - F1: 0.6585
2026-02-12 16:26:15 - INFO - Time taken for Epoch 1:2.82 - F1: 0.6585
Time taken for Epoch 2:3.86 - F1: 0.6606
2026-02-12 16:26:19 - INFO - Time taken for Epoch 2:3.86 - F1: 0.6606
Time taken for Epoch 3:3.97 - F1: 0.6632
2026-02-12 16:26:23 - INFO - Time taken for Epoch 3:3.97 - F1: 0.6632
Time taken for Epoch 4:3.97 - F1: 0.6561
2026-02-12 16:26:27 - INFO - Time taken for Epoch 4:3.97 - F1: 0.6561
Time taken for Epoch 5:2.80 - F1: 0.6580
2026-02-12 16:26:29 - INFO - Time taken for Epoch 5:2.80 - F1: 0.6580
Time taken for Epoch 6:2.80 - F1: 0.6506
2026-02-12 16:26:32 - INFO - Time taken for Epoch 6:2.80 - F1: 0.6506
Time taken for Epoch 7:2.80 - F1: 0.6467
2026-02-12 16:26:35 - INFO - Time taken for Epoch 7:2.80 - F1: 0.6467
Time taken for Epoch 8:2.80 - F1: 0.6514
2026-02-12 16:26:38 - INFO - Time taken for Epoch 8:2.80 - F1: 0.6514
Time taken for Epoch 9:2.80 - F1: 0.6539
2026-02-12 16:26:41 - INFO - Time taken for Epoch 9:2.80 - F1: 0.6539
Time taken for Epoch 10:2.80 - F1: 0.6559
2026-02-12 16:26:43 - INFO - Time taken for Epoch 10:2.80 - F1: 0.6559
Time taken for Epoch 11:2.80 - F1: 0.6514
2026-02-12 16:26:46 - INFO - Time taken for Epoch 11:2.80 - F1: 0.6514
Time taken for Epoch 12:2.80 - F1: 0.6464
2026-02-12 16:26:49 - INFO - Time taken for Epoch 12:2.80 - F1: 0.6464
Time taken for Epoch 13:2.80 - F1: 0.6437
2026-02-12 16:26:52 - INFO - Time taken for Epoch 13:2.80 - F1: 0.6437
Performance not improving for 10 consecutive epochs.
2026-02-12 16:26:52 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6632 - Best Epoch:2
2026-02-12 16:26:52 - INFO - Best F1:0.6632 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6754, Test ECE: 0.0120
2026-02-12 16:27:00 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6754, Test ECE: 0.0120
All results: {'f1_macro': 0.6753987457257488, 'ece': np.float64(0.011995281698634324)}
2026-02-12 16:27:00 - INFO - All results: {'f1_macro': 0.6753987457257488, 'ece': np.float64(0.011995281698634324)}

Total time taken: 964.58 seconds
2026-02-12 16:27:00 - INFO - 
Total time taken: 964.58 seconds
2026-02-12 16:27:00 - INFO - Trial 5 finished with value: 0.6753987457257488 and parameters: {'learning_rate': 2.4493465650483942e-05, 'weight_decay': 1.4735613660400188e-05, 'batch_size': 32, 'co_train_epochs': 16, 'epoch_patience': 10}. Best is trial 4 with value: 0.677944471493021.
Using devices: cuda, cuda
2026-02-12 16:27:00 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:27:00 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:27:00 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:27:00 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 4.220624117244726e-05
Weight Decay: 1.5243001353075344e-05
Batch Size: 32
No. Epochs: 16
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-12 16:27:01 - INFO - Learning Rate: 4.220624117244726e-05
Weight Decay: 1.5243001353075344e-05
Batch Size: 32
No. Epochs: 16
Epoch Patience: 9
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:27:02 - INFO - Generating initial weights
Time taken for Epoch 1:20.36 - F1: 0.0760
2026-02-12 16:27:25 - INFO - Time taken for Epoch 1:20.36 - F1: 0.0760
Time taken for Epoch 2:20.29 - F1: 0.1356
2026-02-12 16:27:46 - INFO - Time taken for Epoch 2:20.29 - F1: 0.1356
Time taken for Epoch 3:20.35 - F1: 0.0995
2026-02-12 16:28:06 - INFO - Time taken for Epoch 3:20.35 - F1: 0.0995
Time taken for Epoch 4:20.36 - F1: 0.1136
2026-02-12 16:28:26 - INFO - Time taken for Epoch 4:20.36 - F1: 0.1136
Time taken for Epoch 5:20.34 - F1: 0.1453
2026-02-12 16:28:47 - INFO - Time taken for Epoch 5:20.34 - F1: 0.1453
Time taken for Epoch 6:20.36 - F1: 0.1872
2026-02-12 16:29:07 - INFO - Time taken for Epoch 6:20.36 - F1: 0.1872
Time taken for Epoch 7:20.33 - F1: 0.2180
2026-02-12 16:29:27 - INFO - Time taken for Epoch 7:20.33 - F1: 0.2180
Time taken for Epoch 8:20.35 - F1: 0.2447
2026-02-12 16:29:48 - INFO - Time taken for Epoch 8:20.35 - F1: 0.2447
Time taken for Epoch 9:20.35 - F1: 0.2471
2026-02-12 16:30:08 - INFO - Time taken for Epoch 9:20.35 - F1: 0.2471
Time taken for Epoch 10:20.38 - F1: 0.2676
2026-02-12 16:30:29 - INFO - Time taken for Epoch 10:20.38 - F1: 0.2676
Time taken for Epoch 11:20.38 - F1: 0.2655
2026-02-12 16:30:49 - INFO - Time taken for Epoch 11:20.38 - F1: 0.2655
Time taken for Epoch 12:20.39 - F1: 0.2678
2026-02-12 16:31:09 - INFO - Time taken for Epoch 12:20.39 - F1: 0.2678
Time taken for Epoch 13:20.36 - F1: 0.2747
2026-02-12 16:31:30 - INFO - Time taken for Epoch 13:20.36 - F1: 0.2747
Time taken for Epoch 14:20.39 - F1: 0.2816
2026-02-12 16:31:50 - INFO - Time taken for Epoch 14:20.39 - F1: 0.2816
Time taken for Epoch 15:20.42 - F1: 0.2886
2026-02-12 16:32:11 - INFO - Time taken for Epoch 15:20.42 - F1: 0.2886
Time taken for Epoch 16:20.40 - F1: 0.2922
2026-02-12 16:32:31 - INFO - Time taken for Epoch 16:20.40 - F1: 0.2922
Best F1:0.2922 - Best Epoch:16
2026-02-12 16:32:31 - INFO - Best F1:0.2922 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:32:32 - INFO - Starting co-training
Time taken for Epoch 1: 35.75s - F1: 0.57229434
2026-02-12 16:33:08 - INFO - Time taken for Epoch 1: 35.75s - F1: 0.57229434
Time taken for Epoch 2: 36.87s - F1: 0.62311305
2026-02-12 16:33:45 - INFO - Time taken for Epoch 2: 36.87s - F1: 0.62311305
Time taken for Epoch 3: 36.89s - F1: 0.60983148
2026-02-12 16:34:22 - INFO - Time taken for Epoch 3: 36.89s - F1: 0.60983148
Time taken for Epoch 4: 35.75s - F1: 0.63440995
2026-02-12 16:34:58 - INFO - Time taken for Epoch 4: 35.75s - F1: 0.63440995
Time taken for Epoch 5: 36.91s - F1: 0.64353500
2026-02-12 16:35:35 - INFO - Time taken for Epoch 5: 36.91s - F1: 0.64353500
Time taken for Epoch 6: 36.94s - F1: 0.65763321
2026-02-12 16:36:12 - INFO - Time taken for Epoch 6: 36.94s - F1: 0.65763321
Time taken for Epoch 7: 37.04s - F1: 0.64670604
2026-02-12 16:36:49 - INFO - Time taken for Epoch 7: 37.04s - F1: 0.64670604
Time taken for Epoch 8: 35.72s - F1: 0.65362416
2026-02-12 16:37:24 - INFO - Time taken for Epoch 8: 35.72s - F1: 0.65362416
Time taken for Epoch 9: 35.77s - F1: 0.65122121
2026-02-12 16:38:00 - INFO - Time taken for Epoch 9: 35.77s - F1: 0.65122121
Time taken for Epoch 10: 35.68s - F1: 0.64405750
2026-02-12 16:38:36 - INFO - Time taken for Epoch 10: 35.68s - F1: 0.64405750
Time taken for Epoch 11: 35.78s - F1: 0.64624277
2026-02-12 16:39:12 - INFO - Time taken for Epoch 11: 35.78s - F1: 0.64624277
Time taken for Epoch 12: 35.74s - F1: 0.66049726
2026-02-12 16:39:47 - INFO - Time taken for Epoch 12: 35.74s - F1: 0.66049726
Time taken for Epoch 13: 37.04s - F1: 0.65296174
2026-02-12 16:40:24 - INFO - Time taken for Epoch 13: 37.04s - F1: 0.65296174
Time taken for Epoch 14: 35.70s - F1: 0.63410630
2026-02-12 16:41:00 - INFO - Time taken for Epoch 14: 35.70s - F1: 0.63410630
Time taken for Epoch 15: 35.75s - F1: 0.65058710
2026-02-12 16:41:36 - INFO - Time taken for Epoch 15: 35.75s - F1: 0.65058710
Time taken for Epoch 16: 35.84s - F1: 0.64496548
2026-02-12 16:42:12 - INFO - Time taken for Epoch 16: 35.84s - F1: 0.64496548
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:42:14 - INFO - Fine-tuning models
Time taken for Epoch 1:2.82 - F1: 0.6566
2026-02-12 16:42:17 - INFO - Time taken for Epoch 1:2.82 - F1: 0.6566
Time taken for Epoch 2:4.02 - F1: 0.6592
2026-02-12 16:42:21 - INFO - Time taken for Epoch 2:4.02 - F1: 0.6592
Time taken for Epoch 3:4.13 - F1: 0.6546
2026-02-12 16:42:26 - INFO - Time taken for Epoch 3:4.13 - F1: 0.6546
Time taken for Epoch 4:2.80 - F1: 0.6546
2026-02-12 16:42:28 - INFO - Time taken for Epoch 4:2.80 - F1: 0.6546
Time taken for Epoch 5:2.81 - F1: 0.6567
2026-02-12 16:42:31 - INFO - Time taken for Epoch 5:2.81 - F1: 0.6567
Time taken for Epoch 6:2.80 - F1: 0.6556
2026-02-12 16:42:34 - INFO - Time taken for Epoch 6:2.80 - F1: 0.6556
Time taken for Epoch 7:2.80 - F1: 0.6491
2026-02-12 16:42:37 - INFO - Time taken for Epoch 7:2.80 - F1: 0.6491
Time taken for Epoch 8:2.80 - F1: 0.6502
2026-02-12 16:42:40 - INFO - Time taken for Epoch 8:2.80 - F1: 0.6502
Time taken for Epoch 9:2.81 - F1: 0.6474
2026-02-12 16:42:42 - INFO - Time taken for Epoch 9:2.81 - F1: 0.6474
Time taken for Epoch 10:2.80 - F1: 0.6472
2026-02-12 16:42:45 - INFO - Time taken for Epoch 10:2.80 - F1: 0.6472
Time taken for Epoch 11:2.80 - F1: 0.6438
2026-02-12 16:42:48 - INFO - Time taken for Epoch 11:2.80 - F1: 0.6438
Time taken for Epoch 12:2.80 - F1: 0.6426
2026-02-12 16:42:51 - INFO - Time taken for Epoch 12:2.80 - F1: 0.6426
Performance not improving for 10 consecutive epochs.
2026-02-12 16:42:51 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6592 - Best Epoch:1
2026-02-12 16:42:51 - INFO - Best F1:0.6592 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6429, Test ECE: 0.0348
2026-02-12 16:42:59 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6429, Test ECE: 0.0348
All results: {'f1_macro': 0.6429269720654465, 'ece': np.float64(0.03475444446654988)}
2026-02-12 16:42:59 - INFO - All results: {'f1_macro': 0.6429269720654465, 'ece': np.float64(0.03475444446654988)}

Total time taken: 959.00 seconds
2026-02-12 16:42:59 - INFO - 
Total time taken: 959.00 seconds
2026-02-12 16:42:59 - INFO - Trial 6 finished with value: 0.6429269720654465 and parameters: {'learning_rate': 4.220624117244726e-05, 'weight_decay': 1.5243001353075344e-05, 'batch_size': 32, 'co_train_epochs': 16, 'epoch_patience': 9}. Best is trial 4 with value: 0.677944471493021.
Using devices: cuda, cuda
2026-02-12 16:42:59 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:42:59 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:42:59 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:42:59 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 6.796352997323603e-05
Weight Decay: 0.009286391968637415
Batch Size: 32
No. Epochs: 7
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-12 16:42:59 - INFO - Learning Rate: 6.796352997323603e-05
Weight Decay: 0.009286391968637415
Batch Size: 32
No. Epochs: 7
Epoch Patience: 9
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:43:01 - INFO - Generating initial weights
Time taken for Epoch 1:20.35 - F1: 0.1144
2026-02-12 16:43:24 - INFO - Time taken for Epoch 1:20.35 - F1: 0.1144
Time taken for Epoch 2:20.26 - F1: 0.1069
2026-02-12 16:43:45 - INFO - Time taken for Epoch 2:20.26 - F1: 0.1069
Time taken for Epoch 3:20.28 - F1: 0.1160
2026-02-12 16:44:05 - INFO - Time taken for Epoch 3:20.28 - F1: 0.1160
Time taken for Epoch 4:20.50 - F1: 0.1447
2026-02-12 16:44:25 - INFO - Time taken for Epoch 4:20.50 - F1: 0.1447
Time taken for Epoch 5:20.34 - F1: 0.1822
2026-02-12 16:44:46 - INFO - Time taken for Epoch 5:20.34 - F1: 0.1822
Time taken for Epoch 6:20.29 - F1: 0.2168
2026-02-12 16:45:06 - INFO - Time taken for Epoch 6:20.29 - F1: 0.2168
Time taken for Epoch 7:20.34 - F1: 0.2257
2026-02-12 16:45:26 - INFO - Time taken for Epoch 7:20.34 - F1: 0.2257
Best F1:0.2257 - Best Epoch:7
2026-02-12 16:45:26 - INFO - Best F1:0.2257 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:45:28 - INFO - Starting co-training
Time taken for Epoch 1: 35.60s - F1: 0.55977050
2026-02-12 16:46:04 - INFO - Time taken for Epoch 1: 35.60s - F1: 0.55977050
Time taken for Epoch 2: 36.84s - F1: 0.60644905
2026-02-12 16:46:41 - INFO - Time taken for Epoch 2: 36.84s - F1: 0.60644905
Time taken for Epoch 3: 36.97s - F1: 0.62093090
2026-02-12 16:47:17 - INFO - Time taken for Epoch 3: 36.97s - F1: 0.62093090
Time taken for Epoch 4: 36.98s - F1: 0.61550115
2026-02-12 16:47:54 - INFO - Time taken for Epoch 4: 36.98s - F1: 0.61550115
Time taken for Epoch 5: 35.77s - F1: 0.64277908
2026-02-12 16:48:30 - INFO - Time taken for Epoch 5: 35.77s - F1: 0.64277908
Time taken for Epoch 6: 37.06s - F1: 0.62440101
2026-02-12 16:49:07 - INFO - Time taken for Epoch 6: 37.06s - F1: 0.62440101
Time taken for Epoch 7: 35.77s - F1: 0.65854261
2026-02-12 16:49:43 - INFO - Time taken for Epoch 7: 35.77s - F1: 0.65854261
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:49:47 - INFO - Fine-tuning models
Time taken for Epoch 1:2.81 - F1: 0.6610
2026-02-12 16:49:50 - INFO - Time taken for Epoch 1:2.81 - F1: 0.6610
Time taken for Epoch 2:3.93 - F1: 0.6334
2026-02-12 16:49:54 - INFO - Time taken for Epoch 2:3.93 - F1: 0.6334
Time taken for Epoch 3:2.81 - F1: 0.6339
2026-02-12 16:49:57 - INFO - Time taken for Epoch 3:2.81 - F1: 0.6339
Time taken for Epoch 4:2.81 - F1: 0.6214
2026-02-12 16:50:00 - INFO - Time taken for Epoch 4:2.81 - F1: 0.6214
Time taken for Epoch 5:2.82 - F1: 0.6050
2026-02-12 16:50:02 - INFO - Time taken for Epoch 5:2.82 - F1: 0.6050
Time taken for Epoch 6:2.81 - F1: 0.6010
2026-02-12 16:50:05 - INFO - Time taken for Epoch 6:2.81 - F1: 0.6010
Time taken for Epoch 7:2.83 - F1: 0.5998
2026-02-12 16:50:08 - INFO - Time taken for Epoch 7:2.83 - F1: 0.5998
Time taken for Epoch 8:2.82 - F1: 0.6164
2026-02-12 16:50:11 - INFO - Time taken for Epoch 8:2.82 - F1: 0.6164
Time taken for Epoch 9:2.82 - F1: 0.6241
2026-02-12 16:50:14 - INFO - Time taken for Epoch 9:2.82 - F1: 0.6241
Time taken for Epoch 10:2.81 - F1: 0.6195
2026-02-12 16:50:16 - INFO - Time taken for Epoch 10:2.81 - F1: 0.6195
Time taken for Epoch 11:2.82 - F1: 0.6237
2026-02-12 16:50:19 - INFO - Time taken for Epoch 11:2.82 - F1: 0.6237
Performance not improving for 10 consecutive epochs.
2026-02-12 16:50:19 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6610 - Best Epoch:0
2026-02-12 16:50:19 - INFO - Best F1:0.6610 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6538, Test ECE: 0.0295
2026-02-12 16:50:28 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6538, Test ECE: 0.0295
All results: {'f1_macro': 0.6537808072592525, 'ece': np.float64(0.029453983584653985)}
2026-02-12 16:50:28 - INFO - All results: {'f1_macro': 0.6537808072592525, 'ece': np.float64(0.029453983584653985)}

Total time taken: 448.51 seconds
2026-02-12 16:50:28 - INFO - 
Total time taken: 448.51 seconds
2026-02-12 16:50:28 - INFO - Trial 7 finished with value: 0.6537808072592525 and parameters: {'learning_rate': 6.796352997323603e-05, 'weight_decay': 0.009286391968637415, 'batch_size': 32, 'co_train_epochs': 7, 'epoch_patience': 9}. Best is trial 4 with value: 0.677944471493021.
Using devices: cuda, cuda
2026-02-12 16:50:28 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:50:28 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:50:28 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:50:28 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.00016528769089146512
Weight Decay: 0.002985243743433368
Batch Size: 64
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 1
2026-02-12 16:50:28 - INFO - Learning Rate: 0.00016528769089146512
Weight Decay: 0.002985243743433368
Batch Size: 64
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:50:29 - INFO - Generating initial weights
Time taken for Epoch 1:19.28 - F1: 0.0647
2026-02-12 16:50:52 - INFO - Time taken for Epoch 1:19.28 - F1: 0.0647
Time taken for Epoch 2:19.26 - F1: 0.1609
2026-02-12 16:51:11 - INFO - Time taken for Epoch 2:19.26 - F1: 0.1609
Time taken for Epoch 3:19.26 - F1: 0.2457
2026-02-12 16:51:30 - INFO - Time taken for Epoch 3:19.26 - F1: 0.2457
Time taken for Epoch 4:19.26 - F1: 0.3133
2026-02-12 16:51:50 - INFO - Time taken for Epoch 4:19.26 - F1: 0.3133
Time taken for Epoch 5:19.27 - F1: 0.3206
2026-02-12 16:52:09 - INFO - Time taken for Epoch 5:19.27 - F1: 0.3206
Time taken for Epoch 6:19.32 - F1: 0.3293
2026-02-12 16:52:28 - INFO - Time taken for Epoch 6:19.32 - F1: 0.3293
Time taken for Epoch 7:19.32 - F1: 0.3523
2026-02-12 16:52:48 - INFO - Time taken for Epoch 7:19.32 - F1: 0.3523
Time taken for Epoch 8:19.31 - F1: 0.3501
2026-02-12 16:53:07 - INFO - Time taken for Epoch 8:19.31 - F1: 0.3501
Best F1:0.3523 - Best Epoch:7
2026-02-12 16:53:07 - INFO - Best F1:0.3523 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:53:08 - INFO - Starting co-training
Time taken for Epoch 1: 46.68s - F1: 0.62822420
2026-02-12 16:53:55 - INFO - Time taken for Epoch 1: 46.68s - F1: 0.62822420
Time taken for Epoch 2: 47.83s - F1: 0.62059153
2026-02-12 16:54:43 - INFO - Time taken for Epoch 2: 47.83s - F1: 0.62059153
Time taken for Epoch 3: 46.72s - F1: 0.58773991
2026-02-12 16:55:30 - INFO - Time taken for Epoch 3: 46.72s - F1: 0.58773991
Time taken for Epoch 4: 46.75s - F1: 0.64823006
2026-02-12 16:56:16 - INFO - Time taken for Epoch 4: 46.75s - F1: 0.64823006
Time taken for Epoch 5: 48.01s - F1: 0.63823366
2026-02-12 16:57:04 - INFO - Time taken for Epoch 5: 48.01s - F1: 0.63823366
Time taken for Epoch 6: 46.78s - F1: 0.55137671
2026-02-12 16:57:51 - INFO - Time taken for Epoch 6: 46.78s - F1: 0.55137671
Time taken for Epoch 7: 46.76s - F1: 0.59936832
2026-02-12 16:58:38 - INFO - Time taken for Epoch 7: 46.76s - F1: 0.59936832
Time taken for Epoch 8: 46.77s - F1: 0.61125343
2026-02-12 16:59:25 - INFO - Time taken for Epoch 8: 46.77s - F1: 0.61125343
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:59:27 - INFO - Fine-tuning models
Time taken for Epoch 1:2.66 - F1: 0.6356
2026-02-12 16:59:30 - INFO - Time taken for Epoch 1:2.66 - F1: 0.6356
Time taken for Epoch 2:3.85 - F1: 0.6218
2026-02-12 16:59:34 - INFO - Time taken for Epoch 2:3.85 - F1: 0.6218
Time taken for Epoch 3:2.66 - F1: 0.6241
2026-02-12 16:59:37 - INFO - Time taken for Epoch 3:2.66 - F1: 0.6241
Time taken for Epoch 4:2.66 - F1: 0.6107
2026-02-12 16:59:40 - INFO - Time taken for Epoch 4:2.66 - F1: 0.6107
Time taken for Epoch 5:2.66 - F1: 0.6057
2026-02-12 16:59:42 - INFO - Time taken for Epoch 5:2.66 - F1: 0.6057
Time taken for Epoch 6:2.66 - F1: 0.5961
2026-02-12 16:59:45 - INFO - Time taken for Epoch 6:2.66 - F1: 0.5961
Time taken for Epoch 7:2.66 - F1: 0.5988
2026-02-12 16:59:48 - INFO - Time taken for Epoch 7:2.66 - F1: 0.5988
Time taken for Epoch 8:2.67 - F1: 0.5990
2026-02-12 16:59:50 - INFO - Time taken for Epoch 8:2.67 - F1: 0.5990
Time taken for Epoch 9:2.65 - F1: 0.6012
2026-02-12 16:59:53 - INFO - Time taken for Epoch 9:2.65 - F1: 0.6012
Time taken for Epoch 10:2.66 - F1: 0.6112
2026-02-12 16:59:56 - INFO - Time taken for Epoch 10:2.66 - F1: 0.6112
Time taken for Epoch 11:2.66 - F1: 0.6137
2026-02-12 16:59:58 - INFO - Time taken for Epoch 11:2.66 - F1: 0.6137
Performance not improving for 10 consecutive epochs.
2026-02-12 16:59:58 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6356 - Best Epoch:0
2026-02-12 16:59:58 - INFO - Best F1:0.6356 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6171, Test ECE: 0.0940
2026-02-12 17:00:06 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6171, Test ECE: 0.0940
All results: {'f1_macro': 0.6171132245635836, 'ece': np.float64(0.0940083900205636)}
2026-02-12 17:00:06 - INFO - All results: {'f1_macro': 0.6171132245635836, 'ece': np.float64(0.0940083900205636)}

Total time taken: 578.31 seconds
2026-02-12 17:00:06 - INFO - 
Total time taken: 578.31 seconds
2026-02-12 17:00:06 - INFO - Trial 8 finished with value: 0.6171132245635836 and parameters: {'learning_rate': 0.00016528769089146512, 'weight_decay': 0.002985243743433368, 'batch_size': 64, 'co_train_epochs': 8, 'epoch_patience': 6}. Best is trial 4 with value: 0.677944471493021.
Using devices: cuda, cuda
2026-02-12 17:00:06 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 17:00:06 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 17:00:06 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 17:00:06 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0009039577944170932
Weight Decay: 0.0001225968654406683
Batch Size: 16
No. Epochs: 19
Epoch Patience: 6
 Accumulation Steps: 4
2026-02-12 17:00:06 - INFO - Learning Rate: 0.0009039577944170932
Weight Decay: 0.0001225968654406683
Batch Size: 16
No. Epochs: 19
Epoch Patience: 6
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 17:00:07 - INFO - Generating initial weights
Time taken for Epoch 1:21.06 - F1: 0.0189
2026-02-12 17:00:32 - INFO - Time taken for Epoch 1:21.06 - F1: 0.0189
Time taken for Epoch 2:20.90 - F1: 0.0244
2026-02-12 17:00:53 - INFO - Time taken for Epoch 2:20.90 - F1: 0.0244
Time taken for Epoch 3:20.93 - F1: 0.0189
2026-02-12 17:01:14 - INFO - Time taken for Epoch 3:20.93 - F1: 0.0189
Time taken for Epoch 4:20.86 - F1: 0.0189
2026-02-12 17:01:35 - INFO - Time taken for Epoch 4:20.86 - F1: 0.0189
Time taken for Epoch 5:20.88 - F1: 0.0189
2026-02-12 17:01:56 - INFO - Time taken for Epoch 5:20.88 - F1: 0.0189
Time taken for Epoch 6:20.90 - F1: 0.0189
2026-02-12 17:02:17 - INFO - Time taken for Epoch 6:20.90 - F1: 0.0189
Time taken for Epoch 7:20.90 - F1: 0.0189
2026-02-12 17:02:37 - INFO - Time taken for Epoch 7:20.90 - F1: 0.0189
Time taken for Epoch 8:20.91 - F1: 0.0189
2026-02-12 17:02:58 - INFO - Time taken for Epoch 8:20.91 - F1: 0.0189
Time taken for Epoch 9:20.91 - F1: 0.0189
2026-02-12 17:03:19 - INFO - Time taken for Epoch 9:20.91 - F1: 0.0189
Time taken for Epoch 10:20.89 - F1: 0.0189
2026-02-12 17:03:40 - INFO - Time taken for Epoch 10:20.89 - F1: 0.0189
Time taken for Epoch 11:20.91 - F1: 0.0189
2026-02-12 17:04:01 - INFO - Time taken for Epoch 11:20.91 - F1: 0.0189
Time taken for Epoch 12:20.93 - F1: 0.0189
2026-02-12 17:04:22 - INFO - Time taken for Epoch 12:20.93 - F1: 0.0189
Time taken for Epoch 13:20.90 - F1: 0.0189
2026-02-12 17:04:43 - INFO - Time taken for Epoch 13:20.90 - F1: 0.0189
Time taken for Epoch 14:20.91 - F1: 0.0189
2026-02-12 17:05:04 - INFO - Time taken for Epoch 14:20.91 - F1: 0.0189
Time taken for Epoch 15:20.87 - F1: 0.0189
2026-02-12 17:05:25 - INFO - Time taken for Epoch 15:20.87 - F1: 0.0189
Time taken for Epoch 16:20.88 - F1: 0.0189
2026-02-12 17:05:46 - INFO - Time taken for Epoch 16:20.88 - F1: 0.0189
Time taken for Epoch 17:20.87 - F1: 0.0189
2026-02-12 17:06:06 - INFO - Time taken for Epoch 17:20.87 - F1: 0.0189
Time taken for Epoch 18:20.87 - F1: 0.0189
2026-02-12 17:06:27 - INFO - Time taken for Epoch 18:20.87 - F1: 0.0189
Time taken for Epoch 19:20.87 - F1: 0.0189
2026-02-12 17:06:48 - INFO - Time taken for Epoch 19:20.87 - F1: 0.0189
Best F1:0.0244 - Best Epoch:2
2026-02-12 17:06:48 - INFO - Best F1:0.0244 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 17:06:49 - INFO - Starting co-training
Time taken for Epoch 1: 29.60s - F1: 0.01890670
2026-02-12 17:07:20 - INFO - Time taken for Epoch 1: 29.60s - F1: 0.01890670
Time taken for Epoch 2: 30.79s - F1: 0.04755179
2026-02-12 17:07:50 - INFO - Time taken for Epoch 2: 30.79s - F1: 0.04755179
Time taken for Epoch 3: 30.93s - F1: 0.04755179
2026-02-12 17:08:21 - INFO - Time taken for Epoch 3: 30.93s - F1: 0.04755179
Time taken for Epoch 4: 29.66s - F1: 0.04755179
2026-02-12 17:08:51 - INFO - Time taken for Epoch 4: 29.66s - F1: 0.04755179
Time taken for Epoch 5: 29.60s - F1: 0.04755179
2026-02-12 17:09:20 - INFO - Time taken for Epoch 5: 29.60s - F1: 0.04755179
Time taken for Epoch 6: 29.58s - F1: 0.04755179
2026-02-12 17:09:50 - INFO - Time taken for Epoch 6: 29.58s - F1: 0.04755179
Time taken for Epoch 7: 29.63s - F1: 0.04755179
2026-02-12 17:10:20 - INFO - Time taken for Epoch 7: 29.63s - F1: 0.04755179
Time taken for Epoch 8: 29.62s - F1: 0.04755179
2026-02-12 17:10:49 - INFO - Time taken for Epoch 8: 29.62s - F1: 0.04755179
Performance not improving for 6 consecutive epochs.
Performance not improving for 6 consecutive epochs.
2026-02-12 17:10:49 - INFO - Performance not improving for 6 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 17:10:52 - INFO - Fine-tuning models
Time taken for Epoch 1:2.94 - F1: 0.0476
2026-02-12 17:10:55 - INFO - Time taken for Epoch 1:2.94 - F1: 0.0476
Time taken for Epoch 2:4.14 - F1: 0.0476
2026-02-12 17:10:59 - INFO - Time taken for Epoch 2:4.14 - F1: 0.0476
Time taken for Epoch 3:2.92 - F1: 0.0089
2026-02-12 17:11:02 - INFO - Time taken for Epoch 3:2.92 - F1: 0.0089
Time taken for Epoch 4:2.92 - F1: 0.0038
2026-02-12 17:11:05 - INFO - Time taken for Epoch 4:2.92 - F1: 0.0038
Time taken for Epoch 5:2.92 - F1: 0.0189
2026-02-12 17:11:08 - INFO - Time taken for Epoch 5:2.92 - F1: 0.0189
Time taken for Epoch 6:2.92 - F1: 0.0189
2026-02-12 17:11:11 - INFO - Time taken for Epoch 6:2.92 - F1: 0.0189
Time taken for Epoch 7:2.93 - F1: 0.0189
2026-02-12 17:11:14 - INFO - Time taken for Epoch 7:2.93 - F1: 0.0189
Time taken for Epoch 8:2.92 - F1: 0.0189
2026-02-12 17:11:17 - INFO - Time taken for Epoch 8:2.92 - F1: 0.0189
Time taken for Epoch 9:2.93 - F1: 0.0189
2026-02-12 17:11:20 - INFO - Time taken for Epoch 9:2.93 - F1: 0.0189
Time taken for Epoch 10:2.92 - F1: 0.0189
2026-02-12 17:11:23 - INFO - Time taken for Epoch 10:2.92 - F1: 0.0189
Time taken for Epoch 11:2.92 - F1: 0.0189
2026-02-12 17:11:26 - INFO - Time taken for Epoch 11:2.92 - F1: 0.0189
Performance not improving for 10 consecutive epochs.
2026-02-12 17:11:26 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:0
2026-02-12 17:11:26 - INFO - Best F1:0.0476 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.4835
2026-02-12 17:11:34 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.4835
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.483516419975503)}
2026-02-12 17:11:34 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.483516419975503)}

Total time taken: 687.90 seconds
2026-02-12 17:11:34 - INFO - 
Total time taken: 687.90 seconds
2026-02-12 17:11:34 - INFO - Trial 9 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0009039577944170932, 'weight_decay': 0.0001225968654406683, 'batch_size': 16, 'co_train_epochs': 19, 'epoch_patience': 6}. Best is trial 4 with value: 0.677944471493021.

[BEST TRIAL RESULTS]
2026-02-12 17:11:34 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6779
2026-02-12 17:11:34 - INFO - F1 Score: 0.6779
Params: {'learning_rate': 3.9205870955943774e-05, 'weight_decay': 0.004075316568407703, 'batch_size': 64, 'co_train_epochs': 18, 'epoch_patience': 5}
2026-02-12 17:11:34 - INFO - Params: {'learning_rate': 3.9205870955943774e-05, 'weight_decay': 0.004075316568407703, 'batch_size': 64, 'co_train_epochs': 18, 'epoch_patience': 5}
  learning_rate: 3.9205870955943774e-05
2026-02-12 17:11:34 - INFO -   learning_rate: 3.9205870955943774e-05
  weight_decay: 0.004075316568407703
2026-02-12 17:11:34 - INFO -   weight_decay: 0.004075316568407703
  batch_size: 64
2026-02-12 17:11:34 - INFO -   batch_size: 64
  co_train_epochs: 18
2026-02-12 17:11:34 - INFO -   co_train_epochs: 18
  epoch_patience: 5
2026-02-12 17:11:34 - INFO -   epoch_patience: 5

Total time taken: 7813.06 seconds
2026-02-12 17:11:34 - INFO - 
Total time taken: 7813.06 seconds