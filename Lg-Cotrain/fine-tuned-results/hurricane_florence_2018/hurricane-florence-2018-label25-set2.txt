Running with 25 label/class set 2

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 16:21:51 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 16:21:51 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_florence_2018
Using devices: cuda, cuda
2026-02-13 16:21:52 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 16:21:52 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 16:21:52 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 16:21:52 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 8.657880665735881e-05
Weight Decay: 0.0007762957921439329
Batch Size: 16
No. Epochs: 14
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-13 16:21:53 - INFO - Learning Rate: 8.657880665735881e-05
Weight Decay: 0.0007762957921439329
Batch Size: 16
No. Epochs: 14
Epoch Patience: 4
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 16:21:54 - INFO - Generating initial weights
Time taken for Epoch 1:18.40 - F1: 0.1274
2026-02-13 16:22:16 - INFO - Time taken for Epoch 1:18.40 - F1: 0.1274
Time taken for Epoch 2:18.08 - F1: 0.1815
2026-02-13 16:22:34 - INFO - Time taken for Epoch 2:18.08 - F1: 0.1815
Time taken for Epoch 3:18.11 - F1: 0.1837
2026-02-13 16:22:52 - INFO - Time taken for Epoch 3:18.11 - F1: 0.1837
Time taken for Epoch 4:18.23 - F1: 0.2709
2026-02-13 16:23:11 - INFO - Time taken for Epoch 4:18.23 - F1: 0.2709
Time taken for Epoch 5:18.25 - F1: 0.4517
2026-02-13 16:23:29 - INFO - Time taken for Epoch 5:18.25 - F1: 0.4517
Time taken for Epoch 6:18.31 - F1: 0.4916
2026-02-13 16:23:47 - INFO - Time taken for Epoch 6:18.31 - F1: 0.4916
Time taken for Epoch 7:18.37 - F1: 0.5065
2026-02-13 16:24:05 - INFO - Time taken for Epoch 7:18.37 - F1: 0.5065
Time taken for Epoch 8:18.38 - F1: 0.5070
2026-02-13 16:24:24 - INFO - Time taken for Epoch 8:18.38 - F1: 0.5070
Time taken for Epoch 9:18.41 - F1: 0.5194
2026-02-13 16:24:42 - INFO - Time taken for Epoch 9:18.41 - F1: 0.5194
Time taken for Epoch 10:18.46 - F1: 0.5291
2026-02-13 16:25:01 - INFO - Time taken for Epoch 10:18.46 - F1: 0.5291
Time taken for Epoch 11:18.43 - F1: 0.5465
2026-02-13 16:25:19 - INFO - Time taken for Epoch 11:18.43 - F1: 0.5465
Time taken for Epoch 12:18.47 - F1: 0.5480
2026-02-13 16:25:38 - INFO - Time taken for Epoch 12:18.47 - F1: 0.5480
Time taken for Epoch 13:18.47 - F1: 0.5648
2026-02-13 16:25:56 - INFO - Time taken for Epoch 13:18.47 - F1: 0.5648
Time taken for Epoch 14:18.48 - F1: 0.5658
2026-02-13 16:26:15 - INFO - Time taken for Epoch 14:18.48 - F1: 0.5658
Best F1:0.5658 - Best Epoch:14
2026-02-13 16:26:15 - INFO - Best F1:0.5658 - Best Epoch:14
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 16:26:16 - INFO - Starting co-training
Time taken for Epoch 1: 24.43s - F1: 0.57079973
2026-02-13 16:26:41 - INFO - Time taken for Epoch 1: 24.43s - F1: 0.57079973
Time taken for Epoch 2: 25.51s - F1: 0.57983422
2026-02-13 16:27:06 - INFO - Time taken for Epoch 2: 25.51s - F1: 0.57983422
Time taken for Epoch 3: 25.64s - F1: 0.61270325
2026-02-13 16:27:32 - INFO - Time taken for Epoch 3: 25.64s - F1: 0.61270325
Time taken for Epoch 4: 25.61s - F1: 0.60441771
2026-02-13 16:27:57 - INFO - Time taken for Epoch 4: 25.61s - F1: 0.60441771
Time taken for Epoch 5: 24.47s - F1: 0.59898262
2026-02-13 16:28:22 - INFO - Time taken for Epoch 5: 24.47s - F1: 0.59898262
Time taken for Epoch 6: 24.51s - F1: 0.60292207
2026-02-13 16:28:46 - INFO - Time taken for Epoch 6: 24.51s - F1: 0.60292207
Time taken for Epoch 7: 24.52s - F1: 0.60235413
2026-02-13 16:29:11 - INFO - Time taken for Epoch 7: 24.52s - F1: 0.60235413
Performance not improving for 4 consecutive epochs.
Performance not improving for 4 consecutive epochs.
2026-02-13 16:29:11 - INFO - Performance not improving for 4 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 16:29:14 - INFO - Fine-tuning models
Time taken for Epoch 1:3.42 - F1: 0.6051
2026-02-13 16:29:17 - INFO - Time taken for Epoch 1:3.42 - F1: 0.6051
Time taken for Epoch 2:4.47 - F1: 0.5955
2026-02-13 16:29:22 - INFO - Time taken for Epoch 2:4.47 - F1: 0.5955
Time taken for Epoch 3:3.41 - F1: 0.5978
2026-02-13 16:29:25 - INFO - Time taken for Epoch 3:3.41 - F1: 0.5978
Time taken for Epoch 4:3.42 - F1: 0.6109
2026-02-13 16:29:28 - INFO - Time taken for Epoch 4:3.42 - F1: 0.6109
Time taken for Epoch 5:4.57 - F1: 0.6218
2026-02-13 16:29:33 - INFO - Time taken for Epoch 5:4.57 - F1: 0.6218
Time taken for Epoch 6:4.59 - F1: 0.6415
2026-02-13 16:29:38 - INFO - Time taken for Epoch 6:4.59 - F1: 0.6415
Time taken for Epoch 7:4.57 - F1: 0.6518
2026-02-13 16:29:42 - INFO - Time taken for Epoch 7:4.57 - F1: 0.6518
Time taken for Epoch 8:4.58 - F1: 0.6575
2026-02-13 16:29:47 - INFO - Time taken for Epoch 8:4.58 - F1: 0.6575
Time taken for Epoch 9:4.58 - F1: 0.6582
2026-02-13 16:29:51 - INFO - Time taken for Epoch 9:4.58 - F1: 0.6582
Time taken for Epoch 10:4.56 - F1: 0.6545
2026-02-13 16:29:56 - INFO - Time taken for Epoch 10:4.56 - F1: 0.6545
Time taken for Epoch 11:3.40 - F1: 0.6634
2026-02-13 16:29:59 - INFO - Time taken for Epoch 11:3.40 - F1: 0.6634
Time taken for Epoch 12:4.59 - F1: 0.6688
2026-02-13 16:30:04 - INFO - Time taken for Epoch 12:4.59 - F1: 0.6688
Time taken for Epoch 13:4.57 - F1: 0.6599
2026-02-13 16:30:09 - INFO - Time taken for Epoch 13:4.57 - F1: 0.6599
Time taken for Epoch 14:3.42 - F1: 0.6488
2026-02-13 16:30:12 - INFO - Time taken for Epoch 14:3.42 - F1: 0.6488
Time taken for Epoch 15:3.41 - F1: 0.6610
2026-02-13 16:30:15 - INFO - Time taken for Epoch 15:3.41 - F1: 0.6610
Time taken for Epoch 16:3.41 - F1: 0.6626
2026-02-13 16:30:19 - INFO - Time taken for Epoch 16:3.41 - F1: 0.6626
Time taken for Epoch 17:3.41 - F1: 0.6723
2026-02-13 16:30:22 - INFO - Time taken for Epoch 17:3.41 - F1: 0.6723
Time taken for Epoch 18:4.56 - F1: 0.6717
2026-02-13 16:30:27 - INFO - Time taken for Epoch 18:4.56 - F1: 0.6717
Time taken for Epoch 19:3.40 - F1: 0.6698
2026-02-13 16:30:30 - INFO - Time taken for Epoch 19:3.40 - F1: 0.6698
Time taken for Epoch 20:3.41 - F1: 0.6623
2026-02-13 16:30:34 - INFO - Time taken for Epoch 20:3.41 - F1: 0.6623
Time taken for Epoch 21:3.41 - F1: 0.6655
2026-02-13 16:30:37 - INFO - Time taken for Epoch 21:3.41 - F1: 0.6655
Time taken for Epoch 22:3.41 - F1: 0.6655
2026-02-13 16:30:40 - INFO - Time taken for Epoch 22:3.41 - F1: 0.6655
Time taken for Epoch 23:3.41 - F1: 0.6689
2026-02-13 16:30:44 - INFO - Time taken for Epoch 23:3.41 - F1: 0.6689
Time taken for Epoch 24:3.41 - F1: 0.6669
2026-02-13 16:30:47 - INFO - Time taken for Epoch 24:3.41 - F1: 0.6669
Time taken for Epoch 25:3.41 - F1: 0.6640
2026-02-13 16:30:51 - INFO - Time taken for Epoch 25:3.41 - F1: 0.6640
Time taken for Epoch 26:3.41 - F1: 0.6624
2026-02-13 16:30:54 - INFO - Time taken for Epoch 26:3.41 - F1: 0.6624
Time taken for Epoch 27:3.41 - F1: 0.6602
2026-02-13 16:30:57 - INFO - Time taken for Epoch 27:3.41 - F1: 0.6602
Performance not improving for 10 consecutive epochs.
2026-02-13 16:30:57 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6723 - Best Epoch:16
2026-02-13 16:30:57 - INFO - Best F1:0.6723 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6603, Test ECE: 0.0661
2026-02-13 16:31:05 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6603, Test ECE: 0.0661
All results: {'f1_macro': 0.660330468896278, 'ece': np.float64(0.06611576270519198)}
2026-02-13 16:31:05 - INFO - All results: {'f1_macro': 0.660330468896278, 'ece': np.float64(0.06611576270519198)}

Total time taken: 554.05 seconds
2026-02-13 16:31:05 - INFO - 
Total time taken: 554.05 seconds
2026-02-13 16:31:05 - INFO - Trial 0 finished with value: 0.660330468896278 and parameters: {'learning_rate': 8.657880665735881e-05, 'weight_decay': 0.0007762957921439329, 'batch_size': 16, 'co_train_epochs': 14, 'epoch_patience': 4}. Best is trial 0 with value: 0.660330468896278.
Using devices: cuda, cuda
2026-02-13 16:31:05 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 16:31:05 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 16:31:05 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 16:31:05 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.0005750031111624406
Weight Decay: 0.00024854553475975067
Batch Size: 32
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-13 16:31:06 - INFO - Learning Rate: 0.0005750031111624406
Weight Decay: 0.00024854553475975067
Batch Size: 32
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 16:31:07 - INFO - Generating initial weights
Time taken for Epoch 1:18.04 - F1: 0.0205
2026-02-13 16:31:28 - INFO - Time taken for Epoch 1:18.04 - F1: 0.0205
Time taken for Epoch 2:17.91 - F1: 0.0248
2026-02-13 16:31:46 - INFO - Time taken for Epoch 2:17.91 - F1: 0.0248
Time taken for Epoch 3:17.89 - F1: 0.0321
2026-02-13 16:32:04 - INFO - Time taken for Epoch 3:17.89 - F1: 0.0321
Time taken for Epoch 4:17.88 - F1: 0.0155
2026-02-13 16:32:22 - INFO - Time taken for Epoch 4:17.88 - F1: 0.0155
Time taken for Epoch 5:17.95 - F1: 0.0155
2026-02-13 16:32:40 - INFO - Time taken for Epoch 5:17.95 - F1: 0.0155
Time taken for Epoch 6:17.93 - F1: 0.0155
2026-02-13 16:32:58 - INFO - Time taken for Epoch 6:17.93 - F1: 0.0155
Time taken for Epoch 7:17.97 - F1: 0.0155
2026-02-13 16:33:16 - INFO - Time taken for Epoch 7:17.97 - F1: 0.0155
Time taken for Epoch 8:17.91 - F1: 0.0155
2026-02-13 16:33:34 - INFO - Time taken for Epoch 8:17.91 - F1: 0.0155
Best F1:0.0321 - Best Epoch:3
2026-02-13 16:33:34 - INFO - Best F1:0.0321 - Best Epoch:3
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 16:33:35 - INFO - Starting co-training
Time taken for Epoch 1: 29.44s - F1: 0.03212851
2026-02-13 16:34:05 - INFO - Time taken for Epoch 1: 29.44s - F1: 0.03212851
Time taken for Epoch 2: 30.55s - F1: 0.03212851
2026-02-13 16:34:35 - INFO - Time taken for Epoch 2: 30.55s - F1: 0.03212851
Time taken for Epoch 3: 29.45s - F1: 0.03212851
2026-02-13 16:35:05 - INFO - Time taken for Epoch 3: 29.45s - F1: 0.03212851
Time taken for Epoch 4: 29.49s - F1: 0.03212851
2026-02-13 16:35:34 - INFO - Time taken for Epoch 4: 29.49s - F1: 0.03212851
Time taken for Epoch 5: 29.50s - F1: 0.03212851
2026-02-13 16:36:04 - INFO - Time taken for Epoch 5: 29.50s - F1: 0.03212851
Time taken for Epoch 6: 29.51s - F1: 0.03212851
2026-02-13 16:36:33 - INFO - Time taken for Epoch 6: 29.51s - F1: 0.03212851
Time taken for Epoch 7: 29.52s - F1: 0.03212851
2026-02-13 16:37:03 - INFO - Time taken for Epoch 7: 29.52s - F1: 0.03212851
Performance not improving for 6 consecutive epochs.
Performance not improving for 6 consecutive epochs.
2026-02-13 16:37:03 - INFO - Performance not improving for 6 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 16:37:05 - INFO - Fine-tuning models
Time taken for Epoch 1:3.34 - F1: 0.0100
2026-02-13 16:37:09 - INFO - Time taken for Epoch 1:3.34 - F1: 0.0100
Time taken for Epoch 2:4.40 - F1: 0.0100
2026-02-13 16:37:13 - INFO - Time taken for Epoch 2:4.40 - F1: 0.0100
Time taken for Epoch 3:3.31 - F1: 0.0109
2026-02-13 16:37:17 - INFO - Time taken for Epoch 3:3.31 - F1: 0.0109
Time taken for Epoch 4:4.47 - F1: 0.0385
2026-02-13 16:37:21 - INFO - Time taken for Epoch 4:4.47 - F1: 0.0385
Time taken for Epoch 5:4.49 - F1: 0.0321
2026-02-13 16:37:26 - INFO - Time taken for Epoch 5:4.49 - F1: 0.0321
Time taken for Epoch 6:3.30 - F1: 0.0321
2026-02-13 16:37:29 - INFO - Time taken for Epoch 6:3.30 - F1: 0.0321
Time taken for Epoch 7:3.31 - F1: 0.0321
2026-02-13 16:37:32 - INFO - Time taken for Epoch 7:3.31 - F1: 0.0321
Time taken for Epoch 8:3.30 - F1: 0.0155
2026-02-13 16:37:36 - INFO - Time taken for Epoch 8:3.30 - F1: 0.0155
Time taken for Epoch 9:3.31 - F1: 0.0100
2026-02-13 16:37:39 - INFO - Time taken for Epoch 9:3.31 - F1: 0.0100
Time taken for Epoch 10:3.31 - F1: 0.0155
2026-02-13 16:37:42 - INFO - Time taken for Epoch 10:3.31 - F1: 0.0155
Time taken for Epoch 11:3.32 - F1: 0.0155
2026-02-13 16:37:45 - INFO - Time taken for Epoch 11:3.32 - F1: 0.0155
Time taken for Epoch 12:3.31 - F1: 0.0155
2026-02-13 16:37:49 - INFO - Time taken for Epoch 12:3.31 - F1: 0.0155
Time taken for Epoch 13:3.31 - F1: 0.0155
2026-02-13 16:37:52 - INFO - Time taken for Epoch 13:3.31 - F1: 0.0155
Time taken for Epoch 14:3.32 - F1: 0.0155
2026-02-13 16:37:55 - INFO - Time taken for Epoch 14:3.32 - F1: 0.0155
Performance not improving for 10 consecutive epochs.
2026-02-13 16:37:55 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0385 - Best Epoch:3
2026-02-13 16:37:55 - INFO - Best F1:0.0385 - Best Epoch:3
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0384, Test ECE: 0.0836
2026-02-13 16:38:03 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0384, Test ECE: 0.0836
All results: {'f1_macro': 0.03837037037037037, 'ece': np.float64(0.08360578937457513)}
2026-02-13 16:38:03 - INFO - All results: {'f1_macro': 0.03837037037037037, 'ece': np.float64(0.08360578937457513)}

Total time taken: 417.51 seconds
2026-02-13 16:38:03 - INFO - 
Total time taken: 417.51 seconds
2026-02-13 16:38:03 - INFO - Trial 1 finished with value: 0.03837037037037037 and parameters: {'learning_rate': 0.0005750031111624406, 'weight_decay': 0.00024854553475975067, 'batch_size': 32, 'co_train_epochs': 8, 'epoch_patience': 6}. Best is trial 0 with value: 0.660330468896278.
Using devices: cuda, cuda
2026-02-13 16:38:03 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 16:38:03 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 16:38:03 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 16:38:03 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 1.3401600950518019e-05
Weight Decay: 0.00896897594874072
Batch Size: 8
No. Epochs: 17
Epoch Patience: 6
 Accumulation Steps: 8
2026-02-13 16:38:03 - INFO - Learning Rate: 1.3401600950518019e-05
Weight Decay: 0.00896897594874072
Batch Size: 8
No. Epochs: 17
Epoch Patience: 6
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 16:38:04 - INFO - Generating initial weights
Time taken for Epoch 1:20.07 - F1: 0.0603
2026-02-13 16:38:28 - INFO - Time taken for Epoch 1:20.07 - F1: 0.0603
Time taken for Epoch 2:19.97 - F1: 0.0631
2026-02-13 16:38:48 - INFO - Time taken for Epoch 2:19.97 - F1: 0.0631
Time taken for Epoch 3:19.99 - F1: 0.0752
2026-02-13 16:39:08 - INFO - Time taken for Epoch 3:19.99 - F1: 0.0752
Time taken for Epoch 4:20.06 - F1: 0.0910
2026-02-13 16:39:28 - INFO - Time taken for Epoch 4:20.06 - F1: 0.0910
Time taken for Epoch 5:20.04 - F1: 0.1182
2026-02-13 16:39:48 - INFO - Time taken for Epoch 5:20.04 - F1: 0.1182
Time taken for Epoch 6:20.05 - F1: 0.1555
2026-02-13 16:40:08 - INFO - Time taken for Epoch 6:20.05 - F1: 0.1555
Time taken for Epoch 7:20.07 - F1: 0.2196
2026-02-13 16:40:28 - INFO - Time taken for Epoch 7:20.07 - F1: 0.2196
Time taken for Epoch 8:20.06 - F1: 0.2897
2026-02-13 16:40:48 - INFO - Time taken for Epoch 8:20.06 - F1: 0.2897
Time taken for Epoch 9:20.07 - F1: 0.3371
2026-02-13 16:41:08 - INFO - Time taken for Epoch 9:20.07 - F1: 0.3371
Time taken for Epoch 10:20.06 - F1: 0.3733
2026-02-13 16:41:28 - INFO - Time taken for Epoch 10:20.06 - F1: 0.3733
Time taken for Epoch 11:20.05 - F1: 0.3882
2026-02-13 16:41:48 - INFO - Time taken for Epoch 11:20.05 - F1: 0.3882
Time taken for Epoch 12:20.10 - F1: 0.4224
2026-02-13 16:42:08 - INFO - Time taken for Epoch 12:20.10 - F1: 0.4224
Time taken for Epoch 13:20.09 - F1: 0.4301
2026-02-13 16:42:29 - INFO - Time taken for Epoch 13:20.09 - F1: 0.4301
Time taken for Epoch 14:20.03 - F1: 0.4487
2026-02-13 16:42:49 - INFO - Time taken for Epoch 14:20.03 - F1: 0.4487
Time taken for Epoch 15:20.03 - F1: 0.4545
2026-02-13 16:43:09 - INFO - Time taken for Epoch 15:20.03 - F1: 0.4545
Time taken for Epoch 16:20.05 - F1: 0.4587
2026-02-13 16:43:29 - INFO - Time taken for Epoch 16:20.05 - F1: 0.4587
Time taken for Epoch 17:20.04 - F1: 0.4669
2026-02-13 16:43:49 - INFO - Time taken for Epoch 17:20.04 - F1: 0.4669
Best F1:0.4669 - Best Epoch:17
2026-02-13 16:43:49 - INFO - Best F1:0.4669 - Best Epoch:17
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 16:43:50 - INFO - Starting co-training
Time taken for Epoch 1: 22.89s - F1: 0.22378930
2026-02-13 16:44:13 - INFO - Time taken for Epoch 1: 22.89s - F1: 0.22378930
Time taken for Epoch 2: 23.97s - F1: 0.38897769
2026-02-13 16:44:37 - INFO - Time taken for Epoch 2: 23.97s - F1: 0.38897769
Time taken for Epoch 3: 24.04s - F1: 0.45564135
2026-02-13 16:45:01 - INFO - Time taken for Epoch 3: 24.04s - F1: 0.45564135
Time taken for Epoch 4: 24.12s - F1: 0.48706980
2026-02-13 16:45:25 - INFO - Time taken for Epoch 4: 24.12s - F1: 0.48706980
Time taken for Epoch 5: 24.61s - F1: 0.49852848
2026-02-13 16:45:50 - INFO - Time taken for Epoch 5: 24.61s - F1: 0.49852848
Time taken for Epoch 6: 24.10s - F1: 0.51944287
2026-02-13 16:46:14 - INFO - Time taken for Epoch 6: 24.10s - F1: 0.51944287
Time taken for Epoch 7: 24.00s - F1: 0.54919595
2026-02-13 16:46:38 - INFO - Time taken for Epoch 7: 24.00s - F1: 0.54919595
Time taken for Epoch 8: 24.15s - F1: 0.58541317
2026-02-13 16:47:02 - INFO - Time taken for Epoch 8: 24.15s - F1: 0.58541317
Time taken for Epoch 9: 24.14s - F1: 0.59582155
2026-02-13 16:47:26 - INFO - Time taken for Epoch 9: 24.14s - F1: 0.59582155
Time taken for Epoch 10: 24.13s - F1: 0.60408045
2026-02-13 16:47:50 - INFO - Time taken for Epoch 10: 24.13s - F1: 0.60408045
Time taken for Epoch 11: 24.13s - F1: 0.60354218
2026-02-13 16:48:15 - INFO - Time taken for Epoch 11: 24.13s - F1: 0.60354218
Time taken for Epoch 12: 22.98s - F1: 0.61178253
2026-02-13 16:48:38 - INFO - Time taken for Epoch 12: 22.98s - F1: 0.61178253
Time taken for Epoch 13: 24.36s - F1: 0.61907034
2026-02-13 16:49:02 - INFO - Time taken for Epoch 13: 24.36s - F1: 0.61907034
Time taken for Epoch 14: 24.43s - F1: 0.62093755
2026-02-13 16:49:26 - INFO - Time taken for Epoch 14: 24.43s - F1: 0.62093755
Time taken for Epoch 15: 24.06s - F1: 0.61295946
2026-02-13 16:49:50 - INFO - Time taken for Epoch 15: 24.06s - F1: 0.61295946
Time taken for Epoch 16: 22.92s - F1: 0.61334014
2026-02-13 16:50:13 - INFO - Time taken for Epoch 16: 22.92s - F1: 0.61334014
Time taken for Epoch 17: 23.00s - F1: 0.63079153
2026-02-13 16:50:36 - INFO - Time taken for Epoch 17: 23.00s - F1: 0.63079153
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 16:50:40 - INFO - Fine-tuning models
Time taken for Epoch 1:3.71 - F1: 0.6187
2026-02-13 16:50:44 - INFO - Time taken for Epoch 1:3.71 - F1: 0.6187
Time taken for Epoch 2:4.74 - F1: 0.6128
2026-02-13 16:50:49 - INFO - Time taken for Epoch 2:4.74 - F1: 0.6128
Time taken for Epoch 3:3.68 - F1: 0.6240
2026-02-13 16:50:53 - INFO - Time taken for Epoch 3:3.68 - F1: 0.6240
Time taken for Epoch 4:4.84 - F1: 0.6354
2026-02-13 16:50:57 - INFO - Time taken for Epoch 4:4.84 - F1: 0.6354
Time taken for Epoch 5:4.83 - F1: 0.6257
2026-02-13 16:51:02 - INFO - Time taken for Epoch 5:4.83 - F1: 0.6257
Time taken for Epoch 6:3.68 - F1: 0.6169
2026-02-13 16:51:06 - INFO - Time taken for Epoch 6:3.68 - F1: 0.6169
Time taken for Epoch 7:3.68 - F1: 0.6119
2026-02-13 16:51:10 - INFO - Time taken for Epoch 7:3.68 - F1: 0.6119
Time taken for Epoch 8:3.68 - F1: 0.6174
2026-02-13 16:51:13 - INFO - Time taken for Epoch 8:3.68 - F1: 0.6174
Time taken for Epoch 9:3.68 - F1: 0.6092
2026-02-13 16:51:17 - INFO - Time taken for Epoch 9:3.68 - F1: 0.6092
Time taken for Epoch 10:3.73 - F1: 0.6095
2026-02-13 16:51:21 - INFO - Time taken for Epoch 10:3.73 - F1: 0.6095
Time taken for Epoch 11:3.75 - F1: 0.6113
2026-02-13 16:51:24 - INFO - Time taken for Epoch 11:3.75 - F1: 0.6113
Time taken for Epoch 12:3.74 - F1: 0.6096
2026-02-13 16:51:28 - INFO - Time taken for Epoch 12:3.74 - F1: 0.6096
Time taken for Epoch 13:3.73 - F1: 0.6176
2026-02-13 16:51:32 - INFO - Time taken for Epoch 13:3.73 - F1: 0.6176
Time taken for Epoch 14:3.74 - F1: 0.6129
2026-02-13 16:51:36 - INFO - Time taken for Epoch 14:3.74 - F1: 0.6129
Performance not improving for 10 consecutive epochs.
2026-02-13 16:51:36 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6354 - Best Epoch:3
2026-02-13 16:51:36 - INFO - Best F1:0.6354 - Best Epoch:3
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6259, Test ECE: 0.0349
2026-02-13 16:51:44 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6259, Test ECE: 0.0349
All results: {'f1_macro': 0.6259366050231114, 'ece': np.float64(0.034944489970503076)}
2026-02-13 16:51:44 - INFO - All results: {'f1_macro': 0.6259366050231114, 'ece': np.float64(0.034944489970503076)}

Total time taken: 821.16 seconds
2026-02-13 16:51:44 - INFO - 
Total time taken: 821.16 seconds
2026-02-13 16:51:44 - INFO - Trial 2 finished with value: 0.6259366050231114 and parameters: {'learning_rate': 1.3401600950518019e-05, 'weight_decay': 0.00896897594874072, 'batch_size': 8, 'co_train_epochs': 17, 'epoch_patience': 6}. Best is trial 0 with value: 0.660330468896278.
Using devices: cuda, cuda
2026-02-13 16:51:44 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 16:51:44 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 16:51:44 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 16:51:44 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.00012658725202321515
Weight Decay: 0.0004651385698538199
Batch Size: 64
No. Epochs: 5
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-13 16:51:44 - INFO - Learning Rate: 0.00012658725202321515
Weight Decay: 0.0004651385698538199
Batch Size: 64
No. Epochs: 5
Epoch Patience: 9
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 16:51:45 - INFO - Generating initial weights
Time taken for Epoch 1:17.24 - F1: 0.2094
2026-02-13 16:52:06 - INFO - Time taken for Epoch 1:17.24 - F1: 0.2094
Time taken for Epoch 2:17.06 - F1: 0.2405
2026-02-13 16:52:23 - INFO - Time taken for Epoch 2:17.06 - F1: 0.2405
Time taken for Epoch 3:17.05 - F1: 0.2403
2026-02-13 16:52:40 - INFO - Time taken for Epoch 3:17.05 - F1: 0.2403
Time taken for Epoch 4:17.11 - F1: 0.2471
2026-02-13 16:52:57 - INFO - Time taken for Epoch 4:17.11 - F1: 0.2471
Time taken for Epoch 5:17.13 - F1: 0.2544
2026-02-13 16:53:15 - INFO - Time taken for Epoch 5:17.13 - F1: 0.2544
Best F1:0.2544 - Best Epoch:5
2026-02-13 16:53:15 - INFO - Best F1:0.2544 - Best Epoch:5
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 16:53:16 - INFO - Starting co-training
Time taken for Epoch 1: 38.46s - F1: 0.59067028
2026-02-13 16:53:55 - INFO - Time taken for Epoch 1: 38.46s - F1: 0.59067028
Time taken for Epoch 2: 39.61s - F1: 0.61450888
2026-02-13 16:54:34 - INFO - Time taken for Epoch 2: 39.61s - F1: 0.61450888
Time taken for Epoch 3: 39.69s - F1: 0.60364481
2026-02-13 16:55:14 - INFO - Time taken for Epoch 3: 39.69s - F1: 0.60364481
Time taken for Epoch 4: 38.57s - F1: 0.64176262
2026-02-13 16:55:52 - INFO - Time taken for Epoch 4: 38.57s - F1: 0.64176262
Time taken for Epoch 5: 39.72s - F1: 0.62790388
2026-02-13 16:56:32 - INFO - Time taken for Epoch 5: 39.72s - F1: 0.62790388
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 16:56:35 - INFO - Fine-tuning models
Time taken for Epoch 1:3.21 - F1: 0.6330
2026-02-13 16:56:38 - INFO - Time taken for Epoch 1:3.21 - F1: 0.6330
Time taken for Epoch 2:4.26 - F1: 0.6142
2026-02-13 16:56:42 - INFO - Time taken for Epoch 2:4.26 - F1: 0.6142
Time taken for Epoch 3:3.18 - F1: 0.6364
2026-02-13 16:56:45 - INFO - Time taken for Epoch 3:3.18 - F1: 0.6364
Time taken for Epoch 4:4.35 - F1: 0.6371
2026-02-13 16:56:50 - INFO - Time taken for Epoch 4:4.35 - F1: 0.6371
Time taken for Epoch 5:4.35 - F1: 0.6294
2026-02-13 16:56:54 - INFO - Time taken for Epoch 5:4.35 - F1: 0.6294
Time taken for Epoch 6:3.17 - F1: 0.6468
2026-02-13 16:56:57 - INFO - Time taken for Epoch 6:3.17 - F1: 0.6468
Time taken for Epoch 7:4.36 - F1: 0.6549
2026-02-13 16:57:02 - INFO - Time taken for Epoch 7:4.36 - F1: 0.6549
Time taken for Epoch 8:4.33 - F1: 0.6733
2026-02-13 16:57:06 - INFO - Time taken for Epoch 8:4.33 - F1: 0.6733
Time taken for Epoch 9:4.35 - F1: 0.6798
2026-02-13 16:57:10 - INFO - Time taken for Epoch 9:4.35 - F1: 0.6798
Time taken for Epoch 10:4.33 - F1: 0.6764
2026-02-13 16:57:15 - INFO - Time taken for Epoch 10:4.33 - F1: 0.6764
Time taken for Epoch 11:3.16 - F1: 0.6660
2026-02-13 16:57:18 - INFO - Time taken for Epoch 11:3.16 - F1: 0.6660
Time taken for Epoch 12:3.16 - F1: 0.6639
2026-02-13 16:57:21 - INFO - Time taken for Epoch 12:3.16 - F1: 0.6639
Time taken for Epoch 13:3.16 - F1: 0.6636
2026-02-13 16:57:24 - INFO - Time taken for Epoch 13:3.16 - F1: 0.6636
Time taken for Epoch 14:3.16 - F1: 0.6626
2026-02-13 16:57:27 - INFO - Time taken for Epoch 14:3.16 - F1: 0.6626
Time taken for Epoch 15:3.17 - F1: 0.6606
2026-02-13 16:57:31 - INFO - Time taken for Epoch 15:3.17 - F1: 0.6606
Time taken for Epoch 16:3.17 - F1: 0.6545
2026-02-13 16:57:34 - INFO - Time taken for Epoch 16:3.17 - F1: 0.6545
Time taken for Epoch 17:3.17 - F1: 0.6613
2026-02-13 16:57:37 - INFO - Time taken for Epoch 17:3.17 - F1: 0.6613
Time taken for Epoch 18:3.17 - F1: 0.6601
2026-02-13 16:57:40 - INFO - Time taken for Epoch 18:3.17 - F1: 0.6601
Time taken for Epoch 19:3.16 - F1: 0.6630
2026-02-13 16:57:43 - INFO - Time taken for Epoch 19:3.16 - F1: 0.6630
Performance not improving for 10 consecutive epochs.
2026-02-13 16:57:43 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6798 - Best Epoch:8
2026-02-13 16:57:43 - INFO - Best F1:0.6798 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6827, Test ECE: 0.0663
2026-02-13 16:57:50 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6827, Test ECE: 0.0663
All results: {'f1_macro': 0.6826995365537694, 'ece': np.float64(0.06629659820236958)}
2026-02-13 16:57:50 - INFO - All results: {'f1_macro': 0.6826995365537694, 'ece': np.float64(0.06629659820236958)}

Total time taken: 366.25 seconds
2026-02-13 16:57:50 - INFO - 
Total time taken: 366.25 seconds
2026-02-13 16:57:50 - INFO - Trial 3 finished with value: 0.6826995365537694 and parameters: {'learning_rate': 0.00012658725202321515, 'weight_decay': 0.0004651385698538199, 'batch_size': 64, 'co_train_epochs': 5, 'epoch_patience': 9}. Best is trial 3 with value: 0.6826995365537694.
Using devices: cuda, cuda
2026-02-13 16:57:50 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 16:57:50 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 16:57:50 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 16:57:50 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 6.525223525125416e-05
Weight Decay: 0.0001085944529138788
Batch Size: 8
No. Epochs: 12
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-13 16:57:51 - INFO - Learning Rate: 6.525223525125416e-05
Weight Decay: 0.0001085944529138788
Batch Size: 8
No. Epochs: 12
Epoch Patience: 7
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 16:57:52 - INFO - Generating initial weights
Time taken for Epoch 1:20.02 - F1: 0.0430
2026-02-13 16:58:15 - INFO - Time taken for Epoch 1:20.02 - F1: 0.0430
Time taken for Epoch 2:19.89 - F1: 0.1586
2026-02-13 16:58:35 - INFO - Time taken for Epoch 2:19.89 - F1: 0.1586
Time taken for Epoch 3:19.91 - F1: 0.2517
2026-02-13 16:58:55 - INFO - Time taken for Epoch 3:19.91 - F1: 0.2517
Time taken for Epoch 4:19.98 - F1: 0.3633
2026-02-13 16:59:15 - INFO - Time taken for Epoch 4:19.98 - F1: 0.3633
Time taken for Epoch 5:20.01 - F1: 0.4460
2026-02-13 16:59:35 - INFO - Time taken for Epoch 5:20.01 - F1: 0.4460
Time taken for Epoch 6:19.99 - F1: 0.4596
2026-02-13 16:59:55 - INFO - Time taken for Epoch 6:19.99 - F1: 0.4596
Time taken for Epoch 7:19.99 - F1: 0.4578
2026-02-13 17:00:15 - INFO - Time taken for Epoch 7:19.99 - F1: 0.4578
Time taken for Epoch 8:20.01 - F1: 0.4819
2026-02-13 17:00:35 - INFO - Time taken for Epoch 8:20.01 - F1: 0.4819
Time taken for Epoch 9:20.03 - F1: 0.4992
2026-02-13 17:00:55 - INFO - Time taken for Epoch 9:20.03 - F1: 0.4992
Time taken for Epoch 10:19.99 - F1: 0.5206
2026-02-13 17:01:15 - INFO - Time taken for Epoch 10:19.99 - F1: 0.5206
Time taken for Epoch 11:20.02 - F1: 0.4959
2026-02-13 17:01:35 - INFO - Time taken for Epoch 11:20.02 - F1: 0.4959
Time taken for Epoch 12:19.96 - F1: 0.5265
2026-02-13 17:01:55 - INFO - Time taken for Epoch 12:19.96 - F1: 0.5265
Best F1:0.5265 - Best Epoch:12
2026-02-13 17:01:55 - INFO - Best F1:0.5265 - Best Epoch:12
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 17:01:56 - INFO - Starting co-training
Time taken for Epoch 1: 22.91s - F1: 0.43297148
2026-02-13 17:02:20 - INFO - Time taken for Epoch 1: 22.91s - F1: 0.43297148
Time taken for Epoch 2: 23.96s - F1: 0.51657282
2026-02-13 17:02:44 - INFO - Time taken for Epoch 2: 23.96s - F1: 0.51657282
Time taken for Epoch 3: 24.13s - F1: 0.58909666
2026-02-13 17:03:08 - INFO - Time taken for Epoch 3: 24.13s - F1: 0.58909666
Time taken for Epoch 4: 24.07s - F1: 0.58447295
2026-02-13 17:03:32 - INFO - Time taken for Epoch 4: 24.07s - F1: 0.58447295
Time taken for Epoch 5: 22.99s - F1: 0.59859074
2026-02-13 17:03:55 - INFO - Time taken for Epoch 5: 22.99s - F1: 0.59859074
Time taken for Epoch 6: 24.12s - F1: 0.61066710
2026-02-13 17:04:19 - INFO - Time taken for Epoch 6: 24.12s - F1: 0.61066710
Time taken for Epoch 7: 24.03s - F1: 0.59738067
2026-02-13 17:04:43 - INFO - Time taken for Epoch 7: 24.03s - F1: 0.59738067
Time taken for Epoch 8: 22.98s - F1: 0.58696972
2026-02-13 17:05:06 - INFO - Time taken for Epoch 8: 22.98s - F1: 0.58696972
Time taken for Epoch 9: 22.91s - F1: 0.58486654
2026-02-13 17:05:29 - INFO - Time taken for Epoch 9: 22.91s - F1: 0.58486654
Time taken for Epoch 10: 22.92s - F1: 0.60563845
2026-02-13 17:05:52 - INFO - Time taken for Epoch 10: 22.92s - F1: 0.60563845
Time taken for Epoch 11: 22.93s - F1: 0.59244813
2026-02-13 17:06:15 - INFO - Time taken for Epoch 11: 22.93s - F1: 0.59244813
Time taken for Epoch 12: 22.96s - F1: 0.61612036
2026-02-13 17:06:38 - INFO - Time taken for Epoch 12: 22.96s - F1: 0.61612036
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 17:06:41 - INFO - Fine-tuning models
Time taken for Epoch 1:3.71 - F1: 0.6059
2026-02-13 17:06:45 - INFO - Time taken for Epoch 1:3.71 - F1: 0.6059
Time taken for Epoch 2:4.73 - F1: 0.6191
2026-02-13 17:06:50 - INFO - Time taken for Epoch 2:4.73 - F1: 0.6191
Time taken for Epoch 3:4.82 - F1: 0.6226
2026-02-13 17:06:55 - INFO - Time taken for Epoch 3:4.82 - F1: 0.6226
Time taken for Epoch 4:4.82 - F1: 0.6194
2026-02-13 17:07:00 - INFO - Time taken for Epoch 4:4.82 - F1: 0.6194
Time taken for Epoch 5:3.72 - F1: 0.6201
2026-02-13 17:07:03 - INFO - Time taken for Epoch 5:3.72 - F1: 0.6201
Time taken for Epoch 6:3.72 - F1: 0.6283
2026-02-13 17:07:07 - INFO - Time taken for Epoch 6:3.72 - F1: 0.6283
Time taken for Epoch 7:4.90 - F1: 0.6387
2026-02-13 17:07:12 - INFO - Time taken for Epoch 7:4.90 - F1: 0.6387
Time taken for Epoch 8:4.84 - F1: 0.6443
2026-02-13 17:07:17 - INFO - Time taken for Epoch 8:4.84 - F1: 0.6443
Time taken for Epoch 9:4.79 - F1: 0.6464
2026-02-13 17:07:22 - INFO - Time taken for Epoch 9:4.79 - F1: 0.6464
Time taken for Epoch 10:4.81 - F1: 0.6501
2026-02-13 17:07:26 - INFO - Time taken for Epoch 10:4.81 - F1: 0.6501
Time taken for Epoch 11:4.82 - F1: 0.6531
2026-02-13 17:07:31 - INFO - Time taken for Epoch 11:4.82 - F1: 0.6531
Time taken for Epoch 12:4.82 - F1: 0.6572
2026-02-13 17:07:36 - INFO - Time taken for Epoch 12:4.82 - F1: 0.6572
Time taken for Epoch 13:4.83 - F1: 0.6556
2026-02-13 17:07:41 - INFO - Time taken for Epoch 13:4.83 - F1: 0.6556
Time taken for Epoch 14:3.67 - F1: 0.6579
2026-02-13 17:07:45 - INFO - Time taken for Epoch 14:3.67 - F1: 0.6579
Time taken for Epoch 15:4.83 - F1: 0.6550
2026-02-13 17:07:49 - INFO - Time taken for Epoch 15:4.83 - F1: 0.6550
Time taken for Epoch 16:3.67 - F1: 0.6697
2026-02-13 17:07:53 - INFO - Time taken for Epoch 16:3.67 - F1: 0.6697
Time taken for Epoch 17:4.82 - F1: 0.6657
2026-02-13 17:07:58 - INFO - Time taken for Epoch 17:4.82 - F1: 0.6657
Time taken for Epoch 18:3.67 - F1: 0.6600
2026-02-13 17:08:02 - INFO - Time taken for Epoch 18:3.67 - F1: 0.6600
Time taken for Epoch 19:3.67 - F1: 0.6657
2026-02-13 17:08:05 - INFO - Time taken for Epoch 19:3.67 - F1: 0.6657
Time taken for Epoch 20:3.67 - F1: 0.6677
2026-02-13 17:08:09 - INFO - Time taken for Epoch 20:3.67 - F1: 0.6677
Time taken for Epoch 21:3.67 - F1: 0.6676
2026-02-13 17:08:13 - INFO - Time taken for Epoch 21:3.67 - F1: 0.6676
Time taken for Epoch 22:3.71 - F1: 0.6658
2026-02-13 17:08:16 - INFO - Time taken for Epoch 22:3.71 - F1: 0.6658
Time taken for Epoch 23:3.72 - F1: 0.6637
2026-02-13 17:08:20 - INFO - Time taken for Epoch 23:3.72 - F1: 0.6637
Time taken for Epoch 24:3.67 - F1: 0.6624
2026-02-13 17:08:24 - INFO - Time taken for Epoch 24:3.67 - F1: 0.6624
Time taken for Epoch 25:3.67 - F1: 0.6610
2026-02-13 17:08:27 - INFO - Time taken for Epoch 25:3.67 - F1: 0.6610
Time taken for Epoch 26:3.67 - F1: 0.6606
2026-02-13 17:08:31 - INFO - Time taken for Epoch 26:3.67 - F1: 0.6606
Performance not improving for 10 consecutive epochs.
2026-02-13 17:08:31 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6697 - Best Epoch:15
2026-02-13 17:08:31 - INFO - Best F1:0.6697 - Best Epoch:15
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6722, Test ECE: 0.0559
2026-02-13 17:08:39 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6722, Test ECE: 0.0559
All results: {'f1_macro': 0.672226178268582, 'ece': np.float64(0.055862370899470944)}
2026-02-13 17:08:39 - INFO - All results: {'f1_macro': 0.672226178268582, 'ece': np.float64(0.055862370899470944)}

Total time taken: 648.79 seconds
2026-02-13 17:08:39 - INFO - 
Total time taken: 648.79 seconds
2026-02-13 17:08:39 - INFO - Trial 4 finished with value: 0.672226178268582 and parameters: {'learning_rate': 6.525223525125416e-05, 'weight_decay': 0.0001085944529138788, 'batch_size': 8, 'co_train_epochs': 12, 'epoch_patience': 7}. Best is trial 3 with value: 0.6826995365537694.
Using devices: cuda, cuda
2026-02-13 17:08:39 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 17:08:39 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 17:08:39 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 17:08:39 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.000637919710158384
Weight Decay: 0.0003774096602388378
Batch Size: 16
No. Epochs: 7
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-13 17:08:40 - INFO - Learning Rate: 0.000637919710158384
Weight Decay: 0.0003774096602388378
Batch Size: 16
No. Epochs: 7
Epoch Patience: 4
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 17:08:41 - INFO - Generating initial weights
Time taken for Epoch 1:18.48 - F1: 0.0306
2026-02-13 17:09:02 - INFO - Time taken for Epoch 1:18.48 - F1: 0.0306
Time taken for Epoch 2:18.41 - F1: 0.0100
2026-02-13 17:09:21 - INFO - Time taken for Epoch 2:18.41 - F1: 0.0100
Time taken for Epoch 3:18.41 - F1: 0.0321
2026-02-13 17:09:39 - INFO - Time taken for Epoch 3:18.41 - F1: 0.0321
Time taken for Epoch 4:18.42 - F1: 0.0222
2026-02-13 17:09:58 - INFO - Time taken for Epoch 4:18.42 - F1: 0.0222
Time taken for Epoch 5:18.41 - F1: 0.0109
2026-02-13 17:10:16 - INFO - Time taken for Epoch 5:18.41 - F1: 0.0109
Time taken for Epoch 6:18.40 - F1: 0.0385
2026-02-13 17:10:35 - INFO - Time taken for Epoch 6:18.40 - F1: 0.0385
Time taken for Epoch 7:18.40 - F1: 0.0205
2026-02-13 17:10:53 - INFO - Time taken for Epoch 7:18.40 - F1: 0.0205
Best F1:0.0385 - Best Epoch:6
2026-02-13 17:10:53 - INFO - Best F1:0.0385 - Best Epoch:6
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 17:10:54 - INFO - Starting co-training
Time taken for Epoch 1: 24.46s - F1: 0.03212851
2026-02-13 17:11:19 - INFO - Time taken for Epoch 1: 24.46s - F1: 0.03212851
Time taken for Epoch 2: 25.58s - F1: 0.03212851
2026-02-13 17:11:44 - INFO - Time taken for Epoch 2: 25.58s - F1: 0.03212851
Time taken for Epoch 3: 24.45s - F1: 0.03212851
2026-02-13 17:12:09 - INFO - Time taken for Epoch 3: 24.45s - F1: 0.03212851
Time taken for Epoch 4: 24.47s - F1: 0.03212851
2026-02-13 17:12:33 - INFO - Time taken for Epoch 4: 24.47s - F1: 0.03212851
Time taken for Epoch 5: 24.50s - F1: 0.03212851
2026-02-13 17:12:58 - INFO - Time taken for Epoch 5: 24.50s - F1: 0.03212851
Performance not improving for 4 consecutive epochs.
Performance not improving for 4 consecutive epochs.
2026-02-13 17:12:58 - INFO - Performance not improving for 4 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 17:13:00 - INFO - Fine-tuning models
Time taken for Epoch 1:3.41 - F1: 0.0100
2026-02-13 17:13:04 - INFO - Time taken for Epoch 1:3.41 - F1: 0.0100
Time taken for Epoch 2:4.45 - F1: 0.0100
2026-02-13 17:13:08 - INFO - Time taken for Epoch 2:4.45 - F1: 0.0100
Time taken for Epoch 3:3.40 - F1: 0.0109
2026-02-13 17:13:12 - INFO - Time taken for Epoch 3:3.40 - F1: 0.0109
Time taken for Epoch 4:4.56 - F1: 0.0321
2026-02-13 17:13:16 - INFO - Time taken for Epoch 4:4.56 - F1: 0.0321
Time taken for Epoch 5:4.55 - F1: 0.0321
2026-02-13 17:13:21 - INFO - Time taken for Epoch 5:4.55 - F1: 0.0321
Time taken for Epoch 6:3.39 - F1: 0.0321
2026-02-13 17:13:24 - INFO - Time taken for Epoch 6:3.39 - F1: 0.0321
Time taken for Epoch 7:3.39 - F1: 0.0205
2026-02-13 17:13:28 - INFO - Time taken for Epoch 7:3.39 - F1: 0.0205
Time taken for Epoch 8:3.39 - F1: 0.0205
2026-02-13 17:13:31 - INFO - Time taken for Epoch 8:3.39 - F1: 0.0205
Time taken for Epoch 9:3.39 - F1: 0.0100
2026-02-13 17:13:34 - INFO - Time taken for Epoch 9:3.39 - F1: 0.0100
Time taken for Epoch 10:3.40 - F1: 0.0100
2026-02-13 17:13:38 - INFO - Time taken for Epoch 10:3.40 - F1: 0.0100
Time taken for Epoch 11:3.40 - F1: 0.0100
2026-02-13 17:13:41 - INFO - Time taken for Epoch 11:3.40 - F1: 0.0100
Time taken for Epoch 12:3.40 - F1: 0.0109
2026-02-13 17:13:45 - INFO - Time taken for Epoch 12:3.40 - F1: 0.0109
Time taken for Epoch 13:3.40 - F1: 0.0109
2026-02-13 17:13:48 - INFO - Time taken for Epoch 13:3.40 - F1: 0.0109
Time taken for Epoch 14:3.40 - F1: 0.0109
2026-02-13 17:13:51 - INFO - Time taken for Epoch 14:3.40 - F1: 0.0109
Performance not improving for 10 consecutive epochs.
2026-02-13 17:13:51 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0321 - Best Epoch:3
2026-02-13 17:13:51 - INFO - Best F1:0.0321 - Best Epoch:3
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0322, Test ECE: 0.1399
2026-02-13 17:13:59 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0322, Test ECE: 0.1399
All results: {'f1_macro': 0.03216172754422238, 'ece': np.float64(0.139924267575589)}
2026-02-13 17:13:59 - INFO - All results: {'f1_macro': 0.03216172754422238, 'ece': np.float64(0.139924267575589)}

Total time taken: 319.44 seconds
2026-02-13 17:13:59 - INFO - 
Total time taken: 319.44 seconds
2026-02-13 17:13:59 - INFO - Trial 5 finished with value: 0.03216172754422238 and parameters: {'learning_rate': 0.000637919710158384, 'weight_decay': 0.0003774096602388378, 'batch_size': 16, 'co_train_epochs': 7, 'epoch_patience': 4}. Best is trial 3 with value: 0.6826995365537694.
Using devices: cuda, cuda
2026-02-13 17:13:59 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 17:13:59 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 17:13:59 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 17:13:59 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.0002563377431355595
Weight Decay: 0.002422973409852472
Batch Size: 64
No. Epochs: 10
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-13 17:13:59 - INFO - Learning Rate: 0.0002563377431355595
Weight Decay: 0.002422973409852472
Batch Size: 64
No. Epochs: 10
Epoch Patience: 9
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 17:14:00 - INFO - Generating initial weights
Time taken for Epoch 1:17.18 - F1: 0.0602
2026-02-13 17:14:21 - INFO - Time taken for Epoch 1:17.18 - F1: 0.0602
Time taken for Epoch 2:17.07 - F1: 0.0705
2026-02-13 17:14:38 - INFO - Time taken for Epoch 2:17.07 - F1: 0.0705
Time taken for Epoch 3:17.07 - F1: 0.2052
2026-02-13 17:14:55 - INFO - Time taken for Epoch 3:17.07 - F1: 0.2052
Time taken for Epoch 4:17.08 - F1: 0.1993
2026-02-13 17:15:12 - INFO - Time taken for Epoch 4:17.08 - F1: 0.1993
Time taken for Epoch 5:17.10 - F1: 0.1647
2026-02-13 17:15:29 - INFO - Time taken for Epoch 5:17.10 - F1: 0.1647
Time taken for Epoch 6:17.09 - F1: 0.2645
2026-02-13 17:15:46 - INFO - Time taken for Epoch 6:17.09 - F1: 0.2645
Time taken for Epoch 7:17.10 - F1: 0.3506
2026-02-13 17:16:03 - INFO - Time taken for Epoch 7:17.10 - F1: 0.3506
Time taken for Epoch 8:17.09 - F1: 0.4077
2026-02-13 17:16:20 - INFO - Time taken for Epoch 8:17.09 - F1: 0.4077
Time taken for Epoch 9:17.08 - F1: 0.4251
2026-02-13 17:16:37 - INFO - Time taken for Epoch 9:17.08 - F1: 0.4251
Time taken for Epoch 10:17.07 - F1: 0.4358
2026-02-13 17:16:55 - INFO - Time taken for Epoch 10:17.07 - F1: 0.4358
Best F1:0.4358 - Best Epoch:10
2026-02-13 17:16:55 - INFO - Best F1:0.4358 - Best Epoch:10
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 17:16:56 - INFO - Starting co-training
Time taken for Epoch 1: 38.46s - F1: 0.52917844
2026-02-13 17:17:34 - INFO - Time taken for Epoch 1: 38.46s - F1: 0.52917844
Time taken for Epoch 2: 39.57s - F1: 0.56784059
2026-02-13 17:18:14 - INFO - Time taken for Epoch 2: 39.57s - F1: 0.56784059
Time taken for Epoch 3: 39.67s - F1: 0.54672951
2026-02-13 17:18:54 - INFO - Time taken for Epoch 3: 39.67s - F1: 0.54672951
Time taken for Epoch 4: 38.52s - F1: 0.56401484
2026-02-13 17:19:32 - INFO - Time taken for Epoch 4: 38.52s - F1: 0.56401484
Time taken for Epoch 5: 38.55s - F1: 0.58208775
2026-02-13 17:20:11 - INFO - Time taken for Epoch 5: 38.55s - F1: 0.58208775
Time taken for Epoch 6: 39.70s - F1: 0.53422283
2026-02-13 17:20:50 - INFO - Time taken for Epoch 6: 39.70s - F1: 0.53422283
Time taken for Epoch 7: 38.53s - F1: 0.48062367
2026-02-13 17:21:29 - INFO - Time taken for Epoch 7: 38.53s - F1: 0.48062367
Time taken for Epoch 8: 38.54s - F1: 0.57775739
2026-02-13 17:22:08 - INFO - Time taken for Epoch 8: 38.54s - F1: 0.57775739
Time taken for Epoch 9: 38.55s - F1: 0.51159494
2026-02-13 17:22:46 - INFO - Time taken for Epoch 9: 38.55s - F1: 0.51159494
Time taken for Epoch 10: 38.54s - F1: 0.60614991
2026-02-13 17:23:25 - INFO - Time taken for Epoch 10: 38.54s - F1: 0.60614991
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 17:23:28 - INFO - Fine-tuning models
Time taken for Epoch 1:3.19 - F1: 0.5371
2026-02-13 17:23:32 - INFO - Time taken for Epoch 1:3.19 - F1: 0.5371
Time taken for Epoch 2:4.22 - F1: 0.5577
2026-02-13 17:23:36 - INFO - Time taken for Epoch 2:4.22 - F1: 0.5577
Time taken for Epoch 3:4.31 - F1: 0.5925
2026-02-13 17:23:40 - INFO - Time taken for Epoch 3:4.31 - F1: 0.5925
Time taken for Epoch 4:4.31 - F1: 0.5856
2026-02-13 17:23:44 - INFO - Time taken for Epoch 4:4.31 - F1: 0.5856
Time taken for Epoch 5:3.15 - F1: 0.5904
2026-02-13 17:23:48 - INFO - Time taken for Epoch 5:3.15 - F1: 0.5904
Time taken for Epoch 6:3.16 - F1: 0.6019
2026-02-13 17:23:51 - INFO - Time taken for Epoch 6:3.16 - F1: 0.6019
Time taken for Epoch 7:4.32 - F1: 0.6130
2026-02-13 17:23:55 - INFO - Time taken for Epoch 7:4.32 - F1: 0.6130
Time taken for Epoch 8:4.31 - F1: 0.6059
2026-02-13 17:23:59 - INFO - Time taken for Epoch 8:4.31 - F1: 0.6059
Time taken for Epoch 9:3.15 - F1: 0.6097
2026-02-13 17:24:03 - INFO - Time taken for Epoch 9:3.15 - F1: 0.6097
Time taken for Epoch 10:3.16 - F1: 0.6006
2026-02-13 17:24:06 - INFO - Time taken for Epoch 10:3.16 - F1: 0.6006
Time taken for Epoch 11:3.55 - F1: 0.5923
2026-02-13 17:24:09 - INFO - Time taken for Epoch 11:3.55 - F1: 0.5923
Time taken for Epoch 12:3.16 - F1: 0.5720
2026-02-13 17:24:12 - INFO - Time taken for Epoch 12:3.16 - F1: 0.5720
Time taken for Epoch 13:3.15 - F1: 0.5916
2026-02-13 17:24:16 - INFO - Time taken for Epoch 13:3.15 - F1: 0.5916
Time taken for Epoch 14:3.16 - F1: 0.6083
2026-02-13 17:24:19 - INFO - Time taken for Epoch 14:3.16 - F1: 0.6083
Time taken for Epoch 15:3.15 - F1: 0.6013
2026-02-13 17:24:22 - INFO - Time taken for Epoch 15:3.15 - F1: 0.6013
Time taken for Epoch 16:3.16 - F1: 0.5803
2026-02-13 17:24:25 - INFO - Time taken for Epoch 16:3.16 - F1: 0.5803
Time taken for Epoch 17:3.16 - F1: 0.6061
2026-02-13 17:24:28 - INFO - Time taken for Epoch 17:3.16 - F1: 0.6061
Performance not improving for 10 consecutive epochs.
2026-02-13 17:24:28 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6130 - Best Epoch:6
2026-02-13 17:24:28 - INFO - Best F1:0.6130 - Best Epoch:6
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6075, Test ECE: 0.1467
2026-02-13 17:24:35 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6075, Test ECE: 0.1467
All results: {'f1_macro': 0.60749077854443, 'ece': np.float64(0.14666813554348818)}
2026-02-13 17:24:35 - INFO - All results: {'f1_macro': 0.60749077854443, 'ece': np.float64(0.14666813554348818)}

Total time taken: 636.62 seconds
2026-02-13 17:24:35 - INFO - 
Total time taken: 636.62 seconds
2026-02-13 17:24:35 - INFO - Trial 6 finished with value: 0.60749077854443 and parameters: {'learning_rate': 0.0002563377431355595, 'weight_decay': 0.002422973409852472, 'batch_size': 64, 'co_train_epochs': 10, 'epoch_patience': 9}. Best is trial 3 with value: 0.6826995365537694.
Using devices: cuda, cuda
2026-02-13 17:24:35 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 17:24:35 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 17:24:35 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 17:24:35 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.00023377332846645325
Weight Decay: 1.1931146342315725e-05
Batch Size: 64
No. Epochs: 12
Epoch Patience: 5
 Accumulation Steps: 1
2026-02-13 17:24:36 - INFO - Learning Rate: 0.00023377332846645325
Weight Decay: 1.1931146342315725e-05
Batch Size: 64
No. Epochs: 12
Epoch Patience: 5
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 17:24:37 - INFO - Generating initial weights
Time taken for Epoch 1:17.12 - F1: 0.0899
2026-02-13 17:24:57 - INFO - Time taken for Epoch 1:17.12 - F1: 0.0899
Time taken for Epoch 2:17.02 - F1: 0.0740
2026-02-13 17:25:14 - INFO - Time taken for Epoch 2:17.02 - F1: 0.0740
Time taken for Epoch 3:17.00 - F1: 0.2198
2026-02-13 17:25:31 - INFO - Time taken for Epoch 3:17.00 - F1: 0.2198
Time taken for Epoch 4:17.02 - F1: 0.2139
2026-02-13 17:25:48 - INFO - Time taken for Epoch 4:17.02 - F1: 0.2139
Time taken for Epoch 5:17.06 - F1: 0.2479
2026-02-13 17:26:05 - INFO - Time taken for Epoch 5:17.06 - F1: 0.2479
Time taken for Epoch 6:17.08 - F1: 0.2840
2026-02-13 17:26:23 - INFO - Time taken for Epoch 6:17.08 - F1: 0.2840
Time taken for Epoch 7:17.05 - F1: 0.4307
2026-02-13 17:26:40 - INFO - Time taken for Epoch 7:17.05 - F1: 0.4307
Time taken for Epoch 8:17.06 - F1: 0.4586
2026-02-13 17:26:57 - INFO - Time taken for Epoch 8:17.06 - F1: 0.4586
Time taken for Epoch 9:17.09 - F1: 0.4768
2026-02-13 17:27:14 - INFO - Time taken for Epoch 9:17.09 - F1: 0.4768
Time taken for Epoch 10:17.07 - F1: 0.4796
2026-02-13 17:27:31 - INFO - Time taken for Epoch 10:17.07 - F1: 0.4796
Time taken for Epoch 11:17.07 - F1: 0.4993
2026-02-13 17:27:48 - INFO - Time taken for Epoch 11:17.07 - F1: 0.4993
Time taken for Epoch 12:17.10 - F1: 0.4929
2026-02-13 17:28:05 - INFO - Time taken for Epoch 12:17.10 - F1: 0.4929
Best F1:0.4993 - Best Epoch:11
2026-02-13 17:28:05 - INFO - Best F1:0.4993 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 17:28:06 - INFO - Starting co-training
Time taken for Epoch 1: 38.50s - F1: 0.60860070
2026-02-13 17:28:45 - INFO - Time taken for Epoch 1: 38.50s - F1: 0.60860070
Time taken for Epoch 2: 39.61s - F1: 0.60786980
2026-02-13 17:29:25 - INFO - Time taken for Epoch 2: 39.61s - F1: 0.60786980
Time taken for Epoch 3: 38.59s - F1: 0.62414690
2026-02-13 17:30:03 - INFO - Time taken for Epoch 3: 38.59s - F1: 0.62414690
Time taken for Epoch 4: 39.69s - F1: 0.62907168
2026-02-13 17:30:43 - INFO - Time taken for Epoch 4: 39.69s - F1: 0.62907168
Time taken for Epoch 5: 39.66s - F1: 0.62201800
2026-02-13 17:31:22 - INFO - Time taken for Epoch 5: 39.66s - F1: 0.62201800
Time taken for Epoch 6: 38.56s - F1: 0.60524942
2026-02-13 17:32:01 - INFO - Time taken for Epoch 6: 38.56s - F1: 0.60524942
Time taken for Epoch 7: 38.57s - F1: 0.60932937
2026-02-13 17:32:40 - INFO - Time taken for Epoch 7: 38.57s - F1: 0.60932937
Time taken for Epoch 8: 38.58s - F1: 0.60755270
2026-02-13 17:33:18 - INFO - Time taken for Epoch 8: 38.58s - F1: 0.60755270
Time taken for Epoch 9: 38.56s - F1: 0.55827122
2026-02-13 17:33:57 - INFO - Time taken for Epoch 9: 38.56s - F1: 0.55827122
Performance not improving for 5 consecutive epochs.
Performance not improving for 5 consecutive epochs.
2026-02-13 17:33:57 - INFO - Performance not improving for 5 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 17:33:59 - INFO - Fine-tuning models
Time taken for Epoch 1:3.18 - F1: 0.6241
2026-02-13 17:34:03 - INFO - Time taken for Epoch 1:3.18 - F1: 0.6241
Time taken for Epoch 2:4.21 - F1: 0.6042
2026-02-13 17:34:07 - INFO - Time taken for Epoch 2:4.21 - F1: 0.6042
Time taken for Epoch 3:3.15 - F1: 0.5786
2026-02-13 17:34:10 - INFO - Time taken for Epoch 3:3.15 - F1: 0.5786
Time taken for Epoch 4:3.16 - F1: 0.6236
2026-02-13 17:34:13 - INFO - Time taken for Epoch 4:3.16 - F1: 0.6236
Time taken for Epoch 5:3.16 - F1: 0.6199
2026-02-13 17:34:16 - INFO - Time taken for Epoch 5:3.16 - F1: 0.6199
Time taken for Epoch 6:3.16 - F1: 0.6226
2026-02-13 17:34:19 - INFO - Time taken for Epoch 6:3.16 - F1: 0.6226
Time taken for Epoch 7:3.16 - F1: 0.6362
2026-02-13 17:34:23 - INFO - Time taken for Epoch 7:3.16 - F1: 0.6362
Time taken for Epoch 8:4.29 - F1: 0.6351
2026-02-13 17:34:27 - INFO - Time taken for Epoch 8:4.29 - F1: 0.6351
Time taken for Epoch 9:3.15 - F1: 0.6601
2026-02-13 17:34:30 - INFO - Time taken for Epoch 9:3.15 - F1: 0.6601
Time taken for Epoch 10:4.31 - F1: 0.6504
2026-02-13 17:34:34 - INFO - Time taken for Epoch 10:4.31 - F1: 0.6504
Time taken for Epoch 11:3.15 - F1: 0.6488
2026-02-13 17:34:38 - INFO - Time taken for Epoch 11:3.15 - F1: 0.6488
Time taken for Epoch 12:3.15 - F1: 0.6400
2026-02-13 17:34:41 - INFO - Time taken for Epoch 12:3.15 - F1: 0.6400
Time taken for Epoch 13:3.15 - F1: 0.6314
2026-02-13 17:34:44 - INFO - Time taken for Epoch 13:3.15 - F1: 0.6314
Time taken for Epoch 14:3.15 - F1: 0.6239
2026-02-13 17:34:47 - INFO - Time taken for Epoch 14:3.15 - F1: 0.6239
Time taken for Epoch 15:3.15 - F1: 0.6240
2026-02-13 17:34:50 - INFO - Time taken for Epoch 15:3.15 - F1: 0.6240
Time taken for Epoch 16:3.15 - F1: 0.6180
2026-02-13 17:34:53 - INFO - Time taken for Epoch 16:3.15 - F1: 0.6180
Time taken for Epoch 17:3.15 - F1: 0.6152
2026-02-13 17:34:56 - INFO - Time taken for Epoch 17:3.15 - F1: 0.6152
Time taken for Epoch 18:3.15 - F1: 0.6097
2026-02-13 17:35:00 - INFO - Time taken for Epoch 18:3.15 - F1: 0.6097
Time taken for Epoch 19:3.15 - F1: 0.6069
2026-02-13 17:35:03 - INFO - Time taken for Epoch 19:3.15 - F1: 0.6069
Performance not improving for 10 consecutive epochs.
2026-02-13 17:35:03 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6601 - Best Epoch:8
2026-02-13 17:35:03 - INFO - Best F1:0.6601 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6569, Test ECE: 0.0788
2026-02-13 17:35:10 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6569, Test ECE: 0.0788
All results: {'f1_macro': 0.6569217561822733, 'ece': np.float64(0.07879568246177471)}
2026-02-13 17:35:10 - INFO - All results: {'f1_macro': 0.6569217561822733, 'ece': np.float64(0.07879568246177471)}

Total time taken: 634.45 seconds
2026-02-13 17:35:10 - INFO - 
Total time taken: 634.45 seconds
2026-02-13 17:35:10 - INFO - Trial 7 finished with value: 0.6569217561822733 and parameters: {'learning_rate': 0.00023377332846645325, 'weight_decay': 1.1931146342315725e-05, 'batch_size': 64, 'co_train_epochs': 12, 'epoch_patience': 5}. Best is trial 3 with value: 0.6826995365537694.
Using devices: cuda, cuda
2026-02-13 17:35:10 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 17:35:10 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 17:35:10 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 17:35:10 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 1.285697117912946e-05
Weight Decay: 0.003863010881414441
Batch Size: 8
No. Epochs: 20
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-13 17:35:10 - INFO - Learning Rate: 1.285697117912946e-05
Weight Decay: 0.003863010881414441
Batch Size: 8
No. Epochs: 20
Epoch Patience: 7
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 17:35:11 - INFO - Generating initial weights
Time taken for Epoch 1:19.96 - F1: 0.0655
2026-02-13 17:35:35 - INFO - Time taken for Epoch 1:19.96 - F1: 0.0655
Time taken for Epoch 2:19.88 - F1: 0.0631
2026-02-13 17:35:55 - INFO - Time taken for Epoch 2:19.88 - F1: 0.0631
Time taken for Epoch 3:19.86 - F1: 0.0752
2026-02-13 17:36:14 - INFO - Time taken for Epoch 3:19.86 - F1: 0.0752
Time taken for Epoch 4:19.87 - F1: 0.0896
2026-02-13 17:36:34 - INFO - Time taken for Epoch 4:19.87 - F1: 0.0896
Time taken for Epoch 5:19.91 - F1: 0.1155
2026-02-13 17:36:54 - INFO - Time taken for Epoch 5:19.91 - F1: 0.1155
Time taken for Epoch 6:19.93 - F1: 0.1479
2026-02-13 17:37:14 - INFO - Time taken for Epoch 6:19.93 - F1: 0.1479
Time taken for Epoch 7:19.92 - F1: 0.1904
2026-02-13 17:37:34 - INFO - Time taken for Epoch 7:19.92 - F1: 0.1904
Time taken for Epoch 8:19.94 - F1: 0.2678
2026-02-13 17:37:54 - INFO - Time taken for Epoch 8:19.94 - F1: 0.2678
Time taken for Epoch 9:19.96 - F1: 0.3301
2026-02-13 17:38:14 - INFO - Time taken for Epoch 9:19.96 - F1: 0.3301
Time taken for Epoch 10:19.94 - F1: 0.3604
2026-02-13 17:38:34 - INFO - Time taken for Epoch 10:19.94 - F1: 0.3604
Time taken for Epoch 11:19.94 - F1: 0.3800
2026-02-13 17:38:54 - INFO - Time taken for Epoch 11:19.94 - F1: 0.3800
Time taken for Epoch 12:19.95 - F1: 0.4112
2026-02-13 17:39:14 - INFO - Time taken for Epoch 12:19.95 - F1: 0.4112
Time taken for Epoch 13:19.94 - F1: 0.4296
2026-02-13 17:39:34 - INFO - Time taken for Epoch 13:19.94 - F1: 0.4296
Time taken for Epoch 14:19.97 - F1: 0.4462
2026-02-13 17:39:54 - INFO - Time taken for Epoch 14:19.97 - F1: 0.4462
Time taken for Epoch 15:20.00 - F1: 0.4529
2026-02-13 17:40:14 - INFO - Time taken for Epoch 15:20.00 - F1: 0.4529
Time taken for Epoch 16:19.96 - F1: 0.4572
2026-02-13 17:40:34 - INFO - Time taken for Epoch 16:19.96 - F1: 0.4572
Time taken for Epoch 17:19.93 - F1: 0.4597
2026-02-13 17:40:54 - INFO - Time taken for Epoch 17:19.93 - F1: 0.4597
Time taken for Epoch 18:19.95 - F1: 0.4628
2026-02-13 17:41:14 - INFO - Time taken for Epoch 18:19.95 - F1: 0.4628
Time taken for Epoch 19:19.95 - F1: 0.4716
2026-02-13 17:41:33 - INFO - Time taken for Epoch 19:19.95 - F1: 0.4716
Time taken for Epoch 20:19.95 - F1: 0.4712
2026-02-13 17:41:53 - INFO - Time taken for Epoch 20:19.95 - F1: 0.4712
Best F1:0.4716 - Best Epoch:19
2026-02-13 17:41:53 - INFO - Best F1:0.4716 - Best Epoch:19
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 17:41:58 - INFO - Starting co-training
Time taken for Epoch 1: 22.73s - F1: 0.21579847
2026-02-13 17:42:21 - INFO - Time taken for Epoch 1: 22.73s - F1: 0.21579847
Time taken for Epoch 2: 23.87s - F1: 0.31395464
2026-02-13 17:42:45 - INFO - Time taken for Epoch 2: 23.87s - F1: 0.31395464
Time taken for Epoch 3: 23.93s - F1: 0.38559388
2026-02-13 17:43:09 - INFO - Time taken for Epoch 3: 23.93s - F1: 0.38559388
Time taken for Epoch 4: 24.04s - F1: 0.45956323
2026-02-13 17:43:33 - INFO - Time taken for Epoch 4: 24.04s - F1: 0.45956323
Time taken for Epoch 5: 24.22s - F1: 0.48697481
2026-02-13 17:43:57 - INFO - Time taken for Epoch 5: 24.22s - F1: 0.48697481
Time taken for Epoch 6: 23.99s - F1: 0.53383085
2026-02-13 17:44:21 - INFO - Time taken for Epoch 6: 23.99s - F1: 0.53383085
Time taken for Epoch 7: 24.00s - F1: 0.57502057
2026-02-13 17:44:45 - INFO - Time taken for Epoch 7: 24.00s - F1: 0.57502057
Time taken for Epoch 8: 23.91s - F1: 0.57764545
2026-02-13 17:45:09 - INFO - Time taken for Epoch 8: 23.91s - F1: 0.57764545
Time taken for Epoch 9: 23.97s - F1: 0.58940575
2026-02-13 17:45:33 - INFO - Time taken for Epoch 9: 23.97s - F1: 0.58940575
Time taken for Epoch 10: 24.09s - F1: 0.60359183
2026-02-13 17:45:57 - INFO - Time taken for Epoch 10: 24.09s - F1: 0.60359183
Time taken for Epoch 11: 24.03s - F1: 0.60377432
2026-02-13 17:46:21 - INFO - Time taken for Epoch 11: 24.03s - F1: 0.60377432
Time taken for Epoch 12: 24.19s - F1: 0.61873677
2026-02-13 17:46:46 - INFO - Time taken for Epoch 12: 24.19s - F1: 0.61873677
Time taken for Epoch 13: 23.98s - F1: 0.62563355
2026-02-13 17:47:09 - INFO - Time taken for Epoch 13: 23.98s - F1: 0.62563355
Time taken for Epoch 14: 23.96s - F1: 0.61458889
2026-02-13 17:47:33 - INFO - Time taken for Epoch 14: 23.96s - F1: 0.61458889
Time taken for Epoch 15: 22.95s - F1: 0.63171412
2026-02-13 17:47:56 - INFO - Time taken for Epoch 15: 22.95s - F1: 0.63171412
Time taken for Epoch 16: 24.03s - F1: 0.61967377
2026-02-13 17:48:20 - INFO - Time taken for Epoch 16: 24.03s - F1: 0.61967377
Time taken for Epoch 17: 22.96s - F1: 0.63561961
2026-02-13 17:48:43 - INFO - Time taken for Epoch 17: 22.96s - F1: 0.63561961
Time taken for Epoch 18: 24.04s - F1: 0.62889166
2026-02-13 17:49:07 - INFO - Time taken for Epoch 18: 24.04s - F1: 0.62889166
Time taken for Epoch 19: 22.88s - F1: 0.63209832
2026-02-13 17:49:30 - INFO - Time taken for Epoch 19: 22.88s - F1: 0.63209832
Time taken for Epoch 20: 22.89s - F1: 0.63663528
2026-02-13 17:49:53 - INFO - Time taken for Epoch 20: 22.89s - F1: 0.63663528
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 17:49:57 - INFO - Fine-tuning models
Time taken for Epoch 1:3.74 - F1: 0.6303
2026-02-13 17:50:01 - INFO - Time taken for Epoch 1:3.74 - F1: 0.6303
Time taken for Epoch 2:4.78 - F1: 0.6346
2026-02-13 17:50:06 - INFO - Time taken for Epoch 2:4.78 - F1: 0.6346
Time taken for Epoch 3:4.89 - F1: 0.6391
2026-02-13 17:50:10 - INFO - Time taken for Epoch 3:4.89 - F1: 0.6391
Time taken for Epoch 4:4.89 - F1: 0.6367
2026-02-13 17:50:15 - INFO - Time taken for Epoch 4:4.89 - F1: 0.6367
Time taken for Epoch 5:3.72 - F1: 0.6391
2026-02-13 17:50:19 - INFO - Time taken for Epoch 5:3.72 - F1: 0.6391
Time taken for Epoch 6:3.73 - F1: 0.6359
2026-02-13 17:50:23 - INFO - Time taken for Epoch 6:3.73 - F1: 0.6359
Time taken for Epoch 7:3.73 - F1: 0.6363
2026-02-13 17:50:26 - INFO - Time taken for Epoch 7:3.73 - F1: 0.6363
Time taken for Epoch 8:3.72 - F1: 0.6320
2026-02-13 17:50:30 - INFO - Time taken for Epoch 8:3.72 - F1: 0.6320
Time taken for Epoch 9:3.72 - F1: 0.6265
2026-02-13 17:50:34 - INFO - Time taken for Epoch 9:3.72 - F1: 0.6265
Time taken for Epoch 10:3.73 - F1: 0.6299
2026-02-13 17:50:38 - INFO - Time taken for Epoch 10:3.73 - F1: 0.6299
Time taken for Epoch 11:3.72 - F1: 0.6246
2026-02-13 17:50:41 - INFO - Time taken for Epoch 11:3.72 - F1: 0.6246
Time taken for Epoch 12:3.72 - F1: 0.6134
2026-02-13 17:50:45 - INFO - Time taken for Epoch 12:3.72 - F1: 0.6134
Time taken for Epoch 13:3.73 - F1: 0.6184
2026-02-13 17:50:49 - INFO - Time taken for Epoch 13:3.73 - F1: 0.6184
Performance not improving for 10 consecutive epochs.
2026-02-13 17:50:49 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6391 - Best Epoch:2
2026-02-13 17:50:49 - INFO - Best F1:0.6391 - Best Epoch:2
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6422, Test ECE: 0.0243
2026-02-13 17:50:56 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6422, Test ECE: 0.0243
All results: {'f1_macro': 0.6422216894431982, 'ece': np.float64(0.024259299304003105)}
2026-02-13 17:50:56 - INFO - All results: {'f1_macro': 0.6422216894431982, 'ece': np.float64(0.024259299304003105)}

Total time taken: 946.83 seconds
2026-02-13 17:50:56 - INFO - 
Total time taken: 946.83 seconds
2026-02-13 17:50:57 - INFO - Trial 8 finished with value: 0.6422216894431982 and parameters: {'learning_rate': 1.285697117912946e-05, 'weight_decay': 0.003863010881414441, 'batch_size': 8, 'co_train_epochs': 20, 'epoch_patience': 7}. Best is trial 3 with value: 0.6826995365537694.
Using devices: cuda, cuda
2026-02-13 17:50:57 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 17:50:57 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 17:50:57 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-13 17:50:57 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.0006294742513029254
Weight Decay: 0.0005288302659796328
Batch Size: 16
No. Epochs: 5
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-13 17:50:57 - INFO - Learning Rate: 0.0006294742513029254
Weight Decay: 0.0005288302659796328
Batch Size: 16
No. Epochs: 5
Epoch Patience: 4
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 17:50:58 - INFO - Generating initial weights
Time taken for Epoch 1:18.49 - F1: 0.0512
2026-02-13 17:51:20 - INFO - Time taken for Epoch 1:18.49 - F1: 0.0512
Time taken for Epoch 2:18.41 - F1: 0.0150
2026-02-13 17:51:39 - INFO - Time taken for Epoch 2:18.41 - F1: 0.0150
Time taken for Epoch 3:18.37 - F1: 0.0100
2026-02-13 17:51:57 - INFO - Time taken for Epoch 3:18.37 - F1: 0.0100
Time taken for Epoch 4:18.45 - F1: 0.0109
2026-02-13 17:52:16 - INFO - Time taken for Epoch 4:18.45 - F1: 0.0109
Time taken for Epoch 5:18.43 - F1: 0.0109
2026-02-13 17:52:34 - INFO - Time taken for Epoch 5:18.43 - F1: 0.0109
Best F1:0.0512 - Best Epoch:1
2026-02-13 17:52:34 - INFO - Best F1:0.0512 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 17:52:35 - INFO - Starting co-training
Time taken for Epoch 1: 24.39s - F1: 0.03212851
2026-02-13 17:53:00 - INFO - Time taken for Epoch 1: 24.39s - F1: 0.03212851
Time taken for Epoch 2: 25.48s - F1: 0.03212851
2026-02-13 17:53:25 - INFO - Time taken for Epoch 2: 25.48s - F1: 0.03212851
Time taken for Epoch 3: 24.42s - F1: 0.03212851
2026-02-13 17:53:50 - INFO - Time taken for Epoch 3: 24.42s - F1: 0.03212851
Time taken for Epoch 4: 24.44s - F1: 0.03212851
2026-02-13 17:54:14 - INFO - Time taken for Epoch 4: 24.44s - F1: 0.03212851
Time taken for Epoch 5: 24.44s - F1: 0.03212851
2026-02-13 17:54:39 - INFO - Time taken for Epoch 5: 24.44s - F1: 0.03212851
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-13 17:54:41 - INFO - Fine-tuning models
Time taken for Epoch 1:3.41 - F1: 0.0205
2026-02-13 17:54:45 - INFO - Time taken for Epoch 1:3.41 - F1: 0.0205
Time taken for Epoch 2:4.44 - F1: 0.0100
2026-02-13 17:54:49 - INFO - Time taken for Epoch 2:4.44 - F1: 0.0100
Time taken for Epoch 3:3.41 - F1: 0.0385
2026-02-13 17:54:52 - INFO - Time taken for Epoch 3:3.41 - F1: 0.0385
Time taken for Epoch 4:4.54 - F1: 0.0385
2026-02-13 17:54:57 - INFO - Time taken for Epoch 4:4.54 - F1: 0.0385
Time taken for Epoch 5:3.39 - F1: 0.0385
2026-02-13 17:55:00 - INFO - Time taken for Epoch 5:3.39 - F1: 0.0385
Time taken for Epoch 6:3.40 - F1: 0.0205
2026-02-13 17:55:04 - INFO - Time taken for Epoch 6:3.40 - F1: 0.0205
Time taken for Epoch 7:3.40 - F1: 0.0205
2026-02-13 17:55:07 - INFO - Time taken for Epoch 7:3.40 - F1: 0.0205
Time taken for Epoch 8:3.40 - F1: 0.0205
2026-02-13 17:55:11 - INFO - Time taken for Epoch 8:3.40 - F1: 0.0205
Time taken for Epoch 9:3.40 - F1: 0.0100
2026-02-13 17:55:14 - INFO - Time taken for Epoch 9:3.40 - F1: 0.0100
Time taken for Epoch 10:3.41 - F1: 0.0109
2026-02-13 17:55:17 - INFO - Time taken for Epoch 10:3.41 - F1: 0.0109
Time taken for Epoch 11:3.40 - F1: 0.0109
2026-02-13 17:55:21 - INFO - Time taken for Epoch 11:3.40 - F1: 0.0109
Time taken for Epoch 12:3.40 - F1: 0.0109
2026-02-13 17:55:24 - INFO - Time taken for Epoch 12:3.40 - F1: 0.0109
Time taken for Epoch 13:3.41 - F1: 0.0385
2026-02-13 17:55:28 - INFO - Time taken for Epoch 13:3.41 - F1: 0.0385
Performance not improving for 10 consecutive epochs.
2026-02-13 17:55:28 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0385 - Best Epoch:2
2026-02-13 17:55:28 - INFO - Best F1:0.0385 - Best Epoch:2
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label25-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label25-set2_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0384, Test ECE: 0.1033
2026-02-13 17:55:35 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0384, Test ECE: 0.1033
All results: {'f1_macro': 0.03837037037037037, 'ece': np.float64(0.1033123964076077)}
2026-02-13 17:55:35 - INFO - All results: {'f1_macro': 0.03837037037037037, 'ece': np.float64(0.1033123964076077)}

Total time taken: 278.50 seconds
2026-02-13 17:55:35 - INFO - 
Total time taken: 278.50 seconds
2026-02-13 17:55:35 - INFO - Trial 9 finished with value: 0.03837037037037037 and parameters: {'learning_rate': 0.0006294742513029254, 'weight_decay': 0.0005288302659796328, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 4}. Best is trial 3 with value: 0.6826995365537694.

[BEST TRIAL RESULTS]
2026-02-13 17:55:35 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6827
2026-02-13 17:55:35 - INFO - F1 Score: 0.6827
Params: {'learning_rate': 0.00012658725202321515, 'weight_decay': 0.0004651385698538199, 'batch_size': 64, 'co_train_epochs': 5, 'epoch_patience': 9}
2026-02-13 17:55:35 - INFO - Params: {'learning_rate': 0.00012658725202321515, 'weight_decay': 0.0004651385698538199, 'batch_size': 64, 'co_train_epochs': 5, 'epoch_patience': 9}
  learning_rate: 0.00012658725202321515
2026-02-13 17:55:35 - INFO -   learning_rate: 0.00012658725202321515
  weight_decay: 0.0004651385698538199
2026-02-13 17:55:35 - INFO -   weight_decay: 0.0004651385698538199
  batch_size: 64
2026-02-13 17:55:35 - INFO -   batch_size: 64
  co_train_epochs: 5
2026-02-13 17:55:35 - INFO -   co_train_epochs: 5
  epoch_patience: 9
2026-02-13 17:55:35 - INFO -   epoch_patience: 9

Total time taken: 5623.93 seconds
2026-02-13 17:55:35 - INFO - 
Total time taken: 5623.93 seconds