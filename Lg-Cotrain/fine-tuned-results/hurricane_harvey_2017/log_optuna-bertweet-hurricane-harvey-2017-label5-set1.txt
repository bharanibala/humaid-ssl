2026-02-10 14:55:23 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-10 14:55:23 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_harvey_2017
2026-02-10 14:55:23 - INFO - Using devices: cuda, cuda
2026-02-10 14:55:23 - INFO - Devices: cuda, cuda
2026-02-10 14:55:23 - INFO - Starting log
2026-02-10 14:55:23 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 14:55:24 - INFO - Learning Rate: 0.0009228806069277776
Weight Decay: 0.00016317551277653065
Batch Size: 8
No. Epochs: 8
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-10 14:55:25 - INFO - Generating initial weights
2026-02-10 14:55:47 - INFO - Time taken for Epoch 1:21.04 - F1: 0.0145
2026-02-10 14:56:07 - INFO - Time taken for Epoch 2:20.26 - F1: 0.0270
2026-02-10 14:56:28 - INFO - Time taken for Epoch 3:20.29 - F1: 0.0145
2026-02-10 14:56:48 - INFO - Time taken for Epoch 4:20.37 - F1: 0.0145
2026-02-10 14:57:09 - INFO - Time taken for Epoch 5:20.42 - F1: 0.0440
2026-02-10 14:57:29 - INFO - Time taken for Epoch 6:20.44 - F1: 0.0145
2026-02-10 14:57:49 - INFO - Time taken for Epoch 7:20.50 - F1: 0.0145
2026-02-10 14:58:10 - INFO - Time taken for Epoch 8:20.47 - F1: 0.0145
2026-02-10 14:58:10 - INFO - Best F1:0.0440 - Best Epoch:5
2026-02-10 14:58:11 - INFO - Starting co-training
2026-02-10 14:58:41 - INFO - Time taken for Epoch 1: 30.24s - F1: 0.00961208
2026-02-10 14:59:12 - INFO - Time taken for Epoch 2: 30.91s - F1: 0.05258833
2026-02-10 14:59:43 - INFO - Time taken for Epoch 3: 30.86s - F1: 0.05258833
2026-02-10 15:00:13 - INFO - Time taken for Epoch 4: 30.24s - F1: 0.05258833
2026-02-10 15:00:43 - INFO - Time taken for Epoch 5: 30.23s - F1: 0.05258833
2026-02-10 15:01:14 - INFO - Time taken for Epoch 6: 30.14s - F1: 0.05258833
2026-02-10 15:01:44 - INFO - Time taken for Epoch 7: 30.16s - F1: 0.05258833
2026-02-10 15:02:14 - INFO - Time taken for Epoch 8: 30.13s - F1: 0.05258833
2026-02-10 15:02:16 - INFO - Fine-tuning models
2026-02-10 15:02:19 - INFO - Time taken for Epoch 1:2.91 - F1: 0.0526
2026-02-10 15:02:22 - INFO - Time taken for Epoch 2:3.62 - F1: 0.0262
2026-02-10 15:02:25 - INFO - Time taken for Epoch 3:2.90 - F1: 0.0078
2026-02-10 15:02:28 - INFO - Time taken for Epoch 4:2.97 - F1: 0.0078
2026-02-10 15:02:31 - INFO - Time taken for Epoch 5:2.90 - F1: 0.0145
2026-02-10 15:02:34 - INFO - Time taken for Epoch 6:2.90 - F1: 0.0145
2026-02-10 15:02:37 - INFO - Time taken for Epoch 7:2.88 - F1: 0.0145
2026-02-10 15:02:40 - INFO - Time taken for Epoch 8:3.02 - F1: 0.0145
2026-02-10 15:02:43 - INFO - Time taken for Epoch 9:2.89 - F1: 0.0145
2026-02-10 15:02:45 - INFO - Time taken for Epoch 10:2.90 - F1: 0.0158
2026-02-10 15:02:48 - INFO - Time taken for Epoch 11:2.90 - F1: 0.0096
2026-02-10 15:02:48 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 15:02:48 - INFO - Best F1:0.0526 - Best Epoch:0
2026-02-10 15:02:55 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0525, Test ECE: 0.2697
2026-02-10 15:02:55 - INFO - All results: {'f1_macro': 0.052547471329197216, 'ece': np.float64(0.2697497047876057)}
2026-02-10 15:02:55 - INFO - 
Total time taken: 452.37 seconds
2026-02-10 15:02:55 - INFO - Trial 0 finished with value: 0.052547471329197216 and parameters: {'learning_rate': 0.0009228806069277776, 'weight_decay': 0.00016317551277653065, 'batch_size': 8, 'co_train_epochs': 8, 'epoch_patience': 10}. Best is trial 0 with value: 0.052547471329197216.
2026-02-10 15:02:55 - INFO - Using devices: cuda, cuda
2026-02-10 15:02:55 - INFO - Devices: cuda, cuda
2026-02-10 15:02:55 - INFO - Starting log
2026-02-10 15:02:55 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 15:02:56 - INFO - Learning Rate: 4.232614357487488e-05
Weight Decay: 0.00027510209152979535
Batch Size: 24
No. Epochs: 17
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-10 15:02:56 - INFO - Generating initial weights
2026-02-10 15:03:15 - INFO - Time taken for Epoch 1:17.05 - F1: 0.0642
2026-02-10 15:03:31 - INFO - Time taken for Epoch 2:16.68 - F1: 0.0732
2026-02-10 15:03:48 - INFO - Time taken for Epoch 3:16.81 - F1: 0.1015
2026-02-10 15:04:05 - INFO - Time taken for Epoch 4:16.74 - F1: 0.1391
2026-02-10 15:04:22 - INFO - Time taken for Epoch 5:16.70 - F1: 0.1849
2026-02-10 15:04:38 - INFO - Time taken for Epoch 6:16.69 - F1: 0.2125
2026-02-10 15:04:55 - INFO - Time taken for Epoch 7:16.72 - F1: 0.2480
2026-02-10 15:05:12 - INFO - Time taken for Epoch 8:16.78 - F1: 0.2671
2026-02-10 15:05:28 - INFO - Time taken for Epoch 9:16.54 - F1: 0.2710
2026-02-10 15:05:45 - INFO - Time taken for Epoch 10:16.54 - F1: 0.2781
2026-02-10 15:06:01 - INFO - Time taken for Epoch 11:16.53 - F1: 0.2876
2026-02-10 15:06:18 - INFO - Time taken for Epoch 12:16.48 - F1: 0.2882
2026-02-10 15:06:34 - INFO - Time taken for Epoch 13:16.53 - F1: 0.2928
2026-02-10 15:06:51 - INFO - Time taken for Epoch 14:16.48 - F1: 0.2940
2026-02-10 15:07:07 - INFO - Time taken for Epoch 15:16.48 - F1: 0.2954
2026-02-10 15:07:24 - INFO - Time taken for Epoch 16:16.55 - F1: 0.2951
2026-02-10 15:07:40 - INFO - Time taken for Epoch 17:16.53 - F1: 0.2959
2026-02-10 15:07:40 - INFO - Best F1:0.2959 - Best Epoch:17
2026-02-10 15:07:41 - INFO - Starting co-training
2026-02-10 15:08:18 - INFO - Time taken for Epoch 1: 36.35s - F1: 0.54419086
2026-02-10 15:08:55 - INFO - Time taken for Epoch 2: 36.90s - F1: 0.56899471
2026-02-10 15:09:34 - INFO - Time taken for Epoch 3: 39.86s - F1: 0.56968958
2026-02-10 15:10:15 - INFO - Time taken for Epoch 4: 40.83s - F1: 0.60567106
2026-02-10 15:10:56 - INFO - Time taken for Epoch 5: 40.87s - F1: 0.59794680
2026-02-10 15:11:32 - INFO - Time taken for Epoch 6: 36.30s - F1: 0.60608592
2026-02-10 15:12:09 - INFO - Time taken for Epoch 7: 36.86s - F1: 0.60514984
2026-02-10 15:12:46 - INFO - Time taken for Epoch 8: 36.39s - F1: 0.61686604
2026-02-10 15:13:23 - INFO - Time taken for Epoch 9: 36.82s - F1: 0.61130390
2026-02-10 15:13:59 - INFO - Time taken for Epoch 10: 36.31s - F1: 0.59804348
2026-02-10 15:14:35 - INFO - Time taken for Epoch 11: 36.34s - F1: 0.60670801
2026-02-10 15:15:11 - INFO - Time taken for Epoch 12: 36.32s - F1: 0.60707125
2026-02-10 15:15:48 - INFO - Time taken for Epoch 13: 36.32s - F1: 0.60756075
2026-02-10 15:16:24 - INFO - Time taken for Epoch 14: 36.29s - F1: 0.59350619
2026-02-10 15:17:00 - INFO - Time taken for Epoch 15: 36.35s - F1: 0.59745859
2026-02-10 15:17:37 - INFO - Time taken for Epoch 16: 36.30s - F1: 0.59091005
2026-02-10 15:18:13 - INFO - Time taken for Epoch 17: 36.31s - F1: 0.61141458
2026-02-10 15:18:15 - INFO - Fine-tuning models
2026-02-10 15:18:17 - INFO - Time taken for Epoch 1:2.30 - F1: 0.6204
2026-02-10 15:18:20 - INFO - Time taken for Epoch 2:2.92 - F1: 0.6204
2026-02-10 15:18:22 - INFO - Time taken for Epoch 3:2.32 - F1: 0.6111
2026-02-10 15:18:25 - INFO - Time taken for Epoch 4:2.32 - F1: 0.6034
2026-02-10 15:18:27 - INFO - Time taken for Epoch 5:2.29 - F1: 0.6013
2026-02-10 15:18:29 - INFO - Time taken for Epoch 6:2.30 - F1: 0.6059
2026-02-10 15:18:31 - INFO - Time taken for Epoch 7:2.30 - F1: 0.6138
2026-02-10 15:18:34 - INFO - Time taken for Epoch 8:2.30 - F1: 0.6127
2026-02-10 15:18:36 - INFO - Time taken for Epoch 9:2.31 - F1: 0.6119
2026-02-10 15:18:38 - INFO - Time taken for Epoch 10:2.31 - F1: 0.6133
2026-02-10 15:18:41 - INFO - Time taken for Epoch 11:2.30 - F1: 0.6128
2026-02-10 15:18:41 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 15:18:41 - INFO - Best F1:0.6204 - Best Epoch:0
2026-02-10 15:18:46 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6250, Test ECE: 0.0275
2026-02-10 15:18:46 - INFO - All results: {'f1_macro': 0.6249929451787254, 'ece': np.float64(0.02754028844040847)}
2026-02-10 15:18:46 - INFO - 
Total time taken: 950.81 seconds
2026-02-10 15:18:46 - INFO - Trial 1 finished with value: 0.6249929451787254 and parameters: {'learning_rate': 4.232614357487488e-05, 'weight_decay': 0.00027510209152979535, 'batch_size': 24, 'co_train_epochs': 17, 'epoch_patience': 10}. Best is trial 1 with value: 0.6249929451787254.
2026-02-10 15:18:46 - INFO - Using devices: cuda, cuda
2026-02-10 15:18:46 - INFO - Devices: cuda, cuda
2026-02-10 15:18:46 - INFO - Starting log
2026-02-10 15:18:46 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 15:18:46 - INFO - Learning Rate: 2.487094782776448e-05
Weight Decay: 0.0023643695292476763
Batch Size: 24
No. Epochs: 19
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-10 15:18:47 - INFO - Generating initial weights
2026-02-10 15:19:05 - INFO - Time taken for Epoch 1:16.53 - F1: 0.0623
2026-02-10 15:19:21 - INFO - Time taken for Epoch 2:16.49 - F1: 0.0696
2026-02-10 15:19:38 - INFO - Time taken for Epoch 3:16.54 - F1: 0.0663
2026-02-10 15:19:55 - INFO - Time taken for Epoch 4:16.48 - F1: 0.0662
2026-02-10 15:20:11 - INFO - Time taken for Epoch 5:16.50 - F1: 0.0999
2026-02-10 15:20:28 - INFO - Time taken for Epoch 6:16.52 - F1: 0.1292
2026-02-10 15:20:44 - INFO - Time taken for Epoch 7:16.51 - F1: 0.1462
2026-02-10 15:21:01 - INFO - Time taken for Epoch 8:16.52 - F1: 0.1733
2026-02-10 15:21:17 - INFO - Time taken for Epoch 9:16.52 - F1: 0.1811
2026-02-10 15:21:34 - INFO - Time taken for Epoch 10:16.55 - F1: 0.2000
2026-02-10 15:21:50 - INFO - Time taken for Epoch 11:16.52 - F1: 0.2140
2026-02-10 15:22:07 - INFO - Time taken for Epoch 12:16.53 - F1: 0.2403
2026-02-10 15:22:23 - INFO - Time taken for Epoch 13:16.58 - F1: 0.2491
2026-02-10 15:22:40 - INFO - Time taken for Epoch 14:16.52 - F1: 0.2682
2026-02-10 15:22:56 - INFO - Time taken for Epoch 15:16.52 - F1: 0.2687
2026-02-10 15:23:13 - INFO - Time taken for Epoch 16:16.53 - F1: 0.2655
2026-02-10 15:23:29 - INFO - Time taken for Epoch 17:16.54 - F1: 0.2767
2026-02-10 15:23:46 - INFO - Time taken for Epoch 18:16.55 - F1: 0.2825
2026-02-10 15:24:02 - INFO - Time taken for Epoch 19:16.50 - F1: 0.2866
2026-02-10 15:24:02 - INFO - Best F1:0.2866 - Best Epoch:19
2026-02-10 15:24:03 - INFO - Starting co-training
2026-02-10 15:24:40 - INFO - Time taken for Epoch 1: 36.30s - F1: 0.52025944
2026-02-10 15:25:17 - INFO - Time taken for Epoch 2: 36.93s - F1: 0.52695687
2026-02-10 15:25:57 - INFO - Time taken for Epoch 3: 40.80s - F1: 0.56061414
2026-02-10 15:26:38 - INFO - Time taken for Epoch 4: 40.68s - F1: 0.57529962
2026-02-10 15:27:19 - INFO - Time taken for Epoch 5: 41.18s - F1: 0.59706988
2026-02-10 15:28:00 - INFO - Time taken for Epoch 6: 40.96s - F1: 0.60794054
2026-02-10 15:28:41 - INFO - Time taken for Epoch 7: 40.93s - F1: 0.59565441
2026-02-10 15:29:18 - INFO - Time taken for Epoch 8: 36.44s - F1: 0.59182055
2026-02-10 15:29:54 - INFO - Time taken for Epoch 9: 36.34s - F1: 0.60963253
2026-02-10 15:30:31 - INFO - Time taken for Epoch 10: 36.98s - F1: 0.62504808
2026-02-10 15:31:15 - INFO - Time taken for Epoch 11: 43.92s - F1: 0.63625565
2026-02-10 15:31:56 - INFO - Time taken for Epoch 12: 40.87s - F1: 0.61644287
2026-02-10 15:32:32 - INFO - Time taken for Epoch 13: 36.33s - F1: 0.62918517
2026-02-10 15:33:08 - INFO - Time taken for Epoch 14: 36.32s - F1: 0.61574303
2026-02-10 15:33:45 - INFO - Time taken for Epoch 15: 36.36s - F1: 0.60804730
2026-02-10 15:34:21 - INFO - Time taken for Epoch 16: 36.14s - F1: 0.62339038
2026-02-10 15:34:57 - INFO - Time taken for Epoch 17: 36.09s - F1: 0.62492174
2026-02-10 15:34:57 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-10 15:34:59 - INFO - Fine-tuning models
2026-02-10 15:35:01 - INFO - Time taken for Epoch 1:2.30 - F1: 0.6358
2026-02-10 15:35:04 - INFO - Time taken for Epoch 2:2.91 - F1: 0.6206
2026-02-10 15:35:06 - INFO - Time taken for Epoch 3:2.29 - F1: 0.6150
2026-02-10 15:35:09 - INFO - Time taken for Epoch 4:2.29 - F1: 0.6164
2026-02-10 15:35:11 - INFO - Time taken for Epoch 5:2.29 - F1: 0.6182
2026-02-10 15:35:13 - INFO - Time taken for Epoch 6:2.29 - F1: 0.6143
2026-02-10 15:35:16 - INFO - Time taken for Epoch 7:2.29 - F1: 0.6142
2026-02-10 15:35:18 - INFO - Time taken for Epoch 8:2.29 - F1: 0.6100
2026-02-10 15:35:20 - INFO - Time taken for Epoch 9:2.29 - F1: 0.6150
2026-02-10 15:35:22 - INFO - Time taken for Epoch 10:2.29 - F1: 0.6155
2026-02-10 15:35:25 - INFO - Time taken for Epoch 11:2.29 - F1: 0.6142
2026-02-10 15:35:25 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 15:35:25 - INFO - Best F1:0.6358 - Best Epoch:0
2026-02-10 15:35:30 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6478, Test ECE: 0.0203
2026-02-10 15:35:30 - INFO - All results: {'f1_macro': 0.6478073114404596, 'ece': np.float64(0.020254390259528757)}
2026-02-10 15:35:30 - INFO - 
Total time taken: 1004.28 seconds
2026-02-10 15:35:30 - INFO - Trial 2 finished with value: 0.6478073114404596 and parameters: {'learning_rate': 2.487094782776448e-05, 'weight_decay': 0.0023643695292476763, 'batch_size': 24, 'co_train_epochs': 19, 'epoch_patience': 6}. Best is trial 2 with value: 0.6478073114404596.
2026-02-10 15:35:30 - INFO - Using devices: cuda, cuda
2026-02-10 15:35:30 - INFO - Devices: cuda, cuda
2026-02-10 15:35:30 - INFO - Starting log
2026-02-10 15:35:30 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 15:35:31 - INFO - Learning Rate: 0.0003762102926968999
Weight Decay: 9.653302183644446e-05
Batch Size: 24
No. Epochs: 9
Epoch Patience: 8
 Accumulation Steps: 2
2026-02-10 15:35:31 - INFO - Generating initial weights
2026-02-10 15:35:49 - INFO - Time taken for Epoch 1:16.55 - F1: 0.0581
2026-02-10 15:36:06 - INFO - Time taken for Epoch 2:16.54 - F1: 0.1573
2026-02-10 15:36:22 - INFO - Time taken for Epoch 3:16.50 - F1: 0.2393
2026-02-10 15:36:39 - INFO - Time taken for Epoch 4:16.49 - F1: 0.2411
2026-02-10 15:36:55 - INFO - Time taken for Epoch 5:16.49 - F1: 0.3002
2026-02-10 15:37:12 - INFO - Time taken for Epoch 6:16.49 - F1: 0.3074
2026-02-10 15:37:28 - INFO - Time taken for Epoch 7:16.50 - F1: 0.3049
2026-02-10 15:37:45 - INFO - Time taken for Epoch 8:16.50 - F1: 0.3211
2026-02-10 15:38:01 - INFO - Time taken for Epoch 9:16.48 - F1: 0.3393
2026-02-10 15:38:01 - INFO - Best F1:0.3393 - Best Epoch:9
2026-02-10 15:38:02 - INFO - Starting co-training
2026-02-10 15:38:38 - INFO - Time taken for Epoch 1: 36.09s - F1: 0.05258833
2026-02-10 15:39:15 - INFO - Time taken for Epoch 2: 36.66s - F1: 0.05258833
2026-02-10 15:39:51 - INFO - Time taken for Epoch 3: 36.11s - F1: 0.05258833
2026-02-10 15:40:27 - INFO - Time taken for Epoch 4: 36.11s - F1: 0.05258833
2026-02-10 15:41:03 - INFO - Time taken for Epoch 5: 36.12s - F1: 0.05258833
2026-02-10 15:41:39 - INFO - Time taken for Epoch 6: 36.15s - F1: 0.05258833
2026-02-10 15:42:16 - INFO - Time taken for Epoch 7: 36.16s - F1: 0.05258833
2026-02-10 15:42:52 - INFO - Time taken for Epoch 8: 36.23s - F1: 0.05258833
2026-02-10 15:43:28 - INFO - Time taken for Epoch 9: 36.53s - F1: 0.05258833
2026-02-10 15:43:30 - INFO - Fine-tuning models
2026-02-10 15:43:32 - INFO - Time taken for Epoch 1:2.29 - F1: 0.0526
2026-02-10 15:43:35 - INFO - Time taken for Epoch 2:2.90 - F1: 0.0526
2026-02-10 15:43:37 - INFO - Time taken for Epoch 3:2.28 - F1: 0.0262
2026-02-10 15:43:40 - INFO - Time taken for Epoch 4:2.28 - F1: 0.0124
2026-02-10 15:43:42 - INFO - Time taken for Epoch 5:2.29 - F1: 0.0078
2026-02-10 15:43:44 - INFO - Time taken for Epoch 6:2.29 - F1: 0.0078
2026-02-10 15:43:47 - INFO - Time taken for Epoch 7:2.30 - F1: 0.0078
2026-02-10 15:43:49 - INFO - Time taken for Epoch 8:2.29 - F1: 0.0078
2026-02-10 15:43:51 - INFO - Time taken for Epoch 9:2.29 - F1: 0.0158
2026-02-10 15:43:53 - INFO - Time taken for Epoch 10:2.29 - F1: 0.0158
2026-02-10 15:43:56 - INFO - Time taken for Epoch 11:2.35 - F1: 0.0158
2026-02-10 15:43:56 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 15:43:56 - INFO - Best F1:0.0526 - Best Epoch:0
2026-02-10 15:44:01 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0525, Test ECE: 0.2216
2026-02-10 15:44:01 - INFO - All results: {'f1_macro': 0.052547471329197216, 'ece': np.float64(0.22156845433560107)}
2026-02-10 15:44:01 - INFO - 
Total time taken: 511.04 seconds
2026-02-10 15:44:01 - INFO - Trial 3 finished with value: 0.052547471329197216 and parameters: {'learning_rate': 0.0003762102926968999, 'weight_decay': 9.653302183644446e-05, 'batch_size': 24, 'co_train_epochs': 9, 'epoch_patience': 8}. Best is trial 2 with value: 0.6478073114404596.
2026-02-10 15:44:01 - INFO - Using devices: cuda, cuda
2026-02-10 15:44:01 - INFO - Devices: cuda, cuda
2026-02-10 15:44:01 - INFO - Starting log
2026-02-10 15:44:01 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 15:44:02 - INFO - Learning Rate: 0.0001202197942574481
Weight Decay: 4.193313448670122e-05
Batch Size: 8
No. Epochs: 13
Epoch Patience: 6
 Accumulation Steps: 8
2026-02-10 15:44:02 - INFO - Generating initial weights
2026-02-10 15:44:25 - INFO - Time taken for Epoch 1:20.75 - F1: 0.0529
2026-02-10 15:44:45 - INFO - Time taken for Epoch 2:20.42 - F1: 0.0145
2026-02-10 15:45:06 - INFO - Time taken for Epoch 3:20.52 - F1: 0.0145
2026-02-10 15:45:26 - INFO - Time taken for Epoch 4:20.32 - F1: 0.0540
2026-02-10 15:45:46 - INFO - Time taken for Epoch 5:20.35 - F1: 0.1043
2026-02-10 15:46:07 - INFO - Time taken for Epoch 6:20.35 - F1: 0.1756
2026-02-10 15:46:27 - INFO - Time taken for Epoch 7:20.35 - F1: 0.2400
2026-02-10 15:46:47 - INFO - Time taken for Epoch 8:20.36 - F1: 0.2534
2026-02-10 15:47:08 - INFO - Time taken for Epoch 9:20.34 - F1: 0.2651
2026-02-10 15:47:28 - INFO - Time taken for Epoch 10:20.36 - F1: 0.2676
2026-02-10 15:47:48 - INFO - Time taken for Epoch 11:20.35 - F1: 0.2647
2026-02-10 15:48:09 - INFO - Time taken for Epoch 12:20.32 - F1: 0.2710
2026-02-10 15:48:29 - INFO - Time taken for Epoch 13:20.34 - F1: 0.2906
2026-02-10 15:48:29 - INFO - Best F1:0.2906 - Best Epoch:13
2026-02-10 15:48:30 - INFO - Starting co-training
2026-02-10 15:49:00 - INFO - Time taken for Epoch 1: 30.03s - F1: 0.05258833
2026-02-10 15:49:31 - INFO - Time taken for Epoch 2: 31.01s - F1: 0.05258833
2026-02-10 15:50:01 - INFO - Time taken for Epoch 3: 30.54s - F1: 0.05258833
2026-02-10 15:50:32 - INFO - Time taken for Epoch 4: 30.19s - F1: 0.05258833
2026-02-10 15:51:02 - INFO - Time taken for Epoch 5: 30.39s - F1: 0.05258833
2026-02-10 15:51:33 - INFO - Time taken for Epoch 6: 30.74s - F1: 0.05258833
2026-02-10 15:52:04 - INFO - Time taken for Epoch 7: 31.00s - F1: 0.05258833
2026-02-10 15:52:04 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-10 15:52:05 - INFO - Fine-tuning models
2026-02-10 15:52:08 - INFO - Time taken for Epoch 1:2.95 - F1: 0.0526
2026-02-10 15:52:13 - INFO - Time taken for Epoch 2:4.34 - F1: 0.0526
2026-02-10 15:52:15 - INFO - Time taken for Epoch 3:2.97 - F1: 0.0145
2026-02-10 15:52:18 - INFO - Time taken for Epoch 4:2.93 - F1: 0.0145
2026-02-10 15:52:21 - INFO - Time taken for Epoch 5:2.95 - F1: 0.0145
2026-02-10 15:52:24 - INFO - Time taken for Epoch 6:2.91 - F1: 0.0145
2026-02-10 15:52:27 - INFO - Time taken for Epoch 7:2.96 - F1: 0.0145
2026-02-10 15:52:30 - INFO - Time taken for Epoch 8:2.97 - F1: 0.0145
2026-02-10 15:52:33 - INFO - Time taken for Epoch 9:2.88 - F1: 0.0145
2026-02-10 15:52:36 - INFO - Time taken for Epoch 10:2.91 - F1: 0.0145
2026-02-10 15:52:39 - INFO - Time taken for Epoch 11:2.94 - F1: 0.0145
2026-02-10 15:52:39 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 15:52:39 - INFO - Best F1:0.0526 - Best Epoch:0
2026-02-10 15:52:46 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0525, Test ECE: 0.1954
2026-02-10 15:52:46 - INFO - All results: {'f1_macro': 0.052547471329197216, 'ece': np.float64(0.19541781696586397)}
2026-02-10 15:52:46 - INFO - 
Total time taken: 524.28 seconds
2026-02-10 15:52:46 - INFO - Trial 4 finished with value: 0.052547471329197216 and parameters: {'learning_rate': 0.0001202197942574481, 'weight_decay': 4.193313448670122e-05, 'batch_size': 8, 'co_train_epochs': 13, 'epoch_patience': 6}. Best is trial 2 with value: 0.6478073114404596.
2026-02-10 15:52:46 - INFO - Using devices: cuda, cuda
2026-02-10 15:52:46 - INFO - Devices: cuda, cuda
2026-02-10 15:52:46 - INFO - Starting log
2026-02-10 15:52:46 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 15:52:46 - INFO - Learning Rate: 0.0007000212313995803
Weight Decay: 0.0016484824733007384
Batch Size: 16
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 4
2026-02-10 15:52:47 - INFO - Generating initial weights
2026-02-10 15:53:07 - INFO - Time taken for Epoch 1:18.35 - F1: 0.0145
2026-02-10 15:53:25 - INFO - Time taken for Epoch 2:18.02 - F1: 0.0145
2026-02-10 15:53:43 - INFO - Time taken for Epoch 3:18.07 - F1: 0.0145
2026-02-10 15:54:01 - INFO - Time taken for Epoch 4:18.05 - F1: 0.0145
2026-02-10 15:54:19 - INFO - Time taken for Epoch 5:17.97 - F1: 0.0145
2026-02-10 15:54:37 - INFO - Time taken for Epoch 6:18.41 - F1: 0.0145
2026-02-10 15:54:55 - INFO - Time taken for Epoch 7:17.81 - F1: 0.0145
2026-02-10 15:55:13 - INFO - Time taken for Epoch 8:17.82 - F1: 0.0145
2026-02-10 15:55:13 - INFO - Best F1:0.0145 - Best Epoch:1
2026-02-10 15:55:13 - INFO - Starting co-training
2026-02-10 15:55:44 - INFO - Time taken for Epoch 1: 30.74s - F1: 0.05258833
2026-02-10 15:56:16 - INFO - Time taken for Epoch 2: 31.35s - F1: 0.05258833
2026-02-10 15:56:46 - INFO - Time taken for Epoch 3: 30.79s - F1: 0.05258833
2026-02-10 15:57:17 - INFO - Time taken for Epoch 4: 30.74s - F1: 0.05258833
2026-02-10 15:57:48 - INFO - Time taken for Epoch 5: 30.94s - F1: 0.05258833
2026-02-10 15:58:19 - INFO - Time taken for Epoch 6: 30.77s - F1: 0.05258833
2026-02-10 15:58:49 - INFO - Time taken for Epoch 7: 30.46s - F1: 0.05258833
2026-02-10 15:58:49 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-10 15:58:51 - INFO - Fine-tuning models
2026-02-10 15:58:54 - INFO - Time taken for Epoch 1:2.54 - F1: 0.0145
2026-02-10 15:58:57 - INFO - Time taken for Epoch 2:3.41 - F1: 0.0145
2026-02-10 15:58:59 - INFO - Time taken for Epoch 3:2.53 - F1: 0.0145
2026-02-10 15:59:02 - INFO - Time taken for Epoch 4:2.51 - F1: 0.0145
2026-02-10 15:59:05 - INFO - Time taken for Epoch 5:2.51 - F1: 0.0145
2026-02-10 15:59:07 - INFO - Time taken for Epoch 6:2.52 - F1: 0.0145
2026-02-10 15:59:10 - INFO - Time taken for Epoch 7:2.52 - F1: 0.0145
2026-02-10 15:59:12 - INFO - Time taken for Epoch 8:2.52 - F1: 0.0145
2026-02-10 15:59:15 - INFO - Time taken for Epoch 9:2.53 - F1: 0.0145
2026-02-10 15:59:17 - INFO - Time taken for Epoch 10:2.53 - F1: 0.0145
2026-02-10 15:59:20 - INFO - Time taken for Epoch 11:2.51 - F1: 0.0145
2026-02-10 15:59:20 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 15:59:20 - INFO - Best F1:0.0145 - Best Epoch:0
2026-02-10 15:59:26 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0145, Test ECE: 0.5390
2026-02-10 15:59:26 - INFO - All results: {'f1_macro': 0.014500258933195235, 'ece': np.float64(0.5389856476532786)}
2026-02-10 15:59:26 - INFO - 
Total time taken: 399.84 seconds
2026-02-10 15:59:26 - INFO - Trial 5 finished with value: 0.014500258933195235 and parameters: {'learning_rate': 0.0007000212313995803, 'weight_decay': 0.0016484824733007384, 'batch_size': 16, 'co_train_epochs': 8, 'epoch_patience': 6}. Best is trial 2 with value: 0.6478073114404596.
2026-02-10 15:59:26 - INFO - Using devices: cuda, cuda
2026-02-10 15:59:26 - INFO - Devices: cuda, cuda
2026-02-10 15:59:26 - INFO - Starting log
2026-02-10 15:59:26 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 15:59:26 - INFO - Learning Rate: 0.000820357224136913
Weight Decay: 0.0005244457390958424
Batch Size: 16
No. Epochs: 8
Epoch Patience: 8
 Accumulation Steps: 4
2026-02-10 15:59:27 - INFO - Generating initial weights
2026-02-10 15:59:46 - INFO - Time taken for Epoch 1:18.09 - F1: 0.0145
2026-02-10 16:00:04 - INFO - Time taken for Epoch 2:17.86 - F1: 0.0145
2026-02-10 16:00:22 - INFO - Time taken for Epoch 3:17.90 - F1: 0.0145
2026-02-10 16:00:40 - INFO - Time taken for Epoch 4:18.31 - F1: 0.0145
2026-02-10 16:00:58 - INFO - Time taken for Epoch 5:18.20 - F1: 0.0145
2026-02-10 16:01:16 - INFO - Time taken for Epoch 6:17.86 - F1: 0.0145
2026-02-10 16:01:34 - INFO - Time taken for Epoch 7:18.03 - F1: 0.0145
2026-02-10 16:01:52 - INFO - Time taken for Epoch 8:17.95 - F1: 0.0145
2026-02-10 16:01:52 - INFO - Best F1:0.0145 - Best Epoch:1
2026-02-10 16:01:53 - INFO - Starting co-training
2026-02-10 16:02:23 - INFO - Time taken for Epoch 1: 30.41s - F1: 0.05258833
2026-02-10 16:02:56 - INFO - Time taken for Epoch 2: 32.09s - F1: 0.05258833
2026-02-10 16:03:26 - INFO - Time taken for Epoch 3: 30.80s - F1: 0.05258833
2026-02-10 16:03:57 - INFO - Time taken for Epoch 4: 30.45s - F1: 0.05258833
2026-02-10 16:04:27 - INFO - Time taken for Epoch 5: 30.50s - F1: 0.05258833
2026-02-10 16:04:58 - INFO - Time taken for Epoch 6: 30.58s - F1: 0.05258833
2026-02-10 16:05:29 - INFO - Time taken for Epoch 7: 30.79s - F1: 0.05258833
2026-02-10 16:05:59 - INFO - Time taken for Epoch 8: 30.58s - F1: 0.05258833
2026-02-10 16:06:01 - INFO - Fine-tuning models
2026-02-10 16:06:03 - INFO - Time taken for Epoch 1:2.55 - F1: 0.0145
2026-02-10 16:06:07 - INFO - Time taken for Epoch 2:3.16 - F1: 0.0145
2026-02-10 16:06:09 - INFO - Time taken for Epoch 3:2.54 - F1: 0.0145
2026-02-10 16:06:12 - INFO - Time taken for Epoch 4:2.59 - F1: 0.0145
2026-02-10 16:06:14 - INFO - Time taken for Epoch 5:2.55 - F1: 0.0145
2026-02-10 16:06:17 - INFO - Time taken for Epoch 6:2.52 - F1: 0.0145
2026-02-10 16:06:19 - INFO - Time taken for Epoch 7:2.53 - F1: 0.0145
2026-02-10 16:06:22 - INFO - Time taken for Epoch 8:2.56 - F1: 0.0145
2026-02-10 16:06:24 - INFO - Time taken for Epoch 9:2.53 - F1: 0.0145
2026-02-10 16:06:27 - INFO - Time taken for Epoch 10:2.55 - F1: 0.0145
2026-02-10 16:06:30 - INFO - Time taken for Epoch 11:2.53 - F1: 0.0145
2026-02-10 16:06:30 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 16:06:30 - INFO - Best F1:0.0145 - Best Epoch:0
2026-02-10 16:06:36 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0145, Test ECE: 0.5107
2026-02-10 16:06:36 - INFO - All results: {'f1_macro': 0.014500258933195235, 'ece': np.float64(0.510707695695502)}
2026-02-10 16:06:36 - INFO - 
Total time taken: 430.06 seconds
2026-02-10 16:06:36 - INFO - Trial 6 finished with value: 0.014500258933195235 and parameters: {'learning_rate': 0.000820357224136913, 'weight_decay': 0.0005244457390958424, 'batch_size': 16, 'co_train_epochs': 8, 'epoch_patience': 8}. Best is trial 2 with value: 0.6478073114404596.
2026-02-10 16:06:36 - INFO - Using devices: cuda, cuda
2026-02-10 16:06:36 - INFO - Devices: cuda, cuda
2026-02-10 16:06:36 - INFO - Starting log
2026-02-10 16:06:36 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 16:06:36 - INFO - Learning Rate: 0.00018102861234775623
Weight Decay: 0.00589674604237358
Batch Size: 16
No. Epochs: 5
Epoch Patience: 8
 Accumulation Steps: 4
2026-02-10 16:06:37 - INFO - Generating initial weights
2026-02-10 16:06:56 - INFO - Time taken for Epoch 1:17.98 - F1: 0.0145
2026-02-10 16:07:14 - INFO - Time taken for Epoch 2:17.85 - F1: 0.0145
2026-02-10 16:07:32 - INFO - Time taken for Epoch 3:17.91 - F1: 0.0145
2026-02-10 16:07:50 - INFO - Time taken for Epoch 4:18.43 - F1: 0.0145
2026-02-10 16:08:08 - INFO - Time taken for Epoch 5:18.10 - F1: 0.0145
2026-02-10 16:08:08 - INFO - Best F1:0.0145 - Best Epoch:1
2026-02-10 16:08:09 - INFO - Starting co-training
2026-02-10 16:08:40 - INFO - Time taken for Epoch 1: 30.52s - F1: 0.05258833
2026-02-10 16:09:11 - INFO - Time taken for Epoch 2: 31.20s - F1: 0.05258833
2026-02-10 16:09:42 - INFO - Time taken for Epoch 3: 30.66s - F1: 0.05258833
2026-02-10 16:10:13 - INFO - Time taken for Epoch 4: 31.05s - F1: 0.05258833
2026-02-10 16:10:44 - INFO - Time taken for Epoch 5: 30.96s - F1: 0.05258833
2026-02-10 16:10:45 - INFO - Fine-tuning models
2026-02-10 16:10:48 - INFO - Time taken for Epoch 1:2.55 - F1: 0.0526
2026-02-10 16:10:51 - INFO - Time taken for Epoch 2:3.07 - F1: 0.0145
2026-02-10 16:10:53 - INFO - Time taken for Epoch 3:2.57 - F1: 0.0145
2026-02-10 16:10:56 - INFO - Time taken for Epoch 4:2.55 - F1: 0.0145
2026-02-10 16:10:58 - INFO - Time taken for Epoch 5:2.65 - F1: 0.0145
2026-02-10 16:11:01 - INFO - Time taken for Epoch 6:2.67 - F1: 0.0145
2026-02-10 16:11:04 - INFO - Time taken for Epoch 7:2.64 - F1: 0.0145
2026-02-10 16:11:06 - INFO - Time taken for Epoch 8:2.51 - F1: 0.0145
2026-02-10 16:11:09 - INFO - Time taken for Epoch 9:2.50 - F1: 0.0145
2026-02-10 16:11:11 - INFO - Time taken for Epoch 10:2.52 - F1: 0.0145
2026-02-10 16:11:14 - INFO - Time taken for Epoch 11:2.52 - F1: 0.0145
2026-02-10 16:11:14 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 16:11:14 - INFO - Best F1:0.0526 - Best Epoch:0
2026-02-10 16:11:20 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0525, Test ECE: 0.2112
2026-02-10 16:11:20 - INFO - All results: {'f1_macro': 0.052547471329197216, 'ece': np.float64(0.21120563851168944)}
2026-02-10 16:11:20 - INFO - 
Total time taken: 284.42 seconds
2026-02-10 16:11:20 - INFO - Trial 7 finished with value: 0.052547471329197216 and parameters: {'learning_rate': 0.00018102861234775623, 'weight_decay': 0.00589674604237358, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 8}. Best is trial 2 with value: 0.6478073114404596.
2026-02-10 16:11:20 - INFO - Using devices: cuda, cuda
2026-02-10 16:11:20 - INFO - Devices: cuda, cuda
2026-02-10 16:11:20 - INFO - Starting log
2026-02-10 16:11:20 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 16:11:20 - INFO - Learning Rate: 2.0549394471300852e-05
Weight Decay: 0.00026795226642982727
Batch Size: 8
No. Epochs: 5
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-10 16:11:21 - INFO - Generating initial weights
2026-02-10 16:11:44 - INFO - Time taken for Epoch 1:21.10 - F1: 0.0581
2026-02-10 16:12:05 - INFO - Time taken for Epoch 2:21.44 - F1: 0.0575
2026-02-10 16:12:27 - INFO - Time taken for Epoch 3:21.60 - F1: 0.0542
2026-02-10 16:12:48 - INFO - Time taken for Epoch 4:21.12 - F1: 0.0538
2026-02-10 16:13:09 - INFO - Time taken for Epoch 5:20.73 - F1: 0.0612
2026-02-10 16:13:09 - INFO - Best F1:0.0612 - Best Epoch:5
2026-02-10 16:13:09 - INFO - Starting co-training
2026-02-10 16:13:40 - INFO - Time taken for Epoch 1: 30.51s - F1: 0.39910448
2026-02-10 16:14:11 - INFO - Time taken for Epoch 2: 31.07s - F1: 0.49141890
2026-02-10 16:14:46 - INFO - Time taken for Epoch 3: 35.03s - F1: 0.51131612
2026-02-10 16:15:25 - INFO - Time taken for Epoch 4: 39.18s - F1: 0.53164793
2026-02-10 16:15:57 - INFO - Time taken for Epoch 5: 31.40s - F1: 0.55126480
2026-02-10 16:15:59 - INFO - Fine-tuning models
2026-02-10 16:16:02 - INFO - Time taken for Epoch 1:2.97 - F1: 0.5487
2026-02-10 16:16:05 - INFO - Time taken for Epoch 2:3.40 - F1: 0.5600
2026-02-10 16:16:09 - INFO - Time taken for Epoch 3:3.55 - F1: 0.5558
2026-02-10 16:16:11 - INFO - Time taken for Epoch 4:2.92 - F1: 0.5560
2026-02-10 16:16:14 - INFO - Time taken for Epoch 5:2.93 - F1: 0.5570
2026-02-10 16:16:17 - INFO - Time taken for Epoch 6:2.94 - F1: 0.5605
2026-02-10 16:16:21 - INFO - Time taken for Epoch 7:3.48 - F1: 0.5621
2026-02-10 16:16:24 - INFO - Time taken for Epoch 8:3.51 - F1: 0.5659
2026-02-10 16:16:28 - INFO - Time taken for Epoch 9:3.55 - F1: 0.5709
2026-02-10 16:16:31 - INFO - Time taken for Epoch 10:3.51 - F1: 0.5742
2026-02-10 16:16:35 - INFO - Time taken for Epoch 11:3.50 - F1: 0.5719
2026-02-10 16:16:44 - INFO - Time taken for Epoch 12:9.02 - F1: 0.5727
2026-02-10 16:16:47 - INFO - Time taken for Epoch 13:2.95 - F1: 0.5764
2026-02-10 16:16:51 - INFO - Time taken for Epoch 14:3.91 - F1: 0.5776
2026-02-10 16:16:54 - INFO - Time taken for Epoch 15:3.67 - F1: 0.5801
2026-02-10 16:16:58 - INFO - Time taken for Epoch 16:3.60 - F1: 0.5860
2026-02-10 16:17:02 - INFO - Time taken for Epoch 17:3.57 - F1: 0.5847
2026-02-10 16:17:05 - INFO - Time taken for Epoch 18:2.93 - F1: 0.5821
2026-02-10 16:17:07 - INFO - Time taken for Epoch 19:2.92 - F1: 0.5854
2026-02-10 16:17:10 - INFO - Time taken for Epoch 20:2.93 - F1: 0.5868
2026-02-10 16:17:14 - INFO - Time taken for Epoch 21:3.50 - F1: 0.5868
2026-02-10 16:17:17 - INFO - Time taken for Epoch 22:3.00 - F1: 0.5847
2026-02-10 16:17:20 - INFO - Time taken for Epoch 23:2.96 - F1: 0.5891
2026-02-10 16:17:32 - INFO - Time taken for Epoch 24:12.48 - F1: 0.5890
2026-02-10 16:17:35 - INFO - Time taken for Epoch 25:2.90 - F1: 0.5949
2026-02-10 16:17:39 - INFO - Time taken for Epoch 26:3.53 - F1: 0.5957
2026-02-10 16:17:42 - INFO - Time taken for Epoch 27:3.50 - F1: 0.5973
2026-02-10 16:17:46 - INFO - Time taken for Epoch 28:3.50 - F1: 0.5973
2026-02-10 16:17:49 - INFO - Time taken for Epoch 29:2.93 - F1: 0.5963
2026-02-10 16:17:52 - INFO - Time taken for Epoch 30:2.92 - F1: 0.5948
2026-02-10 16:17:55 - INFO - Time taken for Epoch 31:3.00 - F1: 0.5937
2026-02-10 16:17:58 - INFO - Time taken for Epoch 32:2.93 - F1: 0.5939
2026-02-10 16:18:00 - INFO - Time taken for Epoch 33:2.95 - F1: 0.5970
2026-02-10 16:18:03 - INFO - Time taken for Epoch 34:2.93 - F1: 0.5974
2026-02-10 16:18:13 - INFO - Time taken for Epoch 35:9.84 - F1: 0.5974
2026-02-10 16:18:16 - INFO - Time taken for Epoch 36:2.93 - F1: 0.5986
2026-02-10 16:18:20 - INFO - Time taken for Epoch 37:3.48 - F1: 0.6001
2026-02-10 16:18:23 - INFO - Time taken for Epoch 38:3.52 - F1: 0.6052
2026-02-10 16:18:27 - INFO - Time taken for Epoch 39:3.49 - F1: 0.6066
2026-02-10 16:18:30 - INFO - Time taken for Epoch 40:3.52 - F1: 0.6048
2026-02-10 16:18:33 - INFO - Time taken for Epoch 41:2.98 - F1: 0.6044
2026-02-10 16:18:36 - INFO - Time taken for Epoch 42:2.98 - F1: 0.6046
2026-02-10 16:18:39 - INFO - Time taken for Epoch 43:2.96 - F1: 0.6043
2026-02-10 16:18:42 - INFO - Time taken for Epoch 44:2.91 - F1: 0.6044
2026-02-10 16:18:45 - INFO - Time taken for Epoch 45:2.90 - F1: 0.6053
2026-02-10 16:18:48 - INFO - Time taken for Epoch 46:2.92 - F1: 0.6059
2026-02-10 16:18:52 - INFO - Time taken for Epoch 47:4.53 - F1: 0.6059
2026-02-10 16:18:55 - INFO - Time taken for Epoch 48:2.97 - F1: 0.6035
2026-02-10 16:18:58 - INFO - Time taken for Epoch 49:2.92 - F1: 0.6035
2026-02-10 16:18:58 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 16:18:58 - INFO - Best F1:0.6066 - Best Epoch:38
2026-02-10 16:19:05 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6117, Test ECE: 0.0641
2026-02-10 16:19:05 - INFO - All results: {'f1_macro': 0.6117402399459221, 'ece': np.float64(0.06405383774448299)}
2026-02-10 16:19:05 - INFO - 
Total time taken: 464.90 seconds
2026-02-10 16:19:05 - INFO - Trial 8 finished with value: 0.6117402399459221 and parameters: {'learning_rate': 2.0549394471300852e-05, 'weight_decay': 0.00026795226642982727, 'batch_size': 8, 'co_train_epochs': 5, 'epoch_patience': 7}. Best is trial 2 with value: 0.6478073114404596.
2026-02-10 16:19:05 - INFO - Using devices: cuda, cuda
2026-02-10 16:19:05 - INFO - Devices: cuda, cuda
2026-02-10 16:19:05 - INFO - Starting log
2026-02-10 16:19:05 - INFO - Dataset: humanitarian9, Event: hurricane_harvey_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-10 16:19:05 - INFO - Learning Rate: 2.7639340025737998e-05
Weight Decay: 0.0005681665859567847
Batch Size: 8
No. Epochs: 8
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-10 16:19:06 - INFO - Generating initial weights
2026-02-10 16:19:28 - INFO - Time taken for Epoch 1:20.65 - F1: 0.0603
2026-02-10 16:19:49 - INFO - Time taken for Epoch 2:21.02 - F1: 0.0536
2026-02-10 16:20:10 - INFO - Time taken for Epoch 3:21.10 - F1: 0.0556
2026-02-10 16:20:31 - INFO - Time taken for Epoch 4:20.53 - F1: 0.0515
2026-02-10 16:20:51 - INFO - Time taken for Epoch 5:20.31 - F1: 0.0562
2026-02-10 16:21:11 - INFO - Time taken for Epoch 6:20.30 - F1: 0.0557
2026-02-10 16:21:32 - INFO - Time taken for Epoch 7:20.31 - F1: 0.0479
2026-02-10 16:21:52 - INFO - Time taken for Epoch 8:20.33 - F1: 0.0422
2026-02-10 16:21:52 - INFO - Best F1:0.0603 - Best Epoch:1
2026-02-10 16:21:53 - INFO - Starting co-training
2026-02-10 16:22:23 - INFO - Time taken for Epoch 1: 29.92s - F1: 0.42503197
2026-02-10 16:22:53 - INFO - Time taken for Epoch 2: 30.42s - F1: 0.49662773
2026-02-10 16:23:24 - INFO - Time taken for Epoch 3: 30.69s - F1: 0.53441548
2026-02-10 16:23:55 - INFO - Time taken for Epoch 4: 30.75s - F1: 0.53444798
2026-02-10 16:24:29 - INFO - Time taken for Epoch 5: 34.99s - F1: 0.55038777
2026-02-10 16:25:07 - INFO - Time taken for Epoch 6: 37.58s - F1: 0.56733163
2026-02-10 16:25:38 - INFO - Time taken for Epoch 7: 30.90s - F1: 0.58231898
2026-02-10 16:26:10 - INFO - Time taken for Epoch 8: 32.41s - F1: 0.57539530
2026-02-10 16:26:12 - INFO - Fine-tuning models
2026-02-10 16:26:15 - INFO - Time taken for Epoch 1:3.18 - F1: 0.5807
2026-02-10 16:26:19 - INFO - Time taken for Epoch 2:3.70 - F1: 0.5969
2026-02-10 16:26:23 - INFO - Time taken for Epoch 3:3.87 - F1: 0.5973
2026-02-10 16:26:27 - INFO - Time taken for Epoch 4:3.82 - F1: 0.5967
2026-02-10 16:26:30 - INFO - Time taken for Epoch 5:3.08 - F1: 0.5938
2026-02-10 16:26:33 - INFO - Time taken for Epoch 6:2.96 - F1: 0.6019
2026-02-10 16:26:36 - INFO - Time taken for Epoch 7:3.75 - F1: 0.6042
2026-02-10 16:26:40 - INFO - Time taken for Epoch 8:3.58 - F1: 0.6098
2026-02-10 16:26:44 - INFO - Time taken for Epoch 9:3.64 - F1: 0.6150
2026-02-10 16:26:47 - INFO - Time taken for Epoch 10:3.61 - F1: 0.6196
2026-02-10 16:26:51 - INFO - Time taken for Epoch 11:3.60 - F1: 0.6231
2026-02-10 16:27:01 - INFO - Time taken for Epoch 12:10.11 - F1: 0.6205
2026-02-10 16:27:04 - INFO - Time taken for Epoch 13:2.92 - F1: 0.6253
2026-02-10 16:27:07 - INFO - Time taken for Epoch 14:3.57 - F1: 0.6279
2026-02-10 16:27:11 - INFO - Time taken for Epoch 15:3.59 - F1: 0.6313
2026-02-10 16:27:15 - INFO - Time taken for Epoch 16:3.57 - F1: 0.6298
2026-02-10 16:27:18 - INFO - Time taken for Epoch 17:2.95 - F1: 0.6304
2026-02-10 16:27:20 - INFO - Time taken for Epoch 18:2.94 - F1: 0.6254
2026-02-10 16:27:23 - INFO - Time taken for Epoch 19:2.93 - F1: 0.6225
2026-02-10 16:27:26 - INFO - Time taken for Epoch 20:2.92 - F1: 0.6253
2026-02-10 16:27:29 - INFO - Time taken for Epoch 21:2.94 - F1: 0.6254
2026-02-10 16:27:32 - INFO - Time taken for Epoch 22:2.95 - F1: 0.6271
2026-02-10 16:27:35 - INFO - Time taken for Epoch 23:2.94 - F1: 0.6291
2026-02-10 16:27:38 - INFO - Time taken for Epoch 24:2.94 - F1: 0.6303
2026-02-10 16:27:41 - INFO - Time taken for Epoch 25:2.93 - F1: 0.6313
2026-02-10 16:27:41 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-10 16:27:41 - INFO - Best F1:0.6313 - Best Epoch:14
2026-02-10 16:27:48 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6255, Test ECE: 0.0534
2026-02-10 16:27:48 - INFO - All results: {'f1_macro': 0.6254629263849718, 'ece': np.float64(0.05335905720322416)}
2026-02-10 16:27:48 - INFO - 
Total time taken: 522.92 seconds
2026-02-10 16:27:48 - INFO - Trial 9 finished with value: 0.6254629263849718 and parameters: {'learning_rate': 2.7639340025737998e-05, 'weight_decay': 0.0005681665859567847, 'batch_size': 8, 'co_train_epochs': 8, 'epoch_patience': 7}. Best is trial 2 with value: 0.6478073114404596.
2026-02-10 16:27:48 - INFO - 
[BEST TRIAL RESULTS]
2026-02-10 16:27:48 - INFO - F1 Score: 0.6478
2026-02-10 16:27:48 - INFO - Params: {'learning_rate': 2.487094782776448e-05, 'weight_decay': 0.0023643695292476763, 'batch_size': 24, 'co_train_epochs': 19, 'epoch_patience': 6}
2026-02-10 16:27:48 - INFO -   learning_rate: 2.487094782776448e-05
2026-02-10 16:27:48 - INFO -   weight_decay: 0.0023643695292476763
2026-02-10 16:27:48 - INFO -   batch_size: 24
2026-02-10 16:27:48 - INFO -   co_train_epochs: 19
2026-02-10 16:27:48 - INFO -   epoch_patience: 6
2026-02-10 16:27:48 - INFO - 
Total time taken: 5545.08 seconds
