Running with 5 label/class set 3

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-12 16:16:05 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-12 16:16:05 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_florence_2018
Using devices: cuda, cuda
2026-02-12 16:16:06 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:16:06 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:16:06 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:16:06 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 2.689888979899006e-05
Weight Decay: 0.00018383891436475378
Batch Size: 16
No. Epochs: 5
Epoch Patience: 5
 Accumulation Steps: 4
2026-02-12 16:16:06 - INFO - Learning Rate: 2.689888979899006e-05
Weight Decay: 0.00018383891436475378
Batch Size: 16
No. Epochs: 5
Epoch Patience: 5
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:16:08 - INFO - Generating initial weights
Time taken for Epoch 1:18.88 - F1: 0.0232
2026-02-12 16:16:30 - INFO - Time taken for Epoch 1:18.88 - F1: 0.0232
Time taken for Epoch 2:18.56 - F1: 0.0223
2026-02-12 16:16:49 - INFO - Time taken for Epoch 2:18.56 - F1: 0.0223
Time taken for Epoch 3:18.34 - F1: 0.0158
2026-02-12 16:17:07 - INFO - Time taken for Epoch 3:18.34 - F1: 0.0158
Time taken for Epoch 4:18.36 - F1: 0.0155
2026-02-12 16:17:26 - INFO - Time taken for Epoch 4:18.36 - F1: 0.0155
Time taken for Epoch 5:18.37 - F1: 0.0155
2026-02-12 16:17:44 - INFO - Time taken for Epoch 5:18.37 - F1: 0.0155
Best F1:0.0232 - Best Epoch:1
2026-02-12 16:17:44 - INFO - Best F1:0.0232 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:17:45 - INFO - Starting co-training
Time taken for Epoch 1: 25.41s - F1: 0.38988165
2026-02-12 16:18:11 - INFO - Time taken for Epoch 1: 25.41s - F1: 0.38988165
Time taken for Epoch 2: 26.47s - F1: 0.56578141
2026-02-12 16:18:37 - INFO - Time taken for Epoch 2: 26.47s - F1: 0.56578141
Time taken for Epoch 3: 26.61s - F1: 0.60085654
2026-02-12 16:19:04 - INFO - Time taken for Epoch 3: 26.61s - F1: 0.60085654
Time taken for Epoch 4: 26.63s - F1: 0.58588935
2026-02-12 16:19:31 - INFO - Time taken for Epoch 4: 26.63s - F1: 0.58588935
Time taken for Epoch 5: 25.53s - F1: 0.61366910
2026-02-12 16:19:56 - INFO - Time taken for Epoch 5: 25.53s - F1: 0.61366910
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:20:00 - INFO - Fine-tuning models
Time taken for Epoch 1:2.60 - F1: 0.6099
2026-02-12 16:20:03 - INFO - Time taken for Epoch 1:2.60 - F1: 0.6099
Time taken for Epoch 2:3.65 - F1: 0.6148
2026-02-12 16:20:07 - INFO - Time taken for Epoch 2:3.65 - F1: 0.6148
Time taken for Epoch 3:3.75 - F1: 0.6171
2026-02-12 16:20:10 - INFO - Time taken for Epoch 3:3.75 - F1: 0.6171
Time taken for Epoch 4:3.74 - F1: 0.6293
2026-02-12 16:20:14 - INFO - Time taken for Epoch 4:3.74 - F1: 0.6293
Time taken for Epoch 5:3.73 - F1: 0.6228
2026-02-12 16:20:18 - INFO - Time taken for Epoch 5:3.73 - F1: 0.6228
Time taken for Epoch 6:2.58 - F1: 0.6250
2026-02-12 16:20:20 - INFO - Time taken for Epoch 6:2.58 - F1: 0.6250
Time taken for Epoch 7:2.57 - F1: 0.6289
2026-02-12 16:20:23 - INFO - Time taken for Epoch 7:2.57 - F1: 0.6289
Time taken for Epoch 8:2.58 - F1: 0.6263
2026-02-12 16:20:26 - INFO - Time taken for Epoch 8:2.58 - F1: 0.6263
Time taken for Epoch 9:2.58 - F1: 0.6256
2026-02-12 16:20:28 - INFO - Time taken for Epoch 9:2.58 - F1: 0.6256
Time taken for Epoch 10:2.58 - F1: 0.6248
2026-02-12 16:20:31 - INFO - Time taken for Epoch 10:2.58 - F1: 0.6248
Time taken for Epoch 11:2.58 - F1: 0.6301
2026-02-12 16:20:33 - INFO - Time taken for Epoch 11:2.58 - F1: 0.6301
Time taken for Epoch 12:4.20 - F1: 0.6330
2026-02-12 16:20:38 - INFO - Time taken for Epoch 12:4.20 - F1: 0.6330
Time taken for Epoch 13:3.75 - F1: 0.6348
2026-02-12 16:20:41 - INFO - Time taken for Epoch 13:3.75 - F1: 0.6348
Time taken for Epoch 14:3.75 - F1: 0.6355
2026-02-12 16:20:45 - INFO - Time taken for Epoch 14:3.75 - F1: 0.6355
Time taken for Epoch 15:3.73 - F1: 0.6372
2026-02-12 16:20:49 - INFO - Time taken for Epoch 15:3.73 - F1: 0.6372
Time taken for Epoch 16:3.73 - F1: 0.6372
2026-02-12 16:20:53 - INFO - Time taken for Epoch 16:3.73 - F1: 0.6372
Time taken for Epoch 17:3.73 - F1: 0.6424
2026-02-12 16:20:56 - INFO - Time taken for Epoch 17:3.73 - F1: 0.6424
Time taken for Epoch 18:3.76 - F1: 0.6409
2026-02-12 16:21:00 - INFO - Time taken for Epoch 18:3.76 - F1: 0.6409
Time taken for Epoch 19:2.57 - F1: 0.6445
2026-02-12 16:21:03 - INFO - Time taken for Epoch 19:2.57 - F1: 0.6445
Time taken for Epoch 20:3.76 - F1: 0.6452
2026-02-12 16:21:06 - INFO - Time taken for Epoch 20:3.76 - F1: 0.6452
Time taken for Epoch 21:3.72 - F1: 0.6433
2026-02-12 16:21:10 - INFO - Time taken for Epoch 21:3.72 - F1: 0.6433
Time taken for Epoch 22:2.57 - F1: 0.6381
2026-02-12 16:21:13 - INFO - Time taken for Epoch 22:2.57 - F1: 0.6381
Time taken for Epoch 23:2.57 - F1: 0.6395
2026-02-12 16:21:15 - INFO - Time taken for Epoch 23:2.57 - F1: 0.6395
Time taken for Epoch 24:2.57 - F1: 0.6416
2026-02-12 16:21:18 - INFO - Time taken for Epoch 24:2.57 - F1: 0.6416
Time taken for Epoch 25:2.58 - F1: 0.6475
2026-02-12 16:21:20 - INFO - Time taken for Epoch 25:2.58 - F1: 0.6475
Time taken for Epoch 26:3.74 - F1: 0.6475
2026-02-12 16:21:24 - INFO - Time taken for Epoch 26:3.74 - F1: 0.6475
Time taken for Epoch 27:2.57 - F1: 0.6483
2026-02-12 16:21:27 - INFO - Time taken for Epoch 27:2.57 - F1: 0.6483
Time taken for Epoch 28:3.75 - F1: 0.6674
2026-02-12 16:21:30 - INFO - Time taken for Epoch 28:3.75 - F1: 0.6674
Time taken for Epoch 29:3.76 - F1: 0.6671
2026-02-12 16:21:34 - INFO - Time taken for Epoch 29:3.76 - F1: 0.6671
Time taken for Epoch 30:2.57 - F1: 0.6666
2026-02-12 16:21:37 - INFO - Time taken for Epoch 30:2.57 - F1: 0.6666
Time taken for Epoch 31:2.57 - F1: 0.6696
2026-02-12 16:21:39 - INFO - Time taken for Epoch 31:2.57 - F1: 0.6696
Time taken for Epoch 32:4.52 - F1: 0.6689
2026-02-12 16:21:44 - INFO - Time taken for Epoch 32:4.52 - F1: 0.6689
Time taken for Epoch 33:2.57 - F1: 0.6676
2026-02-12 16:21:46 - INFO - Time taken for Epoch 33:2.57 - F1: 0.6676
Time taken for Epoch 34:2.57 - F1: 0.6676
2026-02-12 16:21:49 - INFO - Time taken for Epoch 34:2.57 - F1: 0.6676
Time taken for Epoch 35:2.57 - F1: 0.6659
2026-02-12 16:21:52 - INFO - Time taken for Epoch 35:2.57 - F1: 0.6659
Time taken for Epoch 36:2.57 - F1: 0.6659
2026-02-12 16:21:54 - INFO - Time taken for Epoch 36:2.57 - F1: 0.6659
Time taken for Epoch 37:2.57 - F1: 0.6669
2026-02-12 16:21:57 - INFO - Time taken for Epoch 37:2.57 - F1: 0.6669
Time taken for Epoch 38:2.59 - F1: 0.6689
2026-02-12 16:21:59 - INFO - Time taken for Epoch 38:2.59 - F1: 0.6689
Time taken for Epoch 39:2.58 - F1: 0.6689
2026-02-12 16:22:02 - INFO - Time taken for Epoch 39:2.58 - F1: 0.6689
Time taken for Epoch 40:2.58 - F1: 0.6670
2026-02-12 16:22:04 - INFO - Time taken for Epoch 40:2.58 - F1: 0.6670
Time taken for Epoch 41:2.58 - F1: 0.6638
2026-02-12 16:22:07 - INFO - Time taken for Epoch 41:2.58 - F1: 0.6638
Performance not improving for 10 consecutive epochs.
2026-02-12 16:22:07 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6696 - Best Epoch:30
2026-02-12 16:22:07 - INFO - Best F1:0.6696 - Best Epoch:30
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6666, Test ECE: 0.0686
2026-02-12 16:22:15 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6666, Test ECE: 0.0686
All results: {'f1_macro': 0.6665873635903812, 'ece': np.float64(0.06864773972585833)}
2026-02-12 16:22:15 - INFO - All results: {'f1_macro': 0.6665873635903812, 'ece': np.float64(0.06864773972585833)}

Total time taken: 369.51 seconds
2026-02-12 16:22:15 - INFO - 
Total time taken: 369.51 seconds
2026-02-12 16:22:15 - INFO - Trial 0 finished with value: 0.6665873635903812 and parameters: {'learning_rate': 2.689888979899006e-05, 'weight_decay': 0.00018383891436475378, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 5}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 16:22:15 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:22:15 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:22:15 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:22:15 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 9.083384358031551e-05
Weight Decay: 2.3606924836272686e-05
Batch Size: 16
No. Epochs: 18
Epoch Patience: 9
 Accumulation Steps: 4
2026-02-12 16:22:16 - INFO - Learning Rate: 9.083384358031551e-05
Weight Decay: 2.3606924836272686e-05
Batch Size: 16
No. Epochs: 18
Epoch Patience: 9
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:22:17 - INFO - Generating initial weights
Time taken for Epoch 1:18.38 - F1: 0.0155
2026-02-12 16:22:39 - INFO - Time taken for Epoch 1:18.38 - F1: 0.0155
Time taken for Epoch 2:18.28 - F1: 0.0155
2026-02-12 16:22:57 - INFO - Time taken for Epoch 2:18.28 - F1: 0.0155
Time taken for Epoch 3:18.36 - F1: 0.0155
2026-02-12 16:23:15 - INFO - Time taken for Epoch 3:18.36 - F1: 0.0155
Time taken for Epoch 4:18.32 - F1: 0.0155
2026-02-12 16:23:34 - INFO - Time taken for Epoch 4:18.32 - F1: 0.0155
Time taken for Epoch 5:18.37 - F1: 0.0155
2026-02-12 16:23:52 - INFO - Time taken for Epoch 5:18.37 - F1: 0.0155
Time taken for Epoch 6:18.40 - F1: 0.0155
2026-02-12 16:24:10 - INFO - Time taken for Epoch 6:18.40 - F1: 0.0155
Time taken for Epoch 7:18.39 - F1: 0.0323
2026-02-12 16:24:29 - INFO - Time taken for Epoch 7:18.39 - F1: 0.0323
Time taken for Epoch 8:18.38 - F1: 0.0642
2026-02-12 16:24:47 - INFO - Time taken for Epoch 8:18.38 - F1: 0.0642
Time taken for Epoch 9:18.40 - F1: 0.1338
2026-02-12 16:25:06 - INFO - Time taken for Epoch 9:18.40 - F1: 0.1338
Time taken for Epoch 10:18.38 - F1: 0.1999
2026-02-12 16:25:24 - INFO - Time taken for Epoch 10:18.38 - F1: 0.1999
Time taken for Epoch 11:18.39 - F1: 0.2445
2026-02-12 16:25:42 - INFO - Time taken for Epoch 11:18.39 - F1: 0.2445
Time taken for Epoch 12:18.40 - F1: 0.2638
2026-02-12 16:26:01 - INFO - Time taken for Epoch 12:18.40 - F1: 0.2638
Time taken for Epoch 13:18.46 - F1: 0.2740
2026-02-12 16:26:19 - INFO - Time taken for Epoch 13:18.46 - F1: 0.2740
Time taken for Epoch 14:18.42 - F1: 0.2872
2026-02-12 16:26:38 - INFO - Time taken for Epoch 14:18.42 - F1: 0.2872
Time taken for Epoch 15:18.43 - F1: 0.2845
2026-02-12 16:26:56 - INFO - Time taken for Epoch 15:18.43 - F1: 0.2845
Time taken for Epoch 16:18.42 - F1: 0.2944
2026-02-12 16:27:15 - INFO - Time taken for Epoch 16:18.42 - F1: 0.2944
Time taken for Epoch 17:18.42 - F1: 0.3000
2026-02-12 16:27:33 - INFO - Time taken for Epoch 17:18.42 - F1: 0.3000
Time taken for Epoch 18:18.40 - F1: 0.2939
2026-02-12 16:27:51 - INFO - Time taken for Epoch 18:18.40 - F1: 0.2939
Best F1:0.3000 - Best Epoch:17
2026-02-12 16:27:51 - INFO - Best F1:0.3000 - Best Epoch:17
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:27:53 - INFO - Starting co-training
Time taken for Epoch 1: 25.48s - F1: 0.46179427
2026-02-12 16:28:18 - INFO - Time taken for Epoch 1: 25.48s - F1: 0.46179427
Time taken for Epoch 2: 26.60s - F1: 0.58142548
2026-02-12 16:28:45 - INFO - Time taken for Epoch 2: 26.60s - F1: 0.58142548
Time taken for Epoch 3: 26.70s - F1: 0.58261261
2026-02-12 16:29:12 - INFO - Time taken for Epoch 3: 26.70s - F1: 0.58261261
Time taken for Epoch 4: 26.70s - F1: 0.60224137
2026-02-12 16:29:38 - INFO - Time taken for Epoch 4: 26.70s - F1: 0.60224137
Time taken for Epoch 5: 26.69s - F1: 0.58773826
2026-02-12 16:30:05 - INFO - Time taken for Epoch 5: 26.69s - F1: 0.58773826
Time taken for Epoch 6: 25.52s - F1: 0.60366858
2026-02-12 16:30:31 - INFO - Time taken for Epoch 6: 25.52s - F1: 0.60366858
Time taken for Epoch 7: 26.64s - F1: 0.56767509
2026-02-12 16:30:57 - INFO - Time taken for Epoch 7: 26.64s - F1: 0.56767509
Time taken for Epoch 8: 25.52s - F1: 0.59238010
2026-02-12 16:31:23 - INFO - Time taken for Epoch 8: 25.52s - F1: 0.59238010
Time taken for Epoch 9: 25.56s - F1: 0.59349291
2026-02-12 16:31:48 - INFO - Time taken for Epoch 9: 25.56s - F1: 0.59349291
Time taken for Epoch 10: 25.56s - F1: 0.60039131
2026-02-12 16:32:14 - INFO - Time taken for Epoch 10: 25.56s - F1: 0.60039131
Time taken for Epoch 11: 25.58s - F1: 0.61360991
2026-02-12 16:32:40 - INFO - Time taken for Epoch 11: 25.58s - F1: 0.61360991
Time taken for Epoch 12: 26.73s - F1: 0.57199265
2026-02-12 16:33:06 - INFO - Time taken for Epoch 12: 26.73s - F1: 0.57199265
Time taken for Epoch 13: 25.56s - F1: 0.60673992
2026-02-12 16:33:32 - INFO - Time taken for Epoch 13: 25.56s - F1: 0.60673992
Time taken for Epoch 14: 25.57s - F1: 0.63638190
2026-02-12 16:33:57 - INFO - Time taken for Epoch 14: 25.57s - F1: 0.63638190
Time taken for Epoch 15: 26.77s - F1: 0.60014719
2026-02-12 16:34:24 - INFO - Time taken for Epoch 15: 26.77s - F1: 0.60014719
Time taken for Epoch 16: 25.56s - F1: 0.59474592
2026-02-12 16:34:50 - INFO - Time taken for Epoch 16: 25.56s - F1: 0.59474592
Time taken for Epoch 17: 25.73s - F1: 0.62799361
2026-02-12 16:35:15 - INFO - Time taken for Epoch 17: 25.73s - F1: 0.62799361
Time taken for Epoch 18: 25.58s - F1: 0.60689202
2026-02-12 16:35:41 - INFO - Time taken for Epoch 18: 25.58s - F1: 0.60689202
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:35:44 - INFO - Fine-tuning models
Time taken for Epoch 1:2.60 - F1: 0.5975
2026-02-12 16:35:47 - INFO - Time taken for Epoch 1:2.60 - F1: 0.5975
Time taken for Epoch 2:4.29 - F1: 0.5825
2026-02-12 16:35:51 - INFO - Time taken for Epoch 2:4.29 - F1: 0.5825
Time taken for Epoch 3:2.58 - F1: 0.5970
2026-02-12 16:35:54 - INFO - Time taken for Epoch 3:2.58 - F1: 0.5970
Time taken for Epoch 4:2.58 - F1: 0.6082
2026-02-12 16:35:56 - INFO - Time taken for Epoch 4:2.58 - F1: 0.6082
Time taken for Epoch 5:4.81 - F1: 0.5884
2026-02-12 16:36:01 - INFO - Time taken for Epoch 5:4.81 - F1: 0.5884
Time taken for Epoch 6:2.58 - F1: 0.6199
2026-02-12 16:36:04 - INFO - Time taken for Epoch 6:2.58 - F1: 0.6199
Time taken for Epoch 7:3.96 - F1: 0.6250
2026-02-12 16:36:08 - INFO - Time taken for Epoch 7:3.96 - F1: 0.6250
Time taken for Epoch 8:3.87 - F1: 0.6231
2026-02-12 16:36:11 - INFO - Time taken for Epoch 8:3.87 - F1: 0.6231
Time taken for Epoch 9:2.59 - F1: 0.6140
2026-02-12 16:36:14 - INFO - Time taken for Epoch 9:2.59 - F1: 0.6140
Time taken for Epoch 10:2.57 - F1: 0.6075
2026-02-12 16:36:17 - INFO - Time taken for Epoch 10:2.57 - F1: 0.6075
Time taken for Epoch 11:2.57 - F1: 0.6057
2026-02-12 16:36:19 - INFO - Time taken for Epoch 11:2.57 - F1: 0.6057
Time taken for Epoch 12:2.58 - F1: 0.6046
2026-02-12 16:36:22 - INFO - Time taken for Epoch 12:2.58 - F1: 0.6046
Time taken for Epoch 13:2.58 - F1: 0.6042
2026-02-12 16:36:24 - INFO - Time taken for Epoch 13:2.58 - F1: 0.6042
Time taken for Epoch 14:2.57 - F1: 0.6051
2026-02-12 16:36:27 - INFO - Time taken for Epoch 14:2.57 - F1: 0.6051
Time taken for Epoch 15:2.57 - F1: 0.6086
2026-02-12 16:36:29 - INFO - Time taken for Epoch 15:2.57 - F1: 0.6086
Time taken for Epoch 16:2.58 - F1: 0.6051
2026-02-12 16:36:32 - INFO - Time taken for Epoch 16:2.58 - F1: 0.6051
Time taken for Epoch 17:2.58 - F1: 0.5954
2026-02-12 16:36:35 - INFO - Time taken for Epoch 17:2.58 - F1: 0.5954
Performance not improving for 10 consecutive epochs.
2026-02-12 16:36:35 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6250 - Best Epoch:6
2026-02-12 16:36:35 - INFO - Best F1:0.6250 - Best Epoch:6
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6171, Test ECE: 0.1815
2026-02-12 16:36:42 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6171, Test ECE: 0.1815
All results: {'f1_macro': 0.6171148294309793, 'ece': np.float64(0.18150346537062856)}
2026-02-12 16:36:42 - INFO - All results: {'f1_macro': 0.6171148294309793, 'ece': np.float64(0.18150346537062856)}

Total time taken: 867.36 seconds
2026-02-12 16:36:42 - INFO - 
Total time taken: 867.36 seconds
2026-02-12 16:36:43 - INFO - Trial 1 finished with value: 0.6171148294309793 and parameters: {'learning_rate': 9.083384358031551e-05, 'weight_decay': 2.3606924836272686e-05, 'batch_size': 16, 'co_train_epochs': 18, 'epoch_patience': 9}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 16:36:43 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:36:43 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:36:43 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:36:43 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0002851762197205493
Weight Decay: 0.004544778157169178
Batch Size: 16
No. Epochs: 10
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-12 16:36:43 - INFO - Learning Rate: 0.0002851762197205493
Weight Decay: 0.004544778157169178
Batch Size: 16
No. Epochs: 10
Epoch Patience: 4
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:36:44 - INFO - Generating initial weights
Time taken for Epoch 1:18.42 - F1: 0.0155
2026-02-12 16:37:06 - INFO - Time taken for Epoch 1:18.42 - F1: 0.0155
Time taken for Epoch 2:18.36 - F1: 0.0155
2026-02-12 16:37:25 - INFO - Time taken for Epoch 2:18.36 - F1: 0.0155
Time taken for Epoch 3:18.33 - F1: 0.0155
2026-02-12 16:37:43 - INFO - Time taken for Epoch 3:18.33 - F1: 0.0155
Time taken for Epoch 4:18.36 - F1: 0.0155
2026-02-12 16:38:01 - INFO - Time taken for Epoch 4:18.36 - F1: 0.0155
Time taken for Epoch 5:18.37 - F1: 0.0155
2026-02-12 16:38:20 - INFO - Time taken for Epoch 5:18.37 - F1: 0.0155
Time taken for Epoch 6:18.41 - F1: 0.0191
2026-02-12 16:38:38 - INFO - Time taken for Epoch 6:18.41 - F1: 0.0191
Time taken for Epoch 7:18.40 - F1: 0.1545
2026-02-12 16:38:56 - INFO - Time taken for Epoch 7:18.40 - F1: 0.1545
Time taken for Epoch 8:18.40 - F1: 0.0892
2026-02-12 16:39:15 - INFO - Time taken for Epoch 8:18.40 - F1: 0.0892
Time taken for Epoch 9:18.40 - F1: 0.0911
2026-02-12 16:39:33 - INFO - Time taken for Epoch 9:18.40 - F1: 0.0911
Time taken for Epoch 10:18.43 - F1: 0.1205
2026-02-12 16:39:52 - INFO - Time taken for Epoch 10:18.43 - F1: 0.1205
Best F1:0.1545 - Best Epoch:7
2026-02-12 16:39:52 - INFO - Best F1:0.1545 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:39:53 - INFO - Starting co-training
Time taken for Epoch 1: 25.50s - F1: 0.03212851
2026-02-12 16:40:19 - INFO - Time taken for Epoch 1: 25.50s - F1: 0.03212851
Time taken for Epoch 2: 26.59s - F1: 0.03212851
2026-02-12 16:40:45 - INFO - Time taken for Epoch 2: 26.59s - F1: 0.03212851
Time taken for Epoch 3: 25.62s - F1: 0.03212851
2026-02-12 16:41:11 - INFO - Time taken for Epoch 3: 25.62s - F1: 0.03212851
Time taken for Epoch 4: 25.54s - F1: 0.03212851
2026-02-12 16:41:37 - INFO - Time taken for Epoch 4: 25.54s - F1: 0.03212851
Time taken for Epoch 5: 25.57s - F1: 0.03852235
2026-02-12 16:42:02 - INFO - Time taken for Epoch 5: 25.57s - F1: 0.03852235
Time taken for Epoch 6: 26.81s - F1: 0.03212851
2026-02-12 16:42:29 - INFO - Time taken for Epoch 6: 26.81s - F1: 0.03212851
Time taken for Epoch 7: 25.59s - F1: 0.03212851
2026-02-12 16:42:55 - INFO - Time taken for Epoch 7: 25.59s - F1: 0.03212851
Time taken for Epoch 8: 25.58s - F1: 0.03212851
2026-02-12 16:43:20 - INFO - Time taken for Epoch 8: 25.58s - F1: 0.03212851
Time taken for Epoch 9: 25.56s - F1: 0.03212851
2026-02-12 16:43:46 - INFO - Time taken for Epoch 9: 25.56s - F1: 0.03212851
Performance not improving for 4 consecutive epochs.
Performance not improving for 4 consecutive epochs.
2026-02-12 16:43:46 - INFO - Performance not improving for 4 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:43:49 - INFO - Fine-tuning models
Time taken for Epoch 1:2.60 - F1: 0.0385
2026-02-12 16:43:52 - INFO - Time taken for Epoch 1:2.60 - F1: 0.0385
Time taken for Epoch 2:3.86 - F1: 0.0385
2026-02-12 16:43:55 - INFO - Time taken for Epoch 2:3.86 - F1: 0.0385
Time taken for Epoch 3:2.57 - F1: 0.0385
2026-02-12 16:43:58 - INFO - Time taken for Epoch 3:2.57 - F1: 0.0385
Time taken for Epoch 4:2.57 - F1: 0.0385
2026-02-12 16:44:01 - INFO - Time taken for Epoch 4:2.57 - F1: 0.0385
Time taken for Epoch 5:2.57 - F1: 0.0385
2026-02-12 16:44:03 - INFO - Time taken for Epoch 5:2.57 - F1: 0.0385
Time taken for Epoch 6:2.57 - F1: 0.0385
2026-02-12 16:44:06 - INFO - Time taken for Epoch 6:2.57 - F1: 0.0385
Time taken for Epoch 7:2.58 - F1: 0.0270
2026-02-12 16:44:08 - INFO - Time taken for Epoch 7:2.58 - F1: 0.0270
Time taken for Epoch 8:2.58 - F1: 0.0017
2026-02-12 16:44:11 - INFO - Time taken for Epoch 8:2.58 - F1: 0.0017
Time taken for Epoch 9:2.58 - F1: 0.0155
2026-02-12 16:44:13 - INFO - Time taken for Epoch 9:2.58 - F1: 0.0155
Time taken for Epoch 10:2.58 - F1: 0.0155
2026-02-12 16:44:16 - INFO - Time taken for Epoch 10:2.58 - F1: 0.0155
Time taken for Epoch 11:2.58 - F1: 0.0155
2026-02-12 16:44:19 - INFO - Time taken for Epoch 11:2.58 - F1: 0.0155
Performance not improving for 10 consecutive epochs.
2026-02-12 16:44:19 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0385 - Best Epoch:0
2026-02-12 16:44:19 - INFO - Best F1:0.0385 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0384, Test ECE: 0.3971
2026-02-12 16:44:27 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0384, Test ECE: 0.3971
All results: {'f1_macro': 0.03837037037037037, 'ece': np.float64(0.3970587495159084)}
2026-02-12 16:44:27 - INFO - All results: {'f1_macro': 0.03837037037037037, 'ece': np.float64(0.3970587495159084)}

Total time taken: 464.04 seconds
2026-02-12 16:44:27 - INFO - 
Total time taken: 464.04 seconds
2026-02-12 16:44:27 - INFO - Trial 2 finished with value: 0.03837037037037037 and parameters: {'learning_rate': 0.0002851762197205493, 'weight_decay': 0.004544778157169178, 'batch_size': 16, 'co_train_epochs': 10, 'epoch_patience': 4}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 16:44:27 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:44:27 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:44:27 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:44:27 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0004172018301757342
Weight Decay: 7.305826861016868e-05
Batch Size: 32
No. Epochs: 10
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-12 16:44:27 - INFO - Learning Rate: 0.0004172018301757342
Weight Decay: 7.305826861016868e-05
Batch Size: 32
No. Epochs: 10
Epoch Patience: 4
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:44:28 - INFO - Generating initial weights
Time taken for Epoch 1:17.84 - F1: 0.1545
2026-02-12 16:44:50 - INFO - Time taken for Epoch 1:17.84 - F1: 0.1545
Time taken for Epoch 2:17.80 - F1: 0.2562
2026-02-12 16:45:07 - INFO - Time taken for Epoch 2:17.80 - F1: 0.2562
Time taken for Epoch 3:17.79 - F1: 0.2037
2026-02-12 16:45:25 - INFO - Time taken for Epoch 3:17.79 - F1: 0.2037
Time taken for Epoch 4:17.82 - F1: 0.2928
2026-02-12 16:45:43 - INFO - Time taken for Epoch 4:17.82 - F1: 0.2928
Time taken for Epoch 5:17.82 - F1: 0.3245
2026-02-12 16:46:01 - INFO - Time taken for Epoch 5:17.82 - F1: 0.3245
Time taken for Epoch 6:17.86 - F1: 0.3228
2026-02-12 16:46:19 - INFO - Time taken for Epoch 6:17.86 - F1: 0.3228
Time taken for Epoch 7:17.85 - F1: 0.3244
2026-02-12 16:46:36 - INFO - Time taken for Epoch 7:17.85 - F1: 0.3244
Time taken for Epoch 8:17.86 - F1: 0.3216
2026-02-12 16:46:54 - INFO - Time taken for Epoch 8:17.86 - F1: 0.3216
Time taken for Epoch 9:17.82 - F1: 0.3206
2026-02-12 16:47:12 - INFO - Time taken for Epoch 9:17.82 - F1: 0.3206
Time taken for Epoch 10:17.83 - F1: 0.3266
2026-02-12 16:47:30 - INFO - Time taken for Epoch 10:17.83 - F1: 0.3266
Best F1:0.3266 - Best Epoch:10
2026-02-12 16:47:30 - INFO - Best F1:0.3266 - Best Epoch:10
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:47:31 - INFO - Starting co-training
Time taken for Epoch 1: 30.78s - F1: 0.03212851
2026-02-12 16:48:02 - INFO - Time taken for Epoch 1: 30.78s - F1: 0.03212851
Time taken for Epoch 2: 31.95s - F1: 0.03212851
2026-02-12 16:48:34 - INFO - Time taken for Epoch 2: 31.95s - F1: 0.03212851
Time taken for Epoch 3: 30.78s - F1: 0.03212851
2026-02-12 16:49:05 - INFO - Time taken for Epoch 3: 30.78s - F1: 0.03212851
Time taken for Epoch 4: 30.78s - F1: 0.03212851
2026-02-12 16:49:36 - INFO - Time taken for Epoch 4: 30.78s - F1: 0.03212851
Time taken for Epoch 5: 30.82s - F1: 0.04247539
2026-02-12 16:50:07 - INFO - Time taken for Epoch 5: 30.82s - F1: 0.04247539
Time taken for Epoch 6: 32.06s - F1: 0.04247539
2026-02-12 16:50:39 - INFO - Time taken for Epoch 6: 32.06s - F1: 0.04247539
Time taken for Epoch 7: 30.80s - F1: 0.04247539
2026-02-12 16:51:10 - INFO - Time taken for Epoch 7: 30.80s - F1: 0.04247539
Time taken for Epoch 8: 30.80s - F1: 0.04247539
2026-02-12 16:51:40 - INFO - Time taken for Epoch 8: 30.80s - F1: 0.04247539
Time taken for Epoch 9: 30.80s - F1: 0.04247539
2026-02-12 16:52:11 - INFO - Time taken for Epoch 9: 30.80s - F1: 0.04247539
Performance not improving for 4 consecutive epochs.
Performance not improving for 4 consecutive epochs.
2026-02-12 16:52:11 - INFO - Performance not improving for 4 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:52:14 - INFO - Fine-tuning models
Time taken for Epoch 1:2.48 - F1: 0.0425
2026-02-12 16:52:17 - INFO - Time taken for Epoch 1:2.48 - F1: 0.0425
Time taken for Epoch 2:3.63 - F1: 0.0425
2026-02-12 16:52:20 - INFO - Time taken for Epoch 2:3.63 - F1: 0.0425
Time taken for Epoch 3:2.48 - F1: 0.0425
2026-02-12 16:52:23 - INFO - Time taken for Epoch 3:2.48 - F1: 0.0425
Time taken for Epoch 4:2.47 - F1: 0.0425
2026-02-12 16:52:25 - INFO - Time taken for Epoch 4:2.47 - F1: 0.0425
Time taken for Epoch 5:2.47 - F1: 0.0017
2026-02-12 16:52:28 - INFO - Time taken for Epoch 5:2.47 - F1: 0.0017
Time taken for Epoch 6:2.47 - F1: 0.0017
2026-02-12 16:52:30 - INFO - Time taken for Epoch 6:2.47 - F1: 0.0017
Time taken for Epoch 7:2.47 - F1: 0.0017
2026-02-12 16:52:33 - INFO - Time taken for Epoch 7:2.47 - F1: 0.0017
Time taken for Epoch 8:2.47 - F1: 0.0017
2026-02-12 16:52:35 - INFO - Time taken for Epoch 8:2.47 - F1: 0.0017
Time taken for Epoch 9:2.47 - F1: 0.0205
2026-02-12 16:52:38 - INFO - Time taken for Epoch 9:2.47 - F1: 0.0205
Time taken for Epoch 10:2.47 - F1: 0.0205
2026-02-12 16:52:40 - INFO - Time taken for Epoch 10:2.47 - F1: 0.0205
Time taken for Epoch 11:2.47 - F1: 0.0205
2026-02-12 16:52:43 - INFO - Time taken for Epoch 11:2.47 - F1: 0.0205
Performance not improving for 10 consecutive epochs.
2026-02-12 16:52:43 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-12 16:52:43 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2567
2026-02-12 16:52:50 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2567
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.25667504268630104)}
2026-02-12 16:52:50 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.25667504268630104)}

Total time taken: 503.34 seconds
2026-02-12 16:52:50 - INFO - 
Total time taken: 503.34 seconds
2026-02-12 16:52:50 - INFO - Trial 3 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.0004172018301757342, 'weight_decay': 7.305826861016868e-05, 'batch_size': 32, 'co_train_epochs': 10, 'epoch_patience': 4}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 16:52:50 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 16:52:50 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 16:52:50 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 16:52:50 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.000101023418677921
Weight Decay: 1.2980164923272167e-05
Batch Size: 8
No. Epochs: 8
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-12 16:52:50 - INFO - Learning Rate: 0.000101023418677921
Weight Decay: 1.2980164923272167e-05
Batch Size: 8
No. Epochs: 8
Epoch Patience: 7
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 16:52:51 - INFO - Generating initial weights
Time taken for Epoch 1:19.79 - F1: 0.1339
2026-02-12 16:53:15 - INFO - Time taken for Epoch 1:19.79 - F1: 0.1339
Time taken for Epoch 2:19.63 - F1: 0.0155
2026-02-12 16:53:35 - INFO - Time taken for Epoch 2:19.63 - F1: 0.0155
Time taken for Epoch 3:19.66 - F1: 0.0155
2026-02-12 16:53:54 - INFO - Time taken for Epoch 3:19.66 - F1: 0.0155
Time taken for Epoch 4:19.70 - F1: 0.0188
2026-02-12 16:54:14 - INFO - Time taken for Epoch 4:19.70 - F1: 0.0188
Time taken for Epoch 5:19.74 - F1: 0.0606
2026-02-12 16:54:34 - INFO - Time taken for Epoch 5:19.74 - F1: 0.0606
Time taken for Epoch 6:19.73 - F1: 0.1874
2026-02-12 16:54:53 - INFO - Time taken for Epoch 6:19.73 - F1: 0.1874
Time taken for Epoch 7:19.72 - F1: 0.2851
2026-02-12 16:55:13 - INFO - Time taken for Epoch 7:19.72 - F1: 0.2851
Time taken for Epoch 8:19.74 - F1: 0.3106
2026-02-12 16:55:33 - INFO - Time taken for Epoch 8:19.74 - F1: 0.3106
Best F1:0.3106 - Best Epoch:8
2026-02-12 16:55:33 - INFO - Best F1:0.3106 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 16:55:34 - INFO - Starting co-training
Time taken for Epoch 1: 23.90s - F1: 0.30140453
2026-02-12 16:55:58 - INFO - Time taken for Epoch 1: 23.90s - F1: 0.30140453
Time taken for Epoch 2: 25.12s - F1: 0.41332257
2026-02-12 16:56:23 - INFO - Time taken for Epoch 2: 25.12s - F1: 0.41332257
Time taken for Epoch 3: 25.69s - F1: 0.51481196
2026-02-12 16:56:49 - INFO - Time taken for Epoch 3: 25.69s - F1: 0.51481196
Time taken for Epoch 4: 25.25s - F1: 0.53996065
2026-02-12 16:57:14 - INFO - Time taken for Epoch 4: 25.25s - F1: 0.53996065
Time taken for Epoch 5: 25.21s - F1: 0.47708861
2026-02-12 16:57:40 - INFO - Time taken for Epoch 5: 25.21s - F1: 0.47708861
Time taken for Epoch 6: 23.99s - F1: 0.52779001
2026-02-12 16:58:04 - INFO - Time taken for Epoch 6: 23.99s - F1: 0.52779001
Time taken for Epoch 7: 24.40s - F1: 0.54417714
2026-02-12 16:58:28 - INFO - Time taken for Epoch 7: 24.40s - F1: 0.54417714
Time taken for Epoch 8: 25.06s - F1: 0.48727615
2026-02-12 16:58:53 - INFO - Time taken for Epoch 8: 25.06s - F1: 0.48727615
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 16:58:56 - INFO - Fine-tuning models
Time taken for Epoch 1:2.75 - F1: 0.5282
2026-02-12 16:58:59 - INFO - Time taken for Epoch 1:2.75 - F1: 0.5282
Time taken for Epoch 2:3.93 - F1: 0.5354
2026-02-12 16:59:03 - INFO - Time taken for Epoch 2:3.93 - F1: 0.5354
Time taken for Epoch 3:4.02 - F1: 0.5329
2026-02-12 16:59:07 - INFO - Time taken for Epoch 3:4.02 - F1: 0.5329
Time taken for Epoch 4:2.72 - F1: 0.5303
2026-02-12 16:59:09 - INFO - Time taken for Epoch 4:2.72 - F1: 0.5303
Time taken for Epoch 5:2.72 - F1: 0.5215
2026-02-12 16:59:12 - INFO - Time taken for Epoch 5:2.72 - F1: 0.5215
Time taken for Epoch 6:2.72 - F1: 0.5195
2026-02-12 16:59:15 - INFO - Time taken for Epoch 6:2.72 - F1: 0.5195
Time taken for Epoch 7:2.72 - F1: 0.5208
2026-02-12 16:59:18 - INFO - Time taken for Epoch 7:2.72 - F1: 0.5208
Time taken for Epoch 8:2.72 - F1: 0.5354
2026-02-12 16:59:20 - INFO - Time taken for Epoch 8:2.72 - F1: 0.5354
Time taken for Epoch 9:4.05 - F1: 0.5279
2026-02-12 16:59:24 - INFO - Time taken for Epoch 9:4.05 - F1: 0.5279
Time taken for Epoch 10:2.75 - F1: 0.5252
2026-02-12 16:59:27 - INFO - Time taken for Epoch 10:2.75 - F1: 0.5252
Time taken for Epoch 11:2.73 - F1: 0.5313
2026-02-12 16:59:30 - INFO - Time taken for Epoch 11:2.73 - F1: 0.5313
Time taken for Epoch 12:2.74 - F1: 0.5361
2026-02-12 16:59:33 - INFO - Time taken for Epoch 12:2.74 - F1: 0.5361
Time taken for Epoch 13:4.05 - F1: 0.5363
2026-02-12 16:59:37 - INFO - Time taken for Epoch 13:4.05 - F1: 0.5363
Time taken for Epoch 14:4.03 - F1: 0.5277
2026-02-12 16:59:41 - INFO - Time taken for Epoch 14:4.03 - F1: 0.5277
Time taken for Epoch 15:2.72 - F1: 0.5251
2026-02-12 16:59:43 - INFO - Time taken for Epoch 15:2.72 - F1: 0.5251
Time taken for Epoch 16:2.72 - F1: 0.5310
2026-02-12 16:59:46 - INFO - Time taken for Epoch 16:2.72 - F1: 0.5310
Time taken for Epoch 17:2.72 - F1: 0.5406
2026-02-12 16:59:49 - INFO - Time taken for Epoch 17:2.72 - F1: 0.5406
Time taken for Epoch 18:4.04 - F1: 0.5404
2026-02-12 16:59:53 - INFO - Time taken for Epoch 18:4.04 - F1: 0.5404
Time taken for Epoch 19:2.72 - F1: 0.5450
2026-02-12 16:59:56 - INFO - Time taken for Epoch 19:2.72 - F1: 0.5450
Time taken for Epoch 20:4.06 - F1: 0.5460
2026-02-12 17:00:00 - INFO - Time taken for Epoch 20:4.06 - F1: 0.5460
Time taken for Epoch 21:4.13 - F1: 0.5456
2026-02-12 17:00:04 - INFO - Time taken for Epoch 21:4.13 - F1: 0.5456
Time taken for Epoch 22:2.72 - F1: 0.5453
2026-02-12 17:00:06 - INFO - Time taken for Epoch 22:2.72 - F1: 0.5453
Time taken for Epoch 23:2.73 - F1: 0.5466
2026-02-12 17:00:09 - INFO - Time taken for Epoch 23:2.73 - F1: 0.5466
Time taken for Epoch 24:4.04 - F1: 0.5441
2026-02-12 17:00:13 - INFO - Time taken for Epoch 24:4.04 - F1: 0.5441
Time taken for Epoch 25:2.72 - F1: 0.5470
2026-02-12 17:00:16 - INFO - Time taken for Epoch 25:2.72 - F1: 0.5470
Time taken for Epoch 26:4.06 - F1: 0.5495
2026-02-12 17:00:20 - INFO - Time taken for Epoch 26:4.06 - F1: 0.5495
Time taken for Epoch 27:4.05 - F1: 0.5511
2026-02-12 17:00:24 - INFO - Time taken for Epoch 27:4.05 - F1: 0.5511
Time taken for Epoch 28:4.05 - F1: 0.5552
2026-02-12 17:00:28 - INFO - Time taken for Epoch 28:4.05 - F1: 0.5552
Time taken for Epoch 29:4.05 - F1: 0.5533
2026-02-12 17:00:32 - INFO - Time taken for Epoch 29:4.05 - F1: 0.5533
Time taken for Epoch 30:2.73 - F1: 0.5521
2026-02-12 17:00:35 - INFO - Time taken for Epoch 30:2.73 - F1: 0.5521
Time taken for Epoch 31:2.73 - F1: 0.5521
2026-02-12 17:00:38 - INFO - Time taken for Epoch 31:2.73 - F1: 0.5521
Time taken for Epoch 32:2.73 - F1: 0.5488
2026-02-12 17:00:40 - INFO - Time taken for Epoch 32:2.73 - F1: 0.5488
Time taken for Epoch 33:2.73 - F1: 0.5472
2026-02-12 17:00:43 - INFO - Time taken for Epoch 33:2.73 - F1: 0.5472
Time taken for Epoch 34:2.72 - F1: 0.5427
2026-02-12 17:00:46 - INFO - Time taken for Epoch 34:2.72 - F1: 0.5427
Time taken for Epoch 35:2.72 - F1: 0.5421
2026-02-12 17:00:48 - INFO - Time taken for Epoch 35:2.72 - F1: 0.5421
Time taken for Epoch 36:2.72 - F1: 0.5413
2026-02-12 17:00:51 - INFO - Time taken for Epoch 36:2.72 - F1: 0.5413
Time taken for Epoch 37:2.71 - F1: 0.5386
2026-02-12 17:00:54 - INFO - Time taken for Epoch 37:2.71 - F1: 0.5386
Time taken for Epoch 38:2.72 - F1: 0.5380
2026-02-12 17:00:57 - INFO - Time taken for Epoch 38:2.72 - F1: 0.5380
Performance not improving for 10 consecutive epochs.
2026-02-12 17:00:57 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.5552 - Best Epoch:27
2026-02-12 17:00:57 - INFO - Best F1:0.5552 - Best Epoch:27
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5496, Test ECE: 0.1103
2026-02-12 17:01:04 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5496, Test ECE: 0.1103
All results: {'f1_macro': 0.5495978550801364, 'ece': np.float64(0.11026419851685801)}
2026-02-12 17:01:04 - INFO - All results: {'f1_macro': 0.5495978550801364, 'ece': np.float64(0.11026419851685801)}

Total time taken: 494.24 seconds
2026-02-12 17:01:04 - INFO - 
Total time taken: 494.24 seconds
2026-02-12 17:01:04 - INFO - Trial 4 finished with value: 0.5495978550801364 and parameters: {'learning_rate': 0.000101023418677921, 'weight_decay': 1.2980164923272167e-05, 'batch_size': 8, 'co_train_epochs': 8, 'epoch_patience': 7}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 17:01:04 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 17:01:04 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 17:01:04 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 17:01:04 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 1.175098889341411e-05
Weight Decay: 0.00022220273561352835
Batch Size: 32
No. Epochs: 7
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-12 17:01:05 - INFO - Learning Rate: 1.175098889341411e-05
Weight Decay: 0.00022220273561352835
Batch Size: 32
No. Epochs: 7
Epoch Patience: 4
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 17:01:06 - INFO - Generating initial weights
Time taken for Epoch 1:17.83 - F1: 0.0609
2026-02-12 17:01:27 - INFO - Time taken for Epoch 1:17.83 - F1: 0.0609
Time taken for Epoch 2:17.75 - F1: 0.0689
2026-02-12 17:01:45 - INFO - Time taken for Epoch 2:17.75 - F1: 0.0689
Time taken for Epoch 3:17.81 - F1: 0.0784
2026-02-12 17:02:03 - INFO - Time taken for Epoch 3:17.81 - F1: 0.0784
Time taken for Epoch 4:17.79 - F1: 0.0890
2026-02-12 17:02:21 - INFO - Time taken for Epoch 4:17.79 - F1: 0.0890
Time taken for Epoch 5:17.80 - F1: 0.0894
2026-02-12 17:02:38 - INFO - Time taken for Epoch 5:17.80 - F1: 0.0894
Time taken for Epoch 6:17.81 - F1: 0.0952
2026-02-12 17:02:56 - INFO - Time taken for Epoch 6:17.81 - F1: 0.0952
Time taken for Epoch 7:17.87 - F1: 0.0961
2026-02-12 17:03:14 - INFO - Time taken for Epoch 7:17.87 - F1: 0.0961
Best F1:0.0961 - Best Epoch:7
2026-02-12 17:03:14 - INFO - Best F1:0.0961 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 17:03:15 - INFO - Starting co-training
Time taken for Epoch 1: 30.75s - F1: 0.25496310
2026-02-12 17:03:46 - INFO - Time taken for Epoch 1: 30.75s - F1: 0.25496310
Time taken for Epoch 2: 31.98s - F1: 0.50661851
2026-02-12 17:04:18 - INFO - Time taken for Epoch 2: 31.98s - F1: 0.50661851
Time taken for Epoch 3: 32.07s - F1: 0.59243309
2026-02-12 17:04:50 - INFO - Time taken for Epoch 3: 32.07s - F1: 0.59243309
Time taken for Epoch 4: 32.14s - F1: 0.59633413
2026-02-12 17:05:23 - INFO - Time taken for Epoch 4: 32.14s - F1: 0.59633413
Time taken for Epoch 5: 32.28s - F1: 0.61037157
2026-02-12 17:05:55 - INFO - Time taken for Epoch 5: 32.28s - F1: 0.61037157
Time taken for Epoch 6: 32.33s - F1: 0.61651545
2026-02-12 17:06:27 - INFO - Time taken for Epoch 6: 32.33s - F1: 0.61651545
Time taken for Epoch 7: 32.08s - F1: 0.62150079
2026-02-12 17:06:59 - INFO - Time taken for Epoch 7: 32.08s - F1: 0.62150079
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 17:07:03 - INFO - Fine-tuning models
Time taken for Epoch 1:2.49 - F1: 0.6105
2026-02-12 17:07:06 - INFO - Time taken for Epoch 1:2.49 - F1: 0.6105
Time taken for Epoch 2:3.67 - F1: 0.6048
2026-02-12 17:07:10 - INFO - Time taken for Epoch 2:3.67 - F1: 0.6048
Time taken for Epoch 3:2.49 - F1: 0.6035
2026-02-12 17:07:12 - INFO - Time taken for Epoch 3:2.49 - F1: 0.6035
Time taken for Epoch 4:2.48 - F1: 0.6017
2026-02-12 17:07:15 - INFO - Time taken for Epoch 4:2.48 - F1: 0.6017
Time taken for Epoch 5:2.48 - F1: 0.6071
2026-02-12 17:07:17 - INFO - Time taken for Epoch 5:2.48 - F1: 0.6071
Time taken for Epoch 6:2.48 - F1: 0.6069
2026-02-12 17:07:20 - INFO - Time taken for Epoch 6:2.48 - F1: 0.6069
Time taken for Epoch 7:2.49 - F1: 0.6095
2026-02-12 17:07:22 - INFO - Time taken for Epoch 7:2.49 - F1: 0.6095
Time taken for Epoch 8:2.48 - F1: 0.6115
2026-02-12 17:07:25 - INFO - Time taken for Epoch 8:2.48 - F1: 0.6115
Time taken for Epoch 9:3.77 - F1: 0.6073
2026-02-12 17:07:28 - INFO - Time taken for Epoch 9:3.77 - F1: 0.6073
Time taken for Epoch 10:2.48 - F1: 0.6120
2026-02-12 17:07:31 - INFO - Time taken for Epoch 10:2.48 - F1: 0.6120
Time taken for Epoch 11:3.78 - F1: 0.6128
2026-02-12 17:07:35 - INFO - Time taken for Epoch 11:3.78 - F1: 0.6128
Time taken for Epoch 12:3.78 - F1: 0.6082
2026-02-12 17:07:38 - INFO - Time taken for Epoch 12:3.78 - F1: 0.6082
Time taken for Epoch 13:2.47 - F1: 0.6163
2026-02-12 17:07:41 - INFO - Time taken for Epoch 13:2.47 - F1: 0.6163
Time taken for Epoch 14:3.76 - F1: 0.6176
2026-02-12 17:07:45 - INFO - Time taken for Epoch 14:3.76 - F1: 0.6176
Time taken for Epoch 15:3.77 - F1: 0.6210
2026-02-12 17:07:48 - INFO - Time taken for Epoch 15:3.77 - F1: 0.6210
Time taken for Epoch 16:3.77 - F1: 0.6212
2026-02-12 17:07:52 - INFO - Time taken for Epoch 16:3.77 - F1: 0.6212
Time taken for Epoch 17:3.84 - F1: 0.6213
2026-02-12 17:07:56 - INFO - Time taken for Epoch 17:3.84 - F1: 0.6213
Time taken for Epoch 18:4.14 - F1: 0.6213
2026-02-12 17:08:00 - INFO - Time taken for Epoch 18:4.14 - F1: 0.6213
Time taken for Epoch 19:4.32 - F1: 0.6245
2026-02-12 17:08:04 - INFO - Time taken for Epoch 19:4.32 - F1: 0.6245
Time taken for Epoch 20:4.79 - F1: 0.6266
2026-02-12 17:08:09 - INFO - Time taken for Epoch 20:4.79 - F1: 0.6266
Time taken for Epoch 21:3.76 - F1: 0.6273
2026-02-12 17:08:13 - INFO - Time taken for Epoch 21:3.76 - F1: 0.6273
Time taken for Epoch 22:3.78 - F1: 0.6259
2026-02-12 17:08:17 - INFO - Time taken for Epoch 22:3.78 - F1: 0.6259
Time taken for Epoch 23:2.46 - F1: 0.6255
2026-02-12 17:08:19 - INFO - Time taken for Epoch 23:2.46 - F1: 0.6255
Time taken for Epoch 24:2.46 - F1: 0.6254
2026-02-12 17:08:22 - INFO - Time taken for Epoch 24:2.46 - F1: 0.6254
Time taken for Epoch 25:2.48 - F1: 0.6246
2026-02-12 17:08:24 - INFO - Time taken for Epoch 25:2.48 - F1: 0.6246
Time taken for Epoch 26:2.47 - F1: 0.6277
2026-02-12 17:08:27 - INFO - Time taken for Epoch 26:2.47 - F1: 0.6277
Time taken for Epoch 27:3.77 - F1: 0.6308
2026-02-12 17:08:30 - INFO - Time taken for Epoch 27:3.77 - F1: 0.6308
Time taken for Epoch 28:3.77 - F1: 0.6296
2026-02-12 17:08:34 - INFO - Time taken for Epoch 28:3.77 - F1: 0.6296
Time taken for Epoch 29:2.46 - F1: 0.6368
2026-02-12 17:08:37 - INFO - Time taken for Epoch 29:2.46 - F1: 0.6368
Time taken for Epoch 30:3.74 - F1: 0.6370
2026-02-12 17:08:40 - INFO - Time taken for Epoch 30:3.74 - F1: 0.6370
Time taken for Epoch 31:3.73 - F1: 0.6333
2026-02-12 17:08:44 - INFO - Time taken for Epoch 31:3.73 - F1: 0.6333
Time taken for Epoch 32:2.46 - F1: 0.6352
2026-02-12 17:08:47 - INFO - Time taken for Epoch 32:2.46 - F1: 0.6352
Time taken for Epoch 33:2.46 - F1: 0.6359
2026-02-12 17:08:49 - INFO - Time taken for Epoch 33:2.46 - F1: 0.6359
Time taken for Epoch 34:2.48 - F1: 0.6386
2026-02-12 17:08:51 - INFO - Time taken for Epoch 34:2.48 - F1: 0.6386
Time taken for Epoch 35:3.79 - F1: 0.6362
2026-02-12 17:08:55 - INFO - Time taken for Epoch 35:3.79 - F1: 0.6362
Time taken for Epoch 36:2.47 - F1: 0.6338
2026-02-12 17:08:58 - INFO - Time taken for Epoch 36:2.47 - F1: 0.6338
Time taken for Epoch 37:2.47 - F1: 0.6354
2026-02-12 17:09:00 - INFO - Time taken for Epoch 37:2.47 - F1: 0.6354
Time taken for Epoch 38:2.47 - F1: 0.6359
2026-02-12 17:09:03 - INFO - Time taken for Epoch 38:2.47 - F1: 0.6359
Time taken for Epoch 39:2.47 - F1: 0.6362
2026-02-12 17:09:05 - INFO - Time taken for Epoch 39:2.47 - F1: 0.6362
Time taken for Epoch 40:2.47 - F1: 0.6370
2026-02-12 17:09:08 - INFO - Time taken for Epoch 40:2.47 - F1: 0.6370
Time taken for Epoch 41:2.48 - F1: 0.6376
2026-02-12 17:09:10 - INFO - Time taken for Epoch 41:2.48 - F1: 0.6376
Time taken for Epoch 42:2.47 - F1: 0.6354
2026-02-12 17:09:13 - INFO - Time taken for Epoch 42:2.47 - F1: 0.6354
Time taken for Epoch 43:2.48 - F1: 0.6350
2026-02-12 17:09:15 - INFO - Time taken for Epoch 43:2.48 - F1: 0.6350
Time taken for Epoch 44:2.48 - F1: 0.6361
2026-02-12 17:09:18 - INFO - Time taken for Epoch 44:2.48 - F1: 0.6361
Performance not improving for 10 consecutive epochs.
2026-02-12 17:09:18 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6386 - Best Epoch:33
2026-02-12 17:09:18 - INFO - Best F1:0.6386 - Best Epoch:33
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6560, Test ECE: 0.0652
2026-02-12 17:09:25 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6560, Test ECE: 0.0652
All results: {'f1_macro': 0.6559845388889841, 'ece': np.float64(0.06522913525702011)}
2026-02-12 17:09:25 - INFO - All results: {'f1_macro': 0.6559845388889841, 'ece': np.float64(0.06522913525702011)}

Total time taken: 500.82 seconds
2026-02-12 17:09:25 - INFO - 
Total time taken: 500.82 seconds
2026-02-12 17:09:25 - INFO - Trial 5 finished with value: 0.6559845388889841 and parameters: {'learning_rate': 1.175098889341411e-05, 'weight_decay': 0.00022220273561352835, 'batch_size': 32, 'co_train_epochs': 7, 'epoch_patience': 4}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 17:09:25 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 17:09:25 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 17:09:25 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 17:09:25 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.00023785349100988046
Weight Decay: 0.0005559862087440526
Batch Size: 32
No. Epochs: 6
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-12 17:09:26 - INFO - Learning Rate: 0.00023785349100988046
Weight Decay: 0.0005559862087440526
Batch Size: 32
No. Epochs: 6
Epoch Patience: 6
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 17:09:27 - INFO - Generating initial weights
Time taken for Epoch 1:17.75 - F1: 0.1456
2026-02-12 17:09:48 - INFO - Time taken for Epoch 1:17.75 - F1: 0.1456
Time taken for Epoch 2:17.76 - F1: 0.2364
2026-02-12 17:10:06 - INFO - Time taken for Epoch 2:17.76 - F1: 0.2364
Time taken for Epoch 3:17.76 - F1: 0.2432
2026-02-12 17:10:23 - INFO - Time taken for Epoch 3:17.76 - F1: 0.2432
Time taken for Epoch 4:17.77 - F1: 0.2866
2026-02-12 17:10:41 - INFO - Time taken for Epoch 4:17.77 - F1: 0.2866
Time taken for Epoch 5:17.83 - F1: 0.2919
2026-02-12 17:10:59 - INFO - Time taken for Epoch 5:17.83 - F1: 0.2919
Time taken for Epoch 6:17.79 - F1: 0.3067
2026-02-12 17:11:17 - INFO - Time taken for Epoch 6:17.79 - F1: 0.3067
Best F1:0.3067 - Best Epoch:6
2026-02-12 17:11:17 - INFO - Best F1:0.3067 - Best Epoch:6
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 17:11:18 - INFO - Starting co-training
Time taken for Epoch 1: 30.75s - F1: 0.59901196
2026-02-12 17:11:49 - INFO - Time taken for Epoch 1: 30.75s - F1: 0.59901196
Time taken for Epoch 2: 31.86s - F1: 0.55921165
2026-02-12 17:12:21 - INFO - Time taken for Epoch 2: 31.86s - F1: 0.55921165
Time taken for Epoch 3: 30.79s - F1: 0.57183072
2026-02-12 17:12:52 - INFO - Time taken for Epoch 3: 30.79s - F1: 0.57183072
Time taken for Epoch 4: 30.82s - F1: 0.58252464
2026-02-12 17:13:23 - INFO - Time taken for Epoch 4: 30.82s - F1: 0.58252464
Time taken for Epoch 5: 30.82s - F1: 0.59112313
2026-02-12 17:13:53 - INFO - Time taken for Epoch 5: 30.82s - F1: 0.59112313
Time taken for Epoch 6: 30.85s - F1: 0.52563268
2026-02-12 17:14:24 - INFO - Time taken for Epoch 6: 30.85s - F1: 0.52563268
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 17:14:27 - INFO - Fine-tuning models
Time taken for Epoch 1:2.50 - F1: 0.3453
2026-02-12 17:14:29 - INFO - Time taken for Epoch 1:2.50 - F1: 0.3453
Time taken for Epoch 2:3.65 - F1: 0.4921
2026-02-12 17:14:33 - INFO - Time taken for Epoch 2:3.65 - F1: 0.4921
Time taken for Epoch 3:3.68 - F1: 0.4124
2026-02-12 17:14:37 - INFO - Time taken for Epoch 3:3.68 - F1: 0.4124
Time taken for Epoch 4:2.49 - F1: 0.4540
2026-02-12 17:14:39 - INFO - Time taken for Epoch 4:2.49 - F1: 0.4540
Time taken for Epoch 5:2.49 - F1: 0.4798
2026-02-12 17:14:42 - INFO - Time taken for Epoch 5:2.49 - F1: 0.4798
Time taken for Epoch 6:2.49 - F1: 0.5070
2026-02-12 17:14:44 - INFO - Time taken for Epoch 6:2.49 - F1: 0.5070
Time taken for Epoch 7:3.76 - F1: 0.4403
2026-02-12 17:14:48 - INFO - Time taken for Epoch 7:3.76 - F1: 0.4403
Time taken for Epoch 8:2.49 - F1: 0.4740
2026-02-12 17:14:51 - INFO - Time taken for Epoch 8:2.49 - F1: 0.4740
Time taken for Epoch 9:2.49 - F1: 0.4789
2026-02-12 17:14:53 - INFO - Time taken for Epoch 9:2.49 - F1: 0.4789
Time taken for Epoch 10:2.49 - F1: 0.5241
2026-02-12 17:14:56 - INFO - Time taken for Epoch 10:2.49 - F1: 0.5241
Time taken for Epoch 11:3.88 - F1: 0.5366
2026-02-12 17:14:59 - INFO - Time taken for Epoch 11:3.88 - F1: 0.5366
Time taken for Epoch 12:3.65 - F1: 0.5632
2026-02-12 17:15:03 - INFO - Time taken for Epoch 12:3.65 - F1: 0.5632
Time taken for Epoch 13:3.67 - F1: 0.5614
2026-02-12 17:15:07 - INFO - Time taken for Epoch 13:3.67 - F1: 0.5614
Time taken for Epoch 14:2.49 - F1: 0.5445
2026-02-12 17:15:09 - INFO - Time taken for Epoch 14:2.49 - F1: 0.5445
Time taken for Epoch 15:2.49 - F1: 0.4862
2026-02-12 17:15:12 - INFO - Time taken for Epoch 15:2.49 - F1: 0.4862
Time taken for Epoch 16:2.48 - F1: 0.4794
2026-02-12 17:15:14 - INFO - Time taken for Epoch 16:2.48 - F1: 0.4794
Time taken for Epoch 17:2.48 - F1: 0.4977
2026-02-12 17:15:17 - INFO - Time taken for Epoch 17:2.48 - F1: 0.4977
Time taken for Epoch 18:2.49 - F1: 0.4944
2026-02-12 17:15:19 - INFO - Time taken for Epoch 18:2.49 - F1: 0.4944
Time taken for Epoch 19:2.48 - F1: 0.4958
2026-02-12 17:15:22 - INFO - Time taken for Epoch 19:2.48 - F1: 0.4958
Time taken for Epoch 20:2.49 - F1: 0.4930
2026-02-12 17:15:24 - INFO - Time taken for Epoch 20:2.49 - F1: 0.4930
Time taken for Epoch 21:2.49 - F1: 0.4950
2026-02-12 17:15:27 - INFO - Time taken for Epoch 21:2.49 - F1: 0.4950
Time taken for Epoch 22:2.49 - F1: 0.5032
2026-02-12 17:15:29 - INFO - Time taken for Epoch 22:2.49 - F1: 0.5032
Performance not improving for 10 consecutive epochs.
2026-02-12 17:15:29 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.5632 - Best Epoch:11
2026-02-12 17:15:29 - INFO - Best F1:0.5632 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5845, Test ECE: 0.1156
2026-02-12 17:15:36 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5845, Test ECE: 0.1156
All results: {'f1_macro': 0.5844993409768144, 'ece': np.float64(0.11561909498081008)}
2026-02-12 17:15:36 - INFO - All results: {'f1_macro': 0.5844993409768144, 'ece': np.float64(0.11561909498081008)}

Total time taken: 371.25 seconds
2026-02-12 17:15:36 - INFO - 
Total time taken: 371.25 seconds
2026-02-12 17:15:36 - INFO - Trial 6 finished with value: 0.5844993409768144 and parameters: {'learning_rate': 0.00023785349100988046, 'weight_decay': 0.0005559862087440526, 'batch_size': 32, 'co_train_epochs': 6, 'epoch_patience': 6}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 17:15:36 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 17:15:36 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 17:15:36 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 17:15:36 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0005112174851761188
Weight Decay: 0.0014836469851733157
Batch Size: 8
No. Epochs: 12
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-12 17:15:37 - INFO - Learning Rate: 0.0005112174851761188
Weight Decay: 0.0014836469851733157
Batch Size: 8
No. Epochs: 12
Epoch Patience: 4
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 17:15:38 - INFO - Generating initial weights
Time taken for Epoch 1:19.70 - F1: 0.0155
2026-02-12 17:16:02 - INFO - Time taken for Epoch 1:19.70 - F1: 0.0155
Time taken for Epoch 2:19.64 - F1: 0.0155
2026-02-12 17:16:21 - INFO - Time taken for Epoch 2:19.64 - F1: 0.0155
Time taken for Epoch 3:19.67 - F1: 0.0493
2026-02-12 17:16:41 - INFO - Time taken for Epoch 3:19.67 - F1: 0.0493
Time taken for Epoch 4:19.63 - F1: 0.1099
2026-02-12 17:17:00 - INFO - Time taken for Epoch 4:19.63 - F1: 0.1099
Time taken for Epoch 5:19.61 - F1: 0.1680
2026-02-12 17:17:20 - INFO - Time taken for Epoch 5:19.61 - F1: 0.1680
Time taken for Epoch 6:19.71 - F1: 0.2563
2026-02-12 17:17:40 - INFO - Time taken for Epoch 6:19.71 - F1: 0.2563
Time taken for Epoch 7:19.65 - F1: 0.2587
2026-02-12 17:17:59 - INFO - Time taken for Epoch 7:19.65 - F1: 0.2587
Time taken for Epoch 8:19.64 - F1: 0.2415
2026-02-12 17:18:19 - INFO - Time taken for Epoch 8:19.64 - F1: 0.2415
Time taken for Epoch 9:19.66 - F1: 0.3031
2026-02-12 17:18:39 - INFO - Time taken for Epoch 9:19.66 - F1: 0.3031
Time taken for Epoch 10:19.64 - F1: 0.2904
2026-02-12 17:18:58 - INFO - Time taken for Epoch 10:19.64 - F1: 0.2904
Time taken for Epoch 11:19.65 - F1: 0.2659
2026-02-12 17:19:18 - INFO - Time taken for Epoch 11:19.65 - F1: 0.2659
Time taken for Epoch 12:19.61 - F1: 0.2605
2026-02-12 17:19:38 - INFO - Time taken for Epoch 12:19.61 - F1: 0.2605
Best F1:0.3031 - Best Epoch:9
2026-02-12 17:19:38 - INFO - Best F1:0.3031 - Best Epoch:9
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 17:19:39 - INFO - Starting co-training
Time taken for Epoch 1: 24.18s - F1: 0.03852235
2026-02-12 17:20:03 - INFO - Time taken for Epoch 1: 24.18s - F1: 0.03852235
Time taken for Epoch 2: 25.19s - F1: 0.03852235
2026-02-12 17:20:29 - INFO - Time taken for Epoch 2: 25.19s - F1: 0.03852235
Time taken for Epoch 3: 23.90s - F1: 0.03212851
2026-02-12 17:20:53 - INFO - Time taken for Epoch 3: 23.90s - F1: 0.03212851
Time taken for Epoch 4: 24.00s - F1: 0.04247539
2026-02-12 17:21:17 - INFO - Time taken for Epoch 4: 24.00s - F1: 0.04247539
Time taken for Epoch 5: 25.26s - F1: 0.04247539
2026-02-12 17:21:42 - INFO - Time taken for Epoch 5: 25.26s - F1: 0.04247539
Time taken for Epoch 6: 24.05s - F1: 0.04247539
2026-02-12 17:22:06 - INFO - Time taken for Epoch 6: 24.05s - F1: 0.04247539
Time taken for Epoch 7: 24.08s - F1: 0.04247539
2026-02-12 17:22:30 - INFO - Time taken for Epoch 7: 24.08s - F1: 0.04247539
Time taken for Epoch 8: 24.13s - F1: 0.04247539
2026-02-12 17:22:54 - INFO - Time taken for Epoch 8: 24.13s - F1: 0.04247539
Performance not improving for 4 consecutive epochs.
Performance not improving for 4 consecutive epochs.
2026-02-12 17:22:54 - INFO - Performance not improving for 4 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 17:22:56 - INFO - Fine-tuning models
Time taken for Epoch 1:2.75 - F1: 0.0425
2026-02-12 17:22:59 - INFO - Time taken for Epoch 1:2.75 - F1: 0.0425
Time taken for Epoch 2:3.79 - F1: 0.0425
2026-02-12 17:23:03 - INFO - Time taken for Epoch 2:3.79 - F1: 0.0425
Time taken for Epoch 3:2.74 - F1: 0.0425
2026-02-12 17:23:06 - INFO - Time taken for Epoch 3:2.74 - F1: 0.0425
Time taken for Epoch 4:2.74 - F1: 0.0017
2026-02-12 17:23:09 - INFO - Time taken for Epoch 4:2.74 - F1: 0.0017
Time taken for Epoch 5:2.74 - F1: 0.0017
2026-02-12 17:23:11 - INFO - Time taken for Epoch 5:2.74 - F1: 0.0017
Time taken for Epoch 6:2.74 - F1: 0.0100
2026-02-12 17:23:14 - INFO - Time taken for Epoch 6:2.74 - F1: 0.0100
Time taken for Epoch 7:2.74 - F1: 0.0100
2026-02-12 17:23:17 - INFO - Time taken for Epoch 7:2.74 - F1: 0.0100
Time taken for Epoch 8:2.74 - F1: 0.0155
2026-02-12 17:23:20 - INFO - Time taken for Epoch 8:2.74 - F1: 0.0155
Time taken for Epoch 9:2.74 - F1: 0.0155
2026-02-12 17:23:22 - INFO - Time taken for Epoch 9:2.74 - F1: 0.0155
Time taken for Epoch 10:2.73 - F1: 0.0155
2026-02-12 17:23:25 - INFO - Time taken for Epoch 10:2.73 - F1: 0.0155
Time taken for Epoch 11:2.74 - F1: 0.0155
2026-02-12 17:23:28 - INFO - Time taken for Epoch 11:2.74 - F1: 0.0155
Performance not improving for 10 consecutive epochs.
2026-02-12 17:23:28 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-12 17:23:28 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.4147
2026-02-12 17:23:35 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.4147
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.4146615626541094)}
2026-02-12 17:23:35 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.4146615626541094)}

Total time taken: 478.83 seconds
2026-02-12 17:23:35 - INFO - 
Total time taken: 478.83 seconds
2026-02-12 17:23:35 - INFO - Trial 7 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.0005112174851761188, 'weight_decay': 0.0014836469851733157, 'batch_size': 8, 'co_train_epochs': 12, 'epoch_patience': 4}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 17:23:35 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 17:23:35 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 17:23:35 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 17:23:35 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0006627051570686611
Weight Decay: 0.006437068770626472
Batch Size: 32
No. Epochs: 13
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-12 17:23:36 - INFO - Learning Rate: 0.0006627051570686611
Weight Decay: 0.006437068770626472
Batch Size: 32
No. Epochs: 13
Epoch Patience: 7
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 17:23:37 - INFO - Generating initial weights
Time taken for Epoch 1:17.84 - F1: 0.1250
2026-02-12 17:23:58 - INFO - Time taken for Epoch 1:17.84 - F1: 0.1250
Time taken for Epoch 2:17.72 - F1: 0.0621
2026-02-12 17:24:16 - INFO - Time taken for Epoch 2:17.72 - F1: 0.0621
Time taken for Epoch 3:17.72 - F1: 0.0322
2026-02-12 17:24:34 - INFO - Time taken for Epoch 3:17.72 - F1: 0.0322
Time taken for Epoch 4:17.72 - F1: 0.0440
2026-02-12 17:24:51 - INFO - Time taken for Epoch 4:17.72 - F1: 0.0440
Time taken for Epoch 5:17.72 - F1: 0.0427
2026-02-12 17:25:09 - INFO - Time taken for Epoch 5:17.72 - F1: 0.0427
Time taken for Epoch 6:17.71 - F1: 0.0639
2026-02-12 17:25:27 - INFO - Time taken for Epoch 6:17.71 - F1: 0.0639
Time taken for Epoch 7:17.73 - F1: 0.0463
2026-02-12 17:25:45 - INFO - Time taken for Epoch 7:17.73 - F1: 0.0463
Time taken for Epoch 8:17.72 - F1: 0.0422
2026-02-12 17:26:02 - INFO - Time taken for Epoch 8:17.72 - F1: 0.0422
Time taken for Epoch 9:17.73 - F1: 0.0249
2026-02-12 17:26:20 - INFO - Time taken for Epoch 9:17.73 - F1: 0.0249
Time taken for Epoch 10:17.72 - F1: 0.0275
2026-02-12 17:26:38 - INFO - Time taken for Epoch 10:17.72 - F1: 0.0275
Time taken for Epoch 11:17.72 - F1: 0.0395
2026-02-12 17:26:55 - INFO - Time taken for Epoch 11:17.72 - F1: 0.0395
Time taken for Epoch 12:17.72 - F1: 0.0376
2026-02-12 17:27:13 - INFO - Time taken for Epoch 12:17.72 - F1: 0.0376
Time taken for Epoch 13:17.73 - F1: 0.0510
2026-02-12 17:27:31 - INFO - Time taken for Epoch 13:17.73 - F1: 0.0510
Best F1:0.1250 - Best Epoch:1
2026-02-12 17:27:31 - INFO - Best F1:0.1250 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 17:27:32 - INFO - Starting co-training
Time taken for Epoch 1: 30.77s - F1: 0.03212851
2026-02-12 17:28:03 - INFO - Time taken for Epoch 1: 30.77s - F1: 0.03212851
Time taken for Epoch 2: 31.86s - F1: 0.03212851
2026-02-12 17:28:35 - INFO - Time taken for Epoch 2: 31.86s - F1: 0.03212851
Time taken for Epoch 3: 30.78s - F1: 0.04247539
2026-02-12 17:29:06 - INFO - Time taken for Epoch 3: 30.78s - F1: 0.04247539
Time taken for Epoch 4: 31.94s - F1: 0.04247539
2026-02-12 17:29:38 - INFO - Time taken for Epoch 4: 31.94s - F1: 0.04247539
Time taken for Epoch 5: 30.80s - F1: 0.04247539
2026-02-12 17:30:09 - INFO - Time taken for Epoch 5: 30.80s - F1: 0.04247539
Time taken for Epoch 6: 30.80s - F1: 0.04247539
2026-02-12 17:30:39 - INFO - Time taken for Epoch 6: 30.80s - F1: 0.04247539
Time taken for Epoch 7: 30.79s - F1: 0.04247539
2026-02-12 17:31:10 - INFO - Time taken for Epoch 7: 30.79s - F1: 0.04247539
Time taken for Epoch 8: 30.80s - F1: 0.04247539
2026-02-12 17:31:41 - INFO - Time taken for Epoch 8: 30.80s - F1: 0.04247539
Time taken for Epoch 9: 30.79s - F1: 0.04247539
2026-02-12 17:32:12 - INFO - Time taken for Epoch 9: 30.79s - F1: 0.04247539
Time taken for Epoch 10: 30.80s - F1: 0.04247539
2026-02-12 17:32:43 - INFO - Time taken for Epoch 10: 30.80s - F1: 0.04247539
Performance not improving for 7 consecutive epochs.
Performance not improving for 7 consecutive epochs.
2026-02-12 17:32:43 - INFO - Performance not improving for 7 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 17:32:45 - INFO - Fine-tuning models
Time taken for Epoch 1:2.47 - F1: 0.0425
2026-02-12 17:32:48 - INFO - Time taken for Epoch 1:2.47 - F1: 0.0425
Time taken for Epoch 2:3.83 - F1: 0.0321
2026-02-12 17:32:52 - INFO - Time taken for Epoch 2:3.83 - F1: 0.0321
Time taken for Epoch 3:2.47 - F1: 0.0205
2026-02-12 17:32:54 - INFO - Time taken for Epoch 3:2.47 - F1: 0.0205
Time taken for Epoch 4:2.47 - F1: 0.0017
2026-02-12 17:32:57 - INFO - Time taken for Epoch 4:2.47 - F1: 0.0017
Time taken for Epoch 5:2.47 - F1: 0.0017
2026-02-12 17:32:59 - INFO - Time taken for Epoch 5:2.47 - F1: 0.0017
Time taken for Epoch 6:2.48 - F1: 0.0155
2026-02-12 17:33:02 - INFO - Time taken for Epoch 6:2.48 - F1: 0.0155
Time taken for Epoch 7:2.47 - F1: 0.0155
2026-02-12 17:33:04 - INFO - Time taken for Epoch 7:2.47 - F1: 0.0155
Time taken for Epoch 8:2.47 - F1: 0.0155
2026-02-12 17:33:07 - INFO - Time taken for Epoch 8:2.47 - F1: 0.0155
Time taken for Epoch 9:2.47 - F1: 0.0155
2026-02-12 17:33:09 - INFO - Time taken for Epoch 9:2.47 - F1: 0.0155
Time taken for Epoch 10:2.47 - F1: 0.0155
2026-02-12 17:33:12 - INFO - Time taken for Epoch 10:2.47 - F1: 0.0155
Time taken for Epoch 11:2.48 - F1: 0.0425
2026-02-12 17:33:14 - INFO - Time taken for Epoch 11:2.48 - F1: 0.0425
Performance not improving for 10 consecutive epochs.
2026-02-12 17:33:14 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-12 17:33:14 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2499
2026-02-12 17:33:21 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2499
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.24985856334112233)}
2026-02-12 17:33:21 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.24985856334112233)}

Total time taken: 586.22 seconds
2026-02-12 17:33:21 - INFO - 
Total time taken: 586.22 seconds
2026-02-12 17:33:21 - INFO - Trial 8 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.0006627051570686611, 'weight_decay': 0.006437068770626472, 'batch_size': 32, 'co_train_epochs': 13, 'epoch_patience': 7}. Best is trial 0 with value: 0.6665873635903812.
Using devices: cuda, cuda
2026-02-12 17:33:21 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 17:33:21 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 17:33:21 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 17:33:21 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 1.6398966978584835e-05
Weight Decay: 0.006124807115472876
Batch Size: 64
No. Epochs: 16
Epoch Patience: 8
 Accumulation Steps: 1
2026-02-12 17:33:22 - INFO - Learning Rate: 1.6398966978584835e-05
Weight Decay: 0.006124807115472876
Batch Size: 64
No. Epochs: 16
Epoch Patience: 8
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 17:33:23 - INFO - Generating initial weights
Time taken for Epoch 1:16.96 - F1: 0.0605
2026-02-12 17:33:43 - INFO - Time taken for Epoch 1:16.96 - F1: 0.0605
Time taken for Epoch 2:16.87 - F1: 0.0750
2026-02-12 17:34:00 - INFO - Time taken for Epoch 2:16.87 - F1: 0.0750
Time taken for Epoch 3:16.88 - F1: 0.0942
2026-02-12 17:34:17 - INFO - Time taken for Epoch 3:16.88 - F1: 0.0942
Time taken for Epoch 4:16.90 - F1: 0.1048
2026-02-12 17:34:34 - INFO - Time taken for Epoch 4:16.90 - F1: 0.1048
Time taken for Epoch 5:16.90 - F1: 0.1227
2026-02-12 17:34:51 - INFO - Time taken for Epoch 5:16.90 - F1: 0.1227
Time taken for Epoch 6:16.90 - F1: 0.1335
2026-02-12 17:35:08 - INFO - Time taken for Epoch 6:16.90 - F1: 0.1335
Time taken for Epoch 7:16.90 - F1: 0.1601
2026-02-12 17:35:25 - INFO - Time taken for Epoch 7:16.90 - F1: 0.1601
Time taken for Epoch 8:16.90 - F1: 0.1841
2026-02-12 17:35:42 - INFO - Time taken for Epoch 8:16.90 - F1: 0.1841
Time taken for Epoch 9:16.91 - F1: 0.1981
2026-02-12 17:35:59 - INFO - Time taken for Epoch 9:16.91 - F1: 0.1981
Time taken for Epoch 10:16.93 - F1: 0.2021
2026-02-12 17:36:16 - INFO - Time taken for Epoch 10:16.93 - F1: 0.2021
Time taken for Epoch 11:16.93 - F1: 0.2096
2026-02-12 17:36:33 - INFO - Time taken for Epoch 11:16.93 - F1: 0.2096
Time taken for Epoch 12:16.94 - F1: 0.2166
2026-02-12 17:36:49 - INFO - Time taken for Epoch 12:16.94 - F1: 0.2166
Time taken for Epoch 13:16.93 - F1: 0.2279
2026-02-12 17:37:06 - INFO - Time taken for Epoch 13:16.93 - F1: 0.2279
Time taken for Epoch 14:16.95 - F1: 0.2334
2026-02-12 17:37:23 - INFO - Time taken for Epoch 14:16.95 - F1: 0.2334
Time taken for Epoch 15:16.93 - F1: 0.2394
2026-02-12 17:37:40 - INFO - Time taken for Epoch 15:16.93 - F1: 0.2394
Time taken for Epoch 16:16.91 - F1: 0.2407
2026-02-12 17:37:57 - INFO - Time taken for Epoch 16:16.91 - F1: 0.2407
Best F1:0.2407 - Best Epoch:16
2026-02-12 17:37:57 - INFO - Best F1:0.2407 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 17:37:58 - INFO - Starting co-training
Time taken for Epoch 1: 40.25s - F1: 0.51330845
2026-02-12 17:38:39 - INFO - Time taken for Epoch 1: 40.25s - F1: 0.51330845
Time taken for Epoch 2: 41.45s - F1: 0.58192230
2026-02-12 17:39:21 - INFO - Time taken for Epoch 2: 41.45s - F1: 0.58192230
Time taken for Epoch 3: 41.60s - F1: 0.60303215
2026-02-12 17:40:02 - INFO - Time taken for Epoch 3: 41.60s - F1: 0.60303215
Time taken for Epoch 4: 41.60s - F1: 0.60273470
2026-02-12 17:40:44 - INFO - Time taken for Epoch 4: 41.60s - F1: 0.60273470
Time taken for Epoch 5: 40.37s - F1: 0.62341596
2026-02-12 17:41:24 - INFO - Time taken for Epoch 5: 40.37s - F1: 0.62341596
Time taken for Epoch 6: 41.56s - F1: 0.62357831
2026-02-12 17:42:06 - INFO - Time taken for Epoch 6: 41.56s - F1: 0.62357831
Time taken for Epoch 7: 41.61s - F1: 0.64602503
2026-02-12 17:42:47 - INFO - Time taken for Epoch 7: 41.61s - F1: 0.64602503
Time taken for Epoch 8: 41.58s - F1: 0.65536194
2026-02-12 17:43:29 - INFO - Time taken for Epoch 8: 41.58s - F1: 0.65536194
Time taken for Epoch 9: 41.61s - F1: 0.64066785
2026-02-12 17:44:10 - INFO - Time taken for Epoch 9: 41.61s - F1: 0.64066785
Time taken for Epoch 10: 40.33s - F1: 0.63928480
2026-02-12 17:44:51 - INFO - Time taken for Epoch 10: 40.33s - F1: 0.63928480
Time taken for Epoch 11: 40.37s - F1: 0.62740434
2026-02-12 17:45:31 - INFO - Time taken for Epoch 11: 40.37s - F1: 0.62740434
Time taken for Epoch 12: 40.37s - F1: 0.65287360
2026-02-12 17:46:12 - INFO - Time taken for Epoch 12: 40.37s - F1: 0.65287360
Time taken for Epoch 13: 40.34s - F1: 0.64359147
2026-02-12 17:46:52 - INFO - Time taken for Epoch 13: 40.34s - F1: 0.64359147
Time taken for Epoch 14: 40.35s - F1: 0.63636540
2026-02-12 17:47:32 - INFO - Time taken for Epoch 14: 40.35s - F1: 0.63636540
Time taken for Epoch 15: 40.33s - F1: 0.64653802
2026-02-12 17:48:13 - INFO - Time taken for Epoch 15: 40.33s - F1: 0.64653802
Time taken for Epoch 16: 40.31s - F1: 0.64819891
2026-02-12 17:48:53 - INFO - Time taken for Epoch 16: 40.31s - F1: 0.64819891
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 17:48:56 - INFO - Fine-tuning models
Time taken for Epoch 1:2.36 - F1: 0.6490
2026-02-12 17:48:58 - INFO - Time taken for Epoch 1:2.36 - F1: 0.6490
Time taken for Epoch 2:3.57 - F1: 0.6501
2026-02-12 17:49:02 - INFO - Time taken for Epoch 2:3.57 - F1: 0.6501
Time taken for Epoch 3:3.67 - F1: 0.6533
2026-02-12 17:49:06 - INFO - Time taken for Epoch 3:3.67 - F1: 0.6533
Time taken for Epoch 4:3.66 - F1: 0.6494
2026-02-12 17:49:09 - INFO - Time taken for Epoch 4:3.66 - F1: 0.6494
Time taken for Epoch 5:2.35 - F1: 0.6474
2026-02-12 17:49:12 - INFO - Time taken for Epoch 5:2.35 - F1: 0.6474
Time taken for Epoch 6:2.35 - F1: 0.6483
2026-02-12 17:49:14 - INFO - Time taken for Epoch 6:2.35 - F1: 0.6483
Time taken for Epoch 7:2.36 - F1: 0.6511
2026-02-12 17:49:16 - INFO - Time taken for Epoch 7:2.36 - F1: 0.6511
Time taken for Epoch 8:2.35 - F1: 0.6529
2026-02-12 17:49:19 - INFO - Time taken for Epoch 8:2.35 - F1: 0.6529
Time taken for Epoch 9:2.35 - F1: 0.6538
2026-02-12 17:49:21 - INFO - Time taken for Epoch 9:2.35 - F1: 0.6538
Time taken for Epoch 10:3.65 - F1: 0.6510
2026-02-12 17:49:25 - INFO - Time taken for Epoch 10:3.65 - F1: 0.6510
Time taken for Epoch 11:2.36 - F1: 0.6548
2026-02-12 17:49:27 - INFO - Time taken for Epoch 11:2.36 - F1: 0.6548
Time taken for Epoch 12:3.66 - F1: 0.6518
2026-02-12 17:49:31 - INFO - Time taken for Epoch 12:3.66 - F1: 0.6518
Time taken for Epoch 13:2.36 - F1: 0.6587
2026-02-12 17:49:33 - INFO - Time taken for Epoch 13:2.36 - F1: 0.6587
Time taken for Epoch 14:3.68 - F1: 0.6549
2026-02-12 17:49:37 - INFO - Time taken for Epoch 14:3.68 - F1: 0.6549
Time taken for Epoch 15:2.35 - F1: 0.6594
2026-02-12 17:49:39 - INFO - Time taken for Epoch 15:2.35 - F1: 0.6594
Time taken for Epoch 16:3.67 - F1: 0.6613
2026-02-12 17:49:43 - INFO - Time taken for Epoch 16:3.67 - F1: 0.6613
Time taken for Epoch 17:3.67 - F1: 0.6667
2026-02-12 17:49:46 - INFO - Time taken for Epoch 17:3.67 - F1: 0.6667
Time taken for Epoch 18:3.67 - F1: 0.6668
2026-02-12 17:49:50 - INFO - Time taken for Epoch 18:3.67 - F1: 0.6668
Time taken for Epoch 19:3.68 - F1: 0.6658
2026-02-12 17:49:54 - INFO - Time taken for Epoch 19:3.68 - F1: 0.6658
Time taken for Epoch 20:2.34 - F1: 0.6723
2026-02-12 17:49:56 - INFO - Time taken for Epoch 20:2.34 - F1: 0.6723
Time taken for Epoch 21:3.67 - F1: 0.6639
2026-02-12 17:50:00 - INFO - Time taken for Epoch 21:3.67 - F1: 0.6639
Time taken for Epoch 22:2.35 - F1: 0.6658
2026-02-12 17:50:02 - INFO - Time taken for Epoch 22:2.35 - F1: 0.6658
Time taken for Epoch 23:2.35 - F1: 0.6642
2026-02-12 17:50:04 - INFO - Time taken for Epoch 23:2.35 - F1: 0.6642
Time taken for Epoch 24:2.34 - F1: 0.6654
2026-02-12 17:50:07 - INFO - Time taken for Epoch 24:2.34 - F1: 0.6654
Time taken for Epoch 25:2.35 - F1: 0.6621
2026-02-12 17:50:09 - INFO - Time taken for Epoch 25:2.35 - F1: 0.6621
Time taken for Epoch 26:2.35 - F1: 0.6624
2026-02-12 17:50:11 - INFO - Time taken for Epoch 26:2.35 - F1: 0.6624
Time taken for Epoch 27:2.35 - F1: 0.6627
2026-02-12 17:50:14 - INFO - Time taken for Epoch 27:2.35 - F1: 0.6627
Time taken for Epoch 28:2.36 - F1: 0.6637
2026-02-12 17:50:16 - INFO - Time taken for Epoch 28:2.36 - F1: 0.6637
Time taken for Epoch 29:2.35 - F1: 0.6811
2026-02-12 17:50:19 - INFO - Time taken for Epoch 29:2.35 - F1: 0.6811
Time taken for Epoch 30:3.67 - F1: 0.6809
2026-02-12 17:50:22 - INFO - Time taken for Epoch 30:3.67 - F1: 0.6809
Time taken for Epoch 31:2.35 - F1: 0.6822
2026-02-12 17:50:25 - INFO - Time taken for Epoch 31:2.35 - F1: 0.6822
Time taken for Epoch 32:3.66 - F1: 0.6804
2026-02-12 17:50:28 - INFO - Time taken for Epoch 32:3.66 - F1: 0.6804
Time taken for Epoch 33:2.36 - F1: 0.6804
2026-02-12 17:50:31 - INFO - Time taken for Epoch 33:2.36 - F1: 0.6804
Time taken for Epoch 34:2.35 - F1: 0.6798
2026-02-12 17:50:33 - INFO - Time taken for Epoch 34:2.35 - F1: 0.6798
Time taken for Epoch 35:2.35 - F1: 0.6798
2026-02-12 17:50:35 - INFO - Time taken for Epoch 35:2.35 - F1: 0.6798
Time taken for Epoch 36:2.35 - F1: 0.6822
2026-02-12 17:50:38 - INFO - Time taken for Epoch 36:2.35 - F1: 0.6822
Time taken for Epoch 37:2.35 - F1: 0.6852
2026-02-12 17:50:40 - INFO - Time taken for Epoch 37:2.35 - F1: 0.6852
Time taken for Epoch 38:3.69 - F1: 0.6842
2026-02-12 17:50:44 - INFO - Time taken for Epoch 38:3.69 - F1: 0.6842
Time taken for Epoch 39:2.35 - F1: 0.6840
2026-02-12 17:50:46 - INFO - Time taken for Epoch 39:2.35 - F1: 0.6840
Time taken for Epoch 40:2.36 - F1: 0.6860
2026-02-12 17:50:48 - INFO - Time taken for Epoch 40:2.36 - F1: 0.6860
Time taken for Epoch 41:3.68 - F1: 0.6860
2026-02-12 17:50:52 - INFO - Time taken for Epoch 41:3.68 - F1: 0.6860
Time taken for Epoch 42:2.35 - F1: 0.6847
2026-02-12 17:50:54 - INFO - Time taken for Epoch 42:2.35 - F1: 0.6847
Time taken for Epoch 43:2.35 - F1: 0.6847
2026-02-12 17:50:57 - INFO - Time taken for Epoch 43:2.35 - F1: 0.6847
Time taken for Epoch 44:2.36 - F1: 0.6847
2026-02-12 17:50:59 - INFO - Time taken for Epoch 44:2.36 - F1: 0.6847
Time taken for Epoch 45:2.36 - F1: 0.6828
2026-02-12 17:51:01 - INFO - Time taken for Epoch 45:2.36 - F1: 0.6828
Time taken for Epoch 46:2.35 - F1: 0.6828
2026-02-12 17:51:04 - INFO - Time taken for Epoch 46:2.35 - F1: 0.6828
Time taken for Epoch 47:2.35 - F1: 0.6837
2026-02-12 17:51:06 - INFO - Time taken for Epoch 47:2.35 - F1: 0.6837
Time taken for Epoch 48:2.35 - F1: 0.6837
2026-02-12 17:51:08 - INFO - Time taken for Epoch 48:2.35 - F1: 0.6837
Time taken for Epoch 49:2.35 - F1: 0.6837
2026-02-12 17:51:11 - INFO - Time taken for Epoch 49:2.35 - F1: 0.6837
Time taken for Epoch 50:2.36 - F1: 0.6827
2026-02-12 17:51:13 - INFO - Time taken for Epoch 50:2.36 - F1: 0.6827
Performance not improving for 10 consecutive epochs.
2026-02-12 17:51:13 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6860 - Best Epoch:39
2026-02-12 17:51:13 - INFO - Best F1:0.6860 - Best Epoch:39
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set3/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6926, Test ECE: 0.0411
2026-02-12 17:51:20 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6926, Test ECE: 0.0411
All results: {'f1_macro': 0.6925538083486352, 'ece': np.float64(0.04106358757526235)}
2026-02-12 17:51:20 - INFO - All results: {'f1_macro': 0.6925538083486352, 'ece': np.float64(0.04106358757526235)}

Total time taken: 1078.85 seconds
2026-02-12 17:51:20 - INFO - 
Total time taken: 1078.85 seconds
2026-02-12 17:51:20 - INFO - Trial 9 finished with value: 0.6925538083486352 and parameters: {'learning_rate': 1.6398966978584835e-05, 'weight_decay': 0.006124807115472876, 'batch_size': 64, 'co_train_epochs': 16, 'epoch_patience': 8}. Best is trial 9 with value: 0.6925538083486352.

[BEST TRIAL RESULTS]
2026-02-12 17:51:20 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6926
2026-02-12 17:51:20 - INFO - F1 Score: 0.6926
Params: {'learning_rate': 1.6398966978584835e-05, 'weight_decay': 0.006124807115472876, 'batch_size': 64, 'co_train_epochs': 16, 'epoch_patience': 8}
2026-02-12 17:51:20 - INFO - Params: {'learning_rate': 1.6398966978584835e-05, 'weight_decay': 0.006124807115472876, 'batch_size': 64, 'co_train_epochs': 16, 'epoch_patience': 8}
  learning_rate: 1.6398966978584835e-05
2026-02-12 17:51:20 - INFO -   learning_rate: 1.6398966978584835e-05
  weight_decay: 0.006124807115472876
2026-02-12 17:51:20 - INFO -   weight_decay: 0.006124807115472876
  batch_size: 64
2026-02-12 17:51:20 - INFO -   batch_size: 64
  co_train_epochs: 16
2026-02-12 17:51:20 - INFO -   co_train_epochs: 16
  epoch_patience: 8
2026-02-12 17:51:20 - INFO -   epoch_patience: 8

Total time taken: 5714.89 seconds
2026-02-12 17:51:20 - INFO - 
Total time taken: 5714.89 seconds