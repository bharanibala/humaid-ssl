Running with 10 label/class set 2

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-14 04:01:49 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-14 04:01:49 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_maria_2017
Using devices: cuda, cuda
2026-02-14 04:01:49 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 04:01:49 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 04:01:49 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 04:01:49 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 0.0001264164245345984
Weight Decay: 0.0002880465666272556
Batch Size: 64
No. Epochs: 14
Epoch Patience: 8
 Accumulation Steps: 1
2026-02-14 04:01:50 - INFO - Learning Rate: 0.0001264164245345984
Weight Decay: 0.0002880465666272556
Batch Size: 64
No. Epochs: 14
Epoch Patience: 8
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 04:01:51 - INFO - Generating initial weights
Time taken for Epoch 1:19.08 - F1: 0.1690
2026-02-14 04:02:14 - INFO - Time taken for Epoch 1:19.08 - F1: 0.1690
Time taken for Epoch 2:18.77 - F1: 0.1910
2026-02-14 04:02:33 - INFO - Time taken for Epoch 2:18.77 - F1: 0.1910
Time taken for Epoch 3:18.85 - F1: 0.2779
2026-02-14 04:02:52 - INFO - Time taken for Epoch 3:18.85 - F1: 0.2779
Time taken for Epoch 4:18.96 - F1: 0.3730
2026-02-14 04:03:11 - INFO - Time taken for Epoch 4:18.96 - F1: 0.3730
Time taken for Epoch 5:18.99 - F1: 0.4051
2026-02-14 04:03:30 - INFO - Time taken for Epoch 5:18.99 - F1: 0.4051
Time taken for Epoch 6:19.06 - F1: 0.4312
2026-02-14 04:03:49 - INFO - Time taken for Epoch 6:19.06 - F1: 0.4312
Time taken for Epoch 7:19.12 - F1: 0.4278
2026-02-14 04:04:08 - INFO - Time taken for Epoch 7:19.12 - F1: 0.4278
Time taken for Epoch 8:19.17 - F1: 0.4209
2026-02-14 04:04:27 - INFO - Time taken for Epoch 8:19.17 - F1: 0.4209
Time taken for Epoch 9:19.18 - F1: 0.4363
2026-02-14 04:04:46 - INFO - Time taken for Epoch 9:19.18 - F1: 0.4363
Time taken for Epoch 10:19.16 - F1: 0.4381
2026-02-14 04:05:05 - INFO - Time taken for Epoch 10:19.16 - F1: 0.4381
Time taken for Epoch 11:19.25 - F1: 0.4404
2026-02-14 04:05:25 - INFO - Time taken for Epoch 11:19.25 - F1: 0.4404
Time taken for Epoch 12:19.24 - F1: 0.4472
2026-02-14 04:05:44 - INFO - Time taken for Epoch 12:19.24 - F1: 0.4472
Time taken for Epoch 13:19.25 - F1: 0.4502
2026-02-14 04:06:03 - INFO - Time taken for Epoch 13:19.25 - F1: 0.4502
Time taken for Epoch 14:19.25 - F1: 0.4508
2026-02-14 04:06:22 - INFO - Time taken for Epoch 14:19.25 - F1: 0.4508
Best F1:0.4508 - Best Epoch:14
2026-02-14 04:06:22 - INFO - Best F1:0.4508 - Best Epoch:14
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 04:06:24 - INFO - Starting co-training
Time taken for Epoch 1: 46.10s - F1: 0.56411040
2026-02-14 04:07:10 - INFO - Time taken for Epoch 1: 46.10s - F1: 0.56411040
Time taken for Epoch 2: 47.21s - F1: 0.58327915
2026-02-14 04:07:57 - INFO - Time taken for Epoch 2: 47.21s - F1: 0.58327915
Time taken for Epoch 3: 47.30s - F1: 0.61488341
2026-02-14 04:08:45 - INFO - Time taken for Epoch 3: 47.30s - F1: 0.61488341
Time taken for Epoch 4: 47.32s - F1: 0.65104120
2026-02-14 04:09:32 - INFO - Time taken for Epoch 4: 47.32s - F1: 0.65104120
Time taken for Epoch 5: 47.31s - F1: 0.63688151
2026-02-14 04:10:19 - INFO - Time taken for Epoch 5: 47.31s - F1: 0.63688151
Time taken for Epoch 6: 46.18s - F1: 0.65837633
2026-02-14 04:11:06 - INFO - Time taken for Epoch 6: 46.18s - F1: 0.65837633
Time taken for Epoch 7: 47.32s - F1: 0.64379642
2026-02-14 04:11:53 - INFO - Time taken for Epoch 7: 47.32s - F1: 0.64379642
Time taken for Epoch 8: 46.17s - F1: 0.63879533
2026-02-14 04:12:39 - INFO - Time taken for Epoch 8: 46.17s - F1: 0.63879533
Time taken for Epoch 9: 46.21s - F1: 0.65205672
2026-02-14 04:13:25 - INFO - Time taken for Epoch 9: 46.21s - F1: 0.65205672
Time taken for Epoch 10: 46.19s - F1: 0.64226659
2026-02-14 04:14:11 - INFO - Time taken for Epoch 10: 46.19s - F1: 0.64226659
Time taken for Epoch 11: 46.19s - F1: 0.63938716
2026-02-14 04:14:58 - INFO - Time taken for Epoch 11: 46.19s - F1: 0.63938716
Time taken for Epoch 12: 46.20s - F1: 0.62086525
2026-02-14 04:15:44 - INFO - Time taken for Epoch 12: 46.20s - F1: 0.62086525
Time taken for Epoch 13: 46.19s - F1: 0.63600689
2026-02-14 04:16:30 - INFO - Time taken for Epoch 13: 46.19s - F1: 0.63600689
Time taken for Epoch 14: 46.21s - F1: 0.63137093
2026-02-14 04:17:16 - INFO - Time taken for Epoch 14: 46.21s - F1: 0.63137093
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 04:17:19 - INFO - Fine-tuning models
Time taken for Epoch 1:2.86 - F1: 0.6680
2026-02-14 04:17:22 - INFO - Time taken for Epoch 1:2.86 - F1: 0.6680
Time taken for Epoch 2:3.92 - F1: 0.6596
2026-02-14 04:17:26 - INFO - Time taken for Epoch 2:3.92 - F1: 0.6596
Time taken for Epoch 3:2.85 - F1: 0.6529
2026-02-14 04:17:29 - INFO - Time taken for Epoch 3:2.85 - F1: 0.6529
Time taken for Epoch 4:2.85 - F1: 0.6403
2026-02-14 04:17:32 - INFO - Time taken for Epoch 4:2.85 - F1: 0.6403
Time taken for Epoch 5:2.85 - F1: 0.6464
2026-02-14 04:17:35 - INFO - Time taken for Epoch 5:2.85 - F1: 0.6464
Time taken for Epoch 6:2.85 - F1: 0.6312
2026-02-14 04:17:37 - INFO - Time taken for Epoch 6:2.85 - F1: 0.6312
Time taken for Epoch 7:2.85 - F1: 0.6319
2026-02-14 04:17:40 - INFO - Time taken for Epoch 7:2.85 - F1: 0.6319
Time taken for Epoch 8:2.85 - F1: 0.6342
2026-02-14 04:17:43 - INFO - Time taken for Epoch 8:2.85 - F1: 0.6342
Time taken for Epoch 9:2.85 - F1: 0.6554
2026-02-14 04:17:46 - INFO - Time taken for Epoch 9:2.85 - F1: 0.6554
Time taken for Epoch 10:2.85 - F1: 0.6535
2026-02-14 04:17:49 - INFO - Time taken for Epoch 10:2.85 - F1: 0.6535
Time taken for Epoch 11:2.85 - F1: 0.6561
2026-02-14 04:17:52 - INFO - Time taken for Epoch 11:2.85 - F1: 0.6561
Performance not improving for 10 consecutive epochs.
2026-02-14 04:17:52 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6680 - Best Epoch:0
2026-02-14 04:17:52 - INFO - Best F1:0.6680 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6757, Test ECE: 0.0413
2026-02-14 04:18:00 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6757, Test ECE: 0.0413
All results: {'f1_macro': 0.6756986096899137, 'ece': np.float64(0.041290345122513256)}
2026-02-14 04:18:00 - INFO - All results: {'f1_macro': 0.6756986096899137, 'ece': np.float64(0.041290345122513256)}

Total time taken: 970.42 seconds
2026-02-14 04:18:00 - INFO - 
Total time taken: 970.42 seconds
2026-02-14 04:18:00 - INFO - Trial 0 finished with value: 0.6756986096899137 and parameters: {'learning_rate': 0.0001264164245345984, 'weight_decay': 0.0002880465666272556, 'batch_size': 64, 'co_train_epochs': 14, 'epoch_patience': 8}. Best is trial 0 with value: 0.6756986096899137.
Using devices: cuda, cuda
2026-02-14 04:18:00 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 04:18:00 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 04:18:00 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 04:18:00 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 3.674873390278807e-05
Weight Decay: 3.7049123048205384e-05
Batch Size: 32
No. Epochs: 19
Epoch Patience: 8
 Accumulation Steps: 2
2026-02-14 04:18:00 - INFO - Learning Rate: 3.674873390278807e-05
Weight Decay: 3.7049123048205384e-05
Batch Size: 32
No. Epochs: 19
Epoch Patience: 8
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 04:18:01 - INFO - Generating initial weights
Time taken for Epoch 1:20.16 - F1: 0.0750
2026-02-14 04:18:25 - INFO - Time taken for Epoch 1:20.16 - F1: 0.0750
Time taken for Epoch 2:20.09 - F1: 0.0711
2026-02-14 04:18:45 - INFO - Time taken for Epoch 2:20.09 - F1: 0.0711
Time taken for Epoch 3:20.21 - F1: 0.0714
2026-02-14 04:19:05 - INFO - Time taken for Epoch 3:20.21 - F1: 0.0714
Time taken for Epoch 4:20.27 - F1: 0.0662
2026-02-14 04:19:25 - INFO - Time taken for Epoch 4:20.27 - F1: 0.0662
Time taken for Epoch 5:20.23 - F1: 0.0648
2026-02-14 04:19:46 - INFO - Time taken for Epoch 5:20.23 - F1: 0.0648
Time taken for Epoch 6:20.28 - F1: 0.0719
2026-02-14 04:20:06 - INFO - Time taken for Epoch 6:20.28 - F1: 0.0719
Time taken for Epoch 7:20.29 - F1: 0.0781
2026-02-14 04:20:26 - INFO - Time taken for Epoch 7:20.29 - F1: 0.0781
Time taken for Epoch 8:20.29 - F1: 0.0811
2026-02-14 04:20:47 - INFO - Time taken for Epoch 8:20.29 - F1: 0.0811
Time taken for Epoch 9:20.28 - F1: 0.0827
2026-02-14 04:21:07 - INFO - Time taken for Epoch 9:20.28 - F1: 0.0827
Time taken for Epoch 10:20.28 - F1: 0.0891
2026-02-14 04:21:27 - INFO - Time taken for Epoch 10:20.28 - F1: 0.0891
Time taken for Epoch 11:20.34 - F1: 0.1000
2026-02-14 04:21:47 - INFO - Time taken for Epoch 11:20.34 - F1: 0.1000
Time taken for Epoch 12:20.31 - F1: 0.1051
2026-02-14 04:22:08 - INFO - Time taken for Epoch 12:20.31 - F1: 0.1051
Time taken for Epoch 13:20.29 - F1: 0.1235
2026-02-14 04:22:28 - INFO - Time taken for Epoch 13:20.29 - F1: 0.1235
Time taken for Epoch 14:20.26 - F1: 0.1318
2026-02-14 04:22:48 - INFO - Time taken for Epoch 14:20.26 - F1: 0.1318
Time taken for Epoch 15:20.32 - F1: 0.1362
2026-02-14 04:23:09 - INFO - Time taken for Epoch 15:20.32 - F1: 0.1362
Time taken for Epoch 16:20.27 - F1: 0.1376
2026-02-14 04:23:29 - INFO - Time taken for Epoch 16:20.27 - F1: 0.1376
Time taken for Epoch 17:20.26 - F1: 0.1433
2026-02-14 04:23:49 - INFO - Time taken for Epoch 17:20.26 - F1: 0.1433
Time taken for Epoch 18:20.30 - F1: 0.1586
2026-02-14 04:24:09 - INFO - Time taken for Epoch 18:20.30 - F1: 0.1586
Time taken for Epoch 19:20.29 - F1: 0.1608
2026-02-14 04:24:30 - INFO - Time taken for Epoch 19:20.29 - F1: 0.1608
Best F1:0.1608 - Best Epoch:19
2026-02-14 04:24:30 - INFO - Best F1:0.1608 - Best Epoch:19
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 04:24:31 - INFO - Starting co-training
Time taken for Epoch 1: 35.27s - F1: 0.33190850
2026-02-14 04:25:07 - INFO - Time taken for Epoch 1: 35.27s - F1: 0.33190850
Time taken for Epoch 2: 36.35s - F1: 0.51510129
2026-02-14 04:25:43 - INFO - Time taken for Epoch 2: 36.35s - F1: 0.51510129
Time taken for Epoch 3: 36.50s - F1: 0.59988038
2026-02-14 04:26:20 - INFO - Time taken for Epoch 3: 36.50s - F1: 0.59988038
Time taken for Epoch 4: 36.40s - F1: 0.65740691
2026-02-14 04:26:56 - INFO - Time taken for Epoch 4: 36.40s - F1: 0.65740691
Time taken for Epoch 5: 36.40s - F1: 0.65605539
2026-02-14 04:27:32 - INFO - Time taken for Epoch 5: 36.40s - F1: 0.65605539
Time taken for Epoch 6: 35.34s - F1: 0.63153861
2026-02-14 04:28:08 - INFO - Time taken for Epoch 6: 35.34s - F1: 0.63153861
Time taken for Epoch 7: 35.35s - F1: 0.63213181
2026-02-14 04:28:43 - INFO - Time taken for Epoch 7: 35.35s - F1: 0.63213181
Time taken for Epoch 8: 35.34s - F1: 0.62702694
2026-02-14 04:29:18 - INFO - Time taken for Epoch 8: 35.34s - F1: 0.62702694
Time taken for Epoch 9: 35.34s - F1: 0.63260517
2026-02-14 04:29:54 - INFO - Time taken for Epoch 9: 35.34s - F1: 0.63260517
Time taken for Epoch 10: 35.35s - F1: 0.63442521
2026-02-14 04:30:29 - INFO - Time taken for Epoch 10: 35.35s - F1: 0.63442521
Time taken for Epoch 11: 35.34s - F1: 0.62478488
2026-02-14 04:31:04 - INFO - Time taken for Epoch 11: 35.34s - F1: 0.62478488
Time taken for Epoch 12: 35.36s - F1: 0.62897827
2026-02-14 04:31:40 - INFO - Time taken for Epoch 12: 35.36s - F1: 0.62897827
Performance not improving for 8 consecutive epochs.
Performance not improving for 8 consecutive epochs.
2026-02-14 04:31:40 - INFO - Performance not improving for 8 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 04:31:43 - INFO - Fine-tuning models
Time taken for Epoch 1:3.02 - F1: 0.6545
2026-02-14 04:31:46 - INFO - Time taken for Epoch 1:3.02 - F1: 0.6545
Time taken for Epoch 2:4.07 - F1: 0.6459
2026-02-14 04:31:50 - INFO - Time taken for Epoch 2:4.07 - F1: 0.6459
Time taken for Epoch 3:3.01 - F1: 0.6326
2026-02-14 04:31:53 - INFO - Time taken for Epoch 3:3.01 - F1: 0.6326
Time taken for Epoch 4:3.00 - F1: 0.6310
2026-02-14 04:31:56 - INFO - Time taken for Epoch 4:3.00 - F1: 0.6310
Time taken for Epoch 5:3.01 - F1: 0.6337
2026-02-14 04:31:59 - INFO - Time taken for Epoch 5:3.01 - F1: 0.6337
Time taken for Epoch 6:3.01 - F1: 0.6367
2026-02-14 04:32:02 - INFO - Time taken for Epoch 6:3.01 - F1: 0.6367
Time taken for Epoch 7:3.01 - F1: 0.6335
2026-02-14 04:32:05 - INFO - Time taken for Epoch 7:3.01 - F1: 0.6335
Time taken for Epoch 8:3.01 - F1: 0.6393
2026-02-14 04:32:08 - INFO - Time taken for Epoch 8:3.01 - F1: 0.6393
Time taken for Epoch 9:3.01 - F1: 0.6538
2026-02-14 04:32:11 - INFO - Time taken for Epoch 9:3.01 - F1: 0.6538
Time taken for Epoch 10:3.01 - F1: 0.6409
2026-02-14 04:32:14 - INFO - Time taken for Epoch 10:3.01 - F1: 0.6409
Time taken for Epoch 11:3.01 - F1: 0.6384
2026-02-14 04:32:17 - INFO - Time taken for Epoch 11:3.01 - F1: 0.6384
Performance not improving for 10 consecutive epochs.
2026-02-14 04:32:17 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6545 - Best Epoch:0
2026-02-14 04:32:17 - INFO - Best F1:0.6545 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6806, Test ECE: 0.0433
2026-02-14 04:32:25 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6806, Test ECE: 0.0433
All results: {'f1_macro': 0.6806226545745653, 'ece': np.float64(0.04333899617608477)}
2026-02-14 04:32:25 - INFO - All results: {'f1_macro': 0.6806226545745653, 'ece': np.float64(0.04333899617608477)}

Total time taken: 865.60 seconds
2026-02-14 04:32:25 - INFO - 
Total time taken: 865.60 seconds
2026-02-14 04:32:25 - INFO - Trial 1 finished with value: 0.6806226545745653 and parameters: {'learning_rate': 3.674873390278807e-05, 'weight_decay': 3.7049123048205384e-05, 'batch_size': 32, 'co_train_epochs': 19, 'epoch_patience': 8}. Best is trial 1 with value: 0.6806226545745653.
Using devices: cuda, cuda
2026-02-14 04:32:25 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 04:32:25 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 04:32:25 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 04:32:25 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 0.00010623377048050304
Weight Decay: 2.5117366335544202e-05
Batch Size: 32
No. Epochs: 15
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-14 04:32:26 - INFO - Learning Rate: 0.00010623377048050304
Weight Decay: 2.5117366335544202e-05
Batch Size: 32
No. Epochs: 15
Epoch Patience: 10
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 04:32:27 - INFO - Generating initial weights
Time taken for Epoch 1:20.21 - F1: 0.0744
2026-02-14 04:32:51 - INFO - Time taken for Epoch 1:20.21 - F1: 0.0744
Time taken for Epoch 2:20.16 - F1: 0.0590
2026-02-14 04:33:11 - INFO - Time taken for Epoch 2:20.16 - F1: 0.0590
Time taken for Epoch 3:20.23 - F1: 0.0805
2026-02-14 04:33:31 - INFO - Time taken for Epoch 3:20.23 - F1: 0.0805
Time taken for Epoch 4:20.27 - F1: 0.0842
2026-02-14 04:33:51 - INFO - Time taken for Epoch 4:20.27 - F1: 0.0842
Time taken for Epoch 5:20.32 - F1: 0.1261
2026-02-14 04:34:12 - INFO - Time taken for Epoch 5:20.32 - F1: 0.1261
Time taken for Epoch 6:20.27 - F1: 0.1363
2026-02-14 04:34:32 - INFO - Time taken for Epoch 6:20.27 - F1: 0.1363
Time taken for Epoch 7:20.30 - F1: 0.1376
2026-02-14 04:34:52 - INFO - Time taken for Epoch 7:20.30 - F1: 0.1376
Time taken for Epoch 8:20.28 - F1: 0.1928
2026-02-14 04:35:12 - INFO - Time taken for Epoch 8:20.28 - F1: 0.1928
Time taken for Epoch 9:20.23 - F1: 0.2200
2026-02-14 04:35:33 - INFO - Time taken for Epoch 9:20.23 - F1: 0.2200
Time taken for Epoch 10:20.25 - F1: 0.2794
2026-02-14 04:35:53 - INFO - Time taken for Epoch 10:20.25 - F1: 0.2794
Time taken for Epoch 11:20.29 - F1: 0.3638
2026-02-14 04:36:13 - INFO - Time taken for Epoch 11:20.29 - F1: 0.3638
Time taken for Epoch 12:20.26 - F1: 0.3809
2026-02-14 04:36:33 - INFO - Time taken for Epoch 12:20.26 - F1: 0.3809
Time taken for Epoch 13:20.28 - F1: 0.3863
2026-02-14 04:36:54 - INFO - Time taken for Epoch 13:20.28 - F1: 0.3863
Time taken for Epoch 14:20.36 - F1: 0.3995
2026-02-14 04:37:14 - INFO - Time taken for Epoch 14:20.36 - F1: 0.3995
Time taken for Epoch 15:20.27 - F1: 0.4053
2026-02-14 04:37:34 - INFO - Time taken for Epoch 15:20.27 - F1: 0.4053
Best F1:0.4053 - Best Epoch:15
2026-02-14 04:37:34 - INFO - Best F1:0.4053 - Best Epoch:15
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 04:37:36 - INFO - Starting co-training
Time taken for Epoch 1: 35.28s - F1: 0.57072617
2026-02-14 04:38:11 - INFO - Time taken for Epoch 1: 35.28s - F1: 0.57072617
Time taken for Epoch 2: 36.37s - F1: 0.58580987
2026-02-14 04:38:48 - INFO - Time taken for Epoch 2: 36.37s - F1: 0.58580987
Time taken for Epoch 3: 36.66s - F1: 0.63026056
2026-02-14 04:39:24 - INFO - Time taken for Epoch 3: 36.66s - F1: 0.63026056
Time taken for Epoch 4: 36.48s - F1: 0.63615051
2026-02-14 04:40:01 - INFO - Time taken for Epoch 4: 36.48s - F1: 0.63615051
Time taken for Epoch 5: 36.43s - F1: 0.65412505
2026-02-14 04:40:37 - INFO - Time taken for Epoch 5: 36.43s - F1: 0.65412505
Time taken for Epoch 6: 36.44s - F1: 0.61940566
2026-02-14 04:41:14 - INFO - Time taken for Epoch 6: 36.44s - F1: 0.61940566
Time taken for Epoch 7: 35.34s - F1: 0.61598757
2026-02-14 04:41:49 - INFO - Time taken for Epoch 7: 35.34s - F1: 0.61598757
Time taken for Epoch 8: 35.34s - F1: 0.59971288
2026-02-14 04:42:24 - INFO - Time taken for Epoch 8: 35.34s - F1: 0.59971288
Time taken for Epoch 9: 35.34s - F1: 0.59624102
2026-02-14 04:43:00 - INFO - Time taken for Epoch 9: 35.34s - F1: 0.59624102
Time taken for Epoch 10: 35.36s - F1: 0.63256050
2026-02-14 04:43:35 - INFO - Time taken for Epoch 10: 35.36s - F1: 0.63256050
Time taken for Epoch 11: 35.37s - F1: 0.62820183
2026-02-14 04:44:11 - INFO - Time taken for Epoch 11: 35.37s - F1: 0.62820183
Time taken for Epoch 12: 35.37s - F1: 0.58842282
2026-02-14 04:44:46 - INFO - Time taken for Epoch 12: 35.37s - F1: 0.58842282
Time taken for Epoch 13: 35.54s - F1: 0.60612235
2026-02-14 04:45:21 - INFO - Time taken for Epoch 13: 35.54s - F1: 0.60612235
Time taken for Epoch 14: 35.40s - F1: 0.60801243
2026-02-14 04:45:57 - INFO - Time taken for Epoch 14: 35.40s - F1: 0.60801243
Time taken for Epoch 15: 35.38s - F1: 0.64073610
2026-02-14 04:46:32 - INFO - Time taken for Epoch 15: 35.38s - F1: 0.64073610
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 04:46:35 - INFO - Fine-tuning models
Time taken for Epoch 1:3.01 - F1: 0.6611
2026-02-14 04:46:38 - INFO - Time taken for Epoch 1:3.01 - F1: 0.6611
Time taken for Epoch 2:4.09 - F1: 0.6342
2026-02-14 04:46:42 - INFO - Time taken for Epoch 2:4.09 - F1: 0.6342
Time taken for Epoch 3:3.01 - F1: 0.6303
2026-02-14 04:46:45 - INFO - Time taken for Epoch 3:3.01 - F1: 0.6303
Time taken for Epoch 4:3.01 - F1: 0.6300
2026-02-14 04:46:48 - INFO - Time taken for Epoch 4:3.01 - F1: 0.6300
Time taken for Epoch 5:3.01 - F1: 0.6391
2026-02-14 04:46:51 - INFO - Time taken for Epoch 5:3.01 - F1: 0.6391
Time taken for Epoch 6:3.01 - F1: 0.6394
2026-02-14 04:46:54 - INFO - Time taken for Epoch 6:3.01 - F1: 0.6394
Time taken for Epoch 7:3.01 - F1: 0.6523
2026-02-14 04:46:57 - INFO - Time taken for Epoch 7:3.01 - F1: 0.6523
Time taken for Epoch 8:3.01 - F1: 0.6465
2026-02-14 04:47:00 - INFO - Time taken for Epoch 8:3.01 - F1: 0.6465
Time taken for Epoch 9:3.02 - F1: 0.6426
2026-02-14 04:47:03 - INFO - Time taken for Epoch 9:3.02 - F1: 0.6426
Time taken for Epoch 10:3.01 - F1: 0.6487
2026-02-14 04:47:06 - INFO - Time taken for Epoch 10:3.01 - F1: 0.6487
Time taken for Epoch 11:3.01 - F1: 0.6521
2026-02-14 04:47:09 - INFO - Time taken for Epoch 11:3.01 - F1: 0.6521
Performance not improving for 10 consecutive epochs.
2026-02-14 04:47:09 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6611 - Best Epoch:0
2026-02-14 04:47:09 - INFO - Best F1:0.6611 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6750, Test ECE: 0.0543
2026-02-14 04:47:17 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6750, Test ECE: 0.0543
All results: {'f1_macro': 0.6750355388215019, 'ece': np.float64(0.054266754780662216)}
2026-02-14 04:47:17 - INFO - All results: {'f1_macro': 0.6750355388215019, 'ece': np.float64(0.054266754780662216)}

Total time taken: 892.21 seconds
2026-02-14 04:47:17 - INFO - 
Total time taken: 892.21 seconds
2026-02-14 04:47:18 - INFO - Trial 2 finished with value: 0.6750355388215019 and parameters: {'learning_rate': 0.00010623377048050304, 'weight_decay': 2.5117366335544202e-05, 'batch_size': 32, 'co_train_epochs': 15, 'epoch_patience': 10}. Best is trial 1 with value: 0.6806226545745653.
Using devices: cuda, cuda
2026-02-14 04:47:18 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 04:47:18 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 04:47:18 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 04:47:18 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 3.2601764674743625e-05
Weight Decay: 0.00013742880737070478
Batch Size: 16
No. Epochs: 19
Epoch Patience: 8
 Accumulation Steps: 4
2026-02-14 04:47:18 - INFO - Learning Rate: 3.2601764674743625e-05
Weight Decay: 0.00013742880737070478
Batch Size: 16
No. Epochs: 19
Epoch Patience: 8
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 04:47:19 - INFO - Generating initial weights
Time taken for Epoch 1:20.86 - F1: 0.0660
2026-02-14 04:47:44 - INFO - Time taken for Epoch 1:20.86 - F1: 0.0660
Time taken for Epoch 2:20.81 - F1: 0.1218
2026-02-14 04:48:04 - INFO - Time taken for Epoch 2:20.81 - F1: 0.1218
Time taken for Epoch 3:20.86 - F1: 0.0966
2026-02-14 04:48:25 - INFO - Time taken for Epoch 3:20.86 - F1: 0.0966
Time taken for Epoch 4:20.86 - F1: 0.0952
2026-02-14 04:48:46 - INFO - Time taken for Epoch 4:20.86 - F1: 0.0952
Time taken for Epoch 5:20.90 - F1: 0.0945
2026-02-14 04:49:07 - INFO - Time taken for Epoch 5:20.90 - F1: 0.0945
Time taken for Epoch 6:20.87 - F1: 0.1021
2026-02-14 04:49:28 - INFO - Time taken for Epoch 6:20.87 - F1: 0.1021
Time taken for Epoch 7:20.88 - F1: 0.1035
2026-02-14 04:49:49 - INFO - Time taken for Epoch 7:20.88 - F1: 0.1035
Time taken for Epoch 8:20.89 - F1: 0.1103
2026-02-14 04:50:10 - INFO - Time taken for Epoch 8:20.89 - F1: 0.1103
Time taken for Epoch 9:20.89 - F1: 0.1397
2026-02-14 04:50:30 - INFO - Time taken for Epoch 9:20.89 - F1: 0.1397
Time taken for Epoch 10:20.85 - F1: 0.1928
2026-02-14 04:50:51 - INFO - Time taken for Epoch 10:20.85 - F1: 0.1928
Time taken for Epoch 11:20.91 - F1: 0.2418
2026-02-14 04:51:12 - INFO - Time taken for Epoch 11:20.91 - F1: 0.2418
Time taken for Epoch 12:20.93 - F1: 0.3337
2026-02-14 04:51:33 - INFO - Time taken for Epoch 12:20.93 - F1: 0.3337
Time taken for Epoch 13:20.87 - F1: 0.3524
2026-02-14 04:51:54 - INFO - Time taken for Epoch 13:20.87 - F1: 0.3524
Time taken for Epoch 14:20.89 - F1: 0.3702
2026-02-14 04:52:15 - INFO - Time taken for Epoch 14:20.89 - F1: 0.3702
Time taken for Epoch 15:20.92 - F1: 0.3807
2026-02-14 04:52:36 - INFO - Time taken for Epoch 15:20.92 - F1: 0.3807
Time taken for Epoch 16:20.91 - F1: 0.3981
2026-02-14 04:52:57 - INFO - Time taken for Epoch 16:20.91 - F1: 0.3981
Time taken for Epoch 17:20.94 - F1: 0.4019
2026-02-14 04:53:18 - INFO - Time taken for Epoch 17:20.94 - F1: 0.4019
Time taken for Epoch 18:20.92 - F1: 0.3976
2026-02-14 04:53:39 - INFO - Time taken for Epoch 18:20.92 - F1: 0.3976
Time taken for Epoch 19:20.94 - F1: 0.4015
2026-02-14 04:54:00 - INFO - Time taken for Epoch 19:20.94 - F1: 0.4015
Best F1:0.4019 - Best Epoch:17
2026-02-14 04:54:00 - INFO - Best F1:0.4019 - Best Epoch:17
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 04:54:01 - INFO - Starting co-training
Time taken for Epoch 1: 29.38s - F1: 0.32387667
2026-02-14 04:54:31 - INFO - Time taken for Epoch 1: 29.38s - F1: 0.32387667
Time taken for Epoch 2: 30.40s - F1: 0.48414339
2026-02-14 04:55:01 - INFO - Time taken for Epoch 2: 30.40s - F1: 0.48414339
Time taken for Epoch 3: 30.46s - F1: 0.48807596
2026-02-14 04:55:32 - INFO - Time taken for Epoch 3: 30.46s - F1: 0.48807596
Time taken for Epoch 4: 30.48s - F1: 0.58358213
2026-02-14 04:56:02 - INFO - Time taken for Epoch 4: 30.48s - F1: 0.58358213
Time taken for Epoch 5: 30.46s - F1: 0.61163545
2026-02-14 04:56:33 - INFO - Time taken for Epoch 5: 30.46s - F1: 0.61163545
Time taken for Epoch 6: 30.69s - F1: 0.63498392
2026-02-14 04:57:03 - INFO - Time taken for Epoch 6: 30.69s - F1: 0.63498392
Time taken for Epoch 7: 30.99s - F1: 0.62756543
2026-02-14 04:57:34 - INFO - Time taken for Epoch 7: 30.99s - F1: 0.62756543
Time taken for Epoch 8: 29.39s - F1: 0.61728999
2026-02-14 04:58:04 - INFO - Time taken for Epoch 8: 29.39s - F1: 0.61728999
Time taken for Epoch 9: 29.38s - F1: 0.64840626
2026-02-14 04:58:33 - INFO - Time taken for Epoch 9: 29.38s - F1: 0.64840626
Time taken for Epoch 10: 30.47s - F1: 0.61987806
2026-02-14 04:59:04 - INFO - Time taken for Epoch 10: 30.47s - F1: 0.61987806
Time taken for Epoch 11: 29.38s - F1: 0.62856963
2026-02-14 04:59:33 - INFO - Time taken for Epoch 11: 29.38s - F1: 0.62856963
Time taken for Epoch 12: 29.52s - F1: 0.62345468
2026-02-14 05:00:02 - INFO - Time taken for Epoch 12: 29.52s - F1: 0.62345468
Time taken for Epoch 13: 29.39s - F1: 0.63154302
2026-02-14 05:00:32 - INFO - Time taken for Epoch 13: 29.39s - F1: 0.63154302
Time taken for Epoch 14: 29.40s - F1: 0.63831853
2026-02-14 05:01:01 - INFO - Time taken for Epoch 14: 29.40s - F1: 0.63831853
Time taken for Epoch 15: 29.39s - F1: 0.61982581
2026-02-14 05:01:31 - INFO - Time taken for Epoch 15: 29.39s - F1: 0.61982581
Time taken for Epoch 16: 29.39s - F1: 0.61615274
2026-02-14 05:02:00 - INFO - Time taken for Epoch 16: 29.39s - F1: 0.61615274
Time taken for Epoch 17: 29.39s - F1: 0.63269213
2026-02-14 05:02:29 - INFO - Time taken for Epoch 17: 29.39s - F1: 0.63269213
Performance not improving for 8 consecutive epochs.
Performance not improving for 8 consecutive epochs.
2026-02-14 05:02:29 - INFO - Performance not improving for 8 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 05:02:32 - INFO - Fine-tuning models
Time taken for Epoch 1:3.09 - F1: 0.6563
2026-02-14 05:02:35 - INFO - Time taken for Epoch 1:3.09 - F1: 0.6563
Time taken for Epoch 2:4.14 - F1: 0.6572
2026-02-14 05:02:40 - INFO - Time taken for Epoch 2:4.14 - F1: 0.6572
Time taken for Epoch 3:4.25 - F1: 0.6627
2026-02-14 05:02:44 - INFO - Time taken for Epoch 3:4.25 - F1: 0.6627
Time taken for Epoch 4:4.25 - F1: 0.6470
2026-02-14 05:02:48 - INFO - Time taken for Epoch 4:4.25 - F1: 0.6470
Time taken for Epoch 5:3.08 - F1: 0.6362
2026-02-14 05:02:51 - INFO - Time taken for Epoch 5:3.08 - F1: 0.6362
Time taken for Epoch 6:3.08 - F1: 0.6264
2026-02-14 05:02:54 - INFO - Time taken for Epoch 6:3.08 - F1: 0.6264
Time taken for Epoch 7:3.08 - F1: 0.6151
2026-02-14 05:02:57 - INFO - Time taken for Epoch 7:3.08 - F1: 0.6151
Time taken for Epoch 8:3.09 - F1: 0.6186
2026-02-14 05:03:00 - INFO - Time taken for Epoch 8:3.09 - F1: 0.6186
Time taken for Epoch 9:3.09 - F1: 0.6175
2026-02-14 05:03:04 - INFO - Time taken for Epoch 9:3.09 - F1: 0.6175
Time taken for Epoch 10:3.09 - F1: 0.6237
2026-02-14 05:03:07 - INFO - Time taken for Epoch 10:3.09 - F1: 0.6237
Time taken for Epoch 11:3.09 - F1: 0.6257
2026-02-14 05:03:10 - INFO - Time taken for Epoch 11:3.09 - F1: 0.6257
Time taken for Epoch 12:3.09 - F1: 0.6227
2026-02-14 05:03:13 - INFO - Time taken for Epoch 12:3.09 - F1: 0.6227
Time taken for Epoch 13:3.09 - F1: 0.6225
2026-02-14 05:03:16 - INFO - Time taken for Epoch 13:3.09 - F1: 0.6225
Performance not improving for 10 consecutive epochs.
2026-02-14 05:03:16 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6627 - Best Epoch:2
2026-02-14 05:03:16 - INFO - Best F1:0.6627 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6649, Test ECE: 0.0343
2026-02-14 05:03:24 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6649, Test ECE: 0.0343
All results: {'f1_macro': 0.6648518353708452, 'ece': np.float64(0.03428051826361975)}
2026-02-14 05:03:24 - INFO - All results: {'f1_macro': 0.6648518353708452, 'ece': np.float64(0.03428051826361975)}

Total time taken: 966.42 seconds
2026-02-14 05:03:24 - INFO - 
Total time taken: 966.42 seconds
2026-02-14 05:03:24 - INFO - Trial 3 finished with value: 0.6648518353708452 and parameters: {'learning_rate': 3.2601764674743625e-05, 'weight_decay': 0.00013742880737070478, 'batch_size': 16, 'co_train_epochs': 19, 'epoch_patience': 8}. Best is trial 1 with value: 0.6806226545745653.
Using devices: cuda, cuda
2026-02-14 05:03:24 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 05:03:24 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 05:03:24 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 05:03:24 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 7.98272797981014e-05
Weight Decay: 0.00023745433722428274
Batch Size: 64
No. Epochs: 15
Epoch Patience: 6
 Accumulation Steps: 1
2026-02-14 05:03:24 - INFO - Learning Rate: 7.98272797981014e-05
Weight Decay: 0.00023745433722428274
Batch Size: 64
No. Epochs: 15
Epoch Patience: 6
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 05:03:26 - INFO - Generating initial weights
Time taken for Epoch 1:19.27 - F1: 0.1220
2026-02-14 05:03:48 - INFO - Time taken for Epoch 1:19.27 - F1: 0.1220
Time taken for Epoch 2:19.21 - F1: 0.1598
2026-02-14 05:04:07 - INFO - Time taken for Epoch 2:19.21 - F1: 0.1598
Time taken for Epoch 3:19.18 - F1: 0.2217
2026-02-14 05:04:27 - INFO - Time taken for Epoch 3:19.18 - F1: 0.2217
Time taken for Epoch 4:19.17 - F1: 0.2830
2026-02-14 05:04:46 - INFO - Time taken for Epoch 4:19.17 - F1: 0.2830
Time taken for Epoch 5:19.21 - F1: 0.3444
2026-02-14 05:05:05 - INFO - Time taken for Epoch 5:19.21 - F1: 0.3444
Time taken for Epoch 6:19.22 - F1: 0.3982
2026-02-14 05:05:24 - INFO - Time taken for Epoch 6:19.22 - F1: 0.3982
Time taken for Epoch 7:19.21 - F1: 0.4114
2026-02-14 05:05:43 - INFO - Time taken for Epoch 7:19.21 - F1: 0.4114
Time taken for Epoch 8:19.23 - F1: 0.4248
2026-02-14 05:06:03 - INFO - Time taken for Epoch 8:19.23 - F1: 0.4248
Time taken for Epoch 9:19.25 - F1: 0.4280
2026-02-14 05:06:22 - INFO - Time taken for Epoch 9:19.25 - F1: 0.4280
Time taken for Epoch 10:19.23 - F1: 0.4317
2026-02-14 05:06:41 - INFO - Time taken for Epoch 10:19.23 - F1: 0.4317
Time taken for Epoch 11:19.23 - F1: 0.4279
2026-02-14 05:07:00 - INFO - Time taken for Epoch 11:19.23 - F1: 0.4279
Time taken for Epoch 12:19.24 - F1: 0.4298
2026-02-14 05:07:20 - INFO - Time taken for Epoch 12:19.24 - F1: 0.4298
Time taken for Epoch 13:19.26 - F1: 0.4346
2026-02-14 05:07:39 - INFO - Time taken for Epoch 13:19.26 - F1: 0.4346
Time taken for Epoch 14:19.25 - F1: 0.4303
2026-02-14 05:07:58 - INFO - Time taken for Epoch 14:19.25 - F1: 0.4303
Time taken for Epoch 15:19.26 - F1: 0.4320
2026-02-14 05:08:17 - INFO - Time taken for Epoch 15:19.26 - F1: 0.4320
Best F1:0.4346 - Best Epoch:13
2026-02-14 05:08:17 - INFO - Best F1:0.4346 - Best Epoch:13
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 05:08:19 - INFO - Starting co-training
Time taken for Epoch 1: 46.09s - F1: 0.62968563
2026-02-14 05:09:05 - INFO - Time taken for Epoch 1: 46.09s - F1: 0.62968563
Time taken for Epoch 2: 47.22s - F1: 0.64136176
2026-02-14 05:09:52 - INFO - Time taken for Epoch 2: 47.22s - F1: 0.64136176
Time taken for Epoch 3: 47.30s - F1: 0.65210402
2026-02-14 05:10:40 - INFO - Time taken for Epoch 3: 47.30s - F1: 0.65210402
Time taken for Epoch 4: 47.31s - F1: 0.63653936
2026-02-14 05:11:27 - INFO - Time taken for Epoch 4: 47.31s - F1: 0.63653936
Time taken for Epoch 5: 46.18s - F1: 0.62625725
2026-02-14 05:12:13 - INFO - Time taken for Epoch 5: 46.18s - F1: 0.62625725
Time taken for Epoch 6: 46.19s - F1: 0.62078823
2026-02-14 05:12:59 - INFO - Time taken for Epoch 6: 46.19s - F1: 0.62078823
Time taken for Epoch 7: 46.19s - F1: 0.65434173
2026-02-14 05:13:46 - INFO - Time taken for Epoch 7: 46.19s - F1: 0.65434173
Time taken for Epoch 8: 47.35s - F1: 0.67527788
2026-02-14 05:14:33 - INFO - Time taken for Epoch 8: 47.35s - F1: 0.67527788
Time taken for Epoch 9: 47.26s - F1: 0.65301588
2026-02-14 05:15:20 - INFO - Time taken for Epoch 9: 47.26s - F1: 0.65301588
Time taken for Epoch 10: 46.19s - F1: 0.65948424
2026-02-14 05:16:06 - INFO - Time taken for Epoch 10: 46.19s - F1: 0.65948424
Time taken for Epoch 11: 46.20s - F1: 0.63291322
2026-02-14 05:16:53 - INFO - Time taken for Epoch 11: 46.20s - F1: 0.63291322
Time taken for Epoch 12: 46.19s - F1: 0.63004033
2026-02-14 05:17:39 - INFO - Time taken for Epoch 12: 46.19s - F1: 0.63004033
Time taken for Epoch 13: 46.20s - F1: 0.64920870
2026-02-14 05:18:25 - INFO - Time taken for Epoch 13: 46.20s - F1: 0.64920870
Time taken for Epoch 14: 46.21s - F1: 0.62916814
2026-02-14 05:19:11 - INFO - Time taken for Epoch 14: 46.21s - F1: 0.62916814
Performance not improving for 6 consecutive epochs.
Performance not improving for 6 consecutive epochs.
2026-02-14 05:19:11 - INFO - Performance not improving for 6 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 05:19:14 - INFO - Fine-tuning models
Time taken for Epoch 1:2.86 - F1: 0.6772
2026-02-14 05:19:17 - INFO - Time taken for Epoch 1:2.86 - F1: 0.6772
Time taken for Epoch 2:3.93 - F1: 0.6852
2026-02-14 05:19:21 - INFO - Time taken for Epoch 2:3.93 - F1: 0.6852
Time taken for Epoch 3:4.01 - F1: 0.6678
2026-02-14 05:19:25 - INFO - Time taken for Epoch 3:4.01 - F1: 0.6678
Time taken for Epoch 4:2.84 - F1: 0.6749
2026-02-14 05:19:28 - INFO - Time taken for Epoch 4:2.84 - F1: 0.6749
Time taken for Epoch 5:2.84 - F1: 0.6730
2026-02-14 05:19:31 - INFO - Time taken for Epoch 5:2.84 - F1: 0.6730
Time taken for Epoch 6:2.85 - F1: 0.6636
2026-02-14 05:19:34 - INFO - Time taken for Epoch 6:2.85 - F1: 0.6636
Time taken for Epoch 7:2.85 - F1: 0.6692
2026-02-14 05:19:37 - INFO - Time taken for Epoch 7:2.85 - F1: 0.6692
Time taken for Epoch 8:2.86 - F1: 0.6623
2026-02-14 05:19:39 - INFO - Time taken for Epoch 8:2.86 - F1: 0.6623
Time taken for Epoch 9:2.86 - F1: 0.6626
2026-02-14 05:19:42 - INFO - Time taken for Epoch 9:2.86 - F1: 0.6626
Time taken for Epoch 10:2.85 - F1: 0.6657
2026-02-14 05:19:45 - INFO - Time taken for Epoch 10:2.85 - F1: 0.6657
Time taken for Epoch 11:2.86 - F1: 0.6636
2026-02-14 05:19:48 - INFO - Time taken for Epoch 11:2.86 - F1: 0.6636
Time taken for Epoch 12:2.86 - F1: 0.6680
2026-02-14 05:19:51 - INFO - Time taken for Epoch 12:2.86 - F1: 0.6680
Performance not improving for 10 consecutive epochs.
2026-02-14 05:19:51 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6852 - Best Epoch:1
2026-02-14 05:19:51 - INFO - Best F1:0.6852 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6701, Test ECE: 0.0229
2026-02-14 05:19:59 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6701, Test ECE: 0.0229
All results: {'f1_macro': 0.6700713617647573, 'ece': np.float64(0.022882257561081823)}
2026-02-14 05:19:59 - INFO - All results: {'f1_macro': 0.6700713617647573, 'ece': np.float64(0.022882257561081823)}

Total time taken: 994.69 seconds
2026-02-14 05:19:59 - INFO - 
Total time taken: 994.69 seconds
2026-02-14 05:19:59 - INFO - Trial 4 finished with value: 0.6700713617647573 and parameters: {'learning_rate': 7.98272797981014e-05, 'weight_decay': 0.00023745433722428274, 'batch_size': 64, 'co_train_epochs': 15, 'epoch_patience': 6}. Best is trial 1 with value: 0.6806226545745653.
Using devices: cuda, cuda
2026-02-14 05:19:59 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 05:19:59 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 05:19:59 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 05:19:59 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 0.0006116819186710884
Weight Decay: 9.623912824362104e-05
Batch Size: 64
No. Epochs: 11
Epoch Patience: 6
 Accumulation Steps: 1
2026-02-14 05:19:59 - INFO - Learning Rate: 0.0006116819186710884
Weight Decay: 9.623912824362104e-05
Batch Size: 64
No. Epochs: 11
Epoch Patience: 6
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 05:20:00 - INFO - Generating initial weights
Time taken for Epoch 1:19.22 - F1: 0.0607
2026-02-14 05:20:23 - INFO - Time taken for Epoch 1:19.22 - F1: 0.0607
Time taken for Epoch 2:19.15 - F1: 0.0388
2026-02-14 05:20:42 - INFO - Time taken for Epoch 2:19.15 - F1: 0.0388
Time taken for Epoch 3:19.18 - F1: 0.0415
2026-02-14 05:21:01 - INFO - Time taken for Epoch 3:19.18 - F1: 0.0415
Time taken for Epoch 4:19.20 - F1: 0.1838
2026-02-14 05:21:21 - INFO - Time taken for Epoch 4:19.20 - F1: 0.1838
Time taken for Epoch 5:19.21 - F1: 0.2161
2026-02-14 05:21:40 - INFO - Time taken for Epoch 5:19.21 - F1: 0.2161
Time taken for Epoch 6:19.23 - F1: 0.3601
2026-02-14 05:21:59 - INFO - Time taken for Epoch 6:19.23 - F1: 0.3601
Time taken for Epoch 7:19.17 - F1: 0.3394
2026-02-14 05:22:18 - INFO - Time taken for Epoch 7:19.17 - F1: 0.3394
Time taken for Epoch 8:19.16 - F1: 0.4034
2026-02-14 05:22:37 - INFO - Time taken for Epoch 8:19.16 - F1: 0.4034
Time taken for Epoch 9:19.17 - F1: 0.4134
2026-02-14 05:22:57 - INFO - Time taken for Epoch 9:19.17 - F1: 0.4134
Time taken for Epoch 10:19.20 - F1: 0.4512
2026-02-14 05:23:16 - INFO - Time taken for Epoch 10:19.20 - F1: 0.4512
Time taken for Epoch 11:19.17 - F1: 0.4684
2026-02-14 05:23:35 - INFO - Time taken for Epoch 11:19.17 - F1: 0.4684
Best F1:0.4684 - Best Epoch:11
2026-02-14 05:23:35 - INFO - Best F1:0.4684 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 05:23:36 - INFO - Starting co-training
Time taken for Epoch 1: 46.06s - F1: 0.04755179
2026-02-14 05:24:23 - INFO - Time taken for Epoch 1: 46.06s - F1: 0.04755179
Time taken for Epoch 2: 47.17s - F1: 0.04755179
2026-02-14 05:25:10 - INFO - Time taken for Epoch 2: 47.17s - F1: 0.04755179
Time taken for Epoch 3: 46.16s - F1: 0.04755179
2026-02-14 05:25:56 - INFO - Time taken for Epoch 3: 46.16s - F1: 0.04755179
Time taken for Epoch 4: 46.16s - F1: 0.04755179
2026-02-14 05:26:42 - INFO - Time taken for Epoch 4: 46.16s - F1: 0.04755179
Time taken for Epoch 5: 46.16s - F1: 0.04755179
2026-02-14 05:27:28 - INFO - Time taken for Epoch 5: 46.16s - F1: 0.04755179
Time taken for Epoch 6: 46.17s - F1: 0.04755179
2026-02-14 05:28:14 - INFO - Time taken for Epoch 6: 46.17s - F1: 0.04755179
Time taken for Epoch 7: 46.17s - F1: 0.04755179
2026-02-14 05:29:01 - INFO - Time taken for Epoch 7: 46.17s - F1: 0.04755179
Performance not improving for 6 consecutive epochs.
Performance not improving for 6 consecutive epochs.
2026-02-14 05:29:01 - INFO - Performance not improving for 6 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 05:29:03 - INFO - Fine-tuning models
Time taken for Epoch 1:2.85 - F1: 0.0363
2026-02-14 05:29:06 - INFO - Time taken for Epoch 1:2.85 - F1: 0.0363
Time taken for Epoch 2:3.92 - F1: 0.0189
2026-02-14 05:29:10 - INFO - Time taken for Epoch 2:3.92 - F1: 0.0189
Time taken for Epoch 3:2.84 - F1: 0.0064
2026-02-14 05:29:13 - INFO - Time taken for Epoch 3:2.84 - F1: 0.0064
Time taken for Epoch 4:2.84 - F1: 0.0064
2026-02-14 05:29:16 - INFO - Time taken for Epoch 4:2.84 - F1: 0.0064
Time taken for Epoch 5:2.84 - F1: 0.0064
2026-02-14 05:29:19 - INFO - Time taken for Epoch 5:2.84 - F1: 0.0064
Time taken for Epoch 6:2.83 - F1: 0.0394
2026-02-14 05:29:22 - INFO - Time taken for Epoch 6:2.83 - F1: 0.0394
Time taken for Epoch 7:4.00 - F1: 0.0394
2026-02-14 05:29:26 - INFO - Time taken for Epoch 7:4.00 - F1: 0.0394
Time taken for Epoch 8:2.83 - F1: 0.0394
2026-02-14 05:29:29 - INFO - Time taken for Epoch 8:2.83 - F1: 0.0394
Time taken for Epoch 9:2.83 - F1: 0.0476
2026-02-14 05:29:31 - INFO - Time taken for Epoch 9:2.83 - F1: 0.0476
Time taken for Epoch 10:4.03 - F1: 0.0476
2026-02-14 05:29:35 - INFO - Time taken for Epoch 10:4.03 - F1: 0.0476
Time taken for Epoch 11:2.83 - F1: 0.0476
2026-02-14 05:29:38 - INFO - Time taken for Epoch 11:2.83 - F1: 0.0476
Time taken for Epoch 12:2.83 - F1: 0.0476
2026-02-14 05:29:41 - INFO - Time taken for Epoch 12:2.83 - F1: 0.0476
Time taken for Epoch 13:2.83 - F1: 0.0189
2026-02-14 05:29:44 - INFO - Time taken for Epoch 13:2.83 - F1: 0.0189
Time taken for Epoch 14:2.83 - F1: 0.0363
2026-02-14 05:29:47 - INFO - Time taken for Epoch 14:2.83 - F1: 0.0363
Time taken for Epoch 15:2.83 - F1: 0.0363
2026-02-14 05:29:50 - INFO - Time taken for Epoch 15:2.83 - F1: 0.0363
Time taken for Epoch 16:2.83 - F1: 0.0363
2026-02-14 05:29:52 - INFO - Time taken for Epoch 16:2.83 - F1: 0.0363
Time taken for Epoch 17:2.83 - F1: 0.0363
2026-02-14 05:29:55 - INFO - Time taken for Epoch 17:2.83 - F1: 0.0363
Time taken for Epoch 18:2.84 - F1: 0.0363
2026-02-14 05:29:58 - INFO - Time taken for Epoch 18:2.84 - F1: 0.0363
Time taken for Epoch 19:2.84 - F1: 0.0363
2026-02-14 05:30:01 - INFO - Time taken for Epoch 19:2.84 - F1: 0.0363
Performance not improving for 10 consecutive epochs.
2026-02-14 05:30:01 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:8
2026-02-14 05:30:01 - INFO - Best F1:0.0476 - Best Epoch:8
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0001
2026-02-14 05:30:09 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0001
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(8.762635073616964e-05)}
2026-02-14 05:30:09 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(8.762635073616964e-05)}

Total time taken: 610.19 seconds
2026-02-14 05:30:09 - INFO - 
Total time taken: 610.19 seconds
2026-02-14 05:30:09 - INFO - Trial 5 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0006116819186710884, 'weight_decay': 9.623912824362104e-05, 'batch_size': 64, 'co_train_epochs': 11, 'epoch_patience': 6}. Best is trial 1 with value: 0.6806226545745653.
Using devices: cuda, cuda
2026-02-14 05:30:09 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 05:30:09 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 05:30:09 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 05:30:09 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 7.1449235076701e-05
Weight Decay: 1.419977226973508e-05
Batch Size: 64
No. Epochs: 19
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-14 05:30:09 - INFO - Learning Rate: 7.1449235076701e-05
Weight Decay: 1.419977226973508e-05
Batch Size: 64
No. Epochs: 19
Epoch Patience: 9
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 05:30:11 - INFO - Generating initial weights
Time taken for Epoch 1:19.22 - F1: 0.1024
2026-02-14 05:30:33 - INFO - Time taken for Epoch 1:19.22 - F1: 0.1024
Time taken for Epoch 2:19.13 - F1: 0.1560
2026-02-14 05:30:52 - INFO - Time taken for Epoch 2:19.13 - F1: 0.1560
Time taken for Epoch 3:19.16 - F1: 0.2063
2026-02-14 05:31:12 - INFO - Time taken for Epoch 3:19.16 - F1: 0.2063
Time taken for Epoch 4:19.15 - F1: 0.2617
2026-02-14 05:31:31 - INFO - Time taken for Epoch 4:19.15 - F1: 0.2617
Time taken for Epoch 5:19.16 - F1: 0.3302
2026-02-14 05:31:50 - INFO - Time taken for Epoch 5:19.16 - F1: 0.3302
Time taken for Epoch 6:19.17 - F1: 0.3830
2026-02-14 05:32:09 - INFO - Time taken for Epoch 6:19.17 - F1: 0.3830
Time taken for Epoch 7:19.17 - F1: 0.4110
2026-02-14 05:32:28 - INFO - Time taken for Epoch 7:19.17 - F1: 0.4110
Time taken for Epoch 8:19.20 - F1: 0.4208
2026-02-14 05:32:47 - INFO - Time taken for Epoch 8:19.20 - F1: 0.4208
Time taken for Epoch 9:19.25 - F1: 0.4290
2026-02-14 05:33:07 - INFO - Time taken for Epoch 9:19.25 - F1: 0.4290
Time taken for Epoch 10:19.24 - F1: 0.4296
2026-02-14 05:33:26 - INFO - Time taken for Epoch 10:19.24 - F1: 0.4296
Time taken for Epoch 11:19.21 - F1: 0.4316
2026-02-14 05:33:45 - INFO - Time taken for Epoch 11:19.21 - F1: 0.4316
Time taken for Epoch 12:19.18 - F1: 0.4327
2026-02-14 05:34:04 - INFO - Time taken for Epoch 12:19.18 - F1: 0.4327
Time taken for Epoch 13:19.20 - F1: 0.4308
2026-02-14 05:34:23 - INFO - Time taken for Epoch 13:19.20 - F1: 0.4308
Time taken for Epoch 14:19.19 - F1: 0.4285
2026-02-14 05:34:43 - INFO - Time taken for Epoch 14:19.19 - F1: 0.4285
Time taken for Epoch 15:19.23 - F1: 0.4331
2026-02-14 05:35:02 - INFO - Time taken for Epoch 15:19.23 - F1: 0.4331
Time taken for Epoch 16:19.23 - F1: 0.4341
2026-02-14 05:35:21 - INFO - Time taken for Epoch 16:19.23 - F1: 0.4341
Time taken for Epoch 17:19.22 - F1: 0.4348
2026-02-14 05:35:40 - INFO - Time taken for Epoch 17:19.22 - F1: 0.4348
Time taken for Epoch 18:19.24 - F1: 0.4374
2026-02-14 05:36:00 - INFO - Time taken for Epoch 18:19.24 - F1: 0.4374
Time taken for Epoch 19:19.22 - F1: 0.4359
2026-02-14 05:36:19 - INFO - Time taken for Epoch 19:19.22 - F1: 0.4359
Best F1:0.4374 - Best Epoch:18
2026-02-14 05:36:19 - INFO - Best F1:0.4374 - Best Epoch:18
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 05:36:20 - INFO - Starting co-training
Time taken for Epoch 1: 46.11s - F1: 0.63503884
2026-02-14 05:37:07 - INFO - Time taken for Epoch 1: 46.11s - F1: 0.63503884
Time taken for Epoch 2: 47.23s - F1: 0.61285672
2026-02-14 05:37:54 - INFO - Time taken for Epoch 2: 47.23s - F1: 0.61285672
Time taken for Epoch 3: 46.17s - F1: 0.65533394
2026-02-14 05:38:40 - INFO - Time taken for Epoch 3: 46.17s - F1: 0.65533394
Time taken for Epoch 4: 47.35s - F1: 0.63171653
2026-02-14 05:39:27 - INFO - Time taken for Epoch 4: 47.35s - F1: 0.63171653
Time taken for Epoch 5: 46.19s - F1: 0.63089050
2026-02-14 05:40:14 - INFO - Time taken for Epoch 5: 46.19s - F1: 0.63089050
Time taken for Epoch 6: 46.20s - F1: 0.64914302
2026-02-14 05:41:00 - INFO - Time taken for Epoch 6: 46.20s - F1: 0.64914302
Time taken for Epoch 7: 46.20s - F1: 0.63533889
2026-02-14 05:41:46 - INFO - Time taken for Epoch 7: 46.20s - F1: 0.63533889
Time taken for Epoch 8: 46.20s - F1: 0.64216916
2026-02-14 05:42:32 - INFO - Time taken for Epoch 8: 46.20s - F1: 0.64216916
Time taken for Epoch 9: 46.19s - F1: 0.62823907
2026-02-14 05:43:18 - INFO - Time taken for Epoch 9: 46.19s - F1: 0.62823907
Time taken for Epoch 10: 46.21s - F1: 0.59914979
2026-02-14 05:44:05 - INFO - Time taken for Epoch 10: 46.21s - F1: 0.59914979
Time taken for Epoch 11: 46.21s - F1: 0.63514751
2026-02-14 05:44:51 - INFO - Time taken for Epoch 11: 46.21s - F1: 0.63514751
Time taken for Epoch 12: 46.21s - F1: 0.63655168
2026-02-14 05:45:37 - INFO - Time taken for Epoch 12: 46.21s - F1: 0.63655168
Performance not improving for 9 consecutive epochs.
Performance not improving for 9 consecutive epochs.
2026-02-14 05:45:37 - INFO - Performance not improving for 9 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 05:45:49 - INFO - Fine-tuning models
Time taken for Epoch 1:2.84 - F1: 0.6506
2026-02-14 05:45:52 - INFO - Time taken for Epoch 1:2.84 - F1: 0.6506
Time taken for Epoch 2:3.90 - F1: 0.6747
2026-02-14 05:45:56 - INFO - Time taken for Epoch 2:3.90 - F1: 0.6747
Time taken for Epoch 3:3.99 - F1: 0.6590
2026-02-14 05:46:00 - INFO - Time taken for Epoch 3:3.99 - F1: 0.6590
Time taken for Epoch 4:2.83 - F1: 0.6611
2026-02-14 05:46:02 - INFO - Time taken for Epoch 4:2.83 - F1: 0.6611
Time taken for Epoch 5:2.83 - F1: 0.6519
2026-02-14 05:46:05 - INFO - Time taken for Epoch 5:2.83 - F1: 0.6519
Time taken for Epoch 6:2.84 - F1: 0.6700
2026-02-14 05:46:08 - INFO - Time taken for Epoch 6:2.84 - F1: 0.6700
Time taken for Epoch 7:2.83 - F1: 0.6714
2026-02-14 05:46:11 - INFO - Time taken for Epoch 7:2.83 - F1: 0.6714
Time taken for Epoch 8:2.83 - F1: 0.6591
2026-02-14 05:46:14 - INFO - Time taken for Epoch 8:2.83 - F1: 0.6591
Time taken for Epoch 9:2.83 - F1: 0.6648
2026-02-14 05:46:17 - INFO - Time taken for Epoch 9:2.83 - F1: 0.6648
Time taken for Epoch 10:2.83 - F1: 0.6608
2026-02-14 05:46:19 - INFO - Time taken for Epoch 10:2.83 - F1: 0.6608
Time taken for Epoch 11:2.83 - F1: 0.6709
2026-02-14 05:46:22 - INFO - Time taken for Epoch 11:2.83 - F1: 0.6709
Time taken for Epoch 12:2.83 - F1: 0.6725
2026-02-14 05:46:25 - INFO - Time taken for Epoch 12:2.83 - F1: 0.6725
Performance not improving for 10 consecutive epochs.
2026-02-14 05:46:25 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6747 - Best Epoch:1
2026-02-14 05:46:25 - INFO - Best F1:0.6747 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6702, Test ECE: 0.0378
2026-02-14 05:46:33 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6702, Test ECE: 0.0378
All results: {'f1_macro': 0.670185383484491, 'ece': np.float64(0.037818443634639)}
2026-02-14 05:46:33 - INFO - All results: {'f1_macro': 0.670185383484491, 'ece': np.float64(0.037818443634639)}

Total time taken: 983.78 seconds
2026-02-14 05:46:33 - INFO - 
Total time taken: 983.78 seconds
2026-02-14 05:46:33 - INFO - Trial 6 finished with value: 0.670185383484491 and parameters: {'learning_rate': 7.1449235076701e-05, 'weight_decay': 1.419977226973508e-05, 'batch_size': 64, 'co_train_epochs': 19, 'epoch_patience': 9}. Best is trial 1 with value: 0.6806226545745653.
Using devices: cuda, cuda
2026-02-14 05:46:33 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 05:46:33 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 05:46:33 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 05:46:33 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 0.0006334393393909541
Weight Decay: 7.000924429564021e-05
Batch Size: 64
No. Epochs: 10
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-14 05:46:34 - INFO - Learning Rate: 0.0006334393393909541
Weight Decay: 7.000924429564021e-05
Batch Size: 64
No. Epochs: 10
Epoch Patience: 10
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 05:46:35 - INFO - Generating initial weights
Time taken for Epoch 1:19.14 - F1: 0.0580
2026-02-14 05:46:57 - INFO - Time taken for Epoch 1:19.14 - F1: 0.0580
Time taken for Epoch 2:19.09 - F1: 0.0296
2026-02-14 05:47:16 - INFO - Time taken for Epoch 2:19.09 - F1: 0.0296
Time taken for Epoch 3:19.10 - F1: 0.0621
2026-02-14 05:47:35 - INFO - Time taken for Epoch 3:19.10 - F1: 0.0621
Time taken for Epoch 4:19.13 - F1: 0.3117
2026-02-14 05:47:55 - INFO - Time taken for Epoch 4:19.13 - F1: 0.3117
Time taken for Epoch 5:19.12 - F1: 0.3252
2026-02-14 05:48:14 - INFO - Time taken for Epoch 5:19.12 - F1: 0.3252
Time taken for Epoch 6:19.16 - F1: 0.3749
2026-02-14 05:48:33 - INFO - Time taken for Epoch 6:19.16 - F1: 0.3749
Time taken for Epoch 7:19.15 - F1: 0.3956
2026-02-14 05:48:52 - INFO - Time taken for Epoch 7:19.15 - F1: 0.3956
Time taken for Epoch 8:19.16 - F1: 0.4100
2026-02-14 05:49:11 - INFO - Time taken for Epoch 8:19.16 - F1: 0.4100
Time taken for Epoch 9:19.15 - F1: 0.4414
2026-02-14 05:49:30 - INFO - Time taken for Epoch 9:19.15 - F1: 0.4414
Time taken for Epoch 10:19.15 - F1: 0.4547
2026-02-14 05:49:49 - INFO - Time taken for Epoch 10:19.15 - F1: 0.4547
Best F1:0.4547 - Best Epoch:10
2026-02-14 05:49:49 - INFO - Best F1:0.4547 - Best Epoch:10
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 05:49:50 - INFO - Starting co-training
Time taken for Epoch 1: 46.08s - F1: 0.04755179
2026-02-14 05:50:37 - INFO - Time taken for Epoch 1: 46.08s - F1: 0.04755179
Time taken for Epoch 2: 47.19s - F1: 0.04755179
2026-02-14 05:51:24 - INFO - Time taken for Epoch 2: 47.19s - F1: 0.04755179
Time taken for Epoch 3: 46.16s - F1: 0.04755179
2026-02-14 05:52:10 - INFO - Time taken for Epoch 3: 46.16s - F1: 0.04755179
Time taken for Epoch 4: 46.19s - F1: 0.04755179
2026-02-14 05:52:57 - INFO - Time taken for Epoch 4: 46.19s - F1: 0.04755179
Time taken for Epoch 5: 46.18s - F1: 0.04755179
2026-02-14 05:53:43 - INFO - Time taken for Epoch 5: 46.18s - F1: 0.04755179
Time taken for Epoch 6: 46.19s - F1: 0.04755179
2026-02-14 05:54:29 - INFO - Time taken for Epoch 6: 46.19s - F1: 0.04755179
Time taken for Epoch 7: 46.19s - F1: 0.04755179
2026-02-14 05:55:15 - INFO - Time taken for Epoch 7: 46.19s - F1: 0.04755179
Time taken for Epoch 8: 46.19s - F1: 0.04755179
2026-02-14 05:56:01 - INFO - Time taken for Epoch 8: 46.19s - F1: 0.04755179
Time taken for Epoch 9: 46.18s - F1: 0.04755179
2026-02-14 05:56:47 - INFO - Time taken for Epoch 9: 46.18s - F1: 0.04755179
Time taken for Epoch 10: 46.15s - F1: 0.04755179
2026-02-14 05:57:34 - INFO - Time taken for Epoch 10: 46.15s - F1: 0.04755179
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 05:57:36 - INFO - Fine-tuning models
Time taken for Epoch 1:2.85 - F1: 0.0363
2026-02-14 05:57:39 - INFO - Time taken for Epoch 1:2.85 - F1: 0.0363
Time taken for Epoch 2:3.90 - F1: 0.0189
2026-02-14 05:57:43 - INFO - Time taken for Epoch 2:3.90 - F1: 0.0189
Time taken for Epoch 3:2.83 - F1: 0.0064
2026-02-14 05:57:46 - INFO - Time taken for Epoch 3:2.83 - F1: 0.0064
Time taken for Epoch 4:2.83 - F1: 0.0064
2026-02-14 05:57:49 - INFO - Time taken for Epoch 4:2.83 - F1: 0.0064
Time taken for Epoch 5:2.83 - F1: 0.0394
2026-02-14 05:57:52 - INFO - Time taken for Epoch 5:2.83 - F1: 0.0394
Time taken for Epoch 6:3.99 - F1: 0.0394
2026-02-14 05:57:56 - INFO - Time taken for Epoch 6:3.99 - F1: 0.0394
Time taken for Epoch 7:2.84 - F1: 0.0394
2026-02-14 05:57:58 - INFO - Time taken for Epoch 7:2.84 - F1: 0.0394
Time taken for Epoch 8:2.84 - F1: 0.0476
2026-02-14 05:58:01 - INFO - Time taken for Epoch 8:2.84 - F1: 0.0476
Time taken for Epoch 9:4.00 - F1: 0.0476
2026-02-14 05:58:05 - INFO - Time taken for Epoch 9:4.00 - F1: 0.0476
Time taken for Epoch 10:2.83 - F1: 0.0476
2026-02-14 05:58:08 - INFO - Time taken for Epoch 10:2.83 - F1: 0.0476
Time taken for Epoch 11:2.83 - F1: 0.0476
2026-02-14 05:58:11 - INFO - Time taken for Epoch 11:2.83 - F1: 0.0476
Time taken for Epoch 12:2.83 - F1: 0.0476
2026-02-14 05:58:14 - INFO - Time taken for Epoch 12:2.83 - F1: 0.0476
Time taken for Epoch 13:2.83 - F1: 0.0189
2026-02-14 05:58:17 - INFO - Time taken for Epoch 13:2.83 - F1: 0.0189
Time taken for Epoch 14:2.83 - F1: 0.0189
2026-02-14 05:58:19 - INFO - Time taken for Epoch 14:2.83 - F1: 0.0189
Time taken for Epoch 15:2.83 - F1: 0.0189
2026-02-14 05:58:22 - INFO - Time taken for Epoch 15:2.83 - F1: 0.0189
Time taken for Epoch 16:2.83 - F1: 0.0189
2026-02-14 05:58:25 - INFO - Time taken for Epoch 16:2.83 - F1: 0.0189
Time taken for Epoch 17:2.83 - F1: 0.0189
2026-02-14 05:58:28 - INFO - Time taken for Epoch 17:2.83 - F1: 0.0189
Time taken for Epoch 18:2.83 - F1: 0.0363
2026-02-14 05:58:31 - INFO - Time taken for Epoch 18:2.83 - F1: 0.0363
Performance not improving for 10 consecutive epochs.
2026-02-14 05:58:31 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:7
2026-02-14 05:58:31 - INFO - Best F1:0.0476 - Best Epoch:7
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0125
2026-02-14 05:58:38 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0125
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.012540686663741363)}
2026-02-14 05:58:38 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.012540686663741363)}

Total time taken: 725.45 seconds
2026-02-14 05:58:38 - INFO - 
Total time taken: 725.45 seconds
2026-02-14 05:58:38 - INFO - Trial 7 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0006334393393909541, 'weight_decay': 7.000924429564021e-05, 'batch_size': 64, 'co_train_epochs': 10, 'epoch_patience': 10}. Best is trial 1 with value: 0.6806226545745653.
Using devices: cuda, cuda
2026-02-14 05:58:38 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 05:58:38 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 05:58:38 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 05:58:38 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 1.9404671837319212e-05
Weight Decay: 0.006718304122261106
Batch Size: 32
No. Epochs: 9
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-14 05:58:39 - INFO - Learning Rate: 1.9404671837319212e-05
Weight Decay: 0.006718304122261106
Batch Size: 32
No. Epochs: 9
Epoch Patience: 6
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 05:58:40 - INFO - Generating initial weights
Time taken for Epoch 1:20.13 - F1: 0.0549
2026-02-14 05:59:03 - INFO - Time taken for Epoch 1:20.13 - F1: 0.0549
Time taken for Epoch 2:20.05 - F1: 0.0744
2026-02-14 05:59:23 - INFO - Time taken for Epoch 2:20.05 - F1: 0.0744
Time taken for Epoch 3:20.07 - F1: 0.0593
2026-02-14 05:59:43 - INFO - Time taken for Epoch 3:20.07 - F1: 0.0593
Time taken for Epoch 4:20.12 - F1: 0.0705
2026-02-14 06:00:03 - INFO - Time taken for Epoch 4:20.12 - F1: 0.0705
Time taken for Epoch 5:20.20 - F1: 0.0720
2026-02-14 06:00:24 - INFO - Time taken for Epoch 5:20.20 - F1: 0.0720
Time taken for Epoch 6:20.19 - F1: 0.0713
2026-02-14 06:00:44 - INFO - Time taken for Epoch 6:20.19 - F1: 0.0713
Time taken for Epoch 7:20.21 - F1: 0.0742
2026-02-14 06:01:04 - INFO - Time taken for Epoch 7:20.21 - F1: 0.0742
Time taken for Epoch 8:20.20 - F1: 0.0740
2026-02-14 06:01:24 - INFO - Time taken for Epoch 8:20.20 - F1: 0.0740
Time taken for Epoch 9:20.21 - F1: 0.0747
2026-02-14 06:01:44 - INFO - Time taken for Epoch 9:20.21 - F1: 0.0747
Best F1:0.0747 - Best Epoch:9
2026-02-14 06:01:44 - INFO - Best F1:0.0747 - Best Epoch:9
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 06:01:45 - INFO - Starting co-training
Time taken for Epoch 1: 35.28s - F1: 0.31095094
2026-02-14 06:02:21 - INFO - Time taken for Epoch 1: 35.28s - F1: 0.31095094
Time taken for Epoch 2: 36.33s - F1: 0.47303954
2026-02-14 06:02:58 - INFO - Time taken for Epoch 2: 36.33s - F1: 0.47303954
Time taken for Epoch 3: 36.74s - F1: 0.51091132
2026-02-14 06:03:34 - INFO - Time taken for Epoch 3: 36.74s - F1: 0.51091132
Time taken for Epoch 4: 36.43s - F1: 0.51540892
2026-02-14 06:04:11 - INFO - Time taken for Epoch 4: 36.43s - F1: 0.51540892
Time taken for Epoch 5: 36.45s - F1: 0.56305244
2026-02-14 06:04:47 - INFO - Time taken for Epoch 5: 36.45s - F1: 0.56305244
Time taken for Epoch 6: 36.44s - F1: 0.62146233
2026-02-14 06:05:24 - INFO - Time taken for Epoch 6: 36.44s - F1: 0.62146233
Time taken for Epoch 7: 36.46s - F1: 0.64137692
2026-02-14 06:06:00 - INFO - Time taken for Epoch 7: 36.46s - F1: 0.64137692
Time taken for Epoch 8: 36.46s - F1: 0.63574589
2026-02-14 06:06:37 - INFO - Time taken for Epoch 8: 36.46s - F1: 0.63574589
Time taken for Epoch 9: 35.29s - F1: 0.64455972
2026-02-14 06:07:12 - INFO - Time taken for Epoch 9: 35.29s - F1: 0.64455972
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 06:07:16 - INFO - Fine-tuning models
Time taken for Epoch 1:3.00 - F1: 0.6425
2026-02-14 06:07:19 - INFO - Time taken for Epoch 1:3.00 - F1: 0.6425
Time taken for Epoch 2:4.06 - F1: 0.6536
2026-02-14 06:07:23 - INFO - Time taken for Epoch 2:4.06 - F1: 0.6536
Time taken for Epoch 3:4.16 - F1: 0.6427
2026-02-14 06:07:27 - INFO - Time taken for Epoch 3:4.16 - F1: 0.6427
Time taken for Epoch 4:2.99 - F1: 0.6468
2026-02-14 06:07:30 - INFO - Time taken for Epoch 4:2.99 - F1: 0.6468
Time taken for Epoch 5:2.99 - F1: 0.6456
2026-02-14 06:07:33 - INFO - Time taken for Epoch 5:2.99 - F1: 0.6456
Time taken for Epoch 6:2.99 - F1: 0.6403
2026-02-14 06:07:36 - INFO - Time taken for Epoch 6:2.99 - F1: 0.6403
Time taken for Epoch 7:3.00 - F1: 0.6405
2026-02-14 06:07:39 - INFO - Time taken for Epoch 7:3.00 - F1: 0.6405
Time taken for Epoch 8:2.99 - F1: 0.6427
2026-02-14 06:07:42 - INFO - Time taken for Epoch 8:2.99 - F1: 0.6427
Time taken for Epoch 9:2.99 - F1: 0.6462
2026-02-14 06:07:45 - INFO - Time taken for Epoch 9:2.99 - F1: 0.6462
Time taken for Epoch 10:2.99 - F1: 0.6551
2026-02-14 06:07:48 - INFO - Time taken for Epoch 10:2.99 - F1: 0.6551
Time taken for Epoch 11:4.16 - F1: 0.6645
2026-02-14 06:07:52 - INFO - Time taken for Epoch 11:4.16 - F1: 0.6645
Time taken for Epoch 12:4.64 - F1: 0.6548
2026-02-14 06:07:57 - INFO - Time taken for Epoch 12:4.64 - F1: 0.6548
Time taken for Epoch 13:2.99 - F1: 0.6544
2026-02-14 06:08:00 - INFO - Time taken for Epoch 13:2.99 - F1: 0.6544
Time taken for Epoch 14:3.00 - F1: 0.6431
2026-02-14 06:08:03 - INFO - Time taken for Epoch 14:3.00 - F1: 0.6431
Time taken for Epoch 15:2.99 - F1: 0.6447
2026-02-14 06:08:06 - INFO - Time taken for Epoch 15:2.99 - F1: 0.6447
Time taken for Epoch 16:2.99 - F1: 0.6440
2026-02-14 06:08:09 - INFO - Time taken for Epoch 16:2.99 - F1: 0.6440
Time taken for Epoch 17:2.99 - F1: 0.6425
2026-02-14 06:08:12 - INFO - Time taken for Epoch 17:2.99 - F1: 0.6425
Time taken for Epoch 18:2.99 - F1: 0.6405
2026-02-14 06:08:15 - INFO - Time taken for Epoch 18:2.99 - F1: 0.6405
Time taken for Epoch 19:2.99 - F1: 0.6447
2026-02-14 06:08:18 - INFO - Time taken for Epoch 19:2.99 - F1: 0.6447
Time taken for Epoch 20:3.00 - F1: 0.6477
2026-02-14 06:08:21 - INFO - Time taken for Epoch 20:3.00 - F1: 0.6477
Time taken for Epoch 21:2.99 - F1: 0.6546
2026-02-14 06:08:24 - INFO - Time taken for Epoch 21:2.99 - F1: 0.6546
Performance not improving for 10 consecutive epochs.
2026-02-14 06:08:24 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6645 - Best Epoch:10
2026-02-14 06:08:24 - INFO - Best F1:0.6645 - Best Epoch:10
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6716, Test ECE: 0.0352
2026-02-14 06:08:32 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6716, Test ECE: 0.0352
All results: {'f1_macro': 0.6715752728566211, 'ece': np.float64(0.03519716032169728)}
2026-02-14 06:08:32 - INFO - All results: {'f1_macro': 0.6715752728566211, 'ece': np.float64(0.03519716032169728)}

Total time taken: 593.63 seconds
2026-02-14 06:08:32 - INFO - 
Total time taken: 593.63 seconds
2026-02-14 06:08:32 - INFO - Trial 8 finished with value: 0.6715752728566211 and parameters: {'learning_rate': 1.9404671837319212e-05, 'weight_decay': 0.006718304122261106, 'batch_size': 32, 'co_train_epochs': 9, 'epoch_patience': 6}. Best is trial 1 with value: 0.6806226545745653.
Using devices: cuda, cuda
2026-02-14 06:08:32 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 06:08:32 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 06:08:32 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-14 06:08:32 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 0.00011394044051567972
Weight Decay: 0.006343822868551813
Batch Size: 16
No. Epochs: 13
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-14 06:08:32 - INFO - Learning Rate: 0.00011394044051567972
Weight Decay: 0.006343822868551813
Batch Size: 16
No. Epochs: 13
Epoch Patience: 4
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 06:08:33 - INFO - Generating initial weights
Time taken for Epoch 1:20.77 - F1: 0.0655
2026-02-14 06:08:58 - INFO - Time taken for Epoch 1:20.77 - F1: 0.0655
Time taken for Epoch 2:20.69 - F1: 0.0826
2026-02-14 06:09:18 - INFO - Time taken for Epoch 2:20.69 - F1: 0.0826
Time taken for Epoch 3:20.69 - F1: 0.1137
2026-02-14 06:09:39 - INFO - Time taken for Epoch 3:20.69 - F1: 0.1137
Time taken for Epoch 4:20.72 - F1: 0.3250
2026-02-14 06:10:00 - INFO - Time taken for Epoch 4:20.72 - F1: 0.3250
Time taken for Epoch 5:20.76 - F1: 0.4005
2026-02-14 06:10:21 - INFO - Time taken for Epoch 5:20.76 - F1: 0.4005
Time taken for Epoch 6:20.74 - F1: 0.4099
2026-02-14 06:10:41 - INFO - Time taken for Epoch 6:20.74 - F1: 0.4099
Time taken for Epoch 7:20.80 - F1: 0.4069
2026-02-14 06:11:02 - INFO - Time taken for Epoch 7:20.80 - F1: 0.4069
Time taken for Epoch 8:20.78 - F1: 0.4150
2026-02-14 06:11:23 - INFO - Time taken for Epoch 8:20.78 - F1: 0.4150
Time taken for Epoch 9:20.79 - F1: 0.4414
2026-02-14 06:11:44 - INFO - Time taken for Epoch 9:20.79 - F1: 0.4414
Time taken for Epoch 10:20.78 - F1: 0.4629
2026-02-14 06:12:04 - INFO - Time taken for Epoch 10:20.78 - F1: 0.4629
Time taken for Epoch 11:20.76 - F1: 0.4749
2026-02-14 06:12:25 - INFO - Time taken for Epoch 11:20.76 - F1: 0.4749
Time taken for Epoch 12:20.79 - F1: 0.4730
2026-02-14 06:12:46 - INFO - Time taken for Epoch 12:20.79 - F1: 0.4730
Time taken for Epoch 13:20.80 - F1: 0.4682
2026-02-14 06:13:07 - INFO - Time taken for Epoch 13:20.80 - F1: 0.4682
Best F1:0.4749 - Best Epoch:11
2026-02-14 06:13:07 - INFO - Best F1:0.4749 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 06:13:08 - INFO - Starting co-training
Time taken for Epoch 1: 29.35s - F1: 0.56061353
2026-02-14 06:13:38 - INFO - Time taken for Epoch 1: 29.35s - F1: 0.56061353
Time taken for Epoch 2: 30.44s - F1: 0.49240884
2026-02-14 06:14:08 - INFO - Time taken for Epoch 2: 30.44s - F1: 0.49240884
Time taken for Epoch 3: 29.38s - F1: 0.56873804
2026-02-14 06:14:37 - INFO - Time taken for Epoch 3: 29.38s - F1: 0.56873804
Time taken for Epoch 4: 30.52s - F1: 0.57114845
2026-02-14 06:15:08 - INFO - Time taken for Epoch 4: 30.52s - F1: 0.57114845
Time taken for Epoch 5: 30.53s - F1: 0.57052228
2026-02-14 06:15:39 - INFO - Time taken for Epoch 5: 30.53s - F1: 0.57052228
Time taken for Epoch 6: 29.37s - F1: 0.58027833
2026-02-14 06:16:08 - INFO - Time taken for Epoch 6: 29.37s - F1: 0.58027833
Time taken for Epoch 7: 30.53s - F1: 0.57930416
2026-02-14 06:16:38 - INFO - Time taken for Epoch 7: 30.53s - F1: 0.57930416
Time taken for Epoch 8: 29.37s - F1: 0.59365844
2026-02-14 06:17:08 - INFO - Time taken for Epoch 8: 29.37s - F1: 0.59365844
Time taken for Epoch 9: 30.53s - F1: 0.58041073
2026-02-14 06:17:38 - INFO - Time taken for Epoch 9: 30.53s - F1: 0.58041073
Time taken for Epoch 10: 29.37s - F1: 0.58045042
2026-02-14 06:18:08 - INFO - Time taken for Epoch 10: 29.37s - F1: 0.58045042
Time taken for Epoch 11: 29.37s - F1: 0.60489160
2026-02-14 06:18:37 - INFO - Time taken for Epoch 11: 29.37s - F1: 0.60489160
Time taken for Epoch 12: 30.53s - F1: 0.59412959
2026-02-14 06:19:08 - INFO - Time taken for Epoch 12: 30.53s - F1: 0.59412959
Time taken for Epoch 13: 29.47s - F1: 0.60299976
2026-02-14 06:19:37 - INFO - Time taken for Epoch 13: 29.47s - F1: 0.60299976
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-14 06:19:40 - INFO - Fine-tuning models
Time taken for Epoch 1:3.08 - F1: 0.6016
2026-02-14 06:19:43 - INFO - Time taken for Epoch 1:3.08 - F1: 0.6016
Time taken for Epoch 2:4.16 - F1: 0.6137
2026-02-14 06:19:47 - INFO - Time taken for Epoch 2:4.16 - F1: 0.6137
Time taken for Epoch 3:4.25 - F1: 0.6141
2026-02-14 06:19:51 - INFO - Time taken for Epoch 3:4.25 - F1: 0.6141
Time taken for Epoch 4:4.23 - F1: 0.6212
2026-02-14 06:19:56 - INFO - Time taken for Epoch 4:4.23 - F1: 0.6212
Time taken for Epoch 5:4.25 - F1: 0.6303
2026-02-14 06:20:00 - INFO - Time taken for Epoch 5:4.25 - F1: 0.6303
Time taken for Epoch 6:4.26 - F1: 0.6277
2026-02-14 06:20:04 - INFO - Time taken for Epoch 6:4.26 - F1: 0.6277
Time taken for Epoch 7:3.07 - F1: 0.6195
2026-02-14 06:20:07 - INFO - Time taken for Epoch 7:3.07 - F1: 0.6195
Time taken for Epoch 8:3.07 - F1: 0.6128
2026-02-14 06:20:10 - INFO - Time taken for Epoch 8:3.07 - F1: 0.6128
Time taken for Epoch 9:3.07 - F1: 0.6304
2026-02-14 06:20:13 - INFO - Time taken for Epoch 9:3.07 - F1: 0.6304
Time taken for Epoch 10:4.23 - F1: 0.6210
2026-02-14 06:20:18 - INFO - Time taken for Epoch 10:4.23 - F1: 0.6210
Time taken for Epoch 11:3.08 - F1: 0.6263
2026-02-14 06:20:21 - INFO - Time taken for Epoch 11:3.08 - F1: 0.6263
Time taken for Epoch 12:3.08 - F1: 0.6261
2026-02-14 06:20:24 - INFO - Time taken for Epoch 12:3.08 - F1: 0.6261
Time taken for Epoch 13:3.07 - F1: 0.6274
2026-02-14 06:20:27 - INFO - Time taken for Epoch 13:3.07 - F1: 0.6274
Time taken for Epoch 14:3.07 - F1: 0.6229
2026-02-14 06:20:30 - INFO - Time taken for Epoch 14:3.07 - F1: 0.6229
Time taken for Epoch 15:3.08 - F1: 0.6131
2026-02-14 06:20:33 - INFO - Time taken for Epoch 15:3.08 - F1: 0.6131
Time taken for Epoch 16:3.08 - F1: 0.6216
2026-02-14 06:20:36 - INFO - Time taken for Epoch 16:3.08 - F1: 0.6216
Time taken for Epoch 17:3.08 - F1: 0.6196
2026-02-14 06:20:39 - INFO - Time taken for Epoch 17:3.08 - F1: 0.6196
Time taken for Epoch 18:3.08 - F1: 0.6147
2026-02-14 06:20:42 - INFO - Time taken for Epoch 18:3.08 - F1: 0.6147
Time taken for Epoch 19:3.08 - F1: 0.6205
2026-02-14 06:20:45 - INFO - Time taken for Epoch 19:3.08 - F1: 0.6205
Performance not improving for 10 consecutive epochs.
2026-02-14 06:20:45 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6304 - Best Epoch:8
2026-02-14 06:20:45 - INFO - Best F1:0.6304 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label10-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5840, Test ECE: 0.1198
2026-02-14 06:20:54 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5840, Test ECE: 0.1198
All results: {'f1_macro': 0.5840278861676791, 'ece': np.float64(0.11978876305519294)}
2026-02-14 06:20:54 - INFO - All results: {'f1_macro': 0.5840278861676791, 'ece': np.float64(0.11978876305519294)}

Total time taken: 741.78 seconds
2026-02-14 06:20:54 - INFO - 
Total time taken: 741.78 seconds
2026-02-14 06:20:54 - INFO - Trial 9 finished with value: 0.5840278861676791 and parameters: {'learning_rate': 0.00011394044051567972, 'weight_decay': 0.006343822868551813, 'batch_size': 16, 'co_train_epochs': 13, 'epoch_patience': 4}. Best is trial 1 with value: 0.6806226545745653.

[BEST TRIAL RESULTS]
2026-02-14 06:20:54 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6806
2026-02-14 06:20:54 - INFO - F1 Score: 0.6806
Params: {'learning_rate': 3.674873390278807e-05, 'weight_decay': 3.7049123048205384e-05, 'batch_size': 32, 'co_train_epochs': 19, 'epoch_patience': 8}
2026-02-14 06:20:54 - INFO - Params: {'learning_rate': 3.674873390278807e-05, 'weight_decay': 3.7049123048205384e-05, 'batch_size': 32, 'co_train_epochs': 19, 'epoch_patience': 8}
  learning_rate: 3.674873390278807e-05
2026-02-14 06:20:54 - INFO -   learning_rate: 3.674873390278807e-05
  weight_decay: 3.7049123048205384e-05
2026-02-14 06:20:54 - INFO -   weight_decay: 3.7049123048205384e-05
  batch_size: 32
2026-02-14 06:20:54 - INFO -   batch_size: 32
  co_train_epochs: 19
2026-02-14 06:20:54 - INFO -   co_train_epochs: 19
  epoch_patience: 8
2026-02-14 06:20:54 - INFO -   epoch_patience: 8

Total time taken: 8344.64 seconds
2026-02-14 06:20:54 - INFO - 
Total time taken: 8344.64 seconds