[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 03:54:49 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 03:54:49 - INFO - A new study created in memory with name: study_humanitarian10_cyclone_idai_2019
Using devices: cuda, cuda
2026-02-13 03:54:49 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 03:54:49 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 03:54:49 - INFO - Starting log
Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 03:54:49 - INFO - Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
Learning Rate: 5.354946433608293e-05
Weight Decay: 0.006461378682575525
Batch Size: 32
No. Epochs: 16
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-13 03:54:50 - INFO - Learning Rate: 5.354946433608293e-05
Weight Decay: 0.006461378682575525
Batch Size: 32
No. Epochs: 16
Epoch Patience: 6
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 03:54:51 - INFO - Generating initial weights
Time taken for Epoch 1:9.39 - F1: 0.0044
2026-02-13 03:55:01 - INFO - Time taken for Epoch 1:9.39 - F1: 0.0044
Time taken for Epoch 2:9.24 - F1: 0.0091
2026-02-13 03:55:11 - INFO - Time taken for Epoch 2:9.24 - F1: 0.0091
Time taken for Epoch 3:9.20 - F1: 0.0711
2026-02-13 03:55:20 - INFO - Time taken for Epoch 3:9.20 - F1: 0.0711
Time taken for Epoch 4:9.23 - F1: 0.1178
2026-02-13 03:55:29 - INFO - Time taken for Epoch 4:9.23 - F1: 0.1178
Time taken for Epoch 5:9.23 - F1: 0.1903
2026-02-13 03:55:38 - INFO - Time taken for Epoch 5:9.23 - F1: 0.1903
Time taken for Epoch 6:9.18 - F1: 0.1975
2026-02-13 03:55:47 - INFO - Time taken for Epoch 6:9.18 - F1: 0.1975
Time taken for Epoch 7:9.23 - F1: 0.2515
2026-02-13 03:55:57 - INFO - Time taken for Epoch 7:9.23 - F1: 0.2515
Time taken for Epoch 8:9.23 - F1: 0.2938
2026-02-13 03:56:06 - INFO - Time taken for Epoch 8:9.23 - F1: 0.2938
Time taken for Epoch 9:9.17 - F1: 0.3560
2026-02-13 03:56:15 - INFO - Time taken for Epoch 9:9.17 - F1: 0.3560
Time taken for Epoch 10:9.23 - F1: 0.3942
2026-02-13 03:56:24 - INFO - Time taken for Epoch 10:9.23 - F1: 0.3942
Time taken for Epoch 11:9.21 - F1: 0.3940
2026-02-13 03:56:34 - INFO - Time taken for Epoch 11:9.21 - F1: 0.3940
Time taken for Epoch 12:9.17 - F1: 0.4068
2026-02-13 03:56:43 - INFO - Time taken for Epoch 12:9.17 - F1: 0.4068
Time taken for Epoch 13:9.17 - F1: 0.4067
2026-02-13 03:56:52 - INFO - Time taken for Epoch 13:9.17 - F1: 0.4067
Time taken for Epoch 14:9.17 - F1: 0.3941
2026-02-13 03:57:01 - INFO - Time taken for Epoch 14:9.17 - F1: 0.3941
Time taken for Epoch 15:9.18 - F1: 0.4191
2026-02-13 03:57:10 - INFO - Time taken for Epoch 15:9.18 - F1: 0.4191
Time taken for Epoch 16:9.17 - F1: 0.4015
2026-02-13 03:57:19 - INFO - Time taken for Epoch 16:9.17 - F1: 0.4015
Best F1:0.4191 - Best Epoch:15
2026-02-13 03:57:19 - INFO - Best F1:0.4191 - Best Epoch:15
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 03:57:21 - INFO - Starting co-training
Time taken for Epoch 1: 12.02s - F1: 0.20091341
2026-02-13 03:57:33 - INFO - Time taken for Epoch 1: 12.02s - F1: 0.20091341
Time taken for Epoch 2: 13.08s - F1: 0.29620639
2026-02-13 03:57:46 - INFO - Time taken for Epoch 2: 13.08s - F1: 0.29620639
Time taken for Epoch 3: 18.24s - F1: 0.30279141
2026-02-13 03:58:04 - INFO - Time taken for Epoch 3: 18.24s - F1: 0.30279141
Time taken for Epoch 4: 18.25s - F1: 0.29026922
2026-02-13 03:58:23 - INFO - Time taken for Epoch 4: 18.25s - F1: 0.29026922
Time taken for Epoch 5: 11.97s - F1: 0.31484375
2026-02-13 03:58:35 - INFO - Time taken for Epoch 5: 11.97s - F1: 0.31484375
Time taken for Epoch 6: 16.71s - F1: 0.30165395
2026-02-13 03:58:51 - INFO - Time taken for Epoch 6: 16.71s - F1: 0.30165395
Time taken for Epoch 7: 11.98s - F1: 0.34315841
2026-02-13 03:59:03 - INFO - Time taken for Epoch 7: 11.98s - F1: 0.34315841
Time taken for Epoch 8: 21.50s - F1: 0.32816527
2026-02-13 03:59:25 - INFO - Time taken for Epoch 8: 21.50s - F1: 0.32816527
Time taken for Epoch 9: 11.99s - F1: 0.30518267
2026-02-13 03:59:37 - INFO - Time taken for Epoch 9: 11.99s - F1: 0.30518267
Time taken for Epoch 10: 11.99s - F1: 0.34429368
2026-02-13 03:59:49 - INFO - Time taken for Epoch 10: 11.99s - F1: 0.34429368
Time taken for Epoch 11: 19.29s - F1: 0.36283859
2026-02-13 04:00:08 - INFO - Time taken for Epoch 11: 19.29s - F1: 0.36283859
Time taken for Epoch 12: 19.51s - F1: 0.35958116
2026-02-13 04:00:28 - INFO - Time taken for Epoch 12: 19.51s - F1: 0.35958116
Time taken for Epoch 13: 11.98s - F1: 0.35460583
2026-02-13 04:00:40 - INFO - Time taken for Epoch 13: 11.98s - F1: 0.35460583
Time taken for Epoch 14: 11.97s - F1: 0.39575032
2026-02-13 04:00:52 - INFO - Time taken for Epoch 14: 11.97s - F1: 0.39575032
Time taken for Epoch 15: 18.53s - F1: 0.41420741
2026-02-13 04:01:10 - INFO - Time taken for Epoch 15: 18.53s - F1: 0.41420741
Time taken for Epoch 16: 18.62s - F1: 0.42294421
2026-02-13 04:01:29 - INFO - Time taken for Epoch 16: 18.62s - F1: 0.42294421
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:776: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Fine-tuning models
2026-02-13 04:01:38 - INFO - Fine-tuning models
Time taken for Epoch 1:3.46 - F1: 0.3656
2026-02-13 04:01:42 - INFO - Time taken for Epoch 1:3.46 - F1: 0.3656
Time taken for Epoch 2:4.38 - F1: 0.3640
2026-02-13 04:01:46 - INFO - Time taken for Epoch 2:4.38 - F1: 0.3640
Time taken for Epoch 3:3.39 - F1: 0.3998
2026-02-13 04:01:50 - INFO - Time taken for Epoch 3:3.39 - F1: 0.3998
Time taken for Epoch 4:10.85 - F1: 0.4153
2026-02-13 04:02:01 - INFO - Time taken for Epoch 4:10.85 - F1: 0.4153
Time taken for Epoch 5:9.65 - F1: 0.4294
2026-02-13 04:02:10 - INFO - Time taken for Epoch 5:9.65 - F1: 0.4294
Time taken for Epoch 6:7.95 - F1: 0.4285
2026-02-13 04:02:18 - INFO - Time taken for Epoch 6:7.95 - F1: 0.4285
Time taken for Epoch 7:3.40 - F1: 0.4494
2026-02-13 04:02:22 - INFO - Time taken for Epoch 7:3.40 - F1: 0.4494
Time taken for Epoch 8:11.32 - F1: 0.4543
2026-02-13 04:02:33 - INFO - Time taken for Epoch 8:11.32 - F1: 0.4543
Time taken for Epoch 9:10.87 - F1: 0.4735
2026-02-13 04:02:44 - INFO - Time taken for Epoch 9:10.87 - F1: 0.4735
Time taken for Epoch 10:9.72 - F1: 0.4762
2026-02-13 04:02:54 - INFO - Time taken for Epoch 10:9.72 - F1: 0.4762
Time taken for Epoch 11:10.00 - F1: 0.4714
2026-02-13 04:03:04 - INFO - Time taken for Epoch 11:10.00 - F1: 0.4714
Time taken for Epoch 12:3.38 - F1: 0.4714
2026-02-13 04:03:07 - INFO - Time taken for Epoch 12:3.38 - F1: 0.4714
Time taken for Epoch 13:3.38 - F1: 0.4753
2026-02-13 04:03:10 - INFO - Time taken for Epoch 13:3.38 - F1: 0.4753
Time taken for Epoch 14:3.38 - F1: 0.4829
2026-02-13 04:03:14 - INFO - Time taken for Epoch 14:3.38 - F1: 0.4829
Time taken for Epoch 15:10.56 - F1: 0.4693
2026-02-13 04:03:24 - INFO - Time taken for Epoch 15:10.56 - F1: 0.4693
Time taken for Epoch 16:3.38 - F1: 0.4616
2026-02-13 04:03:28 - INFO - Time taken for Epoch 16:3.38 - F1: 0.4616
Time taken for Epoch 17:3.41 - F1: 0.4735
2026-02-13 04:03:31 - INFO - Time taken for Epoch 17:3.41 - F1: 0.4735
Time taken for Epoch 18:3.39 - F1: 0.4628
2026-02-13 04:03:34 - INFO - Time taken for Epoch 18:3.39 - F1: 0.4628
Time taken for Epoch 19:3.38 - F1: 0.4615
2026-02-13 04:03:38 - INFO - Time taken for Epoch 19:3.38 - F1: 0.4615
Time taken for Epoch 20:3.38 - F1: 0.4621
2026-02-13 04:03:41 - INFO - Time taken for Epoch 20:3.38 - F1: 0.4621
Time taken for Epoch 21:3.38 - F1: 0.4706
2026-02-13 04:03:45 - INFO - Time taken for Epoch 21:3.38 - F1: 0.4706
Time taken for Epoch 22:3.38 - F1: 0.4748
2026-02-13 04:03:48 - INFO - Time taken for Epoch 22:3.38 - F1: 0.4748
Time taken for Epoch 23:3.38 - F1: 0.4609
2026-02-13 04:03:51 - INFO - Time taken for Epoch 23:3.38 - F1: 0.4609
Time taken for Epoch 24:3.38 - F1: 0.4751
2026-02-13 04:03:55 - INFO - Time taken for Epoch 24:3.38 - F1: 0.4751
Performance not improving for 10 consecutive epochs.
2026-02-13 04:03:55 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.4829 - Best Epoch:13
2026-02-13 04:03:55 - INFO - Best F1:0.4829 - Best Epoch:13
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:816: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:817: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5269, Test ECE: 0.0803
2026-02-13 04:04:00 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5269, Test ECE: 0.0803
All results: {'f1_macro': 0.5268608735057281, 'ece': 0.0802945324024891}
2026-02-13 04:04:00 - INFO - All results: {'f1_macro': 0.5268608735057281, 'ece': 0.0802945324024891}

Total time taken: 551.09 seconds
2026-02-13 04:04:00 - INFO - 
Total time taken: 551.09 seconds
2026-02-13 04:04:00 - INFO - Trial 0 finished with value: 0.5268608735057281 and parameters: {'learning_rate': 5.354946433608293e-05, 'weight_decay': 0.006461378682575525, 'batch_size': 32, 'co_train_epochs': 16, 'epoch_patience': 6}. Best is trial 0 with value: 0.5268608735057281.
Using devices: cuda, cuda
2026-02-13 04:04:00 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 04:04:00 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 04:04:00 - INFO - Starting log
Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 04:04:00 - INFO - Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Learning Rate: 1.3017319889296693e-05
Weight Decay: 0.0001014103524894511
Batch Size: 32
No. Epochs: 9
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-13 04:04:00 - INFO - Learning Rate: 1.3017319889296693e-05
Weight Decay: 0.0001014103524894511
Batch Size: 32
No. Epochs: 9
Epoch Patience: 4
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 04:04:01 - INFO - Generating initial weights
Time taken for Epoch 1:9.40 - F1: 0.0127
2026-02-13 04:04:12 - INFO - Time taken for Epoch 1:9.40 - F1: 0.0127
Time taken for Epoch 2:9.17 - F1: 0.0171
2026-02-13 04:04:21 - INFO - Time taken for Epoch 2:9.17 - F1: 0.0171
Time taken for Epoch 3:9.19 - F1: 0.0751
2026-02-13 04:04:30 - INFO - Time taken for Epoch 3:9.19 - F1: 0.0751
Time taken for Epoch 4:9.22 - F1: 0.0948
2026-02-13 04:04:39 - INFO - Time taken for Epoch 4:9.22 - F1: 0.0948
Time taken for Epoch 5:9.16 - F1: 0.0987
2026-02-13 04:04:49 - INFO - Time taken for Epoch 5:9.16 - F1: 0.0987
Time taken for Epoch 6:9.17 - F1: 0.1082
2026-02-13 04:04:58 - INFO - Time taken for Epoch 6:9.17 - F1: 0.1082
Time taken for Epoch 7:9.18 - F1: 0.1317
2026-02-13 04:05:07 - INFO - Time taken for Epoch 7:9.18 - F1: 0.1317
Time taken for Epoch 8:9.17 - F1: 0.1620
2026-02-13 04:05:16 - INFO - Time taken for Epoch 8:9.17 - F1: 0.1620
Time taken for Epoch 9:9.17 - F1: 0.1650
2026-02-13 04:05:25 - INFO - Time taken for Epoch 9:9.17 - F1: 0.1650
Best F1:0.1650 - Best Epoch:9
2026-02-13 04:05:25 - INFO - Best F1:0.1650 - Best Epoch:9
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 04:05:27 - INFO - Starting co-training
Time taken for Epoch 1: 11.97s - F1: 0.06452703
2026-02-13 04:05:39 - INFO - Time taken for Epoch 1: 11.97s - F1: 0.06452703
Time taken for Epoch 2: 12.96s - F1: 0.12901823
2026-02-13 04:05:52 - INFO - Time taken for Epoch 2: 12.96s - F1: 0.12901823
Time taken for Epoch 3: 19.34s - F1: 0.20162001
2026-02-13 04:06:11 - INFO - Time taken for Epoch 3: 19.34s - F1: 0.20162001
Time taken for Epoch 4: 19.22s - F1: 0.23292780
2026-02-13 04:06:30 - INFO - Time taken for Epoch 4: 19.22s - F1: 0.23292780
Time taken for Epoch 5: 19.20s - F1: 0.29331816
2026-02-13 04:06:50 - INFO - Time taken for Epoch 5: 19.20s - F1: 0.29331816
Time taken for Epoch 6: 19.00s - F1: 0.30132881
2026-02-13 04:07:09 - INFO - Time taken for Epoch 6: 19.00s - F1: 0.30132881
Time taken for Epoch 7: 19.62s - F1: 0.29739155
2026-02-13 04:07:28 - INFO - Time taken for Epoch 7: 19.62s - F1: 0.29739155
Time taken for Epoch 8: 11.98s - F1: 0.29972589
2026-02-13 04:07:40 - INFO - Time taken for Epoch 8: 11.98s - F1: 0.29972589
Time taken for Epoch 9: 11.99s - F1: 0.29956179
2026-02-13 04:07:52 - INFO - Time taken for Epoch 9: 11.99s - F1: 0.29956179
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:776: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Fine-tuning models
2026-02-13 04:07:55 - INFO - Fine-tuning models
Time taken for Epoch 1:3.44 - F1: 0.2583
2026-02-13 04:07:58 - INFO - Time taken for Epoch 1:3.44 - F1: 0.2583
Time taken for Epoch 2:4.27 - F1: 0.2280
2026-02-13 04:08:02 - INFO - Time taken for Epoch 2:4.27 - F1: 0.2280
Time taken for Epoch 3:3.41 - F1: 0.2298
2026-02-13 04:08:06 - INFO - Time taken for Epoch 3:3.41 - F1: 0.2298
Time taken for Epoch 4:3.39 - F1: 0.2549
2026-02-13 04:08:09 - INFO - Time taken for Epoch 4:3.39 - F1: 0.2549
Time taken for Epoch 5:3.39 - F1: 0.2816
2026-02-13 04:08:13 - INFO - Time taken for Epoch 5:3.39 - F1: 0.2816
Time taken for Epoch 6:8.25 - F1: 0.2826
2026-02-13 04:08:21 - INFO - Time taken for Epoch 6:8.25 - F1: 0.2826
Time taken for Epoch 7:8.88 - F1: 0.2848
2026-02-13 04:08:30 - INFO - Time taken for Epoch 7:8.88 - F1: 0.2848
Time taken for Epoch 8:9.26 - F1: 0.3111
2026-02-13 04:08:39 - INFO - Time taken for Epoch 8:9.26 - F1: 0.3111
Time taken for Epoch 9:8.30 - F1: 0.3299
2026-02-13 04:08:47 - INFO - Time taken for Epoch 9:8.30 - F1: 0.3299
Time taken for Epoch 10:10.50 - F1: 0.3254
2026-02-13 04:08:58 - INFO - Time taken for Epoch 10:10.50 - F1: 0.3254
Time taken for Epoch 11:3.39 - F1: 0.3221
2026-02-13 04:09:01 - INFO - Time taken for Epoch 11:3.39 - F1: 0.3221
Time taken for Epoch 12:3.38 - F1: 0.3323
2026-02-13 04:09:05 - INFO - Time taken for Epoch 12:3.38 - F1: 0.3323
Time taken for Epoch 13:9.84 - F1: 0.3295
2026-02-13 04:09:14 - INFO - Time taken for Epoch 13:9.84 - F1: 0.3295
Time taken for Epoch 14:3.39 - F1: 0.3408
2026-02-13 04:09:18 - INFO - Time taken for Epoch 14:3.39 - F1: 0.3408
Time taken for Epoch 15:11.92 - F1: 0.3303
2026-02-13 04:09:30 - INFO - Time taken for Epoch 15:11.92 - F1: 0.3303
Time taken for Epoch 16:3.38 - F1: 0.3503
2026-02-13 04:09:33 - INFO - Time taken for Epoch 16:3.38 - F1: 0.3503
Time taken for Epoch 17:10.84 - F1: 0.3569
2026-02-13 04:09:44 - INFO - Time taken for Epoch 17:10.84 - F1: 0.3569
Time taken for Epoch 18:10.09 - F1: 0.3536
2026-02-13 04:09:54 - INFO - Time taken for Epoch 18:10.09 - F1: 0.3536
Time taken for Epoch 19:3.38 - F1: 0.3554
2026-02-13 04:09:57 - INFO - Time taken for Epoch 19:3.38 - F1: 0.3554
Time taken for Epoch 20:3.38 - F1: 0.3572
2026-02-13 04:10:01 - INFO - Time taken for Epoch 20:3.38 - F1: 0.3572
Time taken for Epoch 21:9.16 - F1: 0.3569
2026-02-13 04:10:10 - INFO - Time taken for Epoch 21:9.16 - F1: 0.3569
Time taken for Epoch 22:3.38 - F1: 0.3602
2026-02-13 04:10:13 - INFO - Time taken for Epoch 22:3.38 - F1: 0.3602
Time taken for Epoch 23:10.09 - F1: 0.3796
2026-02-13 04:10:23 - INFO - Time taken for Epoch 23:10.09 - F1: 0.3796
Time taken for Epoch 24:11.00 - F1: 0.3779
2026-02-13 04:10:34 - INFO - Time taken for Epoch 24:11.00 - F1: 0.3779
Time taken for Epoch 25:3.38 - F1: 0.3975
2026-02-13 04:10:38 - INFO - Time taken for Epoch 25:3.38 - F1: 0.3975
Time taken for Epoch 26:9.12 - F1: 0.3967
2026-02-13 04:10:47 - INFO - Time taken for Epoch 26:9.12 - F1: 0.3967
Time taken for Epoch 27:3.38 - F1: 0.3996
2026-02-13 04:10:50 - INFO - Time taken for Epoch 27:3.38 - F1: 0.3996
Time taken for Epoch 28:8.68 - F1: 0.3957
2026-02-13 04:10:59 - INFO - Time taken for Epoch 28:8.68 - F1: 0.3957
Time taken for Epoch 29:3.38 - F1: 0.3930
2026-02-13 04:11:02 - INFO - Time taken for Epoch 29:3.38 - F1: 0.3930
Time taken for Epoch 30:3.38 - F1: 0.3926
2026-02-13 04:11:06 - INFO - Time taken for Epoch 30:3.38 - F1: 0.3926
Time taken for Epoch 31:3.39 - F1: 0.3920
2026-02-13 04:11:09 - INFO - Time taken for Epoch 31:3.39 - F1: 0.3920
Time taken for Epoch 32:3.38 - F1: 0.4031
2026-02-13 04:11:13 - INFO - Time taken for Epoch 32:3.38 - F1: 0.4031
Time taken for Epoch 33:4.46 - F1: 0.3992
2026-02-13 04:11:17 - INFO - Time taken for Epoch 33:4.46 - F1: 0.3992
Time taken for Epoch 34:3.41 - F1: 0.4180
2026-02-13 04:11:20 - INFO - Time taken for Epoch 34:3.41 - F1: 0.4180
Time taken for Epoch 35:7.87 - F1: 0.4040
2026-02-13 04:11:28 - INFO - Time taken for Epoch 35:7.87 - F1: 0.4040
Time taken for Epoch 36:3.38 - F1: 0.4090
2026-02-13 04:11:32 - INFO - Time taken for Epoch 36:3.38 - F1: 0.4090
Time taken for Epoch 37:3.38 - F1: 0.3890
2026-02-13 04:11:35 - INFO - Time taken for Epoch 37:3.38 - F1: 0.3890
Time taken for Epoch 38:3.39 - F1: 0.4025
2026-02-13 04:11:38 - INFO - Time taken for Epoch 38:3.39 - F1: 0.4025
Time taken for Epoch 39:3.38 - F1: 0.4011
2026-02-13 04:11:42 - INFO - Time taken for Epoch 39:3.38 - F1: 0.4011
Time taken for Epoch 40:3.38 - F1: 0.4156
2026-02-13 04:11:45 - INFO - Time taken for Epoch 40:3.38 - F1: 0.4156
Time taken for Epoch 41:3.40 - F1: 0.4101
2026-02-13 04:11:49 - INFO - Time taken for Epoch 41:3.40 - F1: 0.4101
Time taken for Epoch 42:3.38 - F1: 0.4118
2026-02-13 04:11:52 - INFO - Time taken for Epoch 42:3.38 - F1: 0.4118
Time taken for Epoch 43:3.38 - F1: 0.4019
2026-02-13 04:11:55 - INFO - Time taken for Epoch 43:3.38 - F1: 0.4019
Time taken for Epoch 44:3.38 - F1: 0.4152
2026-02-13 04:11:59 - INFO - Time taken for Epoch 44:3.38 - F1: 0.4152
Performance not improving for 10 consecutive epochs.
2026-02-13 04:11:59 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.4180 - Best Epoch:33
2026-02-13 04:11:59 - INFO - Best F1:0.4180 - Best Epoch:33
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:816: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:817: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.4773, Test ECE: 0.0954
2026-02-13 04:12:05 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.4773, Test ECE: 0.0954
All results: {'f1_macro': 0.4773437739070358, 'ece': 0.09538756764287055}
2026-02-13 04:12:05 - INFO - All results: {'f1_macro': 0.4773437739070358, 'ece': 0.09538756764287055}

Total time taken: 484.71 seconds
2026-02-13 04:12:05 - INFO - 
Total time taken: 484.71 seconds
2026-02-13 04:12:05 - INFO - Trial 1 finished with value: 0.4773437739070358 and parameters: {'learning_rate': 1.3017319889296693e-05, 'weight_decay': 0.0001014103524894511, 'batch_size': 32, 'co_train_epochs': 9, 'epoch_patience': 4}. Best is trial 0 with value: 0.5268608735057281.
Using devices: cuda, cuda
2026-02-13 04:12:05 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 04:12:05 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 04:12:05 - INFO - Starting log
Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 04:12:05 - INFO - Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Learning Rate: 0.00012939255958433317
Weight Decay: 0.0059859631853894785
Batch Size: 32
No. Epochs: 20
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-13 04:12:05 - INFO - Learning Rate: 0.00012939255958433317
Weight Decay: 0.0059859631853894785
Batch Size: 32
No. Epochs: 20
Epoch Patience: 7
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 04:12:07 - INFO - Generating initial weights
Time taken for Epoch 1:9.35 - F1: 0.0516
2026-02-13 04:12:17 - INFO - Time taken for Epoch 1:9.35 - F1: 0.0516
Time taken for Epoch 2:9.16 - F1: 0.0201
2026-02-13 04:12:26 - INFO - Time taken for Epoch 2:9.16 - F1: 0.0201
Time taken for Epoch 3:9.16 - F1: 0.0287
2026-02-13 04:12:35 - INFO - Time taken for Epoch 3:9.16 - F1: 0.0287
Time taken for Epoch 4:9.15 - F1: 0.0897
2026-02-13 04:12:45 - INFO - Time taken for Epoch 4:9.15 - F1: 0.0897
Time taken for Epoch 5:9.17 - F1: 0.1353
2026-02-13 04:12:54 - INFO - Time taken for Epoch 5:9.17 - F1: 0.1353
Time taken for Epoch 6:9.17 - F1: 0.1309
2026-02-13 04:13:03 - INFO - Time taken for Epoch 6:9.17 - F1: 0.1309
Time taken for Epoch 7:9.23 - F1: 0.2085
2026-02-13 04:13:12 - INFO - Time taken for Epoch 7:9.23 - F1: 0.2085
Time taken for Epoch 8:9.25 - F1: 0.1833
2026-02-13 04:13:21 - INFO - Time taken for Epoch 8:9.25 - F1: 0.1833
Time taken for Epoch 9:9.21 - F1: 0.3179
2026-02-13 04:13:31 - INFO - Time taken for Epoch 9:9.21 - F1: 0.3179
Time taken for Epoch 10:9.17 - F1: 0.3800
2026-02-13 04:13:40 - INFO - Time taken for Epoch 10:9.17 - F1: 0.3800
Time taken for Epoch 11:9.17 - F1: 0.3555
2026-02-13 04:13:49 - INFO - Time taken for Epoch 11:9.17 - F1: 0.3555
Time taken for Epoch 12:9.23 - F1: 0.4058
2026-02-13 04:13:58 - INFO - Time taken for Epoch 12:9.23 - F1: 0.4058
Time taken for Epoch 13:9.17 - F1: 0.3894
2026-02-13 04:14:07 - INFO - Time taken for Epoch 13:9.17 - F1: 0.3894
Time taken for Epoch 14:9.22 - F1: 0.4187
2026-02-13 04:14:17 - INFO - Time taken for Epoch 14:9.22 - F1: 0.4187
Time taken for Epoch 15:9.16 - F1: 0.4149
2026-02-13 04:14:26 - INFO - Time taken for Epoch 15:9.16 - F1: 0.4149
Time taken for Epoch 16:9.21 - F1: 0.3972
2026-02-13 04:14:35 - INFO - Time taken for Epoch 16:9.21 - F1: 0.3972
Time taken for Epoch 17:9.23 - F1: 0.4483
2026-02-13 04:14:44 - INFO - Time taken for Epoch 17:9.23 - F1: 0.4483
Time taken for Epoch 18:9.18 - F1: 0.4754
2026-02-13 04:14:53 - INFO - Time taken for Epoch 18:9.18 - F1: 0.4754
Time taken for Epoch 19:9.21 - F1: 0.4286
2026-02-13 04:15:03 - INFO - Time taken for Epoch 19:9.21 - F1: 0.4286
Time taken for Epoch 20:9.20 - F1: 0.4398
2026-02-13 04:15:12 - INFO - Time taken for Epoch 20:9.20 - F1: 0.4398
Best F1:0.4754 - Best Epoch:18
2026-02-13 04:15:12 - INFO - Best F1:0.4754 - Best Epoch:18
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 04:15:13 - INFO - Starting co-training
Time taken for Epoch 1: 11.99s - F1: 0.28199797
2026-02-13 04:15:25 - INFO - Time taken for Epoch 1: 11.99s - F1: 0.28199797
Time taken for Epoch 2: 13.01s - F1: 0.27261914
2026-02-13 04:15:38 - INFO - Time taken for Epoch 2: 13.01s - F1: 0.27261914
Time taken for Epoch 3: 11.99s - F1: 0.30771117
2026-02-13 04:15:50 - INFO - Time taken for Epoch 3: 11.99s - F1: 0.30771117
Time taken for Epoch 4: 16.65s - F1: 0.30578325
2026-02-13 04:16:07 - INFO - Time taken for Epoch 4: 16.65s - F1: 0.30578325
Time taken for Epoch 5: 11.98s - F1: 0.31222464
2026-02-13 04:16:19 - INFO - Time taken for Epoch 5: 11.98s - F1: 0.31222464
Time taken for Epoch 6: 19.51s - F1: 0.33597143
2026-02-13 04:16:39 - INFO - Time taken for Epoch 6: 19.51s - F1: 0.33597143
Time taken for Epoch 7: 18.82s - F1: 0.31800515
2026-02-13 04:16:57 - INFO - Time taken for Epoch 7: 18.82s - F1: 0.31800515
Time taken for Epoch 8: 11.98s - F1: 0.32315770
2026-02-13 04:17:09 - INFO - Time taken for Epoch 8: 11.98s - F1: 0.32315770
Time taken for Epoch 9: 11.98s - F1: 0.30517300
2026-02-13 04:17:21 - INFO - Time taken for Epoch 9: 11.98s - F1: 0.30517300
Time taken for Epoch 10: 11.98s - F1: 0.33528318
2026-02-13 04:17:33 - INFO - Time taken for Epoch 10: 11.98s - F1: 0.33528318
Time taken for Epoch 11: 11.99s - F1: 0.30537642
2026-02-13 04:17:45 - INFO - Time taken for Epoch 11: 11.99s - F1: 0.30537642
Time taken for Epoch 12: 11.97s - F1: 0.34555188
2026-02-13 04:17:57 - INFO - Time taken for Epoch 12: 11.97s - F1: 0.34555188
Time taken for Epoch 13: 17.58s - F1: 0.33981884
2026-02-13 04:18:15 - INFO - Time taken for Epoch 13: 17.58s - F1: 0.33981884
Time taken for Epoch 14: 11.98s - F1: 0.32396781
2026-02-13 04:18:27 - INFO - Time taken for Epoch 14: 11.98s - F1: 0.32396781
Time taken for Epoch 15: 11.97s - F1: 0.28962527
2026-02-13 04:18:39 - INFO - Time taken for Epoch 15: 11.97s - F1: 0.28962527
Time taken for Epoch 16: 11.98s - F1: 0.31380352
2026-02-13 04:18:51 - INFO - Time taken for Epoch 16: 11.98s - F1: 0.31380352
Time taken for Epoch 17: 11.98s - F1: 0.33434177
2026-02-13 04:19:03 - INFO - Time taken for Epoch 17: 11.98s - F1: 0.33434177
Time taken for Epoch 18: 11.98s - F1: 0.33176546
2026-02-13 04:19:15 - INFO - Time taken for Epoch 18: 11.98s - F1: 0.33176546
Time taken for Epoch 19: 11.98s - F1: 0.31815775
2026-02-13 04:19:27 - INFO - Time taken for Epoch 19: 11.98s - F1: 0.31815775
Performance not improving for 7 consecutive epochs.
Performance not improving for 7 consecutive epochs.
2026-02-13 04:19:27 - INFO - Performance not improving for 7 consecutive epochs.
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:776: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Fine-tuning models
2026-02-13 04:19:29 - INFO - Fine-tuning models
Time taken for Epoch 1:3.43 - F1: 0.3246
2026-02-13 04:19:33 - INFO - Time taken for Epoch 1:3.43 - F1: 0.3246
Time taken for Epoch 2:4.30 - F1: 0.2572
2026-02-13 04:19:37 - INFO - Time taken for Epoch 2:4.30 - F1: 0.2572
Time taken for Epoch 3:3.38 - F1: 0.2616
2026-02-13 04:19:40 - INFO - Time taken for Epoch 3:3.38 - F1: 0.2616
Time taken for Epoch 4:3.38 - F1: 0.3742
2026-02-13 04:19:44 - INFO - Time taken for Epoch 4:3.38 - F1: 0.3742
Time taken for Epoch 5:31.23 - F1: 0.3676
2026-02-13 04:20:15 - INFO - Time taken for Epoch 5:31.23 - F1: 0.3676
Time taken for Epoch 6:3.40 - F1: 0.3585
2026-02-13 04:20:18 - INFO - Time taken for Epoch 6:3.40 - F1: 0.3585
Time taken for Epoch 7:3.40 - F1: 0.3497
2026-02-13 04:20:22 - INFO - Time taken for Epoch 7:3.40 - F1: 0.3497
Time taken for Epoch 8:3.39 - F1: 0.3662
2026-02-13 04:20:25 - INFO - Time taken for Epoch 8:3.39 - F1: 0.3662
Time taken for Epoch 9:3.38 - F1: 0.3651
2026-02-13 04:20:29 - INFO - Time taken for Epoch 9:3.38 - F1: 0.3651
Time taken for Epoch 10:3.38 - F1: 0.3595
2026-02-13 04:20:32 - INFO - Time taken for Epoch 10:3.38 - F1: 0.3595
Time taken for Epoch 11:3.38 - F1: 0.3705
2026-02-13 04:20:35 - INFO - Time taken for Epoch 11:3.38 - F1: 0.3705
Time taken for Epoch 12:3.38 - F1: 0.3755
2026-02-13 04:20:39 - INFO - Time taken for Epoch 12:3.38 - F1: 0.3755
Time taken for Epoch 13:7.62 - F1: 0.3563
2026-02-13 04:20:46 - INFO - Time taken for Epoch 13:7.62 - F1: 0.3563
Time taken for Epoch 14:3.39 - F1: 0.3436
2026-02-13 04:20:50 - INFO - Time taken for Epoch 14:3.39 - F1: 0.3436
Time taken for Epoch 15:3.39 - F1: 0.3811
2026-02-13 04:20:53 - INFO - Time taken for Epoch 15:3.39 - F1: 0.3811
Time taken for Epoch 16:9.27 - F1: 0.3676
2026-02-13 04:21:02 - INFO - Time taken for Epoch 16:9.27 - F1: 0.3676
Time taken for Epoch 17:3.39 - F1: 0.3569
2026-02-13 04:21:06 - INFO - Time taken for Epoch 17:3.39 - F1: 0.3569
Time taken for Epoch 18:3.39 - F1: 0.3809
2026-02-13 04:21:09 - INFO - Time taken for Epoch 18:3.39 - F1: 0.3809
Time taken for Epoch 19:3.40 - F1: 0.4155
2026-02-13 04:21:13 - INFO - Time taken for Epoch 19:3.40 - F1: 0.4155
Time taken for Epoch 20:9.35 - F1: 0.4154
2026-02-13 04:21:22 - INFO - Time taken for Epoch 20:9.35 - F1: 0.4154
Time taken for Epoch 21:3.39 - F1: 0.3964
2026-02-13 04:21:25 - INFO - Time taken for Epoch 21:3.39 - F1: 0.3964
Time taken for Epoch 22:3.40 - F1: 0.3912
2026-02-13 04:21:29 - INFO - Time taken for Epoch 22:3.40 - F1: 0.3912
Time taken for Epoch 23:3.39 - F1: 0.4113
2026-02-13 04:21:32 - INFO - Time taken for Epoch 23:3.39 - F1: 0.4113
Time taken for Epoch 24:3.39 - F1: 0.3949
2026-02-13 04:21:36 - INFO - Time taken for Epoch 24:3.39 - F1: 0.3949
Time taken for Epoch 25:3.38 - F1: 0.4091
2026-02-13 04:21:39 - INFO - Time taken for Epoch 25:3.38 - F1: 0.4091
Time taken for Epoch 26:3.40 - F1: 0.4591
2026-02-13 04:21:42 - INFO - Time taken for Epoch 26:3.40 - F1: 0.4591
Time taken for Epoch 27:9.78 - F1: 0.4942
2026-02-13 04:21:52 - INFO - Time taken for Epoch 27:9.78 - F1: 0.4942
Time taken for Epoch 28:9.93 - F1: 0.4542
2026-02-13 04:22:02 - INFO - Time taken for Epoch 28:9.93 - F1: 0.4542
Time taken for Epoch 29:3.38 - F1: 0.4546
2026-02-13 04:22:05 - INFO - Time taken for Epoch 29:3.38 - F1: 0.4546
Time taken for Epoch 30:3.40 - F1: 0.4440
2026-02-13 04:22:09 - INFO - Time taken for Epoch 30:3.40 - F1: 0.4440
Time taken for Epoch 31:3.41 - F1: 0.4018
2026-02-13 04:22:12 - INFO - Time taken for Epoch 31:3.41 - F1: 0.4018
Time taken for Epoch 32:3.39 - F1: 0.4181
2026-02-13 04:22:16 - INFO - Time taken for Epoch 32:3.39 - F1: 0.4181
Time taken for Epoch 33:3.39 - F1: 0.4374
2026-02-13 04:22:19 - INFO - Time taken for Epoch 33:3.39 - F1: 0.4374
Time taken for Epoch 34:3.40 - F1: 0.4352
2026-02-13 04:22:22 - INFO - Time taken for Epoch 34:3.40 - F1: 0.4352
Time taken for Epoch 35:3.40 - F1: 0.4201
2026-02-13 04:22:26 - INFO - Time taken for Epoch 35:3.40 - F1: 0.4201
Time taken for Epoch 36:3.39 - F1: 0.4176
2026-02-13 04:22:29 - INFO - Time taken for Epoch 36:3.39 - F1: 0.4176
Time taken for Epoch 37:3.39 - F1: 0.4133
2026-02-13 04:22:33 - INFO - Time taken for Epoch 37:3.39 - F1: 0.4133
Performance not improving for 10 consecutive epochs.
2026-02-13 04:22:33 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.4942 - Best Epoch:26
2026-02-13 04:22:33 - INFO - Best F1:0.4942 - Best Epoch:26
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:816: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:817: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.4660, Test ECE: 0.1692
2026-02-13 04:22:38 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.4660, Test ECE: 0.1692
All results: {'f1_macro': 0.4659963359478875, 'ece': 0.1691584556874017}
2026-02-13 04:22:38 - INFO - All results: {'f1_macro': 0.4659963359478875, 'ece': 0.1691584556874017}

Total time taken: 633.52 seconds
2026-02-13 04:22:38 - INFO - 
Total time taken: 633.52 seconds
2026-02-13 04:22:38 - INFO - Trial 2 finished with value: 0.4659963359478875 and parameters: {'learning_rate': 0.00012939255958433317, 'weight_decay': 0.0059859631853894785, 'batch_size': 32, 'co_train_epochs': 20, 'epoch_patience': 7}. Best is trial 0 with value: 0.5268608735057281.
Using devices: cuda, cuda
2026-02-13 04:22:38 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 04:22:38 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 04:22:38 - INFO - Starting log
Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 04:22:38 - INFO - Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
Learning Rate: 2.5596234140367675e-05
Weight Decay: 0.0008507294608109866
Batch Size: 16
No. Epochs: 13
Epoch Patience: 1
 Accumulation Steps: 4
2026-02-13 04:22:39 - INFO - Learning Rate: 2.5596234140367675e-05
Weight Decay: 0.0008507294608109866
Batch Size: 16
No. Epochs: 13
Epoch Patience: 1
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 04:22:40 - INFO - Generating initial weights
Time taken for Epoch 1:10.07 - F1: 0.0044
2026-02-13 04:22:51 - INFO - Time taken for Epoch 1:10.07 - F1: 0.0044
Time taken for Epoch 2:9.95 - F1: 0.0263
2026-02-13 04:23:01 - INFO - Time taken for Epoch 2:9.95 - F1: 0.0263
Time taken for Epoch 3:9.97 - F1: 0.0751
2026-02-13 04:23:11 - INFO - Time taken for Epoch 3:9.97 - F1: 0.0751
Time taken for Epoch 4:9.98 - F1: 0.1166
2026-02-13 04:23:21 - INFO - Time taken for Epoch 4:9.98 - F1: 0.1166
Time taken for Epoch 5:9.95 - F1: 0.1765
2026-02-13 04:23:31 - INFO - Time taken for Epoch 5:9.95 - F1: 0.1765
Time taken for Epoch 6:9.95 - F1: 0.1561
2026-02-13 04:23:41 - INFO - Time taken for Epoch 6:9.95 - F1: 0.1561
Time taken for Epoch 7:9.97 - F1: 0.1639
2026-02-13 04:23:51 - INFO - Time taken for Epoch 7:9.97 - F1: 0.1639
Time taken for Epoch 8:9.97 - F1: 0.2027
2026-02-13 04:24:01 - INFO - Time taken for Epoch 8:9.97 - F1: 0.2027
Time taken for Epoch 9:9.94 - F1: 0.2283
2026-02-13 04:24:11 - INFO - Time taken for Epoch 9:9.94 - F1: 0.2283
Time taken for Epoch 10:9.96 - F1: 0.2594
2026-02-13 04:24:21 - INFO - Time taken for Epoch 10:9.96 - F1: 0.2594
Time taken for Epoch 11:9.92 - F1: 0.2730
2026-02-13 04:24:31 - INFO - Time taken for Epoch 11:9.92 - F1: 0.2730
Time taken for Epoch 12:9.92 - F1: 0.3117
2026-02-13 04:24:41 - INFO - Time taken for Epoch 12:9.92 - F1: 0.3117
Time taken for Epoch 13:9.93 - F1: 0.3453
2026-02-13 04:24:51 - INFO - Time taken for Epoch 13:9.93 - F1: 0.3453
Best F1:0.3453 - Best Epoch:13
2026-02-13 04:24:51 - INFO - Best F1:0.3453 - Best Epoch:13
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 04:24:52 - INFO - Starting co-training
Time taken for Epoch 1: 10.29s - F1: 0.06452703
2026-02-13 04:25:02 - INFO - Time taken for Epoch 1: 10.29s - F1: 0.06452703
Time taken for Epoch 2: 11.42s - F1: 0.20208268
2026-02-13 04:25:14 - INFO - Time taken for Epoch 2: 11.42s - F1: 0.20208268
Time taken for Epoch 3: 32.38s - F1: 0.20915926
2026-02-13 04:25:46 - INFO - Time taken for Epoch 3: 32.38s - F1: 0.20915926
Time taken for Epoch 4: 17.75s - F1: 0.25193259
2026-02-13 04:26:04 - INFO - Time taken for Epoch 4: 17.75s - F1: 0.25193259
Time taken for Epoch 5: 18.70s - F1: 0.28694280
2026-02-13 04:26:23 - INFO - Time taken for Epoch 5: 18.70s - F1: 0.28694280
Time taken for Epoch 6: 17.77s - F1: 0.29648052
2026-02-13 04:26:40 - INFO - Time taken for Epoch 6: 17.77s - F1: 0.29648052
Time taken for Epoch 7: 17.91s - F1: 0.29715946
2026-02-13 04:26:58 - INFO - Time taken for Epoch 7: 17.91s - F1: 0.29715946
Time taken for Epoch 8: 17.78s - F1: 0.29288376
2026-02-13 04:27:16 - INFO - Time taken for Epoch 8: 17.78s - F1: 0.29288376
Performance not improving for 1 consecutive epochs.
Performance not improving for 1 consecutive epochs.
2026-02-13 04:27:16 - INFO - Performance not improving for 1 consecutive epochs.
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:776: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Fine-tuning models
2026-02-13 04:27:19 - INFO - Fine-tuning models
Time taken for Epoch 1:3.73 - F1: 0.2932
2026-02-13 04:27:23 - INFO - Time taken for Epoch 1:3.73 - F1: 0.2932
Time taken for Epoch 2:4.91 - F1: 0.3113
2026-02-13 04:27:28 - INFO - Time taken for Epoch 2:4.91 - F1: 0.3113
Time taken for Epoch 3:18.24 - F1: 0.3470
2026-02-13 04:27:46 - INFO - Time taken for Epoch 3:18.24 - F1: 0.3470
Time taken for Epoch 4:9.49 - F1: 0.3570
2026-02-13 04:27:56 - INFO - Time taken for Epoch 4:9.49 - F1: 0.3570
Time taken for Epoch 5:10.57 - F1: 0.3496
2026-02-13 04:28:06 - INFO - Time taken for Epoch 5:10.57 - F1: 0.3496
Time taken for Epoch 6:3.60 - F1: 0.3948
2026-02-13 04:28:10 - INFO - Time taken for Epoch 6:3.60 - F1: 0.3948
Time taken for Epoch 7:8.99 - F1: 0.3970
2026-02-13 04:28:19 - INFO - Time taken for Epoch 7:8.99 - F1: 0.3970
Time taken for Epoch 8:11.19 - F1: 0.3643
2026-02-13 04:28:30 - INFO - Time taken for Epoch 8:11.19 - F1: 0.3643
Time taken for Epoch 9:3.62 - F1: 0.3657
2026-02-13 04:28:34 - INFO - Time taken for Epoch 9:3.62 - F1: 0.3657
Time taken for Epoch 10:3.60 - F1: 0.3586
2026-02-13 04:28:37 - INFO - Time taken for Epoch 10:3.60 - F1: 0.3586
Time taken for Epoch 11:3.60 - F1: 0.3728
2026-02-13 04:28:41 - INFO - Time taken for Epoch 11:3.60 - F1: 0.3728
Time taken for Epoch 12:3.60 - F1: 0.3763
2026-02-13 04:28:44 - INFO - Time taken for Epoch 12:3.60 - F1: 0.3763
Time taken for Epoch 13:3.62 - F1: 0.3649
2026-02-13 04:28:48 - INFO - Time taken for Epoch 13:3.62 - F1: 0.3649
Time taken for Epoch 14:3.60 - F1: 0.3781
2026-02-13 04:28:52 - INFO - Time taken for Epoch 14:3.60 - F1: 0.3781
Time taken for Epoch 15:3.60 - F1: 0.3914
2026-02-13 04:28:55 - INFO - Time taken for Epoch 15:3.60 - F1: 0.3914
Time taken for Epoch 16:3.60 - F1: 0.4009
2026-02-13 04:28:59 - INFO - Time taken for Epoch 16:3.60 - F1: 0.4009
Time taken for Epoch 17:10.91 - F1: 0.4097
2026-02-13 04:29:10 - INFO - Time taken for Epoch 17:10.91 - F1: 0.4097
Time taken for Epoch 18:9.67 - F1: 0.4074
2026-02-13 04:29:19 - INFO - Time taken for Epoch 18:9.67 - F1: 0.4074
Time taken for Epoch 19:3.60 - F1: 0.4281
2026-02-13 04:29:23 - INFO - Time taken for Epoch 19:3.60 - F1: 0.4281
Time taken for Epoch 20:9.49 - F1: 0.4321
2026-02-13 04:29:32 - INFO - Time taken for Epoch 20:9.49 - F1: 0.4321
Time taken for Epoch 21:10.83 - F1: 0.4308
2026-02-13 04:29:43 - INFO - Time taken for Epoch 21:10.83 - F1: 0.4308
Time taken for Epoch 22:3.60 - F1: 0.4569
2026-02-13 04:29:47 - INFO - Time taken for Epoch 22:3.60 - F1: 0.4569
Time taken for Epoch 23:9.55 - F1: 0.4298
2026-02-13 04:29:56 - INFO - Time taken for Epoch 23:9.55 - F1: 0.4298
Time taken for Epoch 24:3.62 - F1: 0.4575
2026-02-13 04:30:00 - INFO - Time taken for Epoch 24:3.62 - F1: 0.4575
Time taken for Epoch 25:9.91 - F1: 0.4327
2026-02-13 04:30:10 - INFO - Time taken for Epoch 25:9.91 - F1: 0.4327
Time taken for Epoch 26:3.62 - F1: 0.4488
2026-02-13 04:30:14 - INFO - Time taken for Epoch 26:3.62 - F1: 0.4488
Time taken for Epoch 27:3.60 - F1: 0.4256
2026-02-13 04:30:17 - INFO - Time taken for Epoch 27:3.60 - F1: 0.4256
Time taken for Epoch 28:3.59 - F1: 0.4509
2026-02-13 04:30:21 - INFO - Time taken for Epoch 28:3.59 - F1: 0.4509
Time taken for Epoch 29:3.60 - F1: 0.4294
2026-02-13 04:30:24 - INFO - Time taken for Epoch 29:3.60 - F1: 0.4294
Time taken for Epoch 30:3.60 - F1: 0.4526
2026-02-13 04:30:28 - INFO - Time taken for Epoch 30:3.60 - F1: 0.4526
Time taken for Epoch 31:3.59 - F1: 0.4323
2026-02-13 04:30:32 - INFO - Time taken for Epoch 31:3.59 - F1: 0.4323
Time taken for Epoch 32:3.59 - F1: 0.4570
2026-02-13 04:30:35 - INFO - Time taken for Epoch 32:3.59 - F1: 0.4570
Time taken for Epoch 33:3.62 - F1: 0.4228
2026-02-13 04:30:39 - INFO - Time taken for Epoch 33:3.62 - F1: 0.4228
Time taken for Epoch 34:3.59 - F1: 0.4275
2026-02-13 04:30:42 - INFO - Time taken for Epoch 34:3.59 - F1: 0.4275
Performance not improving for 10 consecutive epochs.
2026-02-13 04:30:42 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.4575 - Best Epoch:23
2026-02-13 04:30:42 - INFO - Best F1:0.4575 - Best Epoch:23
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:816: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:817: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5603, Test ECE: 0.1226
2026-02-13 04:30:48 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5603, Test ECE: 0.1226
All results: {'f1_macro': 0.5602574144999545, 'ece': 0.12262666359333385}
2026-02-13 04:30:48 - INFO - All results: {'f1_macro': 0.5602574144999545, 'ece': 0.12262666359333385}

Total time taken: 489.77 seconds
2026-02-13 04:30:48 - INFO - 
Total time taken: 489.77 seconds
2026-02-13 04:30:48 - INFO - Trial 3 finished with value: 0.5602574144999545 and parameters: {'learning_rate': 2.5596234140367675e-05, 'weight_decay': 0.0008507294608109866, 'batch_size': 16, 'co_train_epochs': 13, 'epoch_patience': 1}. Best is trial 3 with value: 0.5602574144999545.
Using devices: cuda, cuda
2026-02-13 04:30:48 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 04:30:48 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 04:30:48 - INFO - Starting log
Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 04:30:48 - INFO - Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
Learning Rate: 0.0002586880195660984
Weight Decay: 0.00019874787759847573
Batch Size: 8
No. Epochs: 13
Epoch Patience: 2
 Accumulation Steps: 8
2026-02-13 04:30:48 - INFO - Learning Rate: 0.0002586880195660984
Weight Decay: 0.00019874787759847573
Batch Size: 8
No. Epochs: 13
Epoch Patience: 2
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 04:30:50 - INFO - Generating initial weights
Time taken for Epoch 1:10.96 - F1: 0.0128
2026-02-13 04:31:02 - INFO - Time taken for Epoch 1:10.96 - F1: 0.0128
Time taken for Epoch 2:10.91 - F1: 0.0198
2026-02-13 04:31:13 - INFO - Time taken for Epoch 2:10.91 - F1: 0.0198
Time taken for Epoch 3:10.88 - F1: 0.0303
2026-02-13 04:31:24 - INFO - Time taken for Epoch 3:10.88 - F1: 0.0303
Time taken for Epoch 4:10.88 - F1: 0.0039
2026-02-13 04:31:35 - INFO - Time taken for Epoch 4:10.88 - F1: 0.0039
Time taken for Epoch 5:10.89 - F1: 0.0192
2026-02-13 04:31:46 - INFO - Time taken for Epoch 5:10.89 - F1: 0.0192
Time taken for Epoch 6:10.90 - F1: 0.0218
2026-02-13 04:31:57 - INFO - Time taken for Epoch 6:10.90 - F1: 0.0218
Time taken for Epoch 7:10.93 - F1: 0.0218
2026-02-13 04:32:08 - INFO - Time taken for Epoch 7:10.93 - F1: 0.0218
Time taken for Epoch 8:10.94 - F1: 0.0355
2026-02-13 04:32:18 - INFO - Time taken for Epoch 8:10.94 - F1: 0.0355
Time taken for Epoch 9:10.85 - F1: 0.0225
2026-02-13 04:32:29 - INFO - Time taken for Epoch 9:10.85 - F1: 0.0225
Time taken for Epoch 10:10.92 - F1: 0.0218
2026-02-13 04:32:40 - INFO - Time taken for Epoch 10:10.92 - F1: 0.0218
Time taken for Epoch 11:10.87 - F1: 0.0218
2026-02-13 04:32:51 - INFO - Time taken for Epoch 11:10.87 - F1: 0.0218
Time taken for Epoch 12:10.86 - F1: 0.0218
2026-02-13 04:33:02 - INFO - Time taken for Epoch 12:10.86 - F1: 0.0218
Time taken for Epoch 13:10.86 - F1: 0.0218
2026-02-13 04:33:13 - INFO - Time taken for Epoch 13:10.86 - F1: 0.0218
Best F1:0.0355 - Best Epoch:8
2026-02-13 04:33:13 - INFO - Best F1:0.0355 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 04:33:14 - INFO - Starting co-training
Time taken for Epoch 1: 9.72s - F1: 0.06452703
2026-02-13 04:33:24 - INFO - Time taken for Epoch 1: 9.72s - F1: 0.06452703
Time taken for Epoch 2: 10.79s - F1: 0.06452703
2026-02-13 04:33:35 - INFO - Time taken for Epoch 2: 10.79s - F1: 0.06452703
Time taken for Epoch 3: 9.71s - F1: 0.06452703
2026-02-13 04:33:45 - INFO - Time taken for Epoch 3: 9.71s - F1: 0.06452703
Performance not improving for 2 consecutive epochs.
Performance not improving for 2 consecutive epochs.
2026-02-13 04:33:45 - INFO - Performance not improving for 2 consecutive epochs.
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:776: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Fine-tuning models
2026-02-13 04:33:47 - INFO - Fine-tuning models
Time taken for Epoch 1:4.03 - F1: 0.0645
2026-02-13 04:33:51 - INFO - Time taken for Epoch 1:4.03 - F1: 0.0645
Time taken for Epoch 2:5.12 - F1: 0.0218
2026-02-13 04:33:56 - INFO - Time taken for Epoch 2:5.12 - F1: 0.0218
Time taken for Epoch 3:3.93 - F1: 0.0039
2026-02-13 04:34:00 - INFO - Time taken for Epoch 3:3.93 - F1: 0.0039
Time taken for Epoch 4:3.93 - F1: 0.0218
2026-02-13 04:34:04 - INFO - Time taken for Epoch 4:3.93 - F1: 0.0218
Time taken for Epoch 5:3.93 - F1: 0.0218
2026-02-13 04:34:08 - INFO - Time taken for Epoch 5:3.93 - F1: 0.0218
Time taken for Epoch 6:3.92 - F1: 0.0218
2026-02-13 04:34:12 - INFO - Time taken for Epoch 6:3.92 - F1: 0.0218
Time taken for Epoch 7:3.92 - F1: 0.0218
2026-02-13 04:34:16 - INFO - Time taken for Epoch 7:3.92 - F1: 0.0218
Time taken for Epoch 8:3.91 - F1: 0.0218
2026-02-13 04:34:20 - INFO - Time taken for Epoch 8:3.91 - F1: 0.0218
Time taken for Epoch 9:3.91 - F1: 0.0218
2026-02-13 04:34:24 - INFO - Time taken for Epoch 9:3.91 - F1: 0.0218
Time taken for Epoch 10:3.91 - F1: 0.0218
2026-02-13 04:34:28 - INFO - Time taken for Epoch 10:3.91 - F1: 0.0218
Time taken for Epoch 11:3.92 - F1: 0.0218
2026-02-13 04:34:32 - INFO - Time taken for Epoch 11:3.92 - F1: 0.0218
Performance not improving for 10 consecutive epochs.
2026-02-13 04:34:32 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0645 - Best Epoch:0
2026-02-13 04:34:32 - INFO - Best F1:0.0645 - Best Epoch:0
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:816: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:817: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.0644, Test ECE: 0.0486
2026-02-13 04:34:37 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.0644, Test ECE: 0.0486
All results: {'f1_macro': 0.06440382941688425, 'ece': 0.048616328808707365}
2026-02-13 04:34:37 - INFO - All results: {'f1_macro': 0.06440382941688425, 'ece': 0.048616328808707365}

Total time taken: 229.35 seconds
2026-02-13 04:34:37 - INFO - 
Total time taken: 229.35 seconds
2026-02-13 04:34:37 - INFO - Trial 4 finished with value: 0.06440382941688425 and parameters: {'learning_rate': 0.0002586880195660984, 'weight_decay': 0.00019874787759847573, 'batch_size': 8, 'co_train_epochs': 13, 'epoch_patience': 2}. Best is trial 3 with value: 0.5602574144999545.
Using devices: cuda, cuda
2026-02-13 04:34:37 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 04:34:37 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 04:34:37 - INFO - Starting log
Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 04:34:37 - INFO - Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Learning Rate: 8.437573670609839e-05
Weight Decay: 9.856113626643751e-05
Batch Size: 16
No. Epochs: 18
Epoch Patience: 1
 Accumulation Steps: 4
2026-02-13 04:34:38 - INFO - Learning Rate: 8.437573670609839e-05
Weight Decay: 9.856113626643751e-05
Batch Size: 16
No. Epochs: 18
Epoch Patience: 1
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 04:34:39 - INFO - Generating initial weights
Time taken for Epoch 1:10.10 - F1: 0.0044
2026-02-13 04:34:51 - INFO - Time taken for Epoch 1:10.10 - F1: 0.0044
Time taken for Epoch 2:10.03 - F1: 0.0213
2026-02-13 04:35:01 - INFO - Time taken for Epoch 2:10.03 - F1: 0.0213
Time taken for Epoch 3:9.96 - F1: 0.0456
2026-02-13 04:35:11 - INFO - Time taken for Epoch 3:9.96 - F1: 0.0456
Time taken for Epoch 4:9.95 - F1: 0.1165
2026-02-13 04:35:21 - INFO - Time taken for Epoch 4:9.95 - F1: 0.1165
Time taken for Epoch 5:10.00 - F1: 0.1647
2026-02-13 04:35:31 - INFO - Time taken for Epoch 5:10.00 - F1: 0.1647
Time taken for Epoch 6:10.00 - F1: 0.2444
2026-02-13 04:35:41 - INFO - Time taken for Epoch 6:10.00 - F1: 0.2444
Time taken for Epoch 7:10.04 - F1: 0.2714
2026-02-13 04:35:51 - INFO - Time taken for Epoch 7:10.04 - F1: 0.2714
Time taken for Epoch 8:9.96 - F1: 0.2942
2026-02-13 04:36:01 - INFO - Time taken for Epoch 8:9.96 - F1: 0.2942
Time taken for Epoch 9:9.95 - F1: 0.3692
2026-02-13 04:36:11 - INFO - Time taken for Epoch 9:9.95 - F1: 0.3692
Time taken for Epoch 10:10.01 - F1: 0.3730
2026-02-13 04:36:21 - INFO - Time taken for Epoch 10:10.01 - F1: 0.3730
Time taken for Epoch 11:10.00 - F1: 0.4174
2026-02-13 04:36:31 - INFO - Time taken for Epoch 11:10.00 - F1: 0.4174
Time taken for Epoch 12:9.97 - F1: 0.3642
2026-02-13 04:36:41 - INFO - Time taken for Epoch 12:9.97 - F1: 0.3642
Time taken for Epoch 13:10.01 - F1: 0.4694
2026-02-13 04:36:51 - INFO - Time taken for Epoch 13:10.01 - F1: 0.4694
Time taken for Epoch 14:10.03 - F1: 0.4587
2026-02-13 04:37:01 - INFO - Time taken for Epoch 14:10.03 - F1: 0.4587
Time taken for Epoch 15:9.99 - F1: 0.4136
2026-02-13 04:37:11 - INFO - Time taken for Epoch 15:9.99 - F1: 0.4136
Time taken for Epoch 16:9.96 - F1: 0.4375
2026-02-13 04:37:21 - INFO - Time taken for Epoch 16:9.96 - F1: 0.4375
Time taken for Epoch 17:10.03 - F1: 0.4275
2026-02-13 04:37:31 - INFO - Time taken for Epoch 17:10.03 - F1: 0.4275
Time taken for Epoch 18:10.03 - F1: 0.4906
2026-02-13 04:37:41 - INFO - Time taken for Epoch 18:10.03 - F1: 0.4906
Best F1:0.4906 - Best Epoch:18
2026-02-13 04:37:41 - INFO - Best F1:0.4906 - Best Epoch:18
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 04:37:42 - INFO - Starting co-training
Time taken for Epoch 1: 10.23s - F1: 0.21519521
2026-02-13 04:37:52 - INFO - Time taken for Epoch 1: 10.23s - F1: 0.21519521
Time taken for Epoch 2: 11.41s - F1: 0.20395712
2026-02-13 04:38:04 - INFO - Time taken for Epoch 2: 11.41s - F1: 0.20395712
Performance not improving for 1 consecutive epochs.
Performance not improving for 1 consecutive epochs.
2026-02-13 04:38:04 - INFO - Performance not improving for 1 consecutive epochs.
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:776: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Fine-tuning models
2026-02-13 04:38:08 - INFO - Fine-tuning models
Time taken for Epoch 1:3.66 - F1: 0.1043
2026-02-13 04:38:12 - INFO - Time taken for Epoch 1:3.66 - F1: 0.1043
Time taken for Epoch 2:4.82 - F1: 0.0500
2026-02-13 04:38:16 - INFO - Time taken for Epoch 2:4.82 - F1: 0.0500
Time taken for Epoch 3:3.59 - F1: 0.0836
2026-02-13 04:38:20 - INFO - Time taken for Epoch 3:3.59 - F1: 0.0836
Time taken for Epoch 4:3.59 - F1: 0.1427
2026-02-13 04:38:24 - INFO - Time taken for Epoch 4:3.59 - F1: 0.1427
Time taken for Epoch 5:9.06 - F1: 0.0875
2026-02-13 04:38:33 - INFO - Time taken for Epoch 5:9.06 - F1: 0.0875
Time taken for Epoch 6:3.60 - F1: 0.2415
2026-02-13 04:38:36 - INFO - Time taken for Epoch 6:3.60 - F1: 0.2415
Time taken for Epoch 7:13.52 - F1: 0.2429
2026-02-13 04:38:50 - INFO - Time taken for Epoch 7:13.52 - F1: 0.2429
Time taken for Epoch 8:10.89 - F1: 0.1997
2026-02-13 04:39:01 - INFO - Time taken for Epoch 8:10.89 - F1: 0.1997
Time taken for Epoch 9:3.61 - F1: 0.1632
2026-02-13 04:39:04 - INFO - Time taken for Epoch 9:3.61 - F1: 0.1632
Time taken for Epoch 10:3.61 - F1: 0.2916
2026-02-13 04:39:08 - INFO - Time taken for Epoch 10:3.61 - F1: 0.2916
Time taken for Epoch 11:25.00 - F1: 0.3540
2026-02-13 04:39:33 - INFO - Time taken for Epoch 11:25.00 - F1: 0.3540
Time taken for Epoch 12:8.97 - F1: 0.3301
2026-02-13 04:39:42 - INFO - Time taken for Epoch 12:8.97 - F1: 0.3301
Time taken for Epoch 13:3.60 - F1: 0.2943
2026-02-13 04:39:45 - INFO - Time taken for Epoch 13:3.60 - F1: 0.2943
Time taken for Epoch 14:3.59 - F1: 0.3927
2026-02-13 04:39:49 - INFO - Time taken for Epoch 14:3.59 - F1: 0.3927
Time taken for Epoch 15:9.77 - F1: 0.3339
2026-02-13 04:39:59 - INFO - Time taken for Epoch 15:9.77 - F1: 0.3339
Time taken for Epoch 16:3.60 - F1: 0.3981
2026-02-13 04:40:02 - INFO - Time taken for Epoch 16:3.60 - F1: 0.3981
Time taken for Epoch 17:20.21 - F1: 0.3680
2026-02-13 04:40:23 - INFO - Time taken for Epoch 17:20.21 - F1: 0.3680
Time taken for Epoch 18:3.61 - F1: 0.3727
2026-02-13 04:40:26 - INFO - Time taken for Epoch 18:3.61 - F1: 0.3727
Time taken for Epoch 19:3.60 - F1: 0.4353
2026-02-13 04:40:30 - INFO - Time taken for Epoch 19:3.60 - F1: 0.4353
Time taken for Epoch 20:11.25 - F1: 0.4054
2026-02-13 04:40:41 - INFO - Time taken for Epoch 20:11.25 - F1: 0.4054
Time taken for Epoch 21:3.61 - F1: 0.4582
2026-02-13 04:40:45 - INFO - Time taken for Epoch 21:3.61 - F1: 0.4582
Time taken for Epoch 22:28.30 - F1: 0.4432
2026-02-13 04:41:13 - INFO - Time taken for Epoch 22:28.30 - F1: 0.4432
Time taken for Epoch 23:3.63 - F1: 0.4127
2026-02-13 04:41:17 - INFO - Time taken for Epoch 23:3.63 - F1: 0.4127
Time taken for Epoch 24:3.63 - F1: 0.4755
2026-02-13 04:41:20 - INFO - Time taken for Epoch 24:3.63 - F1: 0.4755
Time taken for Epoch 25:11.41 - F1: 0.4710
2026-02-13 04:41:32 - INFO - Time taken for Epoch 25:11.41 - F1: 0.4710
Time taken for Epoch 26:3.59 - F1: 0.5076
2026-02-13 04:41:35 - INFO - Time taken for Epoch 26:3.59 - F1: 0.5076
Time taken for Epoch 27:10.88 - F1: 0.5026
2026-02-13 04:41:46 - INFO - Time taken for Epoch 27:10.88 - F1: 0.5026
Time taken for Epoch 28:3.62 - F1: 0.5274
2026-02-13 04:41:50 - INFO - Time taken for Epoch 28:3.62 - F1: 0.5274
Time taken for Epoch 29:11.13 - F1: 0.5107
2026-02-13 04:42:01 - INFO - Time taken for Epoch 29:11.13 - F1: 0.5107
Time taken for Epoch 30:3.61 - F1: 0.5309
2026-02-13 04:42:05 - INFO - Time taken for Epoch 30:3.61 - F1: 0.5309
Time taken for Epoch 31:10.20 - F1: 0.5317
2026-02-13 04:42:15 - INFO - Time taken for Epoch 31:10.20 - F1: 0.5317
Time taken for Epoch 32:10.21 - F1: 0.5175
2026-02-13 04:42:25 - INFO - Time taken for Epoch 32:10.21 - F1: 0.5175
Time taken for Epoch 33:3.61 - F1: 0.5222
2026-02-13 04:42:29 - INFO - Time taken for Epoch 33:3.61 - F1: 0.5222
Time taken for Epoch 34:3.60 - F1: 0.5185
2026-02-13 04:42:32 - INFO - Time taken for Epoch 34:3.60 - F1: 0.5185
Time taken for Epoch 35:3.61 - F1: 0.5163
2026-02-13 04:42:36 - INFO - Time taken for Epoch 35:3.61 - F1: 0.5163
Time taken for Epoch 36:3.59 - F1: 0.5167
2026-02-13 04:42:39 - INFO - Time taken for Epoch 36:3.59 - F1: 0.5167
Time taken for Epoch 37:3.59 - F1: 0.5196
2026-02-13 04:42:43 - INFO - Time taken for Epoch 37:3.59 - F1: 0.5196
Time taken for Epoch 38:3.62 - F1: 0.5139
2026-02-13 04:42:47 - INFO - Time taken for Epoch 38:3.62 - F1: 0.5139
Time taken for Epoch 39:3.60 - F1: 0.5141
2026-02-13 04:42:50 - INFO - Time taken for Epoch 39:3.60 - F1: 0.5141
Time taken for Epoch 40:3.59 - F1: 0.5146
2026-02-13 04:42:54 - INFO - Time taken for Epoch 40:3.59 - F1: 0.5146
Time taken for Epoch 41:3.60 - F1: 0.5125
2026-02-13 04:42:57 - INFO - Time taken for Epoch 41:3.60 - F1: 0.5125
Performance not improving for 10 consecutive epochs.
2026-02-13 04:42:57 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.5317 - Best Epoch:30
2026-02-13 04:42:57 - INFO - Best F1:0.5317 - Best Epoch:30
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:816: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:817: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5314, Test ECE: 0.1755
2026-02-13 04:43:04 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5314, Test ECE: 0.1755
All results: {'f1_macro': 0.5313562905252882, 'ece': 0.17546888156788032}
2026-02-13 04:43:04 - INFO - All results: {'f1_macro': 0.5313562905252882, 'ece': 0.17546888156788032}

Total time taken: 506.43 seconds
2026-02-13 04:43:04 - INFO - 
Total time taken: 506.43 seconds
2026-02-13 04:43:04 - INFO - Trial 5 finished with value: 0.5313562905252882 and parameters: {'learning_rate': 8.437573670609839e-05, 'weight_decay': 9.856113626643751e-05, 'batch_size': 16, 'co_train_epochs': 18, 'epoch_patience': 1}. Best is trial 3 with value: 0.5602574144999545.
Using devices: cuda, cuda
2026-02-13 04:43:04 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 04:43:04 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 04:43:04 - INFO - Starting log
Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 04:43:04 - INFO - Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
Learning Rate: 9.642580659798087e-05
Weight Decay: 0.0002793598817603608
Batch Size: 16
No. Epochs: 15
Epoch Patience: 9
 Accumulation Steps: 4
2026-02-13 04:43:04 - INFO - Learning Rate: 9.642580659798087e-05
Weight Decay: 0.0002793598817603608
Batch Size: 16
No. Epochs: 15
Epoch Patience: 9
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 04:43:06 - INFO - Generating initial weights
Time taken for Epoch 1:10.13 - F1: 0.0044
2026-02-13 04:43:17 - INFO - Time taken for Epoch 1:10.13 - F1: 0.0044
Time taken for Epoch 2:9.98 - F1: 0.0184
2026-02-13 04:43:27 - INFO - Time taken for Epoch 2:9.98 - F1: 0.0184
Time taken for Epoch 3:10.00 - F1: 0.0425
2026-02-13 04:43:37 - INFO - Time taken for Epoch 3:10.00 - F1: 0.0425
Time taken for Epoch 4:10.00 - F1: 0.1284
2026-02-13 04:43:47 - INFO - Time taken for Epoch 4:10.00 - F1: 0.1284
Time taken for Epoch 5:9.98 - F1: 0.1896
2026-02-13 04:43:57 - INFO - Time taken for Epoch 5:9.98 - F1: 0.1896
Time taken for Epoch 6:9.98 - F1: 0.2092
2026-02-13 04:44:07 - INFO - Time taken for Epoch 6:9.98 - F1: 0.2092
Time taken for Epoch 7:10.01 - F1: 0.2739
2026-02-13 04:44:17 - INFO - Time taken for Epoch 7:10.01 - F1: 0.2739
Time taken for Epoch 8:9.97 - F1: 0.4071
2026-02-13 04:44:27 - INFO - Time taken for Epoch 8:9.97 - F1: 0.4071
Time taken for Epoch 9:9.97 - F1: 0.3968
2026-02-13 04:44:37 - INFO - Time taken for Epoch 9:9.97 - F1: 0.3968
Time taken for Epoch 10:10.02 - F1: 0.4005
2026-02-13 04:44:47 - INFO - Time taken for Epoch 10:10.02 - F1: 0.4005
Time taken for Epoch 11:9.99 - F1: 0.4376
2026-02-13 04:44:57 - INFO - Time taken for Epoch 11:9.99 - F1: 0.4376
Time taken for Epoch 12:9.97 - F1: 0.4512
2026-02-13 04:45:07 - INFO - Time taken for Epoch 12:9.97 - F1: 0.4512
Time taken for Epoch 13:9.99 - F1: 0.4305
2026-02-13 04:45:17 - INFO - Time taken for Epoch 13:9.99 - F1: 0.4305
Time taken for Epoch 14:9.96 - F1: 0.4758
2026-02-13 04:45:27 - INFO - Time taken for Epoch 14:9.96 - F1: 0.4758
Time taken for Epoch 15:9.95 - F1: 0.4558
2026-02-13 04:45:37 - INFO - Time taken for Epoch 15:9.95 - F1: 0.4558
Best F1:0.4758 - Best Epoch:14
2026-02-13 04:45:37 - INFO - Best F1:0.4758 - Best Epoch:14
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 04:45:38 - INFO - Starting co-training
Time taken for Epoch 1: 10.18s - F1: 0.17881406
2026-02-13 04:45:49 - INFO - Time taken for Epoch 1: 10.18s - F1: 0.17881406
Time taken for Epoch 2: 11.25s - F1: 0.20196749
2026-02-13 04:46:00 - INFO - Time taken for Epoch 2: 11.25s - F1: 0.20196749
Time taken for Epoch 3: 19.15s - F1: 0.27905152
2026-02-13 04:46:19 - INFO - Time taken for Epoch 3: 19.15s - F1: 0.27905152
Time taken for Epoch 4: 17.77s - F1: 0.29442305
2026-02-13 04:46:37 - INFO - Time taken for Epoch 4: 17.77s - F1: 0.29442305
Time taken for Epoch 5: 17.62s - F1: 0.28913909
2026-02-13 04:46:55 - INFO - Time taken for Epoch 5: 17.62s - F1: 0.28913909
Time taken for Epoch 6: 10.18s - F1: 0.30068110
2026-02-13 04:47:05 - INFO - Time taken for Epoch 6: 10.18s - F1: 0.30068110
Time taken for Epoch 7: 45.14s - F1: 0.31835380
2026-02-13 04:47:50 - INFO - Time taken for Epoch 7: 45.14s - F1: 0.31835380
Time taken for Epoch 8: 21.94s - F1: 0.29698621
2026-02-13 04:48:12 - INFO - Time taken for Epoch 8: 21.94s - F1: 0.29698621
Time taken for Epoch 9: 10.20s - F1: 0.31354374
2026-02-13 04:48:22 - INFO - Time taken for Epoch 9: 10.20s - F1: 0.31354374
Time taken for Epoch 10: 10.20s - F1: 0.31601544
2026-02-13 04:48:32 - INFO - Time taken for Epoch 10: 10.20s - F1: 0.31601544
Time taken for Epoch 11: 10.18s - F1: 0.29665570
2026-02-13 04:48:42 - INFO - Time taken for Epoch 11: 10.18s - F1: 0.29665570
Time taken for Epoch 12: 10.19s - F1: 0.30067928
2026-02-13 04:48:53 - INFO - Time taken for Epoch 12: 10.19s - F1: 0.30067928
Time taken for Epoch 13: 10.17s - F1: 0.30006847
2026-02-13 04:49:03 - INFO - Time taken for Epoch 13: 10.17s - F1: 0.30006847
Time taken for Epoch 14: 10.21s - F1: 0.31991573
2026-02-13 04:49:13 - INFO - Time taken for Epoch 14: 10.21s - F1: 0.31991573
Time taken for Epoch 15: 19.51s - F1: 0.32920575
2026-02-13 04:49:33 - INFO - Time taken for Epoch 15: 19.51s - F1: 0.32920575
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:776: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/co_trained_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Fine-tuning models
2026-02-13 04:49:42 - INFO - Fine-tuning models
Time taken for Epoch 1:3.74 - F1: 0.3232
2026-02-13 04:49:46 - INFO - Time taken for Epoch 1:3.74 - F1: 0.3232
Time taken for Epoch 2:4.83 - F1: 0.3232
2026-02-13 04:49:51 - INFO - Time taken for Epoch 2:4.83 - F1: 0.3232
Time taken for Epoch 3:3.60 - F1: 0.3110
2026-02-13 04:49:54 - INFO - Time taken for Epoch 3:3.60 - F1: 0.3110
Time taken for Epoch 4:3.60 - F1: 0.3535
2026-02-13 04:49:58 - INFO - Time taken for Epoch 4:3.60 - F1: 0.3535
Time taken for Epoch 5:10.67 - F1: 0.3294
2026-02-13 04:50:09 - INFO - Time taken for Epoch 5:10.67 - F1: 0.3294
Time taken for Epoch 6:3.60 - F1: 0.3200
2026-02-13 04:50:12 - INFO - Time taken for Epoch 6:3.60 - F1: 0.3200
Time taken for Epoch 7:3.60 - F1: 0.3206
2026-02-13 04:50:16 - INFO - Time taken for Epoch 7:3.60 - F1: 0.3206
Time taken for Epoch 8:3.61 - F1: 0.3208
2026-02-13 04:50:19 - INFO - Time taken for Epoch 8:3.61 - F1: 0.3208
Time taken for Epoch 9:3.59 - F1: 0.3520
2026-02-13 04:50:23 - INFO - Time taken for Epoch 9:3.59 - F1: 0.3520
Time taken for Epoch 10:3.60 - F1: 0.3505
2026-02-13 04:50:27 - INFO - Time taken for Epoch 10:3.60 - F1: 0.3505
Time taken for Epoch 11:3.59 - F1: 0.3605
2026-02-13 04:50:30 - INFO - Time taken for Epoch 11:3.59 - F1: 0.3605
Time taken for Epoch 12:5.17 - F1: 0.3546
2026-02-13 04:50:35 - INFO - Time taken for Epoch 12:5.17 - F1: 0.3546
Time taken for Epoch 13:3.61 - F1: 0.3427
2026-02-13 04:50:39 - INFO - Time taken for Epoch 13:3.61 - F1: 0.3427
Time taken for Epoch 14:3.60 - F1: 0.3487
2026-02-13 04:50:43 - INFO - Time taken for Epoch 14:3.60 - F1: 0.3487
Time taken for Epoch 15:3.62 - F1: 0.3767
2026-02-13 04:50:46 - INFO - Time taken for Epoch 15:3.62 - F1: 0.3767
Time taken for Epoch 16:11.27 - F1: 0.3707
2026-02-13 04:50:57 - INFO - Time taken for Epoch 16:11.27 - F1: 0.3707
Time taken for Epoch 17:3.63 - F1: 0.3515
2026-02-13 04:51:01 - INFO - Time taken for Epoch 17:3.63 - F1: 0.3515
Time taken for Epoch 18:3.60 - F1: 0.3763
2026-02-13 04:51:05 - INFO - Time taken for Epoch 18:3.60 - F1: 0.3763
Time taken for Epoch 19:3.61 - F1: 0.3781
2026-02-13 04:51:08 - INFO - Time taken for Epoch 19:3.61 - F1: 0.3781
Time taken for Epoch 20:9.34 - F1: 0.3833
2026-02-13 04:51:18 - INFO - Time taken for Epoch 20:9.34 - F1: 0.3833
Time taken for Epoch 21:10.09 - F1: 0.4041
2026-02-13 04:51:28 - INFO - Time taken for Epoch 21:10.09 - F1: 0.4041
Time taken for Epoch 22:10.06 - F1: 0.3898
2026-02-13 04:51:38 - INFO - Time taken for Epoch 22:10.06 - F1: 0.3898
Time taken for Epoch 23:3.61 - F1: 0.3929
2026-02-13 04:51:41 - INFO - Time taken for Epoch 23:3.61 - F1: 0.3929
Time taken for Epoch 24:3.62 - F1: 0.4044
2026-02-13 04:51:45 - INFO - Time taken for Epoch 24:3.62 - F1: 0.4044
Time taken for Epoch 25:10.03 - F1: 0.4117
2026-02-13 04:51:55 - INFO - Time taken for Epoch 25:10.03 - F1: 0.4117
Time taken for Epoch 26:10.12 - F1: 0.4022
2026-02-13 04:52:05 - INFO - Time taken for Epoch 26:10.12 - F1: 0.4022
Time taken for Epoch 27:3.59 - F1: 0.3928
2026-02-13 04:52:09 - INFO - Time taken for Epoch 27:3.59 - F1: 0.3928
Time taken for Epoch 28:3.62 - F1: 0.4186
2026-02-13 04:52:12 - INFO - Time taken for Epoch 28:3.62 - F1: 0.4186
Time taken for Epoch 29:10.45 - F1: 0.4208
2026-02-13 04:52:23 - INFO - Time taken for Epoch 29:10.45 - F1: 0.4208
Time taken for Epoch 30:24.55 - F1: 0.4187
2026-02-13 04:52:47 - INFO - Time taken for Epoch 30:24.55 - F1: 0.4187
Time taken for Epoch 31:3.59 - F1: 0.4263
2026-02-13 04:52:51 - INFO - Time taken for Epoch 31:3.59 - F1: 0.4263
Time taken for Epoch 32:12.01 - F1: 0.4515
2026-02-13 04:53:03 - INFO - Time taken for Epoch 32:12.01 - F1: 0.4515
Time taken for Epoch 33:9.98 - F1: 0.4593
2026-02-13 04:53:13 - INFO - Time taken for Epoch 33:9.98 - F1: 0.4593
Time taken for Epoch 34:10.25 - F1: 0.4126
2026-02-13 04:53:23 - INFO - Time taken for Epoch 34:10.25 - F1: 0.4126
Time taken for Epoch 35:3.59 - F1: 0.4213
2026-02-13 04:53:27 - INFO - Time taken for Epoch 35:3.59 - F1: 0.4213
Time taken for Epoch 36:3.59 - F1: 0.4343
2026-02-13 04:53:30 - INFO - Time taken for Epoch 36:3.59 - F1: 0.4343
Time taken for Epoch 37:3.62 - F1: 0.4269
2026-02-13 04:53:34 - INFO - Time taken for Epoch 37:3.62 - F1: 0.4269
Time taken for Epoch 38:3.60 - F1: 0.4141
2026-02-13 04:53:38 - INFO - Time taken for Epoch 38:3.60 - F1: 0.4141
Time taken for Epoch 39:3.61 - F1: 0.4226
2026-02-13 04:53:41 - INFO - Time taken for Epoch 39:3.61 - F1: 0.4226
Time taken for Epoch 40:3.62 - F1: 0.4642
2026-02-13 04:53:45 - INFO - Time taken for Epoch 40:3.62 - F1: 0.4642
Time taken for Epoch 41:29.04 - F1: 0.4655
2026-02-13 04:54:14 - INFO - Time taken for Epoch 41:29.04 - F1: 0.4655
Time taken for Epoch 42:11.45 - F1: 0.4277
2026-02-13 04:54:25 - INFO - Time taken for Epoch 42:11.45 - F1: 0.4277
Time taken for Epoch 43:3.60 - F1: 0.4172
2026-02-13 04:54:29 - INFO - Time taken for Epoch 43:3.60 - F1: 0.4172
Time taken for Epoch 44:3.59 - F1: 0.4214
2026-02-13 04:54:32 - INFO - Time taken for Epoch 44:3.59 - F1: 0.4214
Time taken for Epoch 45:3.59 - F1: 0.4141
2026-02-13 04:54:36 - INFO - Time taken for Epoch 45:3.59 - F1: 0.4141
Time taken for Epoch 46:3.59 - F1: 0.4220
2026-02-13 04:54:40 - INFO - Time taken for Epoch 46:3.59 - F1: 0.4220
Time taken for Epoch 47:3.59 - F1: 0.4154
2026-02-13 04:54:43 - INFO - Time taken for Epoch 47:3.59 - F1: 0.4154
Time taken for Epoch 48:3.59 - F1: 0.4132
2026-02-13 04:54:47 - INFO - Time taken for Epoch 48:3.59 - F1: 0.4132
Time taken for Epoch 49:3.61 - F1: 0.4079
2026-02-13 04:54:50 - INFO - Time taken for Epoch 49:3.61 - F1: 0.4079
Time taken for Epoch 50:3.60 - F1: 0.4142
2026-02-13 04:54:54 - INFO - Time taken for Epoch 50:3.60 - F1: 0.4142
Time taken for Epoch 51:3.60 - F1: 0.4334
2026-02-13 04:54:58 - INFO - Time taken for Epoch 51:3.60 - F1: 0.4334
Performance not improving for 10 consecutive epochs.
2026-02-13 04:54:58 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.4655 - Best Epoch:40
2026-02-13 04:54:58 - INFO - Best F1:0.4655 - Best Epoch:40
Using Bert Tweet model: bert-tweet
/tmp/ipykernel_501788/78231371.py:816: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_1.load_state_dict(torch.load(model_1_path))
/tmp/ipykernel_501788/78231371.py:817: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_2.load_state_dict(torch.load(model_2_path))
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_1_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt
Successfully deleted /home/jovyan/saved_models/humanitarian10/optuna-bertweet-cyclone-idai-2019-label50-set3/final_model_2_optuna-bertweet-cyclone-idai-2019-label50-set3_gpt4o_50_shot_bert-tweet_50_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5318, Test ECE: 0.1408
2026-02-13 04:55:03 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5318, Test ECE: 0.1408
All results: {'f1_macro': 0.5317918097732697, 'ece': 0.14078152072597988}
2026-02-13 04:55:03 - INFO - All results: {'f1_macro': 0.5317918097732697, 'ece': 0.14078152072597988}

Total time taken: 719.42 seconds
2026-02-13 04:55:03 - INFO - 
Total time taken: 719.42 seconds
2026-02-13 04:55:03 - INFO - Trial 6 finished with value: 0.5317918097732697 and parameters: {'learning_rate': 9.642580659798087e-05, 'weight_decay': 0.0002793598817603608, 'batch_size': 16, 'co_train_epochs': 15, 'epoch_patience': 9}. Best is trial 3 with value: 0.5602574144999545.
Using devices: cuda, cuda
2026-02-13 04:55:03 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 04:55:03 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 04:55:03 - INFO - Starting log
Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 04:55:03 - INFO - Dataset: humanitarian10, Event: cyclone_idai_2019, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
Learning Rate: 2.42054932664274e-05
Weight Decay: 0.007822073623269699
Batch Size: 8
No. Epochs: 16
Epoch Patience: 6
 Accumulation Steps: 8
2026-02-13 04:55:04 - INFO - Learning Rate: 2.42054932664274e-05
Weight Decay: 0.007822073623269699
Batch Size: 8
No. Epochs: 16
Epoch Patience: 6
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 04:55:05 - INFO - Generating initial weights
Time taken for Epoch 1:10.94 - F1: 0.0044
2026-02-13 04:55:17 - INFO - Time taken for Epoch 1:10.94 - F1: 0.0044
Time taken for Epoch 2:10.83 - F1: 0.0278
2026-02-13 04:55:28 - INFO - Time taken for Epoch 2:10.83 - F1: 0.0278
Time taken for Epoch 3:10.92 - F1: 0.0903
2026-02-13 04:55:39 - INFO - Time taken for Epoch 3:10.92 - F1: 0.0903
Time taken for Epoch 4:10.97 - F1: 0.1223
2026-02-13 04:55:50 - INFO - Time taken for Epoch 4:10.97 - F1: 0.1223
Time taken for Epoch 5:10.89 - F1: 0.1790
2026-02-13 04:56:01 - INFO - Time taken for Epoch 5:10.89 - F1: 0.1790
Time taken for Epoch 6:10.82 - F1: 0.1800
2026-02-13 04:56:11 - INFO - Time taken for Epoch 6:10.82 - F1: 0.1800
Time taken for Epoch 7:10.85 - F1: 0.2109
2026-02-13 04:56:22 - INFO - Time taken for Epoch 7:10.85 - F1: 0.2109
Time taken for Epoch 8:10.90 - F1: 0.2484
2026-02-13 04:56:33 - INFO - Time taken for Epoch 8:10.90 - F1: 0.2484
Time taken for Epoch 9:10.90 - F1: 0.2646
2026-02-13 04:56:44 - INFO - Time taken for Epoch 9:10.90 - F1: 0.2646
Time taken for Epoch 10:10.87 - F1: 0.2849
2026-02-13 04:56:55 - INFO - Time taken for Epoch 10:10.87 - F1: 0.2849
Time taken for Epoch 11:10.90 - F1: 0.3187
2026-02-13 04:57:06 - INFO - Time taken for Epoch 11:10.90 - F1: 0.3187
Time taken for Epoch 12:10.89 - F1: 0.3455
2026-02-13 04:57:17 - INFO - Time taken for Epoch 12:10.89 - F1: 0.3455
Time taken for Epoch 13:10.87 - F1: 0.3764
2026-02-13 04:57:28 - INFO - Time taken for Epoch 13:10.87 - F1: 0.3764
Time taken for Epoch 14:10.89 - F1: 0.3976
2026-02-13 04:57:38 - INFO - Time taken for Epoch 14:10.89 - F1: 0.3976
Time taken for Epoch 15:10.87 - F1: 0.3886
2026-02-13 04:57:49 - INFO - Time taken for Epoch 15:10.87 - F1: 0.3886
Time taken for Epoch 16:10.87 - F1: 0.4097
2026-02-13 04:58:00 - INFO - Time taken for Epoch 16:10.87 - F1: 0.4097
Best F1:0.4097 - Best Epoch:16
2026-02-13 04:58:00 - INFO - Best F1:0.4097 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 04:58:02 - INFO - Starting co-training
Time taken for Epoch 1: 9.71s - F1: 0.06452703
2026-02-13 04:58:11 - INFO - Time taken for Epoch 1: 9.71s - F1: 0.06452703
Time taken for Epoch 2: 10.66s - F1: 0.19595120
2026-02-13 04:58:22 - INFO - Time taken for Epoch 2: 10.66s - F1: 0.19595120
Time taken for Epoch 3: 15.43s - F1: 0.21657551
2026-02-13 04:58:38 - INFO - Time taken for Epoch 3: 15.43s - F1: 0.21657551
Time taken for Epoch 4: 15.44s - F1: 0.20921952
2026-02-13 04:58:53 - INFO - Time taken for Epoch 4: 15.44s - F1: 0.20921952
Time taken for Epoch 5: 9.66s - F1: 0.25537019
2026-02-13 04:59:03 - INFO - Time taken for Epoch 5: 9.66s - F1: 0.25537019
Time taken for Epoch 6: 17.28s - F1: 0.27491379
2026-02-13 04:59:20 - INFO - Time taken for Epoch 6: 17.28s - F1: 0.27491379
Time taken for Epoch 7: 17.45s - F1: 0.26617528
2026-02-13 04:59:37 - INFO - Time taken for Epoch 7: 17.45s - F1: 0.26617528
Time taken for Epoch 8: 9.71s - F1: 0.28529663
2026-02-13 04:59:47 - INFO - Time taken for Epoch 8: 9.71s - F1: 0.28529663
Time taken for Epoch 9: 27.59s - F1: 0.27496191
2026-02-13 05:00:15 - INFO - Time taken for Epoch 9: 27.59s - F1: 0.27496191