Running with 5 label/class set 3

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 23:46:19 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 23:46:19 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_maria_2017
Using devices: cuda, cuda
2026-02-13 23:46:19 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 23:46:19 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 23:46:19 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 23:46:19 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0009994966241834726
Weight Decay: 0.00031691527955763884
Batch Size: 64
No. Epochs: 6
Epoch Patience: 7
 Accumulation Steps: 1
2026-02-13 23:46:20 - INFO - Learning Rate: 0.0009994966241834726
Weight Decay: 0.00031691527955763884
Batch Size: 64
No. Epochs: 6
Epoch Patience: 7
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 23:46:22 - INFO - Generating initial weights
Time taken for Epoch 1:19.57 - F1: 0.0461
2026-02-13 23:46:45 - INFO - Time taken for Epoch 1:19.57 - F1: 0.0461
Time taken for Epoch 2:19.28 - F1: 0.0752
2026-02-13 23:47:04 - INFO - Time taken for Epoch 2:19.28 - F1: 0.0752
Time taken for Epoch 3:19.25 - F1: 0.0451
2026-02-13 23:47:23 - INFO - Time taken for Epoch 3:19.25 - F1: 0.0451
Time taken for Epoch 4:19.25 - F1: 0.0104
2026-02-13 23:47:43 - INFO - Time taken for Epoch 4:19.25 - F1: 0.0104
Time taken for Epoch 5:19.24 - F1: 0.0189
2026-02-13 23:48:02 - INFO - Time taken for Epoch 5:19.24 - F1: 0.0189
Time taken for Epoch 6:19.28 - F1: 0.0189
2026-02-13 23:48:21 - INFO - Time taken for Epoch 6:19.28 - F1: 0.0189
Best F1:0.0752 - Best Epoch:2
2026-02-13 23:48:21 - INFO - Best F1:0.0752 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 23:48:22 - INFO - Starting co-training
Time taken for Epoch 1: 46.65s - F1: 0.04755179
2026-02-13 23:49:09 - INFO - Time taken for Epoch 1: 46.65s - F1: 0.04755179
Time taken for Epoch 2: 47.87s - F1: 0.04755179
2026-02-13 23:49:57 - INFO - Time taken for Epoch 2: 47.87s - F1: 0.04755179
Time taken for Epoch 3: 46.76s - F1: 0.04755179
2026-02-13 23:50:44 - INFO - Time taken for Epoch 3: 46.76s - F1: 0.04755179
Time taken for Epoch 4: 46.81s - F1: 0.04755179
2026-02-13 23:51:31 - INFO - Time taken for Epoch 4: 46.81s - F1: 0.04755179
Time taken for Epoch 5: 46.78s - F1: 0.04755179
2026-02-13 23:52:18 - INFO - Time taken for Epoch 5: 46.78s - F1: 0.04755179
Time taken for Epoch 6: 46.77s - F1: 0.04755179
2026-02-13 23:53:04 - INFO - Time taken for Epoch 6: 46.77s - F1: 0.04755179
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 23:53:07 - INFO - Fine-tuning models
Time taken for Epoch 1:2.67 - F1: 0.0476
2026-02-13 23:53:10 - INFO - Time taken for Epoch 1:2.67 - F1: 0.0476
Time taken for Epoch 2:3.80 - F1: 0.0089
2026-02-13 23:53:14 - INFO - Time taken for Epoch 2:3.80 - F1: 0.0089
Time taken for Epoch 3:2.66 - F1: 0.0038
2026-02-13 23:53:16 - INFO - Time taken for Epoch 3:2.66 - F1: 0.0038
Time taken for Epoch 4:2.66 - F1: 0.0394
2026-02-13 23:53:19 - INFO - Time taken for Epoch 4:2.66 - F1: 0.0394
Time taken for Epoch 5:2.66 - F1: 0.0394
2026-02-13 23:53:22 - INFO - Time taken for Epoch 5:2.66 - F1: 0.0394
Time taken for Epoch 6:2.66 - F1: 0.0363
2026-02-13 23:53:24 - INFO - Time taken for Epoch 6:2.66 - F1: 0.0363
Time taken for Epoch 7:2.66 - F1: 0.0363
2026-02-13 23:53:27 - INFO - Time taken for Epoch 7:2.66 - F1: 0.0363
Time taken for Epoch 8:2.66 - F1: 0.0089
2026-02-13 23:53:30 - INFO - Time taken for Epoch 8:2.66 - F1: 0.0089
Time taken for Epoch 9:2.66 - F1: 0.0476
2026-02-13 23:53:32 - INFO - Time taken for Epoch 9:2.66 - F1: 0.0476
Time taken for Epoch 10:2.66 - F1: 0.0476
2026-02-13 23:53:35 - INFO - Time taken for Epoch 10:2.66 - F1: 0.0476
Time taken for Epoch 11:2.65 - F1: 0.0476
2026-02-13 23:53:38 - INFO - Time taken for Epoch 11:2.65 - F1: 0.0476
Performance not improving for 10 consecutive epochs.
2026-02-13 23:53:38 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:0
2026-02-13 23:53:38 - INFO - Best F1:0.0476 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0944
2026-02-13 23:53:45 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0944
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.09444762758094294)}
2026-02-13 23:53:45 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.09444762758094294)}

Total time taken: 446.00 seconds
2026-02-13 23:53:45 - INFO - 
Total time taken: 446.00 seconds
2026-02-13 23:53:45 - INFO - Trial 0 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0009994966241834726, 'weight_decay': 0.00031691527955763884, 'batch_size': 64, 'co_train_epochs': 6, 'epoch_patience': 7}. Best is trial 0 with value: 0.04740255804085591.
Using devices: cuda, cuda
2026-02-13 23:53:45 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 23:53:45 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 23:53:45 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 23:53:45 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 7.573279078053609e-05
Weight Decay: 0.0011660662470591536
Batch Size: 8
No. Epochs: 13
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-13 23:53:46 - INFO - Learning Rate: 7.573279078053609e-05
Weight Decay: 0.0011660662470591536
Batch Size: 8
No. Epochs: 13
Epoch Patience: 7
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 23:53:47 - INFO - Generating initial weights
Time taken for Epoch 1:22.52 - F1: 0.0610
2026-02-13 23:54:13 - INFO - Time taken for Epoch 1:22.52 - F1: 0.0610
Time taken for Epoch 2:22.48 - F1: 0.0336
2026-02-13 23:54:35 - INFO - Time taken for Epoch 2:22.48 - F1: 0.0336
Time taken for Epoch 3:22.51 - F1: 0.0189
2026-02-13 23:54:58 - INFO - Time taken for Epoch 3:22.51 - F1: 0.0189
Time taken for Epoch 4:22.57 - F1: 0.0189
2026-02-13 23:55:20 - INFO - Time taken for Epoch 4:22.57 - F1: 0.0189
Time taken for Epoch 5:22.59 - F1: 0.0642
2026-02-13 23:55:43 - INFO - Time taken for Epoch 5:22.59 - F1: 0.0642
Time taken for Epoch 6:22.55 - F1: 0.1850
2026-02-13 23:56:06 - INFO - Time taken for Epoch 6:22.55 - F1: 0.1850
Time taken for Epoch 7:22.56 - F1: 0.2579
2026-02-13 23:56:28 - INFO - Time taken for Epoch 7:22.56 - F1: 0.2579
Time taken for Epoch 8:22.58 - F1: 0.2793
2026-02-13 23:56:51 - INFO - Time taken for Epoch 8:22.58 - F1: 0.2793
Time taken for Epoch 9:22.62 - F1: 0.2883
2026-02-13 23:57:13 - INFO - Time taken for Epoch 9:22.62 - F1: 0.2883
Time taken for Epoch 10:22.61 - F1: 0.2847
2026-02-13 23:57:36 - INFO - Time taken for Epoch 10:22.61 - F1: 0.2847
Time taken for Epoch 11:22.61 - F1: 0.2892
2026-02-13 23:57:59 - INFO - Time taken for Epoch 11:22.61 - F1: 0.2892
Time taken for Epoch 12:22.60 - F1: 0.2920
2026-02-13 23:58:21 - INFO - Time taken for Epoch 12:22.60 - F1: 0.2920
Time taken for Epoch 13:22.57 - F1: 0.3011
2026-02-13 23:58:44 - INFO - Time taken for Epoch 13:22.57 - F1: 0.3011
Best F1:0.3011 - Best Epoch:13
2026-02-13 23:58:44 - INFO - Best F1:0.3011 - Best Epoch:13
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 23:58:45 - INFO - Starting co-training
Time taken for Epoch 1: 27.67s - F1: 0.30246150
2026-02-13 23:59:13 - INFO - Time taken for Epoch 1: 27.67s - F1: 0.30246150
Time taken for Epoch 2: 28.83s - F1: 0.39967773
2026-02-13 23:59:42 - INFO - Time taken for Epoch 2: 28.83s - F1: 0.39967773
Time taken for Epoch 3: 29.03s - F1: 0.44153436
2026-02-14 00:00:11 - INFO - Time taken for Epoch 3: 29.03s - F1: 0.44153436
Time taken for Epoch 4: 28.94s - F1: 0.46079146
2026-02-14 00:00:40 - INFO - Time taken for Epoch 4: 28.94s - F1: 0.46079146
Time taken for Epoch 5: 29.46s - F1: 0.53816737
2026-02-14 00:01:09 - INFO - Time taken for Epoch 5: 29.46s - F1: 0.53816737
Time taken for Epoch 6: 29.03s - F1: 0.55150414
2026-02-14 00:01:38 - INFO - Time taken for Epoch 6: 29.03s - F1: 0.55150414
Time taken for Epoch 7: 28.94s - F1: 0.55298645
2026-02-14 00:02:07 - INFO - Time taken for Epoch 7: 28.94s - F1: 0.55298645
Time taken for Epoch 8: 29.03s - F1: 0.61448539
2026-02-14 00:02:36 - INFO - Time taken for Epoch 8: 29.03s - F1: 0.61448539
Time taken for Epoch 9: 28.96s - F1: 0.57600975
2026-02-14 00:03:05 - INFO - Time taken for Epoch 9: 28.96s - F1: 0.57600975
Time taken for Epoch 10: 27.76s - F1: 0.57038518
2026-02-14 00:03:33 - INFO - Time taken for Epoch 10: 27.76s - F1: 0.57038518
Time taken for Epoch 11: 27.96s - F1: 0.60926131
2026-02-14 00:04:01 - INFO - Time taken for Epoch 11: 27.96s - F1: 0.60926131
Time taken for Epoch 12: 27.79s - F1: 0.58839845
2026-02-14 00:04:29 - INFO - Time taken for Epoch 12: 27.79s - F1: 0.58839845
Time taken for Epoch 13: 27.79s - F1: 0.55909849
2026-02-14 00:04:57 - INFO - Time taken for Epoch 13: 27.79s - F1: 0.55909849
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 00:04:59 - INFO - Fine-tuning models
Time taken for Epoch 1:3.12 - F1: 0.6000
2026-02-14 00:05:03 - INFO - Time taken for Epoch 1:3.12 - F1: 0.6000
Time taken for Epoch 2:4.25 - F1: 0.6050
2026-02-14 00:05:07 - INFO - Time taken for Epoch 2:4.25 - F1: 0.6050
Time taken for Epoch 3:4.37 - F1: 0.6166
2026-02-14 00:05:11 - INFO - Time taken for Epoch 3:4.37 - F1: 0.6166
Time taken for Epoch 4:4.36 - F1: 0.6229
2026-02-14 00:05:16 - INFO - Time taken for Epoch 4:4.36 - F1: 0.6229
Time taken for Epoch 5:4.36 - F1: 0.6454
2026-02-14 00:05:20 - INFO - Time taken for Epoch 5:4.36 - F1: 0.6454
Time taken for Epoch 6:4.38 - F1: 0.6240
2026-02-14 00:05:24 - INFO - Time taken for Epoch 6:4.38 - F1: 0.6240
Time taken for Epoch 7:3.10 - F1: 0.6317
2026-02-14 00:05:27 - INFO - Time taken for Epoch 7:3.10 - F1: 0.6317
Time taken for Epoch 8:3.10 - F1: 0.6291
2026-02-14 00:05:30 - INFO - Time taken for Epoch 8:3.10 - F1: 0.6291
Time taken for Epoch 9:3.10 - F1: 0.6440
2026-02-14 00:05:34 - INFO - Time taken for Epoch 9:3.10 - F1: 0.6440
Time taken for Epoch 10:3.10 - F1: 0.6354
2026-02-14 00:05:37 - INFO - Time taken for Epoch 10:3.10 - F1: 0.6354
Time taken for Epoch 11:3.10 - F1: 0.6322
2026-02-14 00:05:40 - INFO - Time taken for Epoch 11:3.10 - F1: 0.6322
Time taken for Epoch 12:3.10 - F1: 0.6321
2026-02-14 00:05:43 - INFO - Time taken for Epoch 12:3.10 - F1: 0.6321
Time taken for Epoch 13:3.10 - F1: 0.6371
2026-02-14 00:05:46 - INFO - Time taken for Epoch 13:3.10 - F1: 0.6371
Time taken for Epoch 14:3.10 - F1: 0.6395
2026-02-14 00:05:49 - INFO - Time taken for Epoch 14:3.10 - F1: 0.6395
Time taken for Epoch 15:3.10 - F1: 0.6327
2026-02-14 00:05:52 - INFO - Time taken for Epoch 15:3.10 - F1: 0.6327
Performance not improving for 10 consecutive epochs.
2026-02-14 00:05:52 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6454 - Best Epoch:4
2026-02-14 00:05:52 - INFO - Best F1:0.6454 - Best Epoch:4
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6348, Test ECE: 0.0726
2026-02-14 00:06:01 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6348, Test ECE: 0.0726
All results: {'f1_macro': 0.6347768930976512, 'ece': np.float64(0.07263118305583272)}
2026-02-14 00:06:01 - INFO - All results: {'f1_macro': 0.6347768930976512, 'ece': np.float64(0.07263118305583272)}

Total time taken: 735.33 seconds
2026-02-14 00:06:01 - INFO - 
Total time taken: 735.33 seconds
2026-02-14 00:06:01 - INFO - Trial 1 finished with value: 0.6347768930976512 and parameters: {'learning_rate': 7.573279078053609e-05, 'weight_decay': 0.0011660662470591536, 'batch_size': 8, 'co_train_epochs': 13, 'epoch_patience': 7}. Best is trial 1 with value: 0.6347768930976512.
Using devices: cuda, cuda
2026-02-14 00:06:01 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 00:06:01 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 00:06:01 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-14 00:06:01 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 2.851294622438512e-05
Weight Decay: 4.676870477102402e-05
Batch Size: 8
No. Epochs: 12
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-14 00:06:01 - INFO - Learning Rate: 2.851294622438512e-05
Weight Decay: 4.676870477102402e-05
Batch Size: 8
No. Epochs: 12
Epoch Patience: 10
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 00:06:02 - INFO - Generating initial weights
Time taken for Epoch 1:22.52 - F1: 0.0452
2026-02-14 00:06:28 - INFO - Time taken for Epoch 1:22.52 - F1: 0.0452
Time taken for Epoch 2:22.48 - F1: 0.0473
2026-02-14 00:06:51 - INFO - Time taken for Epoch 2:22.48 - F1: 0.0473
Time taken for Epoch 3:22.53 - F1: 0.0497
2026-02-14 00:07:13 - INFO - Time taken for Epoch 3:22.53 - F1: 0.0497
Time taken for Epoch 4:22.58 - F1: 0.0681
2026-02-14 00:07:36 - INFO - Time taken for Epoch 4:22.58 - F1: 0.0681
Time taken for Epoch 5:22.59 - F1: 0.0877
2026-02-14 00:07:58 - INFO - Time taken for Epoch 5:22.59 - F1: 0.0877
Time taken for Epoch 6:22.57 - F1: 0.0755
2026-02-14 00:08:21 - INFO - Time taken for Epoch 6:22.57 - F1: 0.0755
Time taken for Epoch 7:22.55 - F1: 0.0628
2026-02-14 00:08:44 - INFO - Time taken for Epoch 7:22.55 - F1: 0.0628
Time taken for Epoch 8:22.59 - F1: 0.0440
2026-02-14 00:09:06 - INFO - Time taken for Epoch 8:22.59 - F1: 0.0440
Time taken for Epoch 9:22.60 - F1: 0.0415
2026-02-14 00:09:29 - INFO - Time taken for Epoch 9:22.60 - F1: 0.0415
Time taken for Epoch 10:22.59 - F1: 0.0621
2026-02-14 00:09:51 - INFO - Time taken for Epoch 10:22.59 - F1: 0.0621
Time taken for Epoch 11:22.58 - F1: 0.0759
2026-02-14 00:10:14 - INFO - Time taken for Epoch 11:22.58 - F1: 0.0759
Time taken for Epoch 12:22.59 - F1: 0.1069
2026-02-14 00:10:36 - INFO - Time taken for Epoch 12:22.59 - F1: 0.1069
Best F1:0.1069 - Best Epoch:12
2026-02-14 00:10:36 - INFO - Best F1:0.1069 - Best Epoch:12
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 00:10:38 - INFO - Starting co-training
Time taken for Epoch 1: 27.71s - F1: 0.21120983
2026-02-14 00:11:06 - INFO - Time taken for Epoch 1: 27.71s - F1: 0.21120983
Time taken for Epoch 2: 28.92s - F1: 0.38964371
2026-02-14 00:11:35 - INFO - Time taken for Epoch 2: 28.92s - F1: 0.38964371
Time taken for Epoch 3: 28.96s - F1: 0.40518349
2026-02-14 00:12:04 - INFO - Time taken for Epoch 3: 28.96s - F1: 0.40518349
Time taken for Epoch 4: 28.99s - F1: 0.45125130
2026-02-14 00:12:33 - INFO - Time taken for Epoch 4: 28.99s - F1: 0.45125130
Time taken for Epoch 5: 29.00s - F1: 0.45677746
2026-02-14 00:13:02 - INFO - Time taken for Epoch 5: 29.00s - F1: 0.45677746
Time taken for Epoch 6: 28.99s - F1: 0.53434578
2026-02-14 00:13:31 - INFO - Time taken for Epoch 6: 28.99s - F1: 0.53434578
Time taken for Epoch 7: 28.98s - F1: 0.57745941
2026-02-14 00:14:00 - INFO - Time taken for Epoch 7: 28.98s - F1: 0.57745941
Time taken for Epoch 8: 29.01s - F1: 0.63202101
2026-02-14 00:14:29 - INFO - Time taken for Epoch 8: 29.01s - F1: 0.63202101
Time taken for Epoch 9: 28.99s - F1: 0.64218931
2026-02-14 00:14:58 - INFO - Time taken for Epoch 9: 28.99s - F1: 0.64218931
Time taken for Epoch 10: 29.13s - F1: 0.63395866
2026-02-14 00:15:27 - INFO - Time taken for Epoch 10: 29.13s - F1: 0.63395866
Time taken for Epoch 11: 27.76s - F1: 0.63696030
2026-02-14 00:15:54 - INFO - Time taken for Epoch 11: 27.76s - F1: 0.63696030
Time taken for Epoch 12: 27.77s - F1: 0.62907687
2026-02-14 00:16:22 - INFO - Time taken for Epoch 12: 27.77s - F1: 0.62907687
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 00:16:30 - INFO - Fine-tuning models
Time taken for Epoch 1:3.09 - F1: 0.6362
2026-02-14 00:16:33 - INFO - Time taken for Epoch 1:3.09 - F1: 0.6362
Time taken for Epoch 2:4.26 - F1: 0.6323
2026-02-14 00:16:37 - INFO - Time taken for Epoch 2:4.26 - F1: 0.6323
Time taken for Epoch 3:3.09 - F1: 0.6284
2026-02-14 00:16:40 - INFO - Time taken for Epoch 3:3.09 - F1: 0.6284
Time taken for Epoch 4:3.09 - F1: 0.6172
2026-02-14 00:16:44 - INFO - Time taken for Epoch 4:3.09 - F1: 0.6172
Time taken for Epoch 5:3.09 - F1: 0.6223
2026-02-14 00:16:47 - INFO - Time taken for Epoch 5:3.09 - F1: 0.6223
Time taken for Epoch 6:3.10 - F1: 0.6151
2026-02-14 00:16:50 - INFO - Time taken for Epoch 6:3.10 - F1: 0.6151
Time taken for Epoch 7:3.09 - F1: 0.6123
2026-02-14 00:16:53 - INFO - Time taken for Epoch 7:3.09 - F1: 0.6123
Time taken for Epoch 8:3.09 - F1: 0.6143
2026-02-14 00:16:56 - INFO - Time taken for Epoch 8:3.09 - F1: 0.6143
Time taken for Epoch 9:3.10 - F1: 0.6228
2026-02-14 00:16:59 - INFO - Time taken for Epoch 9:3.10 - F1: 0.6228
Time taken for Epoch 10:3.10 - F1: 0.6261
2026-02-14 00:17:02 - INFO - Time taken for Epoch 10:3.10 - F1: 0.6261
Time taken for Epoch 11:3.10 - F1: 0.6296
2026-02-14 00:17:05 - INFO - Time taken for Epoch 11:3.10 - F1: 0.6296
Performance not improving for 10 consecutive epochs.
2026-02-14 00:17:05 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6362 - Best Epoch:0
2026-02-14 00:17:05 - INFO - Best F1:0.6362 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6486, Test ECE: 0.0456
2026-02-14 00:17:13 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6486, Test ECE: 0.0456
All results: {'f1_macro': 0.6486424452307257, 'ece': np.float64(0.045632550769308576)}
2026-02-14 00:17:13 - INFO - All results: {'f1_macro': 0.6486424452307257, 'ece': np.float64(0.045632550769308576)}

Total time taken: 672.59 seconds
2026-02-14 00:17:13 - INFO - 
Total time taken: 672.59 seconds
2026-02-14 00:17:13 - INFO - Trial 2 finished with value: 0.6486424452307257 and parameters: {'learning_rate': 2.851294622438512e-05, 'weight_decay': 4.676870477102402e-05, 'batch_size': 8, 'co_train_epochs': 12, 'epoch_patience': 10}. Best is trial 2 with value: 0.6486424452307257.
Using devices: cuda, cuda
2026-02-14 00:17:13 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 00:17:13 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 00:17:13 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-14 00:17:13 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0001332957390210613
Weight Decay: 0.0015966409186445826
Batch Size: 64
No. Epochs: 7
Epoch Patience: 6
 Accumulation Steps: 1
2026-02-14 00:17:14 - INFO - Learning Rate: 0.0001332957390210613
Weight Decay: 0.0015966409186445826
Batch Size: 64
No. Epochs: 7
Epoch Patience: 6
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 00:17:15 - INFO - Generating initial weights
Time taken for Epoch 1:19.22 - F1: 0.0729
2026-02-14 00:17:37 - INFO - Time taken for Epoch 1:19.22 - F1: 0.0729
Time taken for Epoch 2:19.16 - F1: 0.1249
2026-02-14 00:17:57 - INFO - Time taken for Epoch 2:19.16 - F1: 0.1249
Time taken for Epoch 3:19.18 - F1: 0.1997
2026-02-14 00:18:16 - INFO - Time taken for Epoch 3:19.18 - F1: 0.1997
Time taken for Epoch 4:19.22 - F1: 0.2873
2026-02-14 00:18:35 - INFO - Time taken for Epoch 4:19.22 - F1: 0.2873
Time taken for Epoch 5:19.22 - F1: 0.3121
2026-02-14 00:18:54 - INFO - Time taken for Epoch 5:19.22 - F1: 0.3121
Time taken for Epoch 6:19.23 - F1: 0.3137
2026-02-14 00:19:13 - INFO - Time taken for Epoch 6:19.23 - F1: 0.3137
Time taken for Epoch 7:19.22 - F1: 0.3258
2026-02-14 00:19:33 - INFO - Time taken for Epoch 7:19.22 - F1: 0.3258
Best F1:0.3258 - Best Epoch:7
2026-02-14 00:19:33 - INFO - Best F1:0.3258 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 00:19:34 - INFO - Starting co-training
Time taken for Epoch 1: 46.68s - F1: 0.62466007
2026-02-14 00:20:21 - INFO - Time taken for Epoch 1: 46.68s - F1: 0.62466007
Time taken for Epoch 2: 47.87s - F1: 0.61694039
2026-02-14 00:21:09 - INFO - Time taken for Epoch 2: 47.87s - F1: 0.61694039
Time taken for Epoch 3: 46.76s - F1: 0.63520108
2026-02-14 00:21:55 - INFO - Time taken for Epoch 3: 46.76s - F1: 0.63520108
Time taken for Epoch 4: 48.01s - F1: 0.64083443
2026-02-14 00:22:43 - INFO - Time taken for Epoch 4: 48.01s - F1: 0.64083443
Time taken for Epoch 5: 48.02s - F1: 0.63760751
2026-02-14 00:23:31 - INFO - Time taken for Epoch 5: 48.02s - F1: 0.63760751
Time taken for Epoch 6: 46.78s - F1: 0.63164169
2026-02-14 00:24:18 - INFO - Time taken for Epoch 6: 46.78s - F1: 0.63164169
Time taken for Epoch 7: 46.77s - F1: 0.65171686
2026-02-14 00:25:05 - INFO - Time taken for Epoch 7: 46.77s - F1: 0.65171686
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 00:25:09 - INFO - Fine-tuning models
Time taken for Epoch 1:2.67 - F1: 0.6501
2026-02-14 00:25:12 - INFO - Time taken for Epoch 1:2.67 - F1: 0.6501
Time taken for Epoch 2:3.81 - F1: 0.6338
2026-02-14 00:25:15 - INFO - Time taken for Epoch 2:3.81 - F1: 0.6338
Time taken for Epoch 3:2.66 - F1: 0.6435
2026-02-14 00:25:18 - INFO - Time taken for Epoch 3:2.66 - F1: 0.6435
Time taken for Epoch 4:2.66 - F1: 0.6592
2026-02-14 00:25:21 - INFO - Time taken for Epoch 4:2.66 - F1: 0.6592
Time taken for Epoch 5:3.94 - F1: 0.6603
2026-02-14 00:25:25 - INFO - Time taken for Epoch 5:3.94 - F1: 0.6603
Time taken for Epoch 6:3.93 - F1: 0.6484
2026-02-14 00:25:29 - INFO - Time taken for Epoch 6:3.93 - F1: 0.6484
Time taken for Epoch 7:2.66 - F1: 0.6420
2026-02-14 00:25:31 - INFO - Time taken for Epoch 7:2.66 - F1: 0.6420
Time taken for Epoch 8:2.66 - F1: 0.6396
2026-02-14 00:25:34 - INFO - Time taken for Epoch 8:2.66 - F1: 0.6396
Time taken for Epoch 9:2.65 - F1: 0.6412
2026-02-14 00:25:37 - INFO - Time taken for Epoch 9:2.65 - F1: 0.6412
Time taken for Epoch 10:2.66 - F1: 0.6461
2026-02-14 00:25:39 - INFO - Time taken for Epoch 10:2.66 - F1: 0.6461
Time taken for Epoch 11:2.66 - F1: 0.6426
2026-02-14 00:25:42 - INFO - Time taken for Epoch 11:2.66 - F1: 0.6426
Time taken for Epoch 12:2.66 - F1: 0.6438
2026-02-14 00:25:45 - INFO - Time taken for Epoch 12:2.66 - F1: 0.6438
Time taken for Epoch 13:2.66 - F1: 0.6420
2026-02-14 00:25:47 - INFO - Time taken for Epoch 13:2.66 - F1: 0.6420
Time taken for Epoch 14:2.66 - F1: 0.6431
2026-02-14 00:25:50 - INFO - Time taken for Epoch 14:2.66 - F1: 0.6431
Time taken for Epoch 15:2.65 - F1: 0.6436
2026-02-14 00:25:53 - INFO - Time taken for Epoch 15:2.65 - F1: 0.6436
Performance not improving for 10 consecutive epochs.
2026-02-14 00:25:53 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6603 - Best Epoch:4
2026-02-14 00:25:53 - INFO - Best F1:0.6603 - Best Epoch:4
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6297, Test ECE: 0.0656
2026-02-14 00:26:00 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6297, Test ECE: 0.0656
All results: {'f1_macro': 0.6296522416845939, 'ece': np.float64(0.06556601431763287)}
2026-02-14 00:26:00 - INFO - All results: {'f1_macro': 0.6296522416845939, 'ece': np.float64(0.06556601431763287)}

Total time taken: 526.58 seconds
2026-02-14 00:26:00 - INFO - 
Total time taken: 526.58 seconds
2026-02-14 00:26:00 - INFO - Trial 3 finished with value: 0.6296522416845939 and parameters: {'learning_rate': 0.0001332957390210613, 'weight_decay': 0.0015966409186445826, 'batch_size': 64, 'co_train_epochs': 7, 'epoch_patience': 6}. Best is trial 2 with value: 0.6486424452307257.
Using devices: cuda, cuda
2026-02-14 00:26:00 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 00:26:00 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 00:26:00 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-14 00:26:00 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.00013972898839000164
Weight Decay: 0.0022979911791563266
Batch Size: 64
No. Epochs: 17
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-14 00:26:00 - INFO - Learning Rate: 0.00013972898839000164
Weight Decay: 0.0022979911791563266
Batch Size: 64
No. Epochs: 17
Epoch Patience: 10
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 00:26:01 - INFO - Generating initial weights
Time taken for Epoch 1:19.19 - F1: 0.0735
2026-02-14 00:26:24 - INFO - Time taken for Epoch 1:19.19 - F1: 0.0735
Time taken for Epoch 2:19.18 - F1: 0.1503
2026-02-14 00:26:43 - INFO - Time taken for Epoch 2:19.18 - F1: 0.1503
Time taken for Epoch 3:19.19 - F1: 0.2238
2026-02-14 00:27:02 - INFO - Time taken for Epoch 3:19.19 - F1: 0.2238
Time taken for Epoch 4:19.18 - F1: 0.3126
2026-02-14 00:27:22 - INFO - Time taken for Epoch 4:19.18 - F1: 0.3126
Time taken for Epoch 5:19.19 - F1: 0.3296
2026-02-14 00:27:41 - INFO - Time taken for Epoch 5:19.19 - F1: 0.3296
Time taken for Epoch 6:19.21 - F1: 0.3497
2026-02-14 00:28:00 - INFO - Time taken for Epoch 6:19.21 - F1: 0.3497
Time taken for Epoch 7:19.22 - F1: 0.3393
2026-02-14 00:28:19 - INFO - Time taken for Epoch 7:19.22 - F1: 0.3393
Time taken for Epoch 8:19.23 - F1: 0.3495
2026-02-14 00:28:38 - INFO - Time taken for Epoch 8:19.23 - F1: 0.3495
Time taken for Epoch 9:19.23 - F1: 0.3557
2026-02-14 00:28:58 - INFO - Time taken for Epoch 9:19.23 - F1: 0.3557
Time taken for Epoch 10:19.24 - F1: 0.3603
2026-02-14 00:29:17 - INFO - Time taken for Epoch 10:19.24 - F1: 0.3603
Time taken for Epoch 11:19.24 - F1: 0.3730
2026-02-14 00:29:36 - INFO - Time taken for Epoch 11:19.24 - F1: 0.3730
Time taken for Epoch 12:19.22 - F1: 0.3638
2026-02-14 00:29:55 - INFO - Time taken for Epoch 12:19.22 - F1: 0.3638
Time taken for Epoch 13:19.23 - F1: 0.3473
2026-02-14 00:30:15 - INFO - Time taken for Epoch 13:19.23 - F1: 0.3473
Time taken for Epoch 14:19.26 - F1: 0.3414
2026-02-14 00:30:34 - INFO - Time taken for Epoch 14:19.26 - F1: 0.3414
Time taken for Epoch 15:19.25 - F1: 0.3454
2026-02-14 00:30:53 - INFO - Time taken for Epoch 15:19.25 - F1: 0.3454
Time taken for Epoch 16:19.25 - F1: 0.3417
2026-02-14 00:31:12 - INFO - Time taken for Epoch 16:19.25 - F1: 0.3417
Time taken for Epoch 17:19.24 - F1: 0.3417
2026-02-14 00:31:32 - INFO - Time taken for Epoch 17:19.24 - F1: 0.3417
Best F1:0.3730 - Best Epoch:11
2026-02-14 00:31:32 - INFO - Best F1:0.3730 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 00:31:33 - INFO - Starting co-training
Time taken for Epoch 1: 46.68s - F1: 0.65649423
2026-02-14 00:32:20 - INFO - Time taken for Epoch 1: 46.68s - F1: 0.65649423
Time taken for Epoch 2: 47.88s - F1: 0.61354019
2026-02-14 00:33:08 - INFO - Time taken for Epoch 2: 47.88s - F1: 0.61354019
Time taken for Epoch 3: 46.78s - F1: 0.63317263
2026-02-14 00:33:55 - INFO - Time taken for Epoch 3: 46.78s - F1: 0.63317263
Time taken for Epoch 4: 46.80s - F1: 0.66492404
2026-02-14 00:34:41 - INFO - Time taken for Epoch 4: 46.80s - F1: 0.66492404
Time taken for Epoch 5: 48.01s - F1: 0.62702670
2026-02-14 00:35:29 - INFO - Time taken for Epoch 5: 48.01s - F1: 0.62702670
Time taken for Epoch 6: 46.71s - F1: 0.64774526
2026-02-14 00:36:16 - INFO - Time taken for Epoch 6: 46.71s - F1: 0.64774526
Time taken for Epoch 7: 46.71s - F1: 0.65886042
2026-02-14 00:37:03 - INFO - Time taken for Epoch 7: 46.71s - F1: 0.65886042
Time taken for Epoch 8: 46.71s - F1: 0.64752890
2026-02-14 00:37:49 - INFO - Time taken for Epoch 8: 46.71s - F1: 0.64752890
Time taken for Epoch 9: 46.71s - F1: 0.65226407
2026-02-14 00:38:36 - INFO - Time taken for Epoch 9: 46.71s - F1: 0.65226407
Time taken for Epoch 10: 46.72s - F1: 0.65030730
2026-02-14 00:39:23 - INFO - Time taken for Epoch 10: 46.72s - F1: 0.65030730
Time taken for Epoch 11: 46.73s - F1: 0.63038221
2026-02-14 00:40:10 - INFO - Time taken for Epoch 11: 46.73s - F1: 0.63038221
Time taken for Epoch 12: 46.74s - F1: 0.62864287
2026-02-14 00:40:56 - INFO - Time taken for Epoch 12: 46.74s - F1: 0.62864287
Time taken for Epoch 13: 46.76s - F1: 0.62936962
2026-02-14 00:41:43 - INFO - Time taken for Epoch 13: 46.76s - F1: 0.62936962
Time taken for Epoch 14: 46.73s - F1: 0.64195122
2026-02-14 00:42:30 - INFO - Time taken for Epoch 14: 46.73s - F1: 0.64195122
Performance not improving for 10 consecutive epochs.
Performance not improving for 10 consecutive epochs.
2026-02-14 00:42:30 - INFO - Performance not improving for 10 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 00:42:33 - INFO - Fine-tuning models
Time taken for Epoch 1:2.66 - F1: 0.6373
2026-02-14 00:42:35 - INFO - Time taken for Epoch 1:2.66 - F1: 0.6373
Time taken for Epoch 2:3.78 - F1: 0.5773
2026-02-14 00:42:39 - INFO - Time taken for Epoch 2:3.78 - F1: 0.5773
Time taken for Epoch 3:2.65 - F1: 0.5946
2026-02-14 00:42:42 - INFO - Time taken for Epoch 3:2.65 - F1: 0.5946
Time taken for Epoch 4:2.65 - F1: 0.6374
2026-02-14 00:42:45 - INFO - Time taken for Epoch 4:2.65 - F1: 0.6374
Time taken for Epoch 5:3.92 - F1: 0.6561
2026-02-14 00:42:48 - INFO - Time taken for Epoch 5:3.92 - F1: 0.6561
Time taken for Epoch 6:3.90 - F1: 0.6660
2026-02-14 00:42:52 - INFO - Time taken for Epoch 6:3.90 - F1: 0.6660
Time taken for Epoch 7:3.90 - F1: 0.6784
2026-02-14 00:42:56 - INFO - Time taken for Epoch 7:3.90 - F1: 0.6784
Time taken for Epoch 8:3.90 - F1: 0.6713
2026-02-14 00:43:00 - INFO - Time taken for Epoch 8:3.90 - F1: 0.6713
Time taken for Epoch 9:2.65 - F1: 0.6758
2026-02-14 00:43:03 - INFO - Time taken for Epoch 9:2.65 - F1: 0.6758
Time taken for Epoch 10:2.65 - F1: 0.6729
2026-02-14 00:43:05 - INFO - Time taken for Epoch 10:2.65 - F1: 0.6729
Time taken for Epoch 11:2.65 - F1: 0.6727
2026-02-14 00:43:08 - INFO - Time taken for Epoch 11:2.65 - F1: 0.6727
Time taken for Epoch 12:2.64 - F1: 0.6659
2026-02-14 00:43:11 - INFO - Time taken for Epoch 12:2.64 - F1: 0.6659
Time taken for Epoch 13:2.65 - F1: 0.6663
2026-02-14 00:43:13 - INFO - Time taken for Epoch 13:2.65 - F1: 0.6663
Time taken for Epoch 14:2.65 - F1: 0.6621
2026-02-14 00:43:16 - INFO - Time taken for Epoch 14:2.65 - F1: 0.6621
Time taken for Epoch 15:2.65 - F1: 0.6647
2026-02-14 00:43:19 - INFO - Time taken for Epoch 15:2.65 - F1: 0.6647
Time taken for Epoch 16:2.65 - F1: 0.6651
2026-02-14 00:43:21 - INFO - Time taken for Epoch 16:2.65 - F1: 0.6651
Time taken for Epoch 17:2.65 - F1: 0.6673
2026-02-14 00:43:24 - INFO - Time taken for Epoch 17:2.65 - F1: 0.6673
Performance not improving for 10 consecutive epochs.
2026-02-14 00:43:24 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6784 - Best Epoch:6
2026-02-14 00:43:24 - INFO - Best F1:0.6784 - Best Epoch:6
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6721, Test ECE: 0.0655
2026-02-14 00:43:31 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6721, Test ECE: 0.0655
All results: {'f1_macro': 0.672104687576518, 'ece': np.float64(0.0654879311996095)}
2026-02-14 00:43:31 - INFO - All results: {'f1_macro': 0.672104687576518, 'ece': np.float64(0.0654879311996095)}

Total time taken: 1051.34 seconds
2026-02-14 00:43:31 - INFO - 
Total time taken: 1051.34 seconds
2026-02-14 00:43:31 - INFO - Trial 4 finished with value: 0.672104687576518 and parameters: {'learning_rate': 0.00013972898839000164, 'weight_decay': 0.0022979911791563266, 'batch_size': 64, 'co_train_epochs': 17, 'epoch_patience': 10}. Best is trial 4 with value: 0.672104687576518.
Using devices: cuda, cuda
2026-02-14 00:43:31 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 00:43:31 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 00:43:31 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-14 00:43:31 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0009453359045148147
Weight Decay: 0.0032289380522815877
Batch Size: 32
No. Epochs: 14
Epoch Patience: 8
 Accumulation Steps: 2
2026-02-14 00:43:32 - INFO - Learning Rate: 0.0009453359045148147
Weight Decay: 0.0032289380522815877
Batch Size: 32
No. Epochs: 14
Epoch Patience: 8
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 00:43:33 - INFO - Generating initial weights
Time taken for Epoch 1:20.20 - F1: 0.0514
2026-02-14 00:43:56 - INFO - Time taken for Epoch 1:20.20 - F1: 0.0514
Time taken for Epoch 2:20.16 - F1: 0.0473
2026-02-14 00:44:16 - INFO - Time taken for Epoch 2:20.16 - F1: 0.0473
Time taken for Epoch 3:20.16 - F1: 0.0189
2026-02-14 00:44:37 - INFO - Time taken for Epoch 3:20.16 - F1: 0.0189
Time taken for Epoch 4:20.20 - F1: 0.0197
2026-02-14 00:44:57 - INFO - Time taken for Epoch 4:20.20 - F1: 0.0197
Time taken for Epoch 5:20.19 - F1: 0.0089
2026-02-14 00:45:17 - INFO - Time taken for Epoch 5:20.19 - F1: 0.0089
Time taken for Epoch 6:20.21 - F1: 0.0207
2026-02-14 00:45:37 - INFO - Time taken for Epoch 6:20.21 - F1: 0.0207
Time taken for Epoch 7:20.25 - F1: 0.0064
2026-02-14 00:45:57 - INFO - Time taken for Epoch 7:20.25 - F1: 0.0064
Time taken for Epoch 8:20.26 - F1: 0.0064
2026-02-14 00:46:18 - INFO - Time taken for Epoch 8:20.26 - F1: 0.0064
Time taken for Epoch 9:20.22 - F1: 0.0081
2026-02-14 00:46:38 - INFO - Time taken for Epoch 9:20.22 - F1: 0.0081
Time taken for Epoch 10:20.35 - F1: 0.0363
2026-02-14 00:46:58 - INFO - Time taken for Epoch 10:20.35 - F1: 0.0363
Time taken for Epoch 11:20.36 - F1: 0.0122
2026-02-14 00:47:19 - INFO - Time taken for Epoch 11:20.36 - F1: 0.0122
Time taken for Epoch 12:20.31 - F1: 0.0593
2026-02-14 00:47:39 - INFO - Time taken for Epoch 12:20.31 - F1: 0.0593
Time taken for Epoch 13:20.29 - F1: 0.0566
2026-02-14 00:47:59 - INFO - Time taken for Epoch 13:20.29 - F1: 0.0566
Time taken for Epoch 14:20.24 - F1: 0.0480
2026-02-14 00:48:20 - INFO - Time taken for Epoch 14:20.24 - F1: 0.0480
Best F1:0.0593 - Best Epoch:12
2026-02-14 00:48:20 - INFO - Best F1:0.0593 - Best Epoch:12
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 00:48:21 - INFO - Starting co-training
Time taken for Epoch 1: 35.69s - F1: 0.04755179
2026-02-14 00:48:57 - INFO - Time taken for Epoch 1: 35.69s - F1: 0.04755179
Time taken for Epoch 2: 36.83s - F1: 0.04755179
2026-02-14 00:49:34 - INFO - Time taken for Epoch 2: 36.83s - F1: 0.04755179
Time taken for Epoch 3: 35.68s - F1: 0.03632720
2026-02-14 00:50:09 - INFO - Time taken for Epoch 3: 35.68s - F1: 0.03632720
Time taken for Epoch 4: 35.75s - F1: 0.03632720
2026-02-14 00:50:45 - INFO - Time taken for Epoch 4: 35.75s - F1: 0.03632720
Time taken for Epoch 5: 35.71s - F1: 0.03632720
2026-02-14 00:51:21 - INFO - Time taken for Epoch 5: 35.71s - F1: 0.03632720
Time taken for Epoch 6: 35.70s - F1: 0.03632720
2026-02-14 00:51:56 - INFO - Time taken for Epoch 6: 35.70s - F1: 0.03632720
Time taken for Epoch 7: 35.69s - F1: 0.03632720
2026-02-14 00:52:32 - INFO - Time taken for Epoch 7: 35.69s - F1: 0.03632720
Time taken for Epoch 8: 35.71s - F1: 0.03632720
2026-02-14 00:53:08 - INFO - Time taken for Epoch 8: 35.71s - F1: 0.03632720
Time taken for Epoch 9: 35.71s - F1: 0.03632720
2026-02-14 00:53:44 - INFO - Time taken for Epoch 9: 35.71s - F1: 0.03632720
Performance not improving for 8 consecutive epochs.
Performance not improving for 8 consecutive epochs.
2026-02-14 00:53:44 - INFO - Performance not improving for 8 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 00:53:46 - INFO - Fine-tuning models
Time taken for Epoch 1:2.80 - F1: 0.0189
2026-02-14 00:53:49 - INFO - Time taken for Epoch 1:2.80 - F1: 0.0189
Time taken for Epoch 2:3.93 - F1: 0.0197
2026-02-14 00:53:53 - INFO - Time taken for Epoch 2:3.93 - F1: 0.0197
Time taken for Epoch 3:4.07 - F1: 0.0197
2026-02-14 00:53:57 - INFO - Time taken for Epoch 3:4.07 - F1: 0.0197
Time taken for Epoch 4:2.79 - F1: 0.0038
2026-02-14 00:54:00 - INFO - Time taken for Epoch 4:2.79 - F1: 0.0038
Time taken for Epoch 5:2.79 - F1: 0.0394
2026-02-14 00:54:03 - INFO - Time taken for Epoch 5:2.79 - F1: 0.0394
Time taken for Epoch 6:4.07 - F1: 0.0394
2026-02-14 00:54:07 - INFO - Time taken for Epoch 6:4.07 - F1: 0.0394
Time taken for Epoch 7:2.79 - F1: 0.0394
2026-02-14 00:54:10 - INFO - Time taken for Epoch 7:2.79 - F1: 0.0394
Time taken for Epoch 8:2.79 - F1: 0.0394
2026-02-14 00:54:12 - INFO - Time taken for Epoch 8:2.79 - F1: 0.0394
Time taken for Epoch 9:2.79 - F1: 0.0476
2026-02-14 00:54:15 - INFO - Time taken for Epoch 9:2.79 - F1: 0.0476
Time taken for Epoch 10:4.08 - F1: 0.0476
2026-02-14 00:54:19 - INFO - Time taken for Epoch 10:4.08 - F1: 0.0476
Time taken for Epoch 11:2.78 - F1: 0.0476
2026-02-14 00:54:22 - INFO - Time taken for Epoch 11:2.78 - F1: 0.0476
Time taken for Epoch 12:2.79 - F1: 0.0197
2026-02-14 00:54:25 - INFO - Time taken for Epoch 12:2.79 - F1: 0.0197
Time taken for Epoch 13:2.79 - F1: 0.0189
2026-02-14 00:54:28 - INFO - Time taken for Epoch 13:2.79 - F1: 0.0189
Time taken for Epoch 14:2.78 - F1: 0.0189
2026-02-14 00:54:30 - INFO - Time taken for Epoch 14:2.78 - F1: 0.0189
Time taken for Epoch 15:2.78 - F1: 0.0189
2026-02-14 00:54:33 - INFO - Time taken for Epoch 15:2.78 - F1: 0.0189
Time taken for Epoch 16:2.79 - F1: 0.0189
2026-02-14 00:54:36 - INFO - Time taken for Epoch 16:2.79 - F1: 0.0189
Time taken for Epoch 17:2.79 - F1: 0.0363
2026-02-14 00:54:39 - INFO - Time taken for Epoch 17:2.79 - F1: 0.0363
Time taken for Epoch 18:2.79 - F1: 0.0363
2026-02-14 00:54:42 - INFO - Time taken for Epoch 18:2.79 - F1: 0.0363
Time taken for Epoch 19:2.79 - F1: 0.0363
2026-02-14 00:54:44 - INFO - Time taken for Epoch 19:2.79 - F1: 0.0363
Performance not improving for 10 consecutive epochs.
2026-02-14 00:54:44 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:8
2026-02-14 00:54:44 - INFO - Best F1:0.0476 - Best Epoch:8
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0012
2026-02-14 00:54:52 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0012
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.0012338973695460997)}
2026-02-14 00:54:52 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.0012338973695460997)}

Total time taken: 680.61 seconds
2026-02-14 00:54:52 - INFO - 
Total time taken: 680.61 seconds
2026-02-14 00:54:52 - INFO - Trial 5 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0009453359045148147, 'weight_decay': 0.0032289380522815877, 'batch_size': 32, 'co_train_epochs': 14, 'epoch_patience': 8}. Best is trial 4 with value: 0.672104687576518.
Using devices: cuda, cuda
2026-02-14 00:54:52 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 00:54:52 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 00:54:52 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-14 00:54:52 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0006594409642353401
Weight Decay: 0.0003637156092654307
Batch Size: 8
No. Epochs: 11
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-14 00:54:52 - INFO - Learning Rate: 0.0006594409642353401
Weight Decay: 0.0003637156092654307
Batch Size: 8
No. Epochs: 11
Epoch Patience: 7
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 00:54:53 - INFO - Generating initial weights
Time taken for Epoch 1:22.41 - F1: 0.0189
2026-02-14 00:55:19 - INFO - Time taken for Epoch 1:22.41 - F1: 0.0189
Time taken for Epoch 2:22.31 - F1: 0.0531
2026-02-14 00:55:42 - INFO - Time taken for Epoch 2:22.31 - F1: 0.0531
Time taken for Epoch 3:22.39 - F1: 0.0412
2026-02-14 00:56:04 - INFO - Time taken for Epoch 3:22.39 - F1: 0.0412
Time taken for Epoch 4:22.40 - F1: 0.0089
2026-02-14 00:56:26 - INFO - Time taken for Epoch 4:22.40 - F1: 0.0089
Time taken for Epoch 5:22.42 - F1: 0.0189
2026-02-14 00:56:49 - INFO - Time taken for Epoch 5:22.42 - F1: 0.0189
Time taken for Epoch 6:22.46 - F1: 0.0189
2026-02-14 00:57:11 - INFO - Time taken for Epoch 6:22.46 - F1: 0.0189
Time taken for Epoch 7:22.48 - F1: 0.0206
2026-02-14 00:57:34 - INFO - Time taken for Epoch 7:22.48 - F1: 0.0206
Time taken for Epoch 8:22.53 - F1: 0.0102
2026-02-14 00:57:56 - INFO - Time taken for Epoch 8:22.53 - F1: 0.0102
Time taken for Epoch 9:22.54 - F1: 0.0223
2026-02-14 00:58:19 - INFO - Time taken for Epoch 9:22.54 - F1: 0.0223
Time taken for Epoch 10:22.53 - F1: 0.0570
2026-02-14 00:58:41 - INFO - Time taken for Epoch 10:22.53 - F1: 0.0570
Time taken for Epoch 11:22.58 - F1: 0.0671
2026-02-14 00:59:04 - INFO - Time taken for Epoch 11:22.58 - F1: 0.0671
Best F1:0.0671 - Best Epoch:11
2026-02-14 00:59:04 - INFO - Best F1:0.0671 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 00:59:05 - INFO - Starting co-training
Time taken for Epoch 1: 27.71s - F1: 0.04755179
2026-02-14 00:59:33 - INFO - Time taken for Epoch 1: 27.71s - F1: 0.04755179
Time taken for Epoch 2: 28.80s - F1: 0.04755179
2026-02-14 01:00:02 - INFO - Time taken for Epoch 2: 28.80s - F1: 0.04755179
Time taken for Epoch 3: 27.77s - F1: 0.04755179
2026-02-14 01:00:30 - INFO - Time taken for Epoch 3: 27.77s - F1: 0.04755179
Time taken for Epoch 4: 27.92s - F1: 0.04755179
2026-02-14 01:00:58 - INFO - Time taken for Epoch 4: 27.92s - F1: 0.04755179
Time taken for Epoch 5: 27.81s - F1: 0.04755179
2026-02-14 01:01:25 - INFO - Time taken for Epoch 5: 27.81s - F1: 0.04755179
Time taken for Epoch 6: 27.90s - F1: 0.04755179
2026-02-14 01:01:53 - INFO - Time taken for Epoch 6: 27.90s - F1: 0.04755179
Time taken for Epoch 7: 27.93s - F1: 0.04755179
2026-02-14 01:02:21 - INFO - Time taken for Epoch 7: 27.93s - F1: 0.04755179
Time taken for Epoch 8: 27.91s - F1: 0.04755179
2026-02-14 01:02:49 - INFO - Time taken for Epoch 8: 27.91s - F1: 0.04755179
Performance not improving for 7 consecutive epochs.
Performance not improving for 7 consecutive epochs.
2026-02-14 01:02:49 - INFO - Performance not improving for 7 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 01:02:52 - INFO - Fine-tuning models
Time taken for Epoch 1:3.19 - F1: 0.0189
2026-02-14 01:02:55 - INFO - Time taken for Epoch 1:3.19 - F1: 0.0189
Time taken for Epoch 2:4.26 - F1: 0.0081
2026-02-14 01:02:59 - INFO - Time taken for Epoch 2:4.26 - F1: 0.0081
Time taken for Epoch 3:3.19 - F1: 0.0189
2026-02-14 01:03:02 - INFO - Time taken for Epoch 3:3.19 - F1: 0.0189
Time taken for Epoch 4:3.10 - F1: 0.0189
2026-02-14 01:03:06 - INFO - Time taken for Epoch 4:3.10 - F1: 0.0189
Time taken for Epoch 5:3.09 - F1: 0.0189
2026-02-14 01:03:09 - INFO - Time taken for Epoch 5:3.09 - F1: 0.0189
Time taken for Epoch 6:3.08 - F1: 0.0189
2026-02-14 01:03:12 - INFO - Time taken for Epoch 6:3.08 - F1: 0.0189
Time taken for Epoch 7:3.08 - F1: 0.0189
2026-02-14 01:03:15 - INFO - Time taken for Epoch 7:3.08 - F1: 0.0189
Time taken for Epoch 8:3.08 - F1: 0.0189
2026-02-14 01:03:18 - INFO - Time taken for Epoch 8:3.08 - F1: 0.0189
Time taken for Epoch 9:3.10 - F1: 0.0189
2026-02-14 01:03:21 - INFO - Time taken for Epoch 9:3.10 - F1: 0.0189
Time taken for Epoch 10:3.08 - F1: 0.0189
2026-02-14 01:03:24 - INFO - Time taken for Epoch 10:3.08 - F1: 0.0189
Time taken for Epoch 11:3.08 - F1: 0.0189
2026-02-14 01:03:27 - INFO - Time taken for Epoch 11:3.08 - F1: 0.0189
Performance not improving for 10 consecutive epochs.
2026-02-14 01:03:27 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0189 - Best Epoch:0
2026-02-14 01:03:27 - INFO - Best F1:0.0189 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0188, Test ECE: 0.2311
2026-02-14 01:03:36 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0188, Test ECE: 0.2311
All results: {'f1_macro': 0.018765432098765432, 'ece': np.float64(0.23106401586251513)}
2026-02-14 01:03:36 - INFO - All results: {'f1_macro': 0.018765432098765432, 'ece': np.float64(0.23106401586251513)}

Total time taken: 524.31 seconds
2026-02-14 01:03:36 - INFO - 
Total time taken: 524.31 seconds
2026-02-14 01:03:36 - INFO - Trial 6 finished with value: 0.018765432098765432 and parameters: {'learning_rate': 0.0006594409642353401, 'weight_decay': 0.0003637156092654307, 'batch_size': 8, 'co_train_epochs': 11, 'epoch_patience': 7}. Best is trial 4 with value: 0.672104687576518.
Using devices: cuda, cuda
2026-02-14 01:03:36 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 01:03:36 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 01:03:36 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-14 01:03:36 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 8.694122431788895e-05
Weight Decay: 0.0005897277887122799
Batch Size: 32
No. Epochs: 11
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-14 01:03:37 - INFO - Learning Rate: 8.694122431788895e-05
Weight Decay: 0.0005897277887122799
Batch Size: 32
No. Epochs: 11
Epoch Patience: 9
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 01:03:38 - INFO - Generating initial weights
Time taken for Epoch 1:20.24 - F1: 0.0915
2026-02-14 01:04:02 - INFO - Time taken for Epoch 1:20.24 - F1: 0.0915
Time taken for Epoch 2:20.20 - F1: 0.1113
2026-02-14 01:04:22 - INFO - Time taken for Epoch 2:20.20 - F1: 0.1113
Time taken for Epoch 3:20.19 - F1: 0.1533
2026-02-14 01:04:42 - INFO - Time taken for Epoch 3:20.19 - F1: 0.1533
Time taken for Epoch 4:20.21 - F1: 0.2211
2026-02-14 01:05:02 - INFO - Time taken for Epoch 4:20.21 - F1: 0.2211
Time taken for Epoch 5:20.25 - F1: 0.2845
2026-02-14 01:05:23 - INFO - Time taken for Epoch 5:20.25 - F1: 0.2845
Time taken for Epoch 6:20.25 - F1: 0.2977
2026-02-14 01:05:43 - INFO - Time taken for Epoch 6:20.25 - F1: 0.2977
Time taken for Epoch 7:20.24 - F1: 0.3218
2026-02-14 01:06:03 - INFO - Time taken for Epoch 7:20.24 - F1: 0.3218
Time taken for Epoch 8:20.28 - F1: 0.3312
2026-02-14 01:06:23 - INFO - Time taken for Epoch 8:20.28 - F1: 0.3312
Time taken for Epoch 9:20.24 - F1: 0.3379
2026-02-14 01:06:44 - INFO - Time taken for Epoch 9:20.24 - F1: 0.3379
Time taken for Epoch 10:20.23 - F1: 0.3335
2026-02-14 01:07:04 - INFO - Time taken for Epoch 10:20.23 - F1: 0.3335
Time taken for Epoch 11:20.21 - F1: 0.3336
2026-02-14 01:07:24 - INFO - Time taken for Epoch 11:20.21 - F1: 0.3336
Best F1:0.3379 - Best Epoch:9
2026-02-14 01:07:24 - INFO - Best F1:0.3379 - Best Epoch:9
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 01:07:26 - INFO - Starting co-training
Time taken for Epoch 1: 35.58s - F1: 0.59269080
2026-02-14 01:08:02 - INFO - Time taken for Epoch 1: 35.58s - F1: 0.59269080
Time taken for Epoch 2: 36.68s - F1: 0.59492435
2026-02-14 01:08:38 - INFO - Time taken for Epoch 2: 36.68s - F1: 0.59492435
Time taken for Epoch 3: 36.68s - F1: 0.65231649
2026-02-14 01:09:15 - INFO - Time taken for Epoch 3: 36.68s - F1: 0.65231649
Time taken for Epoch 4: 36.66s - F1: 0.64382601
2026-02-14 01:09:52 - INFO - Time taken for Epoch 4: 36.66s - F1: 0.64382601
Time taken for Epoch 5: 35.59s - F1: 0.63563785
2026-02-14 01:10:27 - INFO - Time taken for Epoch 5: 35.59s - F1: 0.63563785
Time taken for Epoch 6: 35.58s - F1: 0.64982601
2026-02-14 01:11:03 - INFO - Time taken for Epoch 6: 35.58s - F1: 0.64982601
Time taken for Epoch 7: 35.60s - F1: 0.65000681
2026-02-14 01:11:38 - INFO - Time taken for Epoch 7: 35.60s - F1: 0.65000681
Time taken for Epoch 8: 35.62s - F1: 0.60702653
2026-02-14 01:12:14 - INFO - Time taken for Epoch 8: 35.62s - F1: 0.60702653
Time taken for Epoch 9: 35.60s - F1: 0.63608022
2026-02-14 01:12:50 - INFO - Time taken for Epoch 9: 35.60s - F1: 0.63608022
Time taken for Epoch 10: 35.60s - F1: 0.64262602
2026-02-14 01:13:25 - INFO - Time taken for Epoch 10: 35.60s - F1: 0.64262602
Time taken for Epoch 11: 35.59s - F1: 0.61644639
2026-02-14 01:14:01 - INFO - Time taken for Epoch 11: 35.59s - F1: 0.61644639
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 01:14:04 - INFO - Fine-tuning models
Time taken for Epoch 1:2.80 - F1: 0.6388
2026-02-14 01:14:07 - INFO - Time taken for Epoch 1:2.80 - F1: 0.6388
Time taken for Epoch 2:3.85 - F1: 0.6226
2026-02-14 01:14:10 - INFO - Time taken for Epoch 2:3.85 - F1: 0.6226
Time taken for Epoch 3:2.80 - F1: 0.6184
2026-02-14 01:14:13 - INFO - Time taken for Epoch 3:2.80 - F1: 0.6184
Time taken for Epoch 4:2.79 - F1: 0.6380
2026-02-14 01:14:16 - INFO - Time taken for Epoch 4:2.79 - F1: 0.6380
Time taken for Epoch 5:2.79 - F1: 0.6554
2026-02-14 01:14:19 - INFO - Time taken for Epoch 5:2.79 - F1: 0.6554
Time taken for Epoch 6:3.93 - F1: 0.6602
2026-02-14 01:14:23 - INFO - Time taken for Epoch 6:3.93 - F1: 0.6602
Time taken for Epoch 7:3.95 - F1: 0.6698
2026-02-14 01:14:27 - INFO - Time taken for Epoch 7:3.95 - F1: 0.6698
Time taken for Epoch 8:3.95 - F1: 0.6716
2026-02-14 01:14:31 - INFO - Time taken for Epoch 8:3.95 - F1: 0.6716
Time taken for Epoch 9:3.94 - F1: 0.6660
2026-02-14 01:14:35 - INFO - Time taken for Epoch 9:3.94 - F1: 0.6660
Time taken for Epoch 10:2.78 - F1: 0.6628
2026-02-14 01:14:37 - INFO - Time taken for Epoch 10:2.78 - F1: 0.6628
Time taken for Epoch 11:2.79 - F1: 0.6617
2026-02-14 01:14:40 - INFO - Time taken for Epoch 11:2.79 - F1: 0.6617
Time taken for Epoch 12:2.78 - F1: 0.6615
2026-02-14 01:14:43 - INFO - Time taken for Epoch 12:2.78 - F1: 0.6615
Time taken for Epoch 13:2.78 - F1: 0.6601
2026-02-14 01:14:46 - INFO - Time taken for Epoch 13:2.78 - F1: 0.6601
Time taken for Epoch 14:2.79 - F1: 0.6568
2026-02-14 01:14:48 - INFO - Time taken for Epoch 14:2.79 - F1: 0.6568
Time taken for Epoch 15:2.79 - F1: 0.6556
2026-02-14 01:14:51 - INFO - Time taken for Epoch 15:2.79 - F1: 0.6556
Time taken for Epoch 16:2.79 - F1: 0.6578
2026-02-14 01:14:54 - INFO - Time taken for Epoch 16:2.79 - F1: 0.6578
Time taken for Epoch 17:2.79 - F1: 0.6570
2026-02-14 01:14:57 - INFO - Time taken for Epoch 17:2.79 - F1: 0.6570
Time taken for Epoch 18:2.79 - F1: 0.6554
2026-02-14 01:15:00 - INFO - Time taken for Epoch 18:2.79 - F1: 0.6554
Performance not improving for 10 consecutive epochs.
2026-02-14 01:15:00 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6716 - Best Epoch:7
2026-02-14 01:15:00 - INFO - Best F1:0.6716 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6708, Test ECE: 0.0408
2026-02-14 01:15:08 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6708, Test ECE: 0.0408
All results: {'f1_macro': 0.6708047780548766, 'ece': np.float64(0.04078072061485788)}
2026-02-14 01:15:08 - INFO - All results: {'f1_macro': 0.6708047780548766, 'ece': np.float64(0.04078072061485788)}

Total time taken: 691.30 seconds
2026-02-14 01:15:08 - INFO - 
Total time taken: 691.30 seconds
2026-02-14 01:15:08 - INFO - Trial 7 finished with value: 0.6708047780548766 and parameters: {'learning_rate': 8.694122431788895e-05, 'weight_decay': 0.0005897277887122799, 'batch_size': 32, 'co_train_epochs': 11, 'epoch_patience': 9}. Best is trial 4 with value: 0.672104687576518.
Using devices: cuda, cuda
2026-02-14 01:15:08 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 01:15:08 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 01:15:08 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-14 01:15:08 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0002587653001771671
Weight Decay: 0.002732093045910751
Batch Size: 64
No. Epochs: 18
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-14 01:15:08 - INFO - Learning Rate: 0.0002587653001771671
Weight Decay: 0.002732093045910751
Batch Size: 64
No. Epochs: 18
Epoch Patience: 10
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 01:15:09 - INFO - Generating initial weights
Time taken for Epoch 1:19.12 - F1: 0.0586
2026-02-14 01:15:32 - INFO - Time taken for Epoch 1:19.12 - F1: 0.0586
Time taken for Epoch 2:19.08 - F1: 0.2339
2026-02-14 01:15:51 - INFO - Time taken for Epoch 2:19.08 - F1: 0.2339
Time taken for Epoch 3:19.08 - F1: 0.3143
2026-02-14 01:16:10 - INFO - Time taken for Epoch 3:19.08 - F1: 0.3143
Time taken for Epoch 4:19.09 - F1: 0.3054
2026-02-14 01:16:29 - INFO - Time taken for Epoch 4:19.09 - F1: 0.3054
Time taken for Epoch 5:19.12 - F1: 0.3597
2026-02-14 01:16:48 - INFO - Time taken for Epoch 5:19.12 - F1: 0.3597
Time taken for Epoch 6:19.18 - F1: 0.3634
2026-02-14 01:17:07 - INFO - Time taken for Epoch 6:19.18 - F1: 0.3634
Time taken for Epoch 7:19.11 - F1: 0.3570
2026-02-14 01:17:26 - INFO - Time taken for Epoch 7:19.11 - F1: 0.3570
Time taken for Epoch 8:19.11 - F1: 0.3639
2026-02-14 01:17:46 - INFO - Time taken for Epoch 8:19.11 - F1: 0.3639
Time taken for Epoch 9:19.12 - F1: 0.3796
2026-02-14 01:18:05 - INFO - Time taken for Epoch 9:19.12 - F1: 0.3796
Time taken for Epoch 10:19.13 - F1: 0.3815
2026-02-14 01:18:24 - INFO - Time taken for Epoch 10:19.13 - F1: 0.3815
Time taken for Epoch 11:19.12 - F1: 0.3939
2026-02-14 01:18:43 - INFO - Time taken for Epoch 11:19.12 - F1: 0.3939
Time taken for Epoch 12:19.15 - F1: 0.3735
2026-02-14 01:19:02 - INFO - Time taken for Epoch 12:19.15 - F1: 0.3735
Time taken for Epoch 13:19.14 - F1: 0.3569
2026-02-14 01:19:21 - INFO - Time taken for Epoch 13:19.14 - F1: 0.3569
Time taken for Epoch 14:19.12 - F1: 0.3620
2026-02-14 01:19:40 - INFO - Time taken for Epoch 14:19.12 - F1: 0.3620
Time taken for Epoch 15:19.12 - F1: 0.3734
2026-02-14 01:19:59 - INFO - Time taken for Epoch 15:19.12 - F1: 0.3734
Time taken for Epoch 16:19.14 - F1: 0.3741
2026-02-14 01:20:19 - INFO - Time taken for Epoch 16:19.14 - F1: 0.3741
Time taken for Epoch 17:19.13 - F1: 0.3742
2026-02-14 01:20:38 - INFO - Time taken for Epoch 17:19.13 - F1: 0.3742
Time taken for Epoch 18:19.13 - F1: 0.3724
2026-02-14 01:20:57 - INFO - Time taken for Epoch 18:19.13 - F1: 0.3724
Best F1:0.3939 - Best Epoch:11
2026-02-14 01:20:57 - INFO - Best F1:0.3939 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 01:20:58 - INFO - Starting co-training
Time taken for Epoch 1: 46.43s - F1: 0.22186132
2026-02-14 01:21:45 - INFO - Time taken for Epoch 1: 46.43s - F1: 0.22186132
Time taken for Epoch 2: 47.57s - F1: 0.44707981
2026-02-14 01:22:33 - INFO - Time taken for Epoch 2: 47.57s - F1: 0.44707981
Time taken for Epoch 3: 47.69s - F1: 0.49524503
2026-02-14 01:23:20 - INFO - Time taken for Epoch 3: 47.69s - F1: 0.49524503
Time taken for Epoch 4: 47.63s - F1: 0.43389786
2026-02-14 01:24:08 - INFO - Time taken for Epoch 4: 47.63s - F1: 0.43389786
Time taken for Epoch 5: 46.54s - F1: 0.52886504
2026-02-14 01:24:54 - INFO - Time taken for Epoch 5: 46.54s - F1: 0.52886504
Time taken for Epoch 6: 47.68s - F1: 0.52864983
2026-02-14 01:25:42 - INFO - Time taken for Epoch 6: 47.68s - F1: 0.52864983
Time taken for Epoch 7: 46.54s - F1: 0.05688427
2026-02-14 01:26:29 - INFO - Time taken for Epoch 7: 46.54s - F1: 0.05688427
Time taken for Epoch 8: 46.54s - F1: 0.16096732
2026-02-14 01:27:15 - INFO - Time taken for Epoch 8: 46.54s - F1: 0.16096732
Time taken for Epoch 9: 46.56s - F1: 0.04755179
2026-02-14 01:28:02 - INFO - Time taken for Epoch 9: 46.56s - F1: 0.04755179
Time taken for Epoch 10: 46.56s - F1: 0.04755179
2026-02-14 01:28:48 - INFO - Time taken for Epoch 10: 46.56s - F1: 0.04755179
Time taken for Epoch 11: 46.55s - F1: 0.04755179
2026-02-14 01:29:35 - INFO - Time taken for Epoch 11: 46.55s - F1: 0.04755179
Time taken for Epoch 12: 46.55s - F1: 0.04755179
2026-02-14 01:30:21 - INFO - Time taken for Epoch 12: 46.55s - F1: 0.04755179
Time taken for Epoch 13: 46.56s - F1: 0.04755179
2026-02-14 01:31:08 - INFO - Time taken for Epoch 13: 46.56s - F1: 0.04755179
Time taken for Epoch 14: 46.55s - F1: 0.04755179
2026-02-14 01:31:55 - INFO - Time taken for Epoch 14: 46.55s - F1: 0.04755179
Time taken for Epoch 15: 46.58s - F1: 0.04755179
2026-02-14 01:32:41 - INFO - Time taken for Epoch 15: 46.58s - F1: 0.04755179
Performance not improving for 10 consecutive epochs.
Performance not improving for 10 consecutive epochs.
2026-02-14 01:32:41 - INFO - Performance not improving for 10 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 01:32:44 - INFO - Fine-tuning models
Time taken for Epoch 1:2.66 - F1: 0.5835
2026-02-14 01:32:47 - INFO - Time taken for Epoch 1:2.66 - F1: 0.5835
Time taken for Epoch 2:3.72 - F1: 0.6217
2026-02-14 01:32:51 - INFO - Time taken for Epoch 2:3.72 - F1: 0.6217
Time taken for Epoch 3:3.82 - F1: 0.5817
2026-02-14 01:32:54 - INFO - Time taken for Epoch 3:3.82 - F1: 0.5817
Time taken for Epoch 4:2.65 - F1: 0.5628
2026-02-14 01:32:57 - INFO - Time taken for Epoch 4:2.65 - F1: 0.5628
Time taken for Epoch 5:2.65 - F1: 0.5223
2026-02-14 01:33:00 - INFO - Time taken for Epoch 5:2.65 - F1: 0.5223
Time taken for Epoch 6:2.66 - F1: 0.5311
2026-02-14 01:33:02 - INFO - Time taken for Epoch 6:2.66 - F1: 0.5311
Time taken for Epoch 7:2.65 - F1: 0.5420
2026-02-14 01:33:05 - INFO - Time taken for Epoch 7:2.65 - F1: 0.5420
Time taken for Epoch 8:2.65 - F1: 0.5677
2026-02-14 01:33:08 - INFO - Time taken for Epoch 8:2.65 - F1: 0.5677
Time taken for Epoch 9:2.65 - F1: 0.5439
2026-02-14 01:33:10 - INFO - Time taken for Epoch 9:2.65 - F1: 0.5439
Time taken for Epoch 10:2.65 - F1: 0.5164
2026-02-14 01:33:13 - INFO - Time taken for Epoch 10:2.65 - F1: 0.5164
Time taken for Epoch 11:2.65 - F1: 0.4984
2026-02-14 01:33:16 - INFO - Time taken for Epoch 11:2.65 - F1: 0.4984
Time taken for Epoch 12:2.65 - F1: 0.4931
2026-02-14 01:33:18 - INFO - Time taken for Epoch 12:2.65 - F1: 0.4931
Performance not improving for 10 consecutive epochs.
2026-02-14 01:33:18 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6217 - Best Epoch:1
2026-02-14 01:33:18 - INFO - Best F1:0.6217 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6039, Test ECE: 0.0606
2026-02-14 01:33:26 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6039, Test ECE: 0.0606
All results: {'f1_macro': 0.6038996762004187, 'ece': np.float64(0.060585871504183125)}
2026-02-14 01:33:26 - INFO - All results: {'f1_macro': 0.6038996762004187, 'ece': np.float64(0.060585871504183125)}

Total time taken: 1098.70 seconds
2026-02-14 01:33:26 - INFO - 
Total time taken: 1098.70 seconds
2026-02-14 01:33:26 - INFO - Trial 8 finished with value: 0.6038996762004187 and parameters: {'learning_rate': 0.0002587653001771671, 'weight_decay': 0.002732093045910751, 'batch_size': 64, 'co_train_epochs': 18, 'epoch_patience': 10}. Best is trial 4 with value: 0.672104687576518.
Using devices: cuda, cuda
2026-02-14 01:33:26 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 01:33:26 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 01:33:26 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-14 01:33:26 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.00016926627931662246
Weight Decay: 0.00038435487391448375
Batch Size: 64
No. Epochs: 17
Epoch Patience: 8
 Accumulation Steps: 1
2026-02-14 01:33:27 - INFO - Learning Rate: 0.00016926627931662246
Weight Decay: 0.00038435487391448375
Batch Size: 64
No. Epochs: 17
Epoch Patience: 8
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 01:33:28 - INFO - Generating initial weights
Time taken for Epoch 1:19.16 - F1: 0.0651
2026-02-14 01:33:51 - INFO - Time taken for Epoch 1:19.16 - F1: 0.0651
Time taken for Epoch 2:19.12 - F1: 0.1639
2026-02-14 01:34:10 - INFO - Time taken for Epoch 2:19.12 - F1: 0.1639
Time taken for Epoch 3:19.11 - F1: 0.2742
2026-02-14 01:34:29 - INFO - Time taken for Epoch 3:19.11 - F1: 0.2742
Time taken for Epoch 4:19.11 - F1: 0.2819
2026-02-14 01:34:48 - INFO - Time taken for Epoch 4:19.11 - F1: 0.2819
Time taken for Epoch 5:19.14 - F1: 0.3440
2026-02-14 01:35:07 - INFO - Time taken for Epoch 5:19.14 - F1: 0.3440
Time taken for Epoch 6:19.14 - F1: 0.3415
2026-02-14 01:35:26 - INFO - Time taken for Epoch 6:19.14 - F1: 0.3415
Time taken for Epoch 7:19.14 - F1: 0.3418
2026-02-14 01:35:45 - INFO - Time taken for Epoch 7:19.14 - F1: 0.3418
Time taken for Epoch 8:19.14 - F1: 0.3465
2026-02-14 01:36:04 - INFO - Time taken for Epoch 8:19.14 - F1: 0.3465
Time taken for Epoch 9:19.15 - F1: 0.3539
2026-02-14 01:36:24 - INFO - Time taken for Epoch 9:19.15 - F1: 0.3539
Time taken for Epoch 10:19.15 - F1: 0.3596
2026-02-14 01:36:43 - INFO - Time taken for Epoch 10:19.15 - F1: 0.3596
Time taken for Epoch 11:19.15 - F1: 0.3553
2026-02-14 01:37:02 - INFO - Time taken for Epoch 11:19.15 - F1: 0.3553
Time taken for Epoch 12:19.15 - F1: 0.3559
2026-02-14 01:37:21 - INFO - Time taken for Epoch 12:19.15 - F1: 0.3559
Time taken for Epoch 13:19.17 - F1: 0.3506
2026-02-14 01:37:40 - INFO - Time taken for Epoch 13:19.17 - F1: 0.3506
Time taken for Epoch 14:19.15 - F1: 0.3425
2026-02-14 01:37:59 - INFO - Time taken for Epoch 14:19.15 - F1: 0.3425
Time taken for Epoch 15:19.16 - F1: 0.3457
2026-02-14 01:38:19 - INFO - Time taken for Epoch 15:19.16 - F1: 0.3457
Time taken for Epoch 16:19.16 - F1: 0.3497
2026-02-14 01:38:38 - INFO - Time taken for Epoch 16:19.16 - F1: 0.3497
Time taken for Epoch 17:19.16 - F1: 0.3512
2026-02-14 01:38:57 - INFO - Time taken for Epoch 17:19.16 - F1: 0.3512
Best F1:0.3596 - Best Epoch:10
2026-02-14 01:38:57 - INFO - Best F1:0.3596 - Best Epoch:10
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 01:38:58 - INFO - Starting co-training
Time taken for Epoch 1: 46.40s - F1: 0.55187021
2026-02-14 01:39:45 - INFO - Time taken for Epoch 1: 46.40s - F1: 0.55187021
Time taken for Epoch 2: 47.57s - F1: 0.57198014
2026-02-14 01:40:33 - INFO - Time taken for Epoch 2: 47.57s - F1: 0.57198014
Time taken for Epoch 3: 47.67s - F1: 0.59420065
2026-02-14 01:41:20 - INFO - Time taken for Epoch 3: 47.67s - F1: 0.59420065
Time taken for Epoch 4: 48.08s - F1: 0.59323640
2026-02-14 01:42:09 - INFO - Time taken for Epoch 4: 48.08s - F1: 0.59323640
Time taken for Epoch 5: 46.55s - F1: 0.59922246
2026-02-14 01:42:55 - INFO - Time taken for Epoch 5: 46.55s - F1: 0.59922246
Time taken for Epoch 6: 47.71s - F1: 0.62719393
2026-02-14 01:43:43 - INFO - Time taken for Epoch 6: 47.71s - F1: 0.62719393
Time taken for Epoch 7: 47.70s - F1: 0.63542700
2026-02-14 01:44:31 - INFO - Time taken for Epoch 7: 47.70s - F1: 0.63542700
Time taken for Epoch 8: 47.79s - F1: 0.62004335
2026-02-14 01:45:18 - INFO - Time taken for Epoch 8: 47.79s - F1: 0.62004335
Time taken for Epoch 9: 46.56s - F1: 0.61129337
2026-02-14 01:46:05 - INFO - Time taken for Epoch 9: 46.56s - F1: 0.61129337
Time taken for Epoch 10: 46.59s - F1: 0.58774661
2026-02-14 01:46:51 - INFO - Time taken for Epoch 10: 46.59s - F1: 0.58774661
Time taken for Epoch 11: 46.60s - F1: 0.61490798
2026-02-14 01:47:38 - INFO - Time taken for Epoch 11: 46.60s - F1: 0.61490798
Time taken for Epoch 12: 46.59s - F1: 0.60604341
2026-02-14 01:48:25 - INFO - Time taken for Epoch 12: 46.59s - F1: 0.60604341
Time taken for Epoch 13: 46.59s - F1: 0.62084986
2026-02-14 01:49:11 - INFO - Time taken for Epoch 13: 46.59s - F1: 0.62084986
Time taken for Epoch 14: 46.56s - F1: 0.57976349
2026-02-14 01:49:58 - INFO - Time taken for Epoch 14: 46.56s - F1: 0.57976349
Time taken for Epoch 15: 46.58s - F1: 0.60489826
2026-02-14 01:50:44 - INFO - Time taken for Epoch 15: 46.58s - F1: 0.60489826
Performance not improving for 8 consecutive epochs.
Performance not improving for 8 consecutive epochs.
2026-02-14 01:50:44 - INFO - Performance not improving for 8 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-14 01:50:48 - INFO - Fine-tuning models
Time taken for Epoch 1:2.65 - F1: 0.6254
2026-02-14 01:50:50 - INFO - Time taken for Epoch 1:2.65 - F1: 0.6254
Time taken for Epoch 2:3.70 - F1: 0.6089
2026-02-14 01:50:54 - INFO - Time taken for Epoch 2:3.70 - F1: 0.6089
Time taken for Epoch 3:2.64 - F1: 0.6060
2026-02-14 01:50:57 - INFO - Time taken for Epoch 3:2.64 - F1: 0.6060
Time taken for Epoch 4:2.65 - F1: 0.5940
2026-02-14 01:50:59 - INFO - Time taken for Epoch 4:2.65 - F1: 0.5940
Time taken for Epoch 5:2.65 - F1: 0.5803
2026-02-14 01:51:02 - INFO - Time taken for Epoch 5:2.65 - F1: 0.5803
Time taken for Epoch 6:2.65 - F1: 0.5680
2026-02-14 01:51:05 - INFO - Time taken for Epoch 6:2.65 - F1: 0.5680
Time taken for Epoch 7:2.65 - F1: 0.5677
2026-02-14 01:51:07 - INFO - Time taken for Epoch 7:2.65 - F1: 0.5677
Time taken for Epoch 8:2.65 - F1: 0.5730
2026-02-14 01:51:10 - INFO - Time taken for Epoch 8:2.65 - F1: 0.5730
Time taken for Epoch 9:2.65 - F1: 0.5692
2026-02-14 01:51:13 - INFO - Time taken for Epoch 9:2.65 - F1: 0.5692
Time taken for Epoch 10:2.64 - F1: 0.5675
2026-02-14 01:51:15 - INFO - Time taken for Epoch 10:2.64 - F1: 0.5675
Time taken for Epoch 11:2.65 - F1: 0.5608
2026-02-14 01:51:18 - INFO - Time taken for Epoch 11:2.65 - F1: 0.5608
Performance not improving for 10 consecutive epochs.
2026-02-14 01:51:18 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6254 - Best Epoch:0
2026-02-14 01:51:18 - INFO - Best F1:0.6254 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set3/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set3_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6136, Test ECE: 0.1276
2026-02-14 01:51:26 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6136, Test ECE: 0.1276
All results: {'f1_macro': 0.6135756468720622, 'ece': np.float64(0.1275906404079908)}
2026-02-14 01:51:26 - INFO - All results: {'f1_macro': 0.6135756468720622, 'ece': np.float64(0.1275906404079908)}

Total time taken: 1079.49 seconds
2026-02-14 01:51:26 - INFO - 
Total time taken: 1079.49 seconds
2026-02-14 01:51:26 - INFO - Trial 9 finished with value: 0.6135756468720622 and parameters: {'learning_rate': 0.00016926627931662246, 'weight_decay': 0.00038435487391448375, 'batch_size': 64, 'co_train_epochs': 17, 'epoch_patience': 8}. Best is trial 4 with value: 0.672104687576518.

[BEST TRIAL RESULTS]
2026-02-14 01:51:26 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6721
2026-02-14 01:51:26 - INFO - F1 Score: 0.6721
Params: {'learning_rate': 0.00013972898839000164, 'weight_decay': 0.0022979911791563266, 'batch_size': 64, 'co_train_epochs': 17, 'epoch_patience': 10}
2026-02-14 01:51:26 - INFO - Params: {'learning_rate': 0.00013972898839000164, 'weight_decay': 0.0022979911791563266, 'batch_size': 64, 'co_train_epochs': 17, 'epoch_patience': 10}
  learning_rate: 0.00013972898839000164
2026-02-14 01:51:26 - INFO -   learning_rate: 0.00013972898839000164
  weight_decay: 0.0022979911791563266
2026-02-14 01:51:26 - INFO -   weight_decay: 0.0022979911791563266
  batch_size: 64
2026-02-14 01:51:26 - INFO -   batch_size: 64
  co_train_epochs: 17
2026-02-14 01:51:26 - INFO -   co_train_epochs: 17
  epoch_patience: 10
2026-02-14 01:51:26 - INFO -   epoch_patience: 10

Total time taken: 7506.74 seconds
2026-02-14 01:51:26 - INFO - 
Total time taken: 7506.74 seconds