2026-02-08 18:04:24 - INFO - 
[Optuna] Starting hyperparameter search with 25 trials.
2026-02-08 18:04:24 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_irma_2017
2026-02-08 18:04:24 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 18:04:24 - INFO - Devices: cuda:1, cuda:1
2026-02-08 18:04:24 - INFO - Starting log
2026-02-08 18:04:24 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 18:04:25 - INFO - Learning Rate: 0.00012973890129214203
Weight Decay: 0.0010735078707464663
Batch Size: 16
No. Epochs: 18
Epoch Patience: 1
 Accumulation Steps: 4
2026-02-08 18:04:26 - INFO - Generating initial weights
2026-02-08 18:05:35 - INFO - Time taken for Epoch 1:66.05 - F1: 0.0127
2026-02-08 18:06:41 - INFO - Time taken for Epoch 2:65.49 - F1: 0.0127
2026-02-08 18:07:46 - INFO - Time taken for Epoch 3:65.53 - F1: 0.0127
2026-02-08 18:08:52 - INFO - Time taken for Epoch 4:65.56 - F1: 0.0127
2026-02-08 18:09:57 - INFO - Time taken for Epoch 5:65.52 - F1: 0.0127
2026-02-08 18:11:03 - INFO - Time taken for Epoch 6:65.58 - F1: 0.0127
2026-02-08 18:12:08 - INFO - Time taken for Epoch 7:65.45 - F1: 0.0186
2026-02-08 18:13:14 - INFO - Time taken for Epoch 8:65.77 - F1: 0.0715
2026-02-08 18:14:20 - INFO - Time taken for Epoch 9:65.50 - F1: 0.1360
2026-02-08 18:15:25 - INFO - Time taken for Epoch 10:65.73 - F1: 0.2010
2026-02-08 18:16:30 - INFO - Time taken for Epoch 11:64.99 - F1: 0.2081
2026-02-08 18:17:36 - INFO - Time taken for Epoch 12:65.53 - F1: 0.2148
2026-02-08 18:18:41 - INFO - Time taken for Epoch 13:65.31 - F1: 0.2116
2026-02-08 18:19:47 - INFO - Time taken for Epoch 14:65.51 - F1: 0.2162
2026-02-08 18:20:52 - INFO - Time taken for Epoch 15:65.47 - F1: 0.2239
2026-02-08 18:21:58 - INFO - Time taken for Epoch 16:65.55 - F1: 0.2300
2026-02-08 18:23:03 - INFO - Time taken for Epoch 17:65.17 - F1: 0.2311
2026-02-08 18:24:09 - INFO - Time taken for Epoch 18:65.58 - F1: 0.2391
2026-02-08 18:24:09 - INFO - Best F1:0.2391 - Best Epoch:18
2026-02-08 18:24:10 - INFO - Starting co-training
2026-02-08 18:25:59 - INFO - Time taken for Epoch 1: 108.97s - F1: 0.42988321
2026-02-08 18:27:49 - INFO - Time taken for Epoch 2: 109.59s - F1: 0.46264381
2026-02-08 18:29:39 - INFO - Time taken for Epoch 3: 109.67s - F1: 0.43903476
2026-02-08 18:29:39 - INFO - Performance not improving for 1 consecutive epochs.
2026-02-08 18:29:42 - INFO - Fine-tuning models
2026-02-08 18:29:51 - INFO - Time taken for Epoch 1:9.14 - F1: 0.4847
2026-02-08 18:30:01 - INFO - Time taken for Epoch 2:10.14 - F1: 0.4727
2026-02-08 18:30:10 - INFO - Time taken for Epoch 3:9.03 - F1: 0.4479
2026-02-08 18:30:19 - INFO - Time taken for Epoch 4:9.03 - F1: 0.4474
2026-02-08 18:30:28 - INFO - Time taken for Epoch 5:9.03 - F1: 0.4117
2026-02-08 18:30:37 - INFO - Time taken for Epoch 6:9.03 - F1: 0.4386
2026-02-08 18:30:46 - INFO - Time taken for Epoch 7:9.04 - F1: 0.5196
2026-02-08 18:30:56 - INFO - Time taken for Epoch 8:10.03 - F1: 0.5752
2026-02-08 18:31:06 - INFO - Time taken for Epoch 9:10.04 - F1: 0.5880
2026-02-08 18:31:17 - INFO - Time taken for Epoch 10:10.14 - F1: 0.5855
2026-02-08 18:31:26 - INFO - Time taken for Epoch 11:9.05 - F1: 0.5769
2026-02-08 18:31:35 - INFO - Time taken for Epoch 12:9.00 - F1: 0.5808
2026-02-08 18:31:44 - INFO - Time taken for Epoch 13:9.00 - F1: 0.5694
2026-02-08 18:31:53 - INFO - Time taken for Epoch 14:9.00 - F1: 0.5640
2026-02-08 18:32:02 - INFO - Time taken for Epoch 15:8.98 - F1: 0.5708
2026-02-08 18:32:11 - INFO - Time taken for Epoch 16:8.99 - F1: 0.5573
2026-02-08 18:32:20 - INFO - Time taken for Epoch 17:9.00 - F1: 0.5558
2026-02-08 18:32:29 - INFO - Time taken for Epoch 18:8.99 - F1: 0.5567
2026-02-08 18:32:38 - INFO - Time taken for Epoch 19:8.99 - F1: 0.5758
2026-02-08 18:32:38 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 18:32:38 - INFO - Best F1:0.5880 - Best Epoch:8
2026-02-08 18:32:56 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5583, Test ECE: 0.1306
2026-02-08 18:32:56 - INFO - All results: {'f1_macro': 0.5583376374880293, 'ece': np.float64(0.13059387163176056)}
2026-02-08 18:32:56 - INFO - 
Total time taken: 1711.88 seconds
2026-02-08 18:32:56 - INFO - Trial 0 finished with value: 0.5583376374880293 and parameters: {'learning_rate': 0.00012973890129214203, 'weight_decay': 0.0010735078707464663, 'batch_size': 16, 'co_train_epochs': 18, 'epoch_patience': 1}. Best is trial 0 with value: 0.5583376374880293.
2026-02-08 18:32:56 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 18:32:56 - INFO - Devices: cuda:1, cuda:1
2026-02-08 18:32:56 - INFO - Starting log
2026-02-08 18:32:56 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 18:32:56 - INFO - Learning Rate: 0.0007848522033794418
Weight Decay: 3.2647571862262074e-05
Batch Size: 64
No. Epochs: 15
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-08 18:32:57 - INFO - Generating initial weights
2026-02-08 18:33:38 - INFO - Time taken for Epoch 1:37.26 - F1: 0.0127
2026-02-08 18:34:15 - INFO - Time taken for Epoch 2:36.98 - F1: 0.0253
2026-02-08 18:34:52 - INFO - Time taken for Epoch 3:36.99 - F1: 0.0478
2026-02-08 18:35:29 - INFO - Time taken for Epoch 4:37.12 - F1: 0.0788
2026-02-08 18:36:06 - INFO - Time taken for Epoch 5:37.29 - F1: 0.0592
2026-02-08 18:36:43 - INFO - Time taken for Epoch 6:37.26 - F1: 0.0134
2026-02-08 18:37:20 - INFO - Time taken for Epoch 7:37.19 - F1: 0.0135
2026-02-08 18:37:57 - INFO - Time taken for Epoch 8:37.03 - F1: 0.0135
2026-02-08 18:38:34 - INFO - Time taken for Epoch 9:36.96 - F1: 0.0137
2026-02-08 18:39:11 - INFO - Time taken for Epoch 10:37.07 - F1: 0.0137
2026-02-08 18:39:48 - INFO - Time taken for Epoch 11:36.99 - F1: 0.0371
2026-02-08 18:40:25 - INFO - Time taken for Epoch 12:36.87 - F1: 0.0371
2026-02-08 18:41:02 - INFO - Time taken for Epoch 13:37.17 - F1: 0.0371
2026-02-08 18:41:39 - INFO - Time taken for Epoch 14:36.86 - F1: 0.0371
2026-02-08 18:42:16 - INFO - Time taken for Epoch 15:36.84 - F1: 0.0371
2026-02-08 18:42:16 - INFO - Best F1:0.0788 - Best Epoch:4
2026-02-08 18:42:17 - INFO - Starting co-training
2026-02-08 18:44:06 - INFO - Time taken for Epoch 1: 108.23s - F1: 0.03710145
2026-02-08 18:45:56 - INFO - Time taken for Epoch 2: 109.70s - F1: 0.03710145
2026-02-08 18:47:44 - INFO - Time taken for Epoch 3: 108.38s - F1: 0.03710145
2026-02-08 18:49:33 - INFO - Time taken for Epoch 4: 108.61s - F1: 0.03710145
2026-02-08 18:51:21 - INFO - Time taken for Epoch 5: 108.58s - F1: 0.03710145
2026-02-08 18:53:11 - INFO - Time taken for Epoch 6: 109.23s - F1: 0.03710145
2026-02-08 18:54:59 - INFO - Time taken for Epoch 7: 108.13s - F1: 0.03710145
2026-02-08 18:56:47 - INFO - Time taken for Epoch 8: 108.37s - F1: 0.03710145
2026-02-08 18:58:35 - INFO - Time taken for Epoch 9: 108.09s - F1: 0.03710145
2026-02-08 19:00:23 - INFO - Time taken for Epoch 10: 108.36s - F1: 0.03710145
2026-02-08 19:00:24 - INFO - Performance not improving for 9 consecutive epochs.
2026-02-08 19:00:26 - INFO - Fine-tuning models
2026-02-08 19:00:32 - INFO - Time taken for Epoch 1:5.37 - F1: 0.0371
2026-02-08 19:00:38 - INFO - Time taken for Epoch 2:6.67 - F1: 0.0030
2026-02-08 19:00:43 - INFO - Time taken for Epoch 3:5.19 - F1: 0.0030
2026-02-08 19:00:49 - INFO - Time taken for Epoch 4:5.21 - F1: 0.0127
2026-02-08 19:00:54 - INFO - Time taken for Epoch 5:5.21 - F1: 0.0127
2026-02-08 19:00:59 - INFO - Time taken for Epoch 6:5.20 - F1: 0.0137
2026-02-08 19:01:04 - INFO - Time taken for Epoch 7:5.21 - F1: 0.0137
2026-02-08 19:01:09 - INFO - Time taken for Epoch 8:5.22 - F1: 0.0193
2026-02-08 19:01:15 - INFO - Time taken for Epoch 9:5.22 - F1: 0.0193
2026-02-08 19:01:20 - INFO - Time taken for Epoch 10:5.21 - F1: 0.0193
2026-02-08 19:01:25 - INFO - Time taken for Epoch 11:5.22 - F1: 0.0030
2026-02-08 19:01:25 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 19:01:25 - INFO - Best F1:0.0371 - Best Epoch:0
2026-02-08 19:01:37 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0370, Test ECE: 0.1166
2026-02-08 19:01:37 - INFO - All results: {'f1_macro': 0.037003879438973444, 'ece': np.float64(0.1166394632049585)}
2026-02-08 19:01:37 - INFO - 
Total time taken: 1720.61 seconds
2026-02-08 19:01:37 - INFO - Trial 1 finished with value: 0.037003879438973444 and parameters: {'learning_rate': 0.0007848522033794418, 'weight_decay': 3.2647571862262074e-05, 'batch_size': 64, 'co_train_epochs': 15, 'epoch_patience': 9}. Best is trial 0 with value: 0.5583376374880293.
2026-02-08 19:01:37 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 19:01:37 - INFO - Devices: cuda:1, cuda:1
2026-02-08 19:01:37 - INFO - Starting log
2026-02-08 19:01:37 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 19:01:37 - INFO - Learning Rate: 6.704543875683305e-05
Weight Decay: 0.008530036287964211
Batch Size: 16
No. Epochs: 11
Epoch Patience: 5
 Accumulation Steps: 4
2026-02-08 19:01:38 - INFO - Generating initial weights
2026-02-08 19:02:47 - INFO - Time taken for Epoch 1:65.59 - F1: 0.0128
2026-02-08 19:03:52 - INFO - Time taken for Epoch 2:65.37 - F1: 0.0127
2026-02-08 19:04:57 - INFO - Time taken for Epoch 3:65.34 - F1: 0.0127
2026-02-08 19:06:03 - INFO - Time taken for Epoch 4:65.64 - F1: 0.0127
2026-02-08 19:07:09 - INFO - Time taken for Epoch 5:65.59 - F1: 0.0127
2026-02-08 19:08:14 - INFO - Time taken for Epoch 6:65.33 - F1: 0.0127
2026-02-08 19:09:19 - INFO - Time taken for Epoch 7:65.52 - F1: 0.0127
2026-02-08 19:10:25 - INFO - Time taken for Epoch 8:65.57 - F1: 0.0127
2026-02-08 19:11:31 - INFO - Time taken for Epoch 9:65.72 - F1: 0.0127
2026-02-08 19:12:36 - INFO - Time taken for Epoch 10:65.71 - F1: 0.0127
2026-02-08 19:13:42 - INFO - Time taken for Epoch 11:65.92 - F1: 0.0127
2026-02-08 19:13:42 - INFO - Best F1:0.0128 - Best Epoch:1
2026-02-08 19:13:44 - INFO - Starting co-training
2026-02-08 19:15:33 - INFO - Time taken for Epoch 1: 108.90s - F1: 0.49931939
2026-02-08 19:17:23 - INFO - Time taken for Epoch 2: 109.76s - F1: 0.53488296
2026-02-08 19:19:12 - INFO - Time taken for Epoch 3: 109.53s - F1: 0.59133289
2026-02-08 19:21:02 - INFO - Time taken for Epoch 4: 109.91s - F1: 0.58911033
2026-02-08 19:22:51 - INFO - Time taken for Epoch 5: 108.80s - F1: 0.56260646
2026-02-08 19:24:39 - INFO - Time taken for Epoch 6: 108.61s - F1: 0.57117861
2026-02-08 19:26:28 - INFO - Time taken for Epoch 7: 108.74s - F1: 0.61590498
2026-02-08 19:28:18 - INFO - Time taken for Epoch 8: 109.72s - F1: 0.58451315
2026-02-08 19:30:07 - INFO - Time taken for Epoch 9: 109.05s - F1: 0.60650462
2026-02-08 19:31:58 - INFO - Time taken for Epoch 10: 110.63s - F1: 0.61376585
2026-02-08 19:33:46 - INFO - Time taken for Epoch 11: 108.66s - F1: 0.58604641
2026-02-08 19:33:49 - INFO - Fine-tuning models
2026-02-08 19:33:58 - INFO - Time taken for Epoch 1:9.21 - F1: 0.6312
2026-02-08 19:34:08 - INFO - Time taken for Epoch 2:10.00 - F1: 0.6291
2026-02-08 19:34:17 - INFO - Time taken for Epoch 3:9.03 - F1: 0.6111
2026-02-08 19:34:26 - INFO - Time taken for Epoch 4:9.02 - F1: 0.5972
2026-02-08 19:34:35 - INFO - Time taken for Epoch 5:9.02 - F1: 0.5907
2026-02-08 19:34:44 - INFO - Time taken for Epoch 6:9.02 - F1: 0.5852
2026-02-08 19:34:53 - INFO - Time taken for Epoch 7:9.01 - F1: 0.5816
2026-02-08 19:35:02 - INFO - Time taken for Epoch 8:8.99 - F1: 0.5780
2026-02-08 19:35:11 - INFO - Time taken for Epoch 9:9.01 - F1: 0.5800
2026-02-08 19:35:20 - INFO - Time taken for Epoch 10:8.98 - F1: 0.5800
2026-02-08 19:35:29 - INFO - Time taken for Epoch 11:8.99 - F1: 0.5844
2026-02-08 19:35:29 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 19:35:29 - INFO - Best F1:0.6312 - Best Epoch:0
2026-02-08 19:35:47 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6378, Test ECE: 0.0443
2026-02-08 19:35:47 - INFO - All results: {'f1_macro': 0.6378266196018338, 'ece': np.float64(0.044280791257044934)}
2026-02-08 19:35:47 - INFO - 
Total time taken: 2050.68 seconds
2026-02-08 19:35:47 - INFO - Trial 2 finished with value: 0.6378266196018338 and parameters: {'learning_rate': 6.704543875683305e-05, 'weight_decay': 0.008530036287964211, 'batch_size': 16, 'co_train_epochs': 11, 'epoch_patience': 5}. Best is trial 2 with value: 0.6378266196018338.
2026-02-08 19:35:47 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 19:35:47 - INFO - Devices: cuda:1, cuda:1
2026-02-08 19:35:47 - INFO - Starting log
2026-02-08 19:35:47 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 19:35:48 - INFO - Learning Rate: 0.000268647854560452
Weight Decay: 0.0002493586274826732
Batch Size: 8
No. Epochs: 11
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-08 19:35:49 - INFO - Generating initial weights
2026-02-08 19:37:35 - INFO - Time taken for Epoch 1:103.39 - F1: 0.0127
2026-02-08 19:39:18 - INFO - Time taken for Epoch 2:103.15 - F1: 0.0127
2026-02-08 19:41:01 - INFO - Time taken for Epoch 3:102.68 - F1: 0.0432
2026-02-08 19:42:44 - INFO - Time taken for Epoch 4:102.91 - F1: 0.1281
2026-02-08 19:44:27 - INFO - Time taken for Epoch 5:102.99 - F1: 0.3080
2026-02-08 19:46:10 - INFO - Time taken for Epoch 6:102.89 - F1: 0.3010
2026-02-08 19:47:53 - INFO - Time taken for Epoch 7:102.65 - F1: 0.2653
2026-02-08 19:49:35 - INFO - Time taken for Epoch 8:102.80 - F1: 0.2829
2026-02-08 19:51:18 - INFO - Time taken for Epoch 9:102.66 - F1: 0.3109
2026-02-08 19:53:01 - INFO - Time taken for Epoch 10:102.79 - F1: 0.3101
2026-02-08 19:54:44 - INFO - Time taken for Epoch 11:103.25 - F1: 0.3146
2026-02-08 19:54:44 - INFO - Best F1:0.3146 - Best Epoch:11
2026-02-08 19:54:45 - INFO - Starting co-training
2026-02-08 19:57:09 - INFO - Time taken for Epoch 1: 143.43s - F1: 0.03710145
2026-02-08 19:59:33 - INFO - Time taken for Epoch 2: 144.27s - F1: 0.03710145
2026-02-08 20:01:57 - INFO - Time taken for Epoch 3: 143.17s - F1: 0.03710145
2026-02-08 20:04:20 - INFO - Time taken for Epoch 4: 143.42s - F1: 0.03710145
2026-02-08 20:06:44 - INFO - Time taken for Epoch 5: 143.45s - F1: 0.03710145
2026-02-08 20:06:44 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-08 20:06:46 - INFO - Fine-tuning models
2026-02-08 20:07:00 - INFO - Time taken for Epoch 1:14.00 - F1: 0.0371
2026-02-08 20:07:15 - INFO - Time taken for Epoch 2:14.92 - F1: 0.0127
2026-02-08 20:07:29 - INFO - Time taken for Epoch 3:13.84 - F1: 0.0127
2026-02-08 20:07:43 - INFO - Time taken for Epoch 4:13.86 - F1: 0.0127
2026-02-08 20:07:57 - INFO - Time taken for Epoch 5:13.87 - F1: 0.0127
2026-02-08 20:08:10 - INFO - Time taken for Epoch 6:13.88 - F1: 0.0127
2026-02-08 20:08:24 - INFO - Time taken for Epoch 7:13.86 - F1: 0.0127
2026-02-08 20:08:38 - INFO - Time taken for Epoch 8:13.87 - F1: 0.0127
2026-02-08 20:08:52 - INFO - Time taken for Epoch 9:13.84 - F1: 0.0127
2026-02-08 20:09:06 - INFO - Time taken for Epoch 10:13.88 - F1: 0.0127
2026-02-08 20:09:20 - INFO - Time taken for Epoch 11:13.84 - F1: 0.0127
2026-02-08 20:09:20 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 20:09:20 - INFO - Best F1:0.0371 - Best Epoch:0
2026-02-08 20:09:47 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0370, Test ECE: 0.1999
2026-02-08 20:09:47 - INFO - All results: {'f1_macro': 0.037003879438973444, 'ece': np.float64(0.19993276621998318)}
2026-02-08 20:09:47 - INFO - 
Total time taken: 2039.28 seconds
2026-02-08 20:09:47 - INFO - Trial 3 finished with value: 0.037003879438973444 and parameters: {'learning_rate': 0.000268647854560452, 'weight_decay': 0.0002493586274826732, 'batch_size': 8, 'co_train_epochs': 11, 'epoch_patience': 4}. Best is trial 2 with value: 0.6378266196018338.
2026-02-08 20:09:47 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 20:09:47 - INFO - Devices: cuda:1, cuda:1
2026-02-08 20:09:47 - INFO - Starting log
2026-02-08 20:09:47 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 20:09:47 - INFO - Learning Rate: 0.0001695123652526694
Weight Decay: 1.732208828496966e-05
Batch Size: 64
No. Epochs: 13
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-08 20:09:48 - INFO - Generating initial weights
2026-02-08 20:10:28 - INFO - Time taken for Epoch 1:37.06 - F1: 0.1247
2026-02-08 20:11:05 - INFO - Time taken for Epoch 2:37.12 - F1: 0.0544
2026-02-08 20:11:42 - INFO - Time taken for Epoch 3:37.08 - F1: 0.1839
2026-02-08 20:12:19 - INFO - Time taken for Epoch 4:37.11 - F1: 0.2793
2026-02-08 20:12:57 - INFO - Time taken for Epoch 5:37.17 - F1: 0.3485
2026-02-08 20:13:34 - INFO - Time taken for Epoch 6:37.16 - F1: 0.3607
2026-02-08 20:14:11 - INFO - Time taken for Epoch 7:37.06 - F1: 0.3603
2026-02-08 20:14:48 - INFO - Time taken for Epoch 8:37.13 - F1: 0.3523
2026-02-08 20:15:25 - INFO - Time taken for Epoch 9:37.16 - F1: 0.3454
2026-02-08 20:16:02 - INFO - Time taken for Epoch 10:37.26 - F1: 0.3463
2026-02-08 20:16:40 - INFO - Time taken for Epoch 11:37.22 - F1: 0.3497
2026-02-08 20:17:17 - INFO - Time taken for Epoch 12:37.22 - F1: 0.3558
2026-02-08 20:17:54 - INFO - Time taken for Epoch 13:37.11 - F1: 0.3610
2026-02-08 20:17:54 - INFO - Best F1:0.3610 - Best Epoch:13
2026-02-08 20:17:55 - INFO - Starting co-training
2026-02-08 20:19:44 - INFO - Time taken for Epoch 1: 108.78s - F1: 0.59935207
2026-02-08 20:21:34 - INFO - Time taken for Epoch 2: 109.70s - F1: 0.59346366
2026-02-08 20:23:23 - INFO - Time taken for Epoch 3: 108.58s - F1: 0.59519675
2026-02-08 20:25:11 - INFO - Time taken for Epoch 4: 108.52s - F1: 0.59376551
2026-02-08 20:27:04 - INFO - Time taken for Epoch 5: 113.24s - F1: 0.60156403
2026-02-08 20:28:54 - INFO - Time taken for Epoch 6: 109.61s - F1: 0.59941585
2026-02-08 20:30:43 - INFO - Time taken for Epoch 7: 108.54s - F1: 0.60435362
2026-02-08 20:32:32 - INFO - Time taken for Epoch 8: 109.57s - F1: 0.59640035
2026-02-08 20:34:21 - INFO - Time taken for Epoch 9: 108.50s - F1: 0.59509560
2026-02-08 20:36:09 - INFO - Time taken for Epoch 10: 108.46s - F1: 0.60933695
2026-02-08 20:37:59 - INFO - Time taken for Epoch 11: 109.44s - F1: 0.59572932
2026-02-08 20:39:47 - INFO - Time taken for Epoch 12: 108.60s - F1: 0.59788578
2026-02-08 20:41:36 - INFO - Time taken for Epoch 13: 108.81s - F1: 0.58776091
2026-02-08 20:42:11 - INFO - Fine-tuning models
2026-02-08 20:42:17 - INFO - Time taken for Epoch 1:5.33 - F1: 0.6373
2026-02-08 20:42:23 - INFO - Time taken for Epoch 2:6.42 - F1: 0.6474
2026-02-08 20:42:29 - INFO - Time taken for Epoch 3:6.33 - F1: 0.6400
2026-02-08 20:42:35 - INFO - Time taken for Epoch 4:5.17 - F1: 0.6346
2026-02-08 20:42:40 - INFO - Time taken for Epoch 5:5.19 - F1: 0.6320
2026-02-08 20:42:45 - INFO - Time taken for Epoch 6:5.20 - F1: 0.6290
2026-02-08 20:42:50 - INFO - Time taken for Epoch 7:5.19 - F1: 0.6132
2026-02-08 20:42:55 - INFO - Time taken for Epoch 8:5.21 - F1: 0.6132
2026-02-08 20:43:01 - INFO - Time taken for Epoch 9:5.21 - F1: 0.6136
2026-02-08 20:43:06 - INFO - Time taken for Epoch 10:5.21 - F1: 0.6178
2026-02-08 20:43:11 - INFO - Time taken for Epoch 11:5.21 - F1: 0.6228
2026-02-08 20:43:16 - INFO - Time taken for Epoch 12:5.21 - F1: 0.6238
2026-02-08 20:43:16 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 20:43:16 - INFO - Best F1:0.6474 - Best Epoch:1
2026-02-08 20:43:28 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6572, Test ECE: 0.0716
2026-02-08 20:43:28 - INFO - All results: {'f1_macro': 0.6572275378744384, 'ece': np.float64(0.07156862694646055)}
2026-02-08 20:43:28 - INFO - 
Total time taken: 2021.11 seconds
2026-02-08 20:43:28 - INFO - Trial 4 finished with value: 0.6572275378744384 and parameters: {'learning_rate': 0.0001695123652526694, 'weight_decay': 1.732208828496966e-05, 'batch_size': 64, 'co_train_epochs': 13, 'epoch_patience': 10}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 20:43:28 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 20:43:28 - INFO - Devices: cuda:1, cuda:1
2026-02-08 20:43:28 - INFO - Starting log
2026-02-08 20:43:28 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 20:43:29 - INFO - Learning Rate: 1.4201166002873021e-05
Weight Decay: 0.0008532682547098321
Batch Size: 64
No. Epochs: 10
Epoch Patience: 2
 Accumulation Steps: 1
2026-02-08 20:43:30 - INFO - Generating initial weights
2026-02-08 20:44:10 - INFO - Time taken for Epoch 1:37.33 - F1: 0.0557
2026-02-08 20:44:47 - INFO - Time taken for Epoch 2:37.13 - F1: 0.0731
2026-02-08 20:45:25 - INFO - Time taken for Epoch 3:37.20 - F1: 0.0809
2026-02-08 20:46:02 - INFO - Time taken for Epoch 4:37.11 - F1: 0.0840
2026-02-08 20:46:39 - INFO - Time taken for Epoch 5:37.19 - F1: 0.0883
2026-02-08 20:47:16 - INFO - Time taken for Epoch 6:37.03 - F1: 0.0972
2026-02-08 20:47:53 - INFO - Time taken for Epoch 7:37.17 - F1: 0.1099
2026-02-08 20:48:30 - INFO - Time taken for Epoch 8:37.19 - F1: 0.1058
2026-02-08 20:49:07 - INFO - Time taken for Epoch 9:37.17 - F1: 0.1048
2026-02-08 20:49:45 - INFO - Time taken for Epoch 10:37.10 - F1: 0.1030
2026-02-08 20:49:45 - INFO - Best F1:0.1099 - Best Epoch:7
2026-02-08 20:49:46 - INFO - Starting co-training
2026-02-08 20:51:35 - INFO - Time taken for Epoch 1: 108.70s - F1: 0.54017954
2026-02-08 20:53:25 - INFO - Time taken for Epoch 2: 110.29s - F1: 0.55190334
2026-02-08 20:55:15 - INFO - Time taken for Epoch 3: 109.96s - F1: 0.55647004
2026-02-08 20:57:05 - INFO - Time taken for Epoch 4: 110.43s - F1: 0.57587866
2026-02-08 20:58:55 - INFO - Time taken for Epoch 5: 109.99s - F1: 0.58350489
2026-02-08 21:00:45 - INFO - Time taken for Epoch 6: 109.37s - F1: 0.60375131
2026-02-08 21:02:34 - INFO - Time taken for Epoch 7: 109.51s - F1: 0.60580018
2026-02-08 21:04:25 - INFO - Time taken for Epoch 8: 110.36s - F1: 0.59143903
2026-02-08 21:06:13 - INFO - Time taken for Epoch 9: 108.59s - F1: 0.61191682
2026-02-08 21:08:04 - INFO - Time taken for Epoch 10: 110.81s - F1: 0.62616132
2026-02-08 21:08:15 - INFO - Fine-tuning models
2026-02-08 21:08:21 - INFO - Time taken for Epoch 1:5.34 - F1: 0.6321
2026-02-08 21:08:29 - INFO - Time taken for Epoch 2:7.77 - F1: 0.6357
2026-02-08 21:08:35 - INFO - Time taken for Epoch 3:6.91 - F1: 0.6351
2026-02-08 21:08:41 - INFO - Time taken for Epoch 4:5.20 - F1: 0.6364
2026-02-08 21:08:48 - INFO - Time taken for Epoch 5:6.93 - F1: 0.6318
2026-02-08 21:08:53 - INFO - Time taken for Epoch 6:5.20 - F1: 0.6332
2026-02-08 21:08:58 - INFO - Time taken for Epoch 7:5.22 - F1: 0.6323
2026-02-08 21:09:03 - INFO - Time taken for Epoch 8:5.21 - F1: 0.6371
2026-02-08 21:09:10 - INFO - Time taken for Epoch 9:7.01 - F1: 0.6372
2026-02-08 21:09:18 - INFO - Time taken for Epoch 10:7.37 - F1: 0.6409
2026-02-08 21:09:24 - INFO - Time taken for Epoch 11:6.31 - F1: 0.6403
2026-02-08 21:09:29 - INFO - Time taken for Epoch 12:5.20 - F1: 0.6423
2026-02-08 21:09:36 - INFO - Time taken for Epoch 13:6.42 - F1: 0.6453
2026-02-08 21:09:50 - INFO - Time taken for Epoch 14:14.11 - F1: 0.6426
2026-02-08 21:09:55 - INFO - Time taken for Epoch 15:5.19 - F1: 0.6444
2026-02-08 21:10:00 - INFO - Time taken for Epoch 16:5.20 - F1: 0.6427
2026-02-08 21:10:05 - INFO - Time taken for Epoch 17:5.20 - F1: 0.6395
2026-02-08 21:10:10 - INFO - Time taken for Epoch 18:5.21 - F1: 0.6380
2026-02-08 21:10:16 - INFO - Time taken for Epoch 19:5.22 - F1: 0.6367
2026-02-08 21:10:21 - INFO - Time taken for Epoch 20:5.23 - F1: 0.6368
2026-02-08 21:10:26 - INFO - Time taken for Epoch 21:5.20 - F1: 0.6374
2026-02-08 21:10:31 - INFO - Time taken for Epoch 22:5.20 - F1: 0.6373
2026-02-08 21:10:37 - INFO - Time taken for Epoch 23:5.21 - F1: 0.6400
2026-02-08 21:10:37 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 21:10:37 - INFO - Best F1:0.6453 - Best Epoch:12
2026-02-08 21:11:04 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6453, Test ECE: 0.0366
2026-02-08 21:11:04 - INFO - All results: {'f1_macro': 0.6453485237270558, 'ece': np.float64(0.03662438881128354)}
2026-02-08 21:11:04 - INFO - 
Total time taken: 1656.29 seconds
2026-02-08 21:11:04 - INFO - Trial 5 finished with value: 0.6453485237270558 and parameters: {'learning_rate': 1.4201166002873021e-05, 'weight_decay': 0.0008532682547098321, 'batch_size': 64, 'co_train_epochs': 10, 'epoch_patience': 2}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 21:11:05 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 21:11:05 - INFO - Devices: cuda:1, cuda:1
2026-02-08 21:11:05 - INFO - Starting log
2026-02-08 21:11:05 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 21:11:08 - INFO - Learning Rate: 0.0008813133358642097
Weight Decay: 2.091820518037997e-05
Batch Size: 8
No. Epochs: 10
Epoch Patience: 1
 Accumulation Steps: 8
2026-02-08 21:11:14 - INFO - Generating initial weights
2026-02-08 21:13:00 - INFO - Time taken for Epoch 1:103.22 - F1: 0.0127
2026-02-08 21:14:43 - INFO - Time taken for Epoch 2:102.77 - F1: 0.0301
2026-02-08 21:16:26 - INFO - Time taken for Epoch 3:103.38 - F1: 0.0391
2026-02-08 21:22:17 - INFO - Time taken for Epoch 4:350.65 - F1: 0.0193
2026-02-08 21:23:59 - INFO - Time taken for Epoch 5:102.22 - F1: 0.0127
2026-02-08 21:25:41 - INFO - Time taken for Epoch 6:101.84 - F1: 0.0127
2026-02-08 21:27:22 - INFO - Time taken for Epoch 7:101.75 - F1: 0.0127
2026-02-08 21:29:04 - INFO - Time taken for Epoch 8:101.77 - F1: 0.0127
2026-02-08 21:30:46 - INFO - Time taken for Epoch 9:101.81 - F1: 0.0127
2026-02-08 21:32:29 - INFO - Time taken for Epoch 10:102.65 - F1: 0.0127
2026-02-08 21:32:29 - INFO - Best F1:0.0391 - Best Epoch:3
2026-02-08 21:32:35 - INFO - Starting co-training
2026-02-08 21:34:58 - INFO - Time taken for Epoch 1: 142.90s - F1: 0.03710145
2026-02-08 21:37:22 - INFO - Time taken for Epoch 2: 144.27s - F1: 0.03710145
2026-02-08 21:37:23 - INFO - Performance not improving for 1 consecutive epochs.
2026-02-08 21:37:25 - INFO - Fine-tuning models
2026-02-08 21:37:40 - INFO - Time taken for Epoch 1:13.99 - F1: 0.0371
2026-02-08 21:37:55 - INFO - Time taken for Epoch 2:15.14 - F1: 0.0127
2026-02-08 21:38:09 - INFO - Time taken for Epoch 3:13.80 - F1: 0.0127
2026-02-08 21:38:22 - INFO - Time taken for Epoch 4:13.82 - F1: 0.0127
2026-02-08 21:38:36 - INFO - Time taken for Epoch 5:13.81 - F1: 0.0127
2026-02-08 21:38:50 - INFO - Time taken for Epoch 6:13.85 - F1: 0.0127
2026-02-08 21:39:04 - INFO - Time taken for Epoch 7:13.85 - F1: 0.0127
2026-02-08 21:39:18 - INFO - Time taken for Epoch 8:13.86 - F1: 0.0371
2026-02-08 21:39:32 - INFO - Time taken for Epoch 9:13.85 - F1: 0.0321
2026-02-08 21:39:46 - INFO - Time taken for Epoch 10:13.86 - F1: 0.0127
2026-02-08 21:39:59 - INFO - Time taken for Epoch 11:13.84 - F1: 0.0127
2026-02-08 21:39:59 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 21:39:59 - INFO - Best F1:0.0371 - Best Epoch:0
2026-02-08 21:40:26 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0370, Test ECE: 0.2254
2026-02-08 21:40:26 - INFO - All results: {'f1_macro': 0.037003879438973444, 'ece': np.float64(0.22544654347522686)}
2026-02-08 21:40:26 - INFO - 
Total time taken: 1761.97 seconds
2026-02-08 21:40:26 - INFO - Trial 6 finished with value: 0.037003879438973444 and parameters: {'learning_rate': 0.0008813133358642097, 'weight_decay': 2.091820518037997e-05, 'batch_size': 8, 'co_train_epochs': 10, 'epoch_patience': 1}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 21:40:26 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 21:40:26 - INFO - Devices: cuda:1, cuda:1
2026-02-08 21:40:26 - INFO - Starting log
2026-02-08 21:40:26 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 21:40:27 - INFO - Learning Rate: 1.195573138214154e-05
Weight Decay: 0.0002585726985111452
Batch Size: 8
No. Epochs: 12
Epoch Patience: 6
 Accumulation Steps: 8
2026-02-08 21:40:28 - INFO - Generating initial weights
2026-02-08 21:42:15 - INFO - Time taken for Epoch 1:103.11 - F1: 0.0530
2026-02-08 21:43:57 - INFO - Time taken for Epoch 2:102.49 - F1: 0.0569
2026-02-08 21:45:40 - INFO - Time taken for Epoch 3:102.91 - F1: 0.0570
2026-02-08 21:46:59 - INFO - Time taken for Epoch 4:78.76 - F1: 0.0596
2026-02-08 21:47:38 - INFO - Time taken for Epoch 5:39.38 - F1: 0.0650
2026-02-08 21:48:17 - INFO - Time taken for Epoch 6:38.89 - F1: 0.0638
2026-02-08 21:48:56 - INFO - Time taken for Epoch 7:39.49 - F1: 0.0758
2026-02-08 21:49:36 - INFO - Time taken for Epoch 8:39.82 - F1: 0.0751
2026-02-08 21:50:16 - INFO - Time taken for Epoch 9:39.41 - F1: 0.0782
2026-02-08 21:50:55 - INFO - Time taken for Epoch 10:39.24 - F1: 0.0742
2026-02-08 21:51:34 - INFO - Time taken for Epoch 11:39.51 - F1: 0.0675
2026-02-08 21:52:14 - INFO - Time taken for Epoch 12:39.77 - F1: 0.0646
2026-02-08 21:52:14 - INFO - Best F1:0.0782 - Best Epoch:9
2026-02-08 21:52:16 - INFO - Starting co-training
2026-02-08 21:53:09 - INFO - Time taken for Epoch 1: 53.54s - F1: 0.14836502
2026-02-08 21:54:04 - INFO - Time taken for Epoch 2: 54.38s - F1: 0.39817200
2026-02-08 21:54:58 - INFO - Time taken for Epoch 3: 53.83s - F1: 0.45317754
2026-02-08 21:55:52 - INFO - Time taken for Epoch 4: 54.44s - F1: 0.50567050
2026-02-08 21:56:47 - INFO - Time taken for Epoch 5: 54.49s - F1: 0.52797144
2026-02-08 21:57:41 - INFO - Time taken for Epoch 6: 54.41s - F1: 0.50742031
2026-02-08 21:58:34 - INFO - Time taken for Epoch 7: 53.07s - F1: 0.50888537
2026-02-08 21:59:28 - INFO - Time taken for Epoch 8: 53.54s - F1: 0.54303568
2026-02-08 22:00:22 - INFO - Time taken for Epoch 9: 54.07s - F1: 0.54955853
2026-02-08 22:01:16 - INFO - Time taken for Epoch 10: 54.46s - F1: 0.54819023
2026-02-08 22:02:09 - INFO - Time taken for Epoch 11: 53.04s - F1: 0.58297566
2026-02-08 22:03:03 - INFO - Time taken for Epoch 12: 54.17s - F1: 0.58618566
2026-02-08 22:03:07 - INFO - Fine-tuning models
2026-02-08 22:03:12 - INFO - Time taken for Epoch 1:5.34 - F1: 0.5927
2026-02-08 22:03:19 - INFO - Time taken for Epoch 2:6.59 - F1: 0.6036
2026-02-08 22:03:25 - INFO - Time taken for Epoch 3:6.37 - F1: 0.6046
2026-02-08 22:03:32 - INFO - Time taken for Epoch 4:6.45 - F1: 0.5971
2026-02-08 22:03:37 - INFO - Time taken for Epoch 5:5.39 - F1: 0.5891
2026-02-08 22:03:43 - INFO - Time taken for Epoch 6:5.35 - F1: 0.5848
2026-02-08 22:03:48 - INFO - Time taken for Epoch 7:5.31 - F1: 0.5839
2026-02-08 22:03:53 - INFO - Time taken for Epoch 8:5.35 - F1: 0.5859
2026-02-08 22:03:59 - INFO - Time taken for Epoch 9:5.33 - F1: 0.5937
2026-02-08 22:04:04 - INFO - Time taken for Epoch 10:5.32 - F1: 0.5966
2026-02-08 22:04:09 - INFO - Time taken for Epoch 11:5.33 - F1: 0.5956
2026-02-08 22:04:15 - INFO - Time taken for Epoch 12:5.34 - F1: 0.6017
2026-02-08 22:04:20 - INFO - Time taken for Epoch 13:5.33 - F1: 0.6015
2026-02-08 22:04:20 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 22:04:20 - INFO - Best F1:0.6046 - Best Epoch:2
2026-02-08 22:04:32 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6289, Test ECE: 0.0491
2026-02-08 22:04:32 - INFO - All results: {'f1_macro': 0.6288947100796708, 'ece': np.float64(0.04907289326639104)}
2026-02-08 22:04:32 - INFO - 
Total time taken: 1445.82 seconds
2026-02-08 22:04:32 - INFO - Trial 7 finished with value: 0.6288947100796708 and parameters: {'learning_rate': 1.195573138214154e-05, 'weight_decay': 0.0002585726985111452, 'batch_size': 8, 'co_train_epochs': 12, 'epoch_patience': 6}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 22:04:32 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 22:04:32 - INFO - Devices: cuda:1, cuda:1
2026-02-08 22:04:32 - INFO - Starting log
2026-02-08 22:04:32 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 22:04:33 - INFO - Learning Rate: 0.0008973443581144738
Weight Decay: 0.001035576688625401
Batch Size: 8
No. Epochs: 12
Epoch Patience: 5
 Accumulation Steps: 8
2026-02-08 22:04:34 - INFO - Generating initial weights
2026-02-08 22:05:16 - INFO - Time taken for Epoch 1:39.35 - F1: 0.0127
2026-02-08 22:05:55 - INFO - Time taken for Epoch 2:39.30 - F1: 0.0127
2026-02-08 22:06:34 - INFO - Time taken for Epoch 3:38.75 - F1: 0.0361
2026-02-08 22:07:13 - INFO - Time taken for Epoch 4:39.26 - F1: 0.0441
2026-02-08 22:07:52 - INFO - Time taken for Epoch 5:38.87 - F1: 0.0135
2026-02-08 22:08:32 - INFO - Time taken for Epoch 6:39.72 - F1: 0.0127
2026-02-08 22:09:11 - INFO - Time taken for Epoch 7:39.66 - F1: 0.0127
2026-02-08 22:09:51 - INFO - Time taken for Epoch 8:39.85 - F1: 0.0445
2026-02-08 22:10:31 - INFO - Time taken for Epoch 9:39.83 - F1: 0.0127
2026-02-08 22:11:11 - INFO - Time taken for Epoch 10:39.74 - F1: 0.0127
2026-02-08 22:11:50 - INFO - Time taken for Epoch 11:39.27 - F1: 0.0127
2026-02-08 22:12:30 - INFO - Time taken for Epoch 12:40.37 - F1: 0.0127
2026-02-08 22:12:30 - INFO - Best F1:0.0445 - Best Epoch:8
2026-02-08 22:12:32 - INFO - Starting co-training
2026-02-08 22:13:26 - INFO - Time taken for Epoch 1: 53.25s - F1: 0.03710145
2026-02-08 22:14:20 - INFO - Time taken for Epoch 2: 54.25s - F1: 0.03710145
2026-02-08 22:15:13 - INFO - Time taken for Epoch 3: 53.21s - F1: 0.03710145
2026-02-08 22:16:06 - INFO - Time taken for Epoch 4: 53.21s - F1: 0.03710145
2026-02-08 22:16:59 - INFO - Time taken for Epoch 5: 53.18s - F1: 0.03710145
2026-02-08 22:17:52 - INFO - Time taken for Epoch 6: 53.04s - F1: 0.03710145
2026-02-08 22:17:52 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-08 22:17:55 - INFO - Fine-tuning models
2026-02-08 22:18:00 - INFO - Time taken for Epoch 1:5.35 - F1: 0.0371
2026-02-08 22:18:07 - INFO - Time taken for Epoch 2:6.31 - F1: 0.0127
2026-02-08 22:18:12 - INFO - Time taken for Epoch 3:5.32 - F1: 0.0127
2026-02-08 22:18:17 - INFO - Time taken for Epoch 4:5.28 - F1: 0.0127
2026-02-08 22:18:23 - INFO - Time taken for Epoch 5:5.30 - F1: 0.0127
2026-02-08 22:18:28 - INFO - Time taken for Epoch 6:5.34 - F1: 0.0127
2026-02-08 22:18:33 - INFO - Time taken for Epoch 7:5.33 - F1: 0.0127
2026-02-08 22:18:39 - INFO - Time taken for Epoch 8:5.33 - F1: 0.0127
2026-02-08 22:18:44 - INFO - Time taken for Epoch 9:5.35 - F1: 0.0127
2026-02-08 22:18:49 - INFO - Time taken for Epoch 10:5.31 - F1: 0.0127
2026-02-08 22:18:54 - INFO - Time taken for Epoch 11:5.31 - F1: 0.0127
2026-02-08 22:18:54 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 22:18:54 - INFO - Best F1:0.0371 - Best Epoch:0
2026-02-08 22:19:06 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0370, Test ECE: 0.1915
2026-02-08 22:19:06 - INFO - All results: {'f1_macro': 0.037003879438973444, 'ece': np.float64(0.1915047235154696)}
2026-02-08 22:19:06 - INFO - 
Total time taken: 874.41 seconds
2026-02-08 22:19:07 - INFO - Trial 8 finished with value: 0.037003879438973444 and parameters: {'learning_rate': 0.0008973443581144738, 'weight_decay': 0.001035576688625401, 'batch_size': 8, 'co_train_epochs': 12, 'epoch_patience': 5}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 22:19:07 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 22:19:07 - INFO - Devices: cuda:1, cuda:1
2026-02-08 22:19:07 - INFO - Starting log
2026-02-08 22:19:07 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 22:19:07 - INFO - Learning Rate: 3.727531065625212e-05
Weight Decay: 0.0009137043337163979
Batch Size: 8
No. Epochs: 20
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-08 22:19:08 - INFO - Generating initial weights
2026-02-08 22:19:51 - INFO - Time taken for Epoch 1:39.62 - F1: 0.0594
2026-02-08 22:20:30 - INFO - Time taken for Epoch 2:39.51 - F1: 0.0654
2026-02-08 22:21:10 - INFO - Time taken for Epoch 3:39.97 - F1: 0.0645
2026-02-08 22:21:50 - INFO - Time taken for Epoch 4:40.26 - F1: 0.0387
2026-02-08 22:22:30 - INFO - Time taken for Epoch 5:39.99 - F1: 0.0125
2026-02-08 22:23:10 - INFO - Time taken for Epoch 6:40.18 - F1: 0.0127
2026-02-08 22:23:50 - INFO - Time taken for Epoch 7:39.88 - F1: 0.0127
2026-02-08 22:24:31 - INFO - Time taken for Epoch 8:40.16 - F1: 0.0127
2026-02-08 22:25:11 - INFO - Time taken for Epoch 9:40.65 - F1: 0.0127
2026-02-08 22:25:50 - INFO - Time taken for Epoch 10:39.04 - F1: 0.0162
2026-02-08 22:26:30 - INFO - Time taken for Epoch 11:39.90 - F1: 0.0195
2026-02-08 22:27:11 - INFO - Time taken for Epoch 12:40.45 - F1: 0.0277
2026-02-08 22:27:50 - INFO - Time taken for Epoch 13:39.25 - F1: 0.0579
2026-02-08 22:28:29 - INFO - Time taken for Epoch 14:39.03 - F1: 0.1027
2026-02-08 22:29:09 - INFO - Time taken for Epoch 15:40.29 - F1: 0.1468
2026-02-08 22:29:49 - INFO - Time taken for Epoch 16:39.40 - F1: 0.1805
2026-02-08 22:30:27 - INFO - Time taken for Epoch 17:38.67 - F1: 0.2067
2026-02-08 22:31:06 - INFO - Time taken for Epoch 18:39.08 - F1: 0.2253
2026-02-08 22:31:46 - INFO - Time taken for Epoch 19:39.26 - F1: 0.2399
2026-02-08 22:32:26 - INFO - Time taken for Epoch 20:40.22 - F1: 0.2508
2026-02-08 22:32:26 - INFO - Best F1:0.2508 - Best Epoch:20
2026-02-08 22:32:27 - INFO - Starting co-training
2026-02-08 22:33:21 - INFO - Time taken for Epoch 1: 53.57s - F1: 0.36844885
2026-02-08 22:34:15 - INFO - Time taken for Epoch 2: 54.21s - F1: 0.50371396
2026-02-08 22:35:10 - INFO - Time taken for Epoch 3: 54.35s - F1: 0.50831943
2026-02-08 22:36:03 - INFO - Time taken for Epoch 4: 53.93s - F1: 0.52106648
2026-02-08 22:36:58 - INFO - Time taken for Epoch 5: 54.40s - F1: 0.53190448
2026-02-08 22:37:52 - INFO - Time taken for Epoch 6: 54.23s - F1: 0.56681561
2026-02-08 22:38:46 - INFO - Time taken for Epoch 7: 54.31s - F1: 0.57956324
2026-02-08 22:39:41 - INFO - Time taken for Epoch 8: 54.20s - F1: 0.60110582
2026-02-08 22:40:35 - INFO - Time taken for Epoch 9: 54.60s - F1: 0.61094765
2026-02-08 22:41:29 - INFO - Time taken for Epoch 10: 54.10s - F1: 0.57431158
2026-02-08 22:42:23 - INFO - Time taken for Epoch 11: 53.35s - F1: 0.59808074
2026-02-08 22:43:16 - INFO - Time taken for Epoch 12: 52.90s - F1: 0.59846367
2026-02-08 22:44:09 - INFO - Time taken for Epoch 13: 53.14s - F1: 0.59909225
2026-02-08 22:44:09 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-08 22:44:11 - INFO - Fine-tuning models
2026-02-08 22:44:17 - INFO - Time taken for Epoch 1:5.34 - F1: 0.6095
2026-02-08 22:44:23 - INFO - Time taken for Epoch 2:6.68 - F1: 0.6066
2026-02-08 22:44:29 - INFO - Time taken for Epoch 3:5.30 - F1: 0.6005
2026-02-08 22:44:34 - INFO - Time taken for Epoch 4:5.29 - F1: 0.5969
2026-02-08 22:44:39 - INFO - Time taken for Epoch 5:5.32 - F1: 0.5880
2026-02-08 22:44:45 - INFO - Time taken for Epoch 6:5.31 - F1: 0.5823
2026-02-08 22:44:50 - INFO - Time taken for Epoch 7:5.33 - F1: 0.5803
2026-02-08 22:44:55 - INFO - Time taken for Epoch 8:5.31 - F1: 0.5842
2026-02-08 22:45:01 - INFO - Time taken for Epoch 9:5.32 - F1: 0.5802
2026-02-08 22:45:06 - INFO - Time taken for Epoch 10:5.32 - F1: 0.5797
2026-02-08 22:45:11 - INFO - Time taken for Epoch 11:5.34 - F1: 0.5709
2026-02-08 22:45:11 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 22:45:11 - INFO - Best F1:0.6095 - Best Epoch:0
2026-02-08 22:45:23 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6093, Test ECE: 0.0383
2026-02-08 22:45:23 - INFO - All results: {'f1_macro': 0.6093222878127654, 'ece': np.float64(0.03828405834550376)}
2026-02-08 22:45:23 - INFO - 
Total time taken: 1576.81 seconds
2026-02-08 22:45:23 - INFO - Trial 9 finished with value: 0.6093222878127654 and parameters: {'learning_rate': 3.727531065625212e-05, 'weight_decay': 0.0009137043337163979, 'batch_size': 8, 'co_train_epochs': 20, 'epoch_patience': 4}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 22:45:24 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 22:45:24 - INFO - Devices: cuda:1, cuda:1
2026-02-08 22:45:24 - INFO - Starting log
2026-02-08 22:45:24 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 22:45:24 - INFO - Learning Rate: 0.0002374907647394005
Weight Decay: 1.0433934476978967e-05
Batch Size: 32
No. Epochs: 5
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-08 22:45:25 - INFO - Generating initial weights
2026-02-08 22:45:56 - INFO - Time taken for Epoch 1:27.87 - F1: 0.0905
2026-02-08 22:46:24 - INFO - Time taken for Epoch 2:28.32 - F1: 0.0580
2026-02-08 22:46:53 - INFO - Time taken for Epoch 3:28.41 - F1: 0.2055
2026-02-08 22:47:21 - INFO - Time taken for Epoch 4:28.26 - F1: 0.2895
2026-02-08 22:47:49 - INFO - Time taken for Epoch 5:28.39 - F1: 0.3126
2026-02-08 22:47:49 - INFO - Best F1:0.3126 - Best Epoch:5
2026-02-08 22:47:51 - INFO - Starting co-training
2026-02-08 22:48:47 - INFO - Time taken for Epoch 1: 56.24s - F1: 0.03710145
2026-02-08 22:49:44 - INFO - Time taken for Epoch 2: 57.01s - F1: 0.03710145
2026-02-08 22:50:40 - INFO - Time taken for Epoch 3: 55.44s - F1: 0.03710145
2026-02-08 22:51:35 - INFO - Time taken for Epoch 4: 55.24s - F1: 0.03710145
2026-02-08 22:52:31 - INFO - Time taken for Epoch 5: 55.77s - F1: 0.03710145
2026-02-08 22:52:33 - INFO - Fine-tuning models
2026-02-08 22:52:37 - INFO - Time taken for Epoch 1:3.87 - F1: 0.0371
2026-02-08 22:52:42 - INFO - Time taken for Epoch 2:4.85 - F1: 0.0415
2026-02-08 22:52:47 - INFO - Time taken for Epoch 3:4.88 - F1: 0.0426
2026-02-08 22:52:52 - INFO - Time taken for Epoch 4:4.95 - F1: 0.0137
2026-02-08 22:52:56 - INFO - Time taken for Epoch 5:3.84 - F1: 0.0195
2026-02-08 22:52:59 - INFO - Time taken for Epoch 6:3.83 - F1: 0.0030
2026-02-08 22:53:03 - INFO - Time taken for Epoch 7:3.84 - F1: 0.0030
2026-02-08 22:53:07 - INFO - Time taken for Epoch 8:3.83 - F1: 0.0199
2026-02-08 22:53:11 - INFO - Time taken for Epoch 9:3.86 - F1: 0.0404
2026-02-08 22:53:15 - INFO - Time taken for Epoch 10:3.89 - F1: 0.0570
2026-02-08 22:53:25 - INFO - Time taken for Epoch 11:9.90 - F1: 0.0741
2026-02-08 22:53:30 - INFO - Time taken for Epoch 12:4.97 - F1: 0.0612
2026-02-08 22:53:33 - INFO - Time taken for Epoch 13:3.83 - F1: 0.0543
2026-02-08 22:53:37 - INFO - Time taken for Epoch 14:3.83 - F1: 0.0426
2026-02-08 22:53:41 - INFO - Time taken for Epoch 15:3.82 - F1: 0.0436
2026-02-08 22:53:45 - INFO - Time taken for Epoch 16:3.84 - F1: 0.0390
2026-02-08 22:53:49 - INFO - Time taken for Epoch 17:3.89 - F1: 0.0220
2026-02-08 22:53:53 - INFO - Time taken for Epoch 18:3.87 - F1: 0.0195
2026-02-08 22:53:57 - INFO - Time taken for Epoch 19:3.84 - F1: 0.0193
2026-02-08 22:54:00 - INFO - Time taken for Epoch 20:3.84 - F1: 0.0193
2026-02-08 22:54:04 - INFO - Time taken for Epoch 21:3.83 - F1: 0.0138
2026-02-08 22:54:04 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 22:54:04 - INFO - Best F1:0.0741 - Best Epoch:10
2026-02-08 22:54:13 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0597, Test ECE: 0.1397
2026-02-08 22:54:13 - INFO - All results: {'f1_macro': 0.05974745122393003, 'ece': np.float64(0.13970777085080952)}
2026-02-08 22:54:13 - INFO - 
Total time taken: 530.12 seconds
2026-02-08 22:54:14 - INFO - Trial 10 finished with value: 0.05974745122393003 and parameters: {'learning_rate': 0.0002374907647394005, 'weight_decay': 1.0433934476978967e-05, 'batch_size': 32, 'co_train_epochs': 5, 'epoch_patience': 10}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 22:54:14 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 22:54:14 - INFO - Devices: cuda:1, cuda:1
2026-02-08 22:54:14 - INFO - Starting log
2026-02-08 22:54:14 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 22:54:14 - INFO - Learning Rate: 1.0741641700638157e-05
Weight Decay: 5.6619186476013824e-05
Batch Size: 64
No. Epochs: 7
Epoch Patience: 8
 Accumulation Steps: 1
2026-02-08 22:54:15 - INFO - Generating initial weights
2026-02-08 22:54:44 - INFO - Time taken for Epoch 1:25.67 - F1: 0.0557
2026-02-08 22:55:09 - INFO - Time taken for Epoch 2:25.50 - F1: 0.0686
2026-02-08 22:55:35 - INFO - Time taken for Epoch 3:25.44 - F1: 0.0729
2026-02-08 22:56:00 - INFO - Time taken for Epoch 4:25.59 - F1: 0.0776
2026-02-08 22:56:26 - INFO - Time taken for Epoch 5:25.67 - F1: 0.0798
2026-02-08 22:56:51 - INFO - Time taken for Epoch 6:25.63 - F1: 0.0821
2026-02-08 22:57:17 - INFO - Time taken for Epoch 7:25.49 - F1: 0.0818
2026-02-08 22:57:17 - INFO - Best F1:0.0821 - Best Epoch:6
2026-02-08 22:57:18 - INFO - Starting co-training
2026-02-08 22:58:28 - INFO - Time taken for Epoch 1: 69.16s - F1: 0.50653345
2026-02-08 22:59:38 - INFO - Time taken for Epoch 2: 70.36s - F1: 0.54128859
2026-02-08 23:00:48 - INFO - Time taken for Epoch 3: 70.51s - F1: 0.54892644
2026-02-08 23:01:59 - INFO - Time taken for Epoch 4: 70.37s - F1: 0.55175910
2026-02-08 23:03:09 - INFO - Time taken for Epoch 5: 69.88s - F1: 0.57504535
2026-02-08 23:04:19 - INFO - Time taken for Epoch 6: 70.10s - F1: 0.58186028
2026-02-08 23:05:29 - INFO - Time taken for Epoch 7: 70.17s - F1: 0.60280322
2026-02-08 23:05:32 - INFO - Fine-tuning models
2026-02-08 23:05:36 - INFO - Time taken for Epoch 1:3.55 - F1: 0.6109
2026-02-08 23:05:40 - INFO - Time taken for Epoch 2:4.32 - F1: 0.6159
2026-02-08 23:05:45 - INFO - Time taken for Epoch 3:4.49 - F1: 0.6264
2026-02-08 23:05:49 - INFO - Time taken for Epoch 4:4.48 - F1: 0.6327
2026-02-08 23:05:54 - INFO - Time taken for Epoch 5:4.47 - F1: 0.6324
2026-02-08 23:05:57 - INFO - Time taken for Epoch 6:3.51 - F1: 0.6325
2026-02-08 23:06:01 - INFO - Time taken for Epoch 7:3.52 - F1: 0.6384
2026-02-08 23:06:05 - INFO - Time taken for Epoch 8:4.56 - F1: 0.6317
2026-02-08 23:06:09 - INFO - Time taken for Epoch 9:3.53 - F1: 0.6312
2026-02-08 23:06:13 - INFO - Time taken for Epoch 10:3.52 - F1: 0.6368
2026-02-08 23:06:16 - INFO - Time taken for Epoch 11:3.52 - F1: 0.6395
2026-02-08 23:06:26 - INFO - Time taken for Epoch 12:10.25 - F1: 0.6438
2026-02-08 23:06:31 - INFO - Time taken for Epoch 13:4.60 - F1: 0.6407
2026-02-08 23:06:34 - INFO - Time taken for Epoch 14:3.51 - F1: 0.6340
2026-02-08 23:06:38 - INFO - Time taken for Epoch 15:3.51 - F1: 0.6369
2026-02-08 23:06:41 - INFO - Time taken for Epoch 16:3.51 - F1: 0.6366
2026-02-08 23:06:45 - INFO - Time taken for Epoch 17:3.51 - F1: 0.6317
2026-02-08 23:06:48 - INFO - Time taken for Epoch 18:3.51 - F1: 0.6311
2026-02-08 23:06:52 - INFO - Time taken for Epoch 19:3.51 - F1: 0.6315
2026-02-08 23:06:55 - INFO - Time taken for Epoch 20:3.51 - F1: 0.6295
2026-02-08 23:06:59 - INFO - Time taken for Epoch 21:3.51 - F1: 0.6315
2026-02-08 23:07:02 - INFO - Time taken for Epoch 22:3.52 - F1: 0.6316
2026-02-08 23:07:02 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 23:07:02 - INFO - Best F1:0.6438 - Best Epoch:11
2026-02-08 23:07:11 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6552, Test ECE: 0.0468
2026-02-08 23:07:11 - INFO - All results: {'f1_macro': 0.6552292732732043, 'ece': np.float64(0.046787488729433904)}
2026-02-08 23:07:11 - INFO - 
Total time taken: 777.40 seconds
2026-02-08 23:07:11 - INFO - Trial 11 finished with value: 0.6552292732732043 and parameters: {'learning_rate': 1.0741641700638157e-05, 'weight_decay': 5.6619186476013824e-05, 'batch_size': 64, 'co_train_epochs': 7, 'epoch_patience': 8}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 23:07:11 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 23:07:11 - INFO - Devices: cuda:1, cuda:1
2026-02-08 23:07:11 - INFO - Starting log
2026-02-08 23:07:11 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 23:07:11 - INFO - Learning Rate: 3.151372428999275e-05
Weight Decay: 6.5009942982046e-05
Batch Size: 64
No. Epochs: 6
Epoch Patience: 8
 Accumulation Steps: 1
2026-02-08 23:07:13 - INFO - Generating initial weights
2026-02-08 23:07:41 - INFO - Time taken for Epoch 1:25.73 - F1: 0.0711
2026-02-08 23:08:07 - INFO - Time taken for Epoch 2:25.62 - F1: 0.1012
2026-02-08 23:08:32 - INFO - Time taken for Epoch 3:25.43 - F1: 0.1237
2026-02-08 23:08:58 - INFO - Time taken for Epoch 4:25.37 - F1: 0.1584
2026-02-08 23:09:23 - INFO - Time taken for Epoch 5:25.33 - F1: 0.1570
2026-02-08 23:09:48 - INFO - Time taken for Epoch 6:25.49 - F1: 0.1535
2026-02-08 23:09:48 - INFO - Best F1:0.1584 - Best Epoch:4
2026-02-08 23:09:49 - INFO - Starting co-training
2026-02-08 23:10:59 - INFO - Time taken for Epoch 1: 68.92s - F1: 0.55439686
2026-02-08 23:12:09 - INFO - Time taken for Epoch 2: 69.97s - F1: 0.57159392
2026-02-08 23:13:19 - INFO - Time taken for Epoch 3: 70.12s - F1: 0.61069481
2026-02-08 23:14:29 - INFO - Time taken for Epoch 4: 69.84s - F1: 0.62086041
2026-02-08 23:15:39 - INFO - Time taken for Epoch 5: 70.26s - F1: 0.61174909
2026-02-08 23:16:48 - INFO - Time taken for Epoch 6: 69.21s - F1: 0.61018361
2026-02-08 23:16:50 - INFO - Fine-tuning models
2026-02-08 23:16:54 - INFO - Time taken for Epoch 1:3.56 - F1: 0.6184
2026-02-08 23:16:58 - INFO - Time taken for Epoch 2:4.40 - F1: 0.6153
2026-02-08 23:17:02 - INFO - Time taken for Epoch 3:3.51 - F1: 0.6208
2026-02-08 23:17:06 - INFO - Time taken for Epoch 4:4.61 - F1: 0.6253
2026-02-08 23:18:58 - INFO - Time taken for Epoch 5:111.46 - F1: 0.6246
2026-02-08 23:19:01 - INFO - Time taken for Epoch 6:3.51 - F1: 0.6332
2026-02-08 23:19:06 - INFO - Time taken for Epoch 7:4.38 - F1: 0.6307
2026-02-08 23:19:09 - INFO - Time taken for Epoch 8:3.49 - F1: 0.6336
2026-02-08 23:19:14 - INFO - Time taken for Epoch 9:4.64 - F1: 0.6369
2026-02-08 23:19:19 - INFO - Time taken for Epoch 10:4.71 - F1: 0.6384
2026-02-08 23:19:23 - INFO - Time taken for Epoch 11:4.54 - F1: 0.6385
2026-02-08 23:19:28 - INFO - Time taken for Epoch 12:4.50 - F1: 0.6371
2026-02-08 23:19:31 - INFO - Time taken for Epoch 13:3.51 - F1: 0.6378
2026-02-08 23:19:35 - INFO - Time taken for Epoch 14:3.50 - F1: 0.6368
2026-02-08 23:19:38 - INFO - Time taken for Epoch 15:3.51 - F1: 0.6355
2026-02-08 23:19:42 - INFO - Time taken for Epoch 16:3.51 - F1: 0.6362
2026-02-08 23:19:45 - INFO - Time taken for Epoch 17:3.51 - F1: 0.6353
2026-02-08 23:19:49 - INFO - Time taken for Epoch 18:3.51 - F1: 0.6351
2026-02-08 23:19:52 - INFO - Time taken for Epoch 19:3.52 - F1: 0.6364
2026-02-08 23:19:56 - INFO - Time taken for Epoch 20:3.51 - F1: 0.6361
2026-02-08 23:19:59 - INFO - Time taken for Epoch 21:3.51 - F1: 0.6348
2026-02-08 23:19:59 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 23:19:59 - INFO - Best F1:0.6385 - Best Epoch:10
2026-02-08 23:20:08 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6568, Test ECE: 0.0447
2026-02-08 23:20:08 - INFO - All results: {'f1_macro': 0.656842557932108, 'ece': np.float64(0.044677735207288266)}
2026-02-08 23:20:08 - INFO - 
Total time taken: 776.74 seconds
2026-02-08 23:20:08 - INFO - Trial 12 finished with value: 0.656842557932108 and parameters: {'learning_rate': 3.151372428999275e-05, 'weight_decay': 6.5009942982046e-05, 'batch_size': 64, 'co_train_epochs': 6, 'epoch_patience': 8}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 23:20:08 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 23:20:08 - INFO - Devices: cuda:1, cuda:1
2026-02-08 23:20:08 - INFO - Starting log
2026-02-08 23:20:08 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 23:20:08 - INFO - Learning Rate: 3.4063504529995076e-05
Weight Decay: 8.975530070415313e-05
Batch Size: 64
No. Epochs: 16
Epoch Patience: 8
 Accumulation Steps: 1
2026-02-08 23:20:09 - INFO - Generating initial weights
2026-02-08 23:20:38 - INFO - Time taken for Epoch 1:25.59 - F1: 0.0725
2026-02-08 23:21:03 - INFO - Time taken for Epoch 2:25.43 - F1: 0.1006
2026-02-08 23:21:29 - INFO - Time taken for Epoch 3:25.57 - F1: 0.1557
2026-02-08 23:21:54 - INFO - Time taken for Epoch 4:25.56 - F1: 0.1460
2026-02-08 23:22:20 - INFO - Time taken for Epoch 5:25.60 - F1: 0.1368
2026-02-08 23:22:45 - INFO - Time taken for Epoch 6:25.56 - F1: 0.1426
2026-02-08 23:23:11 - INFO - Time taken for Epoch 7:25.52 - F1: 0.1473
2026-02-08 23:23:36 - INFO - Time taken for Epoch 8:25.41 - F1: 0.1451
2026-02-08 23:24:02 - INFO - Time taken for Epoch 9:25.29 - F1: 0.1495
2026-02-08 23:24:27 - INFO - Time taken for Epoch 10:25.30 - F1: 0.1574
2026-02-08 23:24:52 - INFO - Time taken for Epoch 11:25.40 - F1: 0.1675
2026-02-08 23:25:18 - INFO - Time taken for Epoch 12:25.44 - F1: 0.1678
2026-02-08 23:25:43 - INFO - Time taken for Epoch 13:25.41 - F1: 0.1726
2026-02-08 23:26:09 - INFO - Time taken for Epoch 14:25.44 - F1: 0.1768
2026-02-08 23:26:34 - INFO - Time taken for Epoch 15:25.52 - F1: 0.1807
2026-02-08 23:26:59 - INFO - Time taken for Epoch 16:25.40 - F1: 0.1866
2026-02-08 23:26:59 - INFO - Best F1:0.1866 - Best Epoch:16
2026-02-08 23:27:01 - INFO - Starting co-training
2026-02-08 23:28:10 - INFO - Time taken for Epoch 1: 69.30s - F1: 0.54843644
2026-02-08 23:29:20 - INFO - Time taken for Epoch 2: 70.17s - F1: 0.58232138
2026-02-08 23:30:31 - INFO - Time taken for Epoch 3: 70.62s - F1: 0.60202491
2026-02-08 23:31:41 - INFO - Time taken for Epoch 4: 70.39s - F1: 0.62163975
2026-02-08 23:32:51 - INFO - Time taken for Epoch 5: 69.78s - F1: 0.61119310
2026-02-08 23:34:00 - INFO - Time taken for Epoch 6: 69.13s - F1: 0.61812284
2026-02-08 23:35:09 - INFO - Time taken for Epoch 7: 69.07s - F1: 0.61093587
2026-02-08 23:36:18 - INFO - Time taken for Epoch 8: 68.88s - F1: 0.62645358
2026-02-08 23:37:28 - INFO - Time taken for Epoch 9: 69.67s - F1: 0.60887377
2026-02-08 23:38:38 - INFO - Time taken for Epoch 10: 69.67s - F1: 0.61401620
2026-02-08 23:39:47 - INFO - Time taken for Epoch 11: 69.74s - F1: 0.61171720
2026-02-08 23:40:57 - INFO - Time taken for Epoch 12: 69.40s - F1: 0.61244818
2026-02-08 23:42:06 - INFO - Time taken for Epoch 13: 69.40s - F1: 0.61736517
2026-02-08 23:43:15 - INFO - Time taken for Epoch 14: 69.06s - F1: 0.62333051
2026-02-08 23:44:24 - INFO - Time taken for Epoch 15: 69.11s - F1: 0.61686666
2026-02-08 23:45:33 - INFO - Time taken for Epoch 16: 68.92s - F1: 0.61828251
2026-02-08 23:45:36 - INFO - Fine-tuning models
2026-02-08 23:45:39 - INFO - Time taken for Epoch 1:3.57 - F1: 0.6095
2026-02-08 23:45:44 - INFO - Time taken for Epoch 2:4.55 - F1: 0.6160
2026-02-08 23:45:48 - INFO - Time taken for Epoch 3:4.57 - F1: 0.6224
2026-02-08 23:45:53 - INFO - Time taken for Epoch 4:4.59 - F1: 0.6268
2026-02-08 23:45:58 - INFO - Time taken for Epoch 5:4.66 - F1: 0.6285
2026-02-08 23:46:02 - INFO - Time taken for Epoch 6:4.77 - F1: 0.6257
2026-02-08 23:46:06 - INFO - Time taken for Epoch 7:3.51 - F1: 0.6217
2026-02-08 23:46:10 - INFO - Time taken for Epoch 8:3.52 - F1: 0.6225
2026-02-08 23:46:13 - INFO - Time taken for Epoch 9:3.52 - F1: 0.6212
2026-02-08 23:46:17 - INFO - Time taken for Epoch 10:3.52 - F1: 0.6185
2026-02-08 23:46:20 - INFO - Time taken for Epoch 11:3.52 - F1: 0.6186
2026-02-08 23:46:24 - INFO - Time taken for Epoch 12:3.51 - F1: 0.6209
2026-02-08 23:46:27 - INFO - Time taken for Epoch 13:3.52 - F1: 0.6198
2026-02-08 23:46:31 - INFO - Time taken for Epoch 14:3.51 - F1: 0.6224
2026-02-08 23:46:34 - INFO - Time taken for Epoch 15:3.52 - F1: 0.6208
2026-02-08 23:46:34 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-08 23:46:34 - INFO - Best F1:0.6285 - Best Epoch:4
2026-02-08 23:46:43 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6427, Test ECE: 0.0314
2026-02-08 23:46:43 - INFO - All results: {'f1_macro': 0.6426853781101527, 'ece': np.float64(0.0313858161295267)}
2026-02-08 23:46:43 - INFO - 
Total time taken: 1594.99 seconds
2026-02-08 23:46:43 - INFO - Trial 13 finished with value: 0.6426853781101527 and parameters: {'learning_rate': 3.4063504529995076e-05, 'weight_decay': 8.975530070415313e-05, 'batch_size': 64, 'co_train_epochs': 16, 'epoch_patience': 8}. Best is trial 4 with value: 0.6572275378744384.
2026-02-08 23:46:43 - INFO - Using devices: cuda:1, cuda:1
2026-02-08 23:46:43 - INFO - Devices: cuda:1, cuda:1
2026-02-08 23:46:43 - INFO - Starting log
2026-02-08 23:46:43 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-08 23:46:43 - INFO - Learning Rate: 3.0633867483279684e-05
Weight Decay: 0.00010400931529602136
Batch Size: 64
No. Epochs: 8
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-08 23:46:44 - INFO - Generating initial weights
2026-02-08 23:47:13 - INFO - Time taken for Epoch 1:25.59 - F1: 0.0717
2026-02-08 23:47:38 - INFO - Time taken for Epoch 2:25.24 - F1: 0.0980
2026-02-08 23:48:03 - INFO - Time taken for Epoch 3:25.40 - F1: 0.1256
2026-02-08 23:48:29 - INFO - Time taken for Epoch 4:25.69 - F1: 0.1597
2026-02-08 23:48:55 - INFO - Time taken for Epoch 5:25.81 - F1: 0.1519
2026-02-08 23:49:21 - INFO - Time taken for Epoch 6:25.79 - F1: 0.1483
2026-02-08 23:49:46 - INFO - Time taken for Epoch 7:25.67 - F1: 0.1447
2026-02-08 23:50:12 - INFO - Time taken for Epoch 8:25.72 - F1: 0.1460
2026-02-08 23:50:12 - INFO - Best F1:0.1597 - Best Epoch:4
2026-02-08 23:50:13 - INFO - Starting co-training
2026-02-08 23:51:23 - INFO - Time taken for Epoch 1: 69.40s - F1: 0.55540930
2026-02-08 23:52:34 - INFO - Time taken for Epoch 2: 70.71s - F1: 0.57387995
2026-02-08 23:53:45 - INFO - Time taken for Epoch 3: 71.39s - F1: 0.62864175
2026-02-08 23:54:55 - INFO - Time taken for Epoch 4: 70.39s - F1: 0.62468595
2026-02-08 23:56:05 - INFO - Time taken for Epoch 5: 69.56s - F1: 0.62324902
2026-02-08 23:57:15 - INFO - Time taken for Epoch 6: 69.64s - F1: 0.62290299
2026-02-08 23:58:24 - INFO - Time taken for Epoch 7: 69.83s - F1: 0.60777225
2026-02-08 23:59:34 - INFO - Time taken for Epoch 8: 69.55s - F1: 0.61727573
2026-02-08 23:59:36 - INFO - Fine-tuning models
2026-02-08 23:59:40 - INFO - Time taken for Epoch 1:3.57 - F1: 0.6289
2026-02-08 23:59:44 - INFO - Time taken for Epoch 2:4.54 - F1: 0.6250
2026-02-08 23:59:48 - INFO - Time taken for Epoch 3:3.52 - F1: 0.6341
2026-02-08 23:59:53 - INFO - Time taken for Epoch 4:4.64 - F1: 0.6408
2026-02-08 23:59:57 - INFO - Time taken for Epoch 5:4.68 - F1: 0.6408
2026-02-09 00:00:02 - INFO - Time taken for Epoch 6:4.61 - F1: 0.6382
2026-02-09 00:00:05 - INFO - Time taken for Epoch 7:3.51 - F1: 0.6376
2026-02-09 00:00:09 - INFO - Time taken for Epoch 8:3.52 - F1: 0.6388
2026-02-09 00:00:12 - INFO - Time taken for Epoch 9:3.52 - F1: 0.6416
2026-02-09 00:00:17 - INFO - Time taken for Epoch 10:4.60 - F1: 0.6379
2026-02-09 00:00:21 - INFO - Time taken for Epoch 11:3.52 - F1: 0.6356
2026-02-09 00:00:24 - INFO - Time taken for Epoch 12:3.52 - F1: 0.6394
2026-02-09 00:00:28 - INFO - Time taken for Epoch 13:3.51 - F1: 0.6392
2026-02-09 00:00:31 - INFO - Time taken for Epoch 14:3.51 - F1: 0.6425
2026-02-09 00:00:36 - INFO - Time taken for Epoch 15:4.56 - F1: 0.6388
2026-02-09 00:00:39 - INFO - Time taken for Epoch 16:3.52 - F1: 0.6388
2026-02-09 00:00:43 - INFO - Time taken for Epoch 17:3.51 - F1: 0.6405
2026-02-09 00:00:46 - INFO - Time taken for Epoch 18:3.51 - F1: 0.6416
2026-02-09 00:00:50 - INFO - Time taken for Epoch 19:3.51 - F1: 0.6429
2026-02-09 00:00:54 - INFO - Time taken for Epoch 20:4.59 - F1: 0.6416
2026-02-09 00:00:58 - INFO - Time taken for Epoch 21:3.52 - F1: 0.6416
2026-02-09 00:01:01 - INFO - Time taken for Epoch 22:3.52 - F1: 0.6423
2026-02-09 00:01:05 - INFO - Time taken for Epoch 23:3.52 - F1: 0.6455
2026-02-09 00:01:10 - INFO - Time taken for Epoch 24:4.78 - F1: 0.6475
2026-02-09 00:01:20 - INFO - Time taken for Epoch 25:10.38 - F1: 0.6460
2026-02-09 00:01:24 - INFO - Time taken for Epoch 26:3.52 - F1: 0.6450
2026-02-09 00:01:27 - INFO - Time taken for Epoch 27:3.52 - F1: 0.6458
2026-02-09 00:01:31 - INFO - Time taken for Epoch 28:3.52 - F1: 0.6467
2026-02-09 00:01:34 - INFO - Time taken for Epoch 29:3.51 - F1: 0.6459
2026-02-09 00:01:38 - INFO - Time taken for Epoch 30:3.52 - F1: 0.6453
2026-02-09 00:01:41 - INFO - Time taken for Epoch 31:3.51 - F1: 0.6453
2026-02-09 00:01:45 - INFO - Time taken for Epoch 32:3.52 - F1: 0.6453
2026-02-09 00:01:48 - INFO - Time taken for Epoch 33:3.52 - F1: 0.6459
2026-02-09 00:01:52 - INFO - Time taken for Epoch 34:3.52 - F1: 0.6459
2026-02-09 00:01:52 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 00:01:52 - INFO - Best F1:0.6475 - Best Epoch:23
2026-02-09 00:02:00 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6690, Test ECE: 0.0470
2026-02-09 00:02:00 - INFO - All results: {'f1_macro': 0.6690158334893085, 'ece': np.float64(0.04701756899646222)}
2026-02-09 00:02:00 - INFO - 
Total time taken: 917.37 seconds
2026-02-09 00:02:00 - INFO - Trial 14 finished with value: 0.6690158334893085 and parameters: {'learning_rate': 3.0633867483279684e-05, 'weight_decay': 0.00010400931529602136, 'batch_size': 64, 'co_train_epochs': 8, 'epoch_patience': 10}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 00:02:00 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 00:02:00 - INFO - Devices: cuda:1, cuda:1
2026-02-09 00:02:00 - INFO - Starting log
2026-02-09 00:02:00 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 00:02:01 - INFO - Learning Rate: 9.629050115312256e-05
Weight Decay: 1.1332616832321805e-05
Batch Size: 32
No. Epochs: 8
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-09 00:02:02 - INFO - Generating initial weights
2026-02-09 00:02:33 - INFO - Time taken for Epoch 1:28.35 - F1: 0.1274
2026-02-09 00:03:01 - INFO - Time taken for Epoch 2:28.48 - F1: 0.1233
2026-02-09 00:03:30 - INFO - Time taken for Epoch 3:28.33 - F1: 0.1195
2026-02-09 00:03:58 - INFO - Time taken for Epoch 4:28.21 - F1: 0.1447
2026-02-09 00:04:26 - INFO - Time taken for Epoch 5:27.98 - F1: 0.1890
2026-02-09 00:04:54 - INFO - Time taken for Epoch 6:28.44 - F1: 0.2177
2026-02-09 00:05:23 - INFO - Time taken for Epoch 7:28.38 - F1: 0.2470
2026-02-09 00:05:51 - INFO - Time taken for Epoch 8:28.44 - F1: 0.2618
2026-02-09 00:05:51 - INFO - Best F1:0.2618 - Best Epoch:8
2026-02-09 00:05:52 - INFO - Starting co-training
2026-02-09 00:06:49 - INFO - Time taken for Epoch 1: 56.37s - F1: 0.56455310
2026-02-09 00:07:46 - INFO - Time taken for Epoch 2: 57.16s - F1: 0.52382638
2026-02-09 00:08:42 - INFO - Time taken for Epoch 3: 55.94s - F1: 0.60313049
2026-02-09 00:09:39 - INFO - Time taken for Epoch 4: 56.63s - F1: 0.61608455
2026-02-09 00:10:35 - INFO - Time taken for Epoch 5: 56.65s - F1: 0.58062823
2026-02-09 00:11:31 - INFO - Time taken for Epoch 6: 56.07s - F1: 0.60987355
2026-02-09 00:12:27 - INFO - Time taken for Epoch 7: 55.90s - F1: 0.59434587
2026-02-09 00:13:23 - INFO - Time taken for Epoch 8: 55.94s - F1: 0.60328116
2026-02-09 00:13:25 - INFO - Fine-tuning models
2026-02-09 00:13:29 - INFO - Time taken for Epoch 1:3.88 - F1: 0.6188
2026-02-09 00:13:34 - INFO - Time taken for Epoch 2:4.86 - F1: 0.6127
2026-02-09 00:13:38 - INFO - Time taken for Epoch 3:3.83 - F1: 0.6126
2026-02-09 00:13:42 - INFO - Time taken for Epoch 4:3.85 - F1: 0.6049
2026-02-09 00:13:46 - INFO - Time taken for Epoch 5:3.84 - F1: 0.5998
2026-02-09 00:13:50 - INFO - Time taken for Epoch 6:3.84 - F1: 0.6004
2026-02-09 00:13:53 - INFO - Time taken for Epoch 7:3.83 - F1: 0.6014
2026-02-09 00:13:57 - INFO - Time taken for Epoch 8:3.84 - F1: 0.6020
2026-02-09 00:14:01 - INFO - Time taken for Epoch 9:3.83 - F1: 0.6039
2026-02-09 00:14:05 - INFO - Time taken for Epoch 10:3.84 - F1: 0.6041
2026-02-09 00:14:09 - INFO - Time taken for Epoch 11:3.85 - F1: 0.6078
2026-02-09 00:14:09 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 00:14:09 - INFO - Best F1:0.6188 - Best Epoch:0
2026-02-09 00:14:18 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6337, Test ECE: 0.0446
2026-02-09 00:14:18 - INFO - All results: {'f1_macro': 0.6336611701559716, 'ece': np.float64(0.04461148112340081)}
2026-02-09 00:14:18 - INFO - 
Total time taken: 737.82 seconds
2026-02-09 00:14:18 - INFO - Trial 15 finished with value: 0.6336611701559716 and parameters: {'learning_rate': 9.629050115312256e-05, 'weight_decay': 1.1332616832321805e-05, 'batch_size': 32, 'co_train_epochs': 8, 'epoch_patience': 10}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 00:14:18 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 00:14:18 - INFO - Devices: cuda:1, cuda:1
2026-02-09 00:14:18 - INFO - Starting log
2026-02-09 00:14:18 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 00:14:18 - INFO - Learning Rate: 0.0002932547571987407
Weight Decay: 0.00011019620451053202
Batch Size: 64
No. Epochs: 14
Epoch Patience: 7
 Accumulation Steps: 1
2026-02-09 00:14:19 - INFO - Generating initial weights
2026-02-09 00:14:48 - INFO - Time taken for Epoch 1:25.65 - F1: 0.0557
2026-02-09 00:15:13 - INFO - Time taken for Epoch 2:25.50 - F1: 0.0512
2026-02-09 00:15:39 - INFO - Time taken for Epoch 3:25.63 - F1: 0.2500
2026-02-09 00:16:05 - INFO - Time taken for Epoch 4:25.67 - F1: 0.2903
2026-02-09 00:16:30 - INFO - Time taken for Epoch 5:25.75 - F1: 0.3503
2026-02-09 00:16:56 - INFO - Time taken for Epoch 6:25.75 - F1: 0.3660
2026-02-09 00:17:22 - INFO - Time taken for Epoch 7:25.70 - F1: 0.3440
2026-02-09 00:17:48 - INFO - Time taken for Epoch 8:25.58 - F1: 0.3320
2026-02-09 00:18:13 - INFO - Time taken for Epoch 9:25.53 - F1: 0.3415
2026-02-09 00:18:39 - INFO - Time taken for Epoch 10:25.46 - F1: 0.3603
2026-02-09 00:19:04 - INFO - Time taken for Epoch 11:25.55 - F1: 0.3644
2026-02-09 00:19:30 - INFO - Time taken for Epoch 12:25.62 - F1: 0.3677
2026-02-09 00:19:55 - INFO - Time taken for Epoch 13:25.30 - F1: 0.3622
2026-02-09 00:20:21 - INFO - Time taken for Epoch 14:25.50 - F1: 0.3609
2026-02-09 00:20:21 - INFO - Best F1:0.3677 - Best Epoch:12
2026-02-09 00:20:22 - INFO - Starting co-training
2026-02-09 00:21:32 - INFO - Time taken for Epoch 1: 69.35s - F1: 0.03710145
2026-02-09 00:22:42 - INFO - Time taken for Epoch 2: 70.04s - F1: 0.03710145
2026-02-09 00:23:51 - INFO - Time taken for Epoch 3: 69.08s - F1: 0.03710145
2026-02-09 00:25:00 - INFO - Time taken for Epoch 4: 69.25s - F1: 0.03710145
2026-02-09 00:26:09 - INFO - Time taken for Epoch 5: 69.33s - F1: 0.03710145
2026-02-09 00:27:19 - INFO - Time taken for Epoch 6: 69.39s - F1: 0.03710145
2026-02-09 00:28:28 - INFO - Time taken for Epoch 7: 69.51s - F1: 0.03710145
2026-02-09 00:29:38 - INFO - Time taken for Epoch 8: 69.49s - F1: 0.03710145
2026-02-09 00:29:38 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-09 00:29:40 - INFO - Fine-tuning models
2026-02-09 00:29:44 - INFO - Time taken for Epoch 1:3.58 - F1: 0.0371
2026-02-09 00:29:49 - INFO - Time taken for Epoch 2:5.17 - F1: 0.0371
2026-02-09 00:29:52 - INFO - Time taken for Epoch 3:3.51 - F1: 0.0371
2026-02-09 00:29:56 - INFO - Time taken for Epoch 4:3.52 - F1: 0.0137
2026-02-09 00:29:59 - INFO - Time taken for Epoch 5:3.52 - F1: 0.0030
2026-02-09 00:30:03 - INFO - Time taken for Epoch 6:3.51 - F1: 0.0030
2026-02-09 00:30:06 - INFO - Time taken for Epoch 7:3.53 - F1: 0.0030
2026-02-09 00:30:10 - INFO - Time taken for Epoch 8:3.52 - F1: 0.0030
2026-02-09 00:30:14 - INFO - Time taken for Epoch 9:3.54 - F1: 0.0030
2026-02-09 00:30:17 - INFO - Time taken for Epoch 10:3.52 - F1: 0.0030
2026-02-09 00:30:21 - INFO - Time taken for Epoch 11:3.52 - F1: 0.0165
2026-02-09 00:30:21 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 00:30:21 - INFO - Best F1:0.0371 - Best Epoch:0
2026-02-09 00:30:29 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0370, Test ECE: 0.2195
2026-02-09 00:30:29 - INFO - All results: {'f1_macro': 0.037003879438973444, 'ece': np.float64(0.21949232682569714)}
2026-02-09 00:30:29 - INFO - 
Total time taken: 971.06 seconds
2026-02-09 00:30:29 - INFO - Trial 16 finished with value: 0.037003879438973444 and parameters: {'learning_rate': 0.0002932547571987407, 'weight_decay': 0.00011019620451053202, 'batch_size': 64, 'co_train_epochs': 14, 'epoch_patience': 7}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 00:30:29 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 00:30:29 - INFO - Devices: cuda:1, cuda:1
2026-02-09 00:30:29 - INFO - Starting log
2026-02-09 00:30:29 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 00:30:29 - INFO - Learning Rate: 5.9508601847471105e-05
Weight Decay: 2.6792060974064106e-05
Batch Size: 64
No. Epochs: 8
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-09 00:30:30 - INFO - Generating initial weights
2026-02-09 00:30:59 - INFO - Time taken for Epoch 1:25.83 - F1: 0.0847
2026-02-09 00:31:25 - INFO - Time taken for Epoch 2:25.80 - F1: 0.1665
2026-02-09 00:31:50 - INFO - Time taken for Epoch 3:25.52 - F1: 0.1426
2026-02-09 00:32:16 - INFO - Time taken for Epoch 4:25.63 - F1: 0.1280
2026-02-09 00:32:41 - INFO - Time taken for Epoch 5:25.46 - F1: 0.1302
2026-02-09 00:33:07 - INFO - Time taken for Epoch 6:25.73 - F1: 0.1346
2026-02-09 00:33:33 - INFO - Time taken for Epoch 7:25.79 - F1: 0.1367
2026-02-09 00:33:59 - INFO - Time taken for Epoch 8:25.72 - F1: 0.1451
2026-02-09 00:33:59 - INFO - Best F1:0.1665 - Best Epoch:2
2026-02-09 00:34:00 - INFO - Starting co-training
2026-02-09 00:35:10 - INFO - Time taken for Epoch 1: 69.58s - F1: 0.61753103
2026-02-09 00:36:20 - INFO - Time taken for Epoch 2: 70.45s - F1: 0.60648049
2026-02-09 00:37:29 - INFO - Time taken for Epoch 3: 69.37s - F1: 0.61959003
2026-02-09 00:38:40 - INFO - Time taken for Epoch 4: 70.71s - F1: 0.63306446
2026-02-09 00:39:51 - INFO - Time taken for Epoch 5: 70.72s - F1: 0.62254677
2026-02-09 00:41:01 - INFO - Time taken for Epoch 6: 69.72s - F1: 0.61553055
2026-02-09 00:42:10 - INFO - Time taken for Epoch 7: 69.33s - F1: 0.61433079
2026-02-09 00:43:19 - INFO - Time taken for Epoch 8: 69.32s - F1: 0.60843492
2026-02-09 00:43:21 - INFO - Fine-tuning models
2026-02-09 00:43:25 - INFO - Time taken for Epoch 1:3.57 - F1: 0.6381
2026-02-09 00:43:29 - INFO - Time taken for Epoch 2:4.29 - F1: 0.6361
2026-02-09 00:43:33 - INFO - Time taken for Epoch 3:3.52 - F1: 0.6327
2026-02-09 00:43:36 - INFO - Time taken for Epoch 4:3.52 - F1: 0.6316
2026-02-09 00:43:40 - INFO - Time taken for Epoch 5:3.52 - F1: 0.6277
2026-02-09 00:43:43 - INFO - Time taken for Epoch 6:3.52 - F1: 0.6262
2026-02-09 00:43:47 - INFO - Time taken for Epoch 7:3.52 - F1: 0.6365
2026-02-09 00:43:50 - INFO - Time taken for Epoch 8:3.53 - F1: 0.6374
2026-02-09 00:43:54 - INFO - Time taken for Epoch 9:3.52 - F1: 0.6379
2026-02-09 00:43:58 - INFO - Time taken for Epoch 10:3.53 - F1: 0.6397
2026-02-09 00:44:10 - INFO - Time taken for Epoch 11:12.51 - F1: 0.6382
2026-02-09 00:44:14 - INFO - Time taken for Epoch 12:3.51 - F1: 0.6378
2026-02-09 00:44:17 - INFO - Time taken for Epoch 13:3.52 - F1: 0.6374
2026-02-09 00:44:21 - INFO - Time taken for Epoch 14:3.52 - F1: 0.6384
2026-02-09 00:44:24 - INFO - Time taken for Epoch 15:3.52 - F1: 0.6378
2026-02-09 00:44:28 - INFO - Time taken for Epoch 16:3.52 - F1: 0.6399
2026-02-09 00:44:32 - INFO - Time taken for Epoch 17:4.35 - F1: 0.6456
2026-02-09 00:44:36 - INFO - Time taken for Epoch 18:4.40 - F1: 0.6459
2026-02-09 00:44:41 - INFO - Time taken for Epoch 19:4.46 - F1: 0.6428
2026-02-09 00:44:44 - INFO - Time taken for Epoch 20:3.53 - F1: 0.6427
2026-02-09 00:44:48 - INFO - Time taken for Epoch 21:3.53 - F1: 0.6410
2026-02-09 00:44:51 - INFO - Time taken for Epoch 22:3.52 - F1: 0.6426
2026-02-09 00:44:55 - INFO - Time taken for Epoch 23:3.51 - F1: 0.6390
2026-02-09 00:44:58 - INFO - Time taken for Epoch 24:3.52 - F1: 0.6355
2026-02-09 00:45:02 - INFO - Time taken for Epoch 25:3.52 - F1: 0.6353
2026-02-09 00:45:06 - INFO - Time taken for Epoch 26:3.52 - F1: 0.6357
2026-02-09 00:45:09 - INFO - Time taken for Epoch 27:3.52 - F1: 0.6351
2026-02-09 00:45:13 - INFO - Time taken for Epoch 28:3.52 - F1: 0.6352
2026-02-09 00:45:13 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 00:45:13 - INFO - Best F1:0.6459 - Best Epoch:17
2026-02-09 00:45:21 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6515, Test ECE: 0.0426
2026-02-09 00:45:21 - INFO - All results: {'f1_macro': 0.6515358578330108, 'ece': np.float64(0.042559121208211405)}
2026-02-09 00:45:21 - INFO - 
Total time taken: 891.93 seconds
2026-02-09 00:45:21 - INFO - Trial 17 finished with value: 0.6515358578330108 and parameters: {'learning_rate': 5.9508601847471105e-05, 'weight_decay': 2.6792060974064106e-05, 'batch_size': 64, 'co_train_epochs': 8, 'epoch_patience': 10}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 00:45:21 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 00:45:21 - INFO - Devices: cuda:1, cuda:1
2026-02-09 00:45:21 - INFO - Starting log
2026-02-09 00:45:21 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 00:45:21 - INFO - Learning Rate: 0.00013079251234305632
Weight Decay: 0.00017511129097440162
Batch Size: 64
No. Epochs: 14
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-09 00:45:22 - INFO - Generating initial weights
2026-02-09 00:45:51 - INFO - Time taken for Epoch 1:25.74 - F1: 0.1430
2026-02-09 00:46:16 - INFO - Time taken for Epoch 2:25.52 - F1: 0.0706
2026-02-09 00:46:42 - INFO - Time taken for Epoch 3:25.71 - F1: 0.1371
2026-02-09 00:47:08 - INFO - Time taken for Epoch 4:25.56 - F1: 0.1947
2026-02-09 00:47:33 - INFO - Time taken for Epoch 5:25.73 - F1: 0.3019
2026-02-09 00:47:59 - INFO - Time taken for Epoch 6:25.54 - F1: 0.3257
2026-02-09 00:48:25 - INFO - Time taken for Epoch 7:25.73 - F1: 0.3335
2026-02-09 00:48:50 - INFO - Time taken for Epoch 8:25.78 - F1: 0.3417
2026-02-09 00:49:16 - INFO - Time taken for Epoch 9:25.65 - F1: 0.3431
2026-02-09 00:49:42 - INFO - Time taken for Epoch 10:25.63 - F1: 0.3459
2026-02-09 00:50:07 - INFO - Time taken for Epoch 11:25.69 - F1: 0.3492
2026-02-09 00:50:33 - INFO - Time taken for Epoch 12:25.49 - F1: 0.3506
2026-02-09 00:50:59 - INFO - Time taken for Epoch 13:25.61 - F1: 0.3519
2026-02-09 00:51:24 - INFO - Time taken for Epoch 14:25.46 - F1: 0.3548
2026-02-09 00:51:24 - INFO - Best F1:0.3548 - Best Epoch:14
2026-02-09 00:51:25 - INFO - Starting co-training
2026-02-09 00:52:35 - INFO - Time taken for Epoch 1: 69.48s - F1: 0.60778380
2026-02-09 00:53:46 - INFO - Time taken for Epoch 2: 70.59s - F1: 0.61516927
2026-02-09 00:54:56 - INFO - Time taken for Epoch 3: 70.43s - F1: 0.58686173
2026-02-09 00:56:06 - INFO - Time taken for Epoch 4: 69.73s - F1: 0.62397134
2026-02-09 00:57:16 - INFO - Time taken for Epoch 5: 70.54s - F1: 0.59675718
2026-02-09 00:58:26 - INFO - Time taken for Epoch 6: 69.34s - F1: 0.59826832
2026-02-09 00:59:35 - INFO - Time taken for Epoch 7: 69.64s - F1: 0.59963790
2026-02-09 01:00:44 - INFO - Time taken for Epoch 8: 69.09s - F1: 0.59210294
2026-02-09 01:01:54 - INFO - Time taken for Epoch 9: 69.60s - F1: 0.58855659
2026-02-09 01:03:03 - INFO - Time taken for Epoch 10: 68.94s - F1: 0.60484797
2026-02-09 01:04:12 - INFO - Time taken for Epoch 11: 69.23s - F1: 0.60508595
2026-02-09 01:05:21 - INFO - Time taken for Epoch 12: 69.15s - F1: 0.61273251
2026-02-09 01:06:31 - INFO - Time taken for Epoch 13: 69.56s - F1: 0.59593884
2026-02-09 01:06:31 - INFO - Performance not improving for 9 consecutive epochs.
2026-02-09 01:06:47 - INFO - Fine-tuning models
2026-02-09 01:06:50 - INFO - Time taken for Epoch 1:3.56 - F1: 0.6016
2026-02-09 01:06:55 - INFO - Time taken for Epoch 2:4.65 - F1: 0.6337
2026-02-09 01:07:00 - INFO - Time taken for Epoch 3:4.63 - F1: 0.6423
2026-02-09 01:07:04 - INFO - Time taken for Epoch 4:4.58 - F1: 0.6524
2026-02-09 01:07:09 - INFO - Time taken for Epoch 5:4.67 - F1: 0.6532
2026-02-09 01:07:13 - INFO - Time taken for Epoch 6:4.62 - F1: 0.6526
2026-02-09 01:07:17 - INFO - Time taken for Epoch 7:3.53 - F1: 0.6491
2026-02-09 01:07:21 - INFO - Time taken for Epoch 8:3.53 - F1: 0.6462
2026-02-09 01:07:24 - INFO - Time taken for Epoch 9:3.52 - F1: 0.6466
2026-02-09 01:07:28 - INFO - Time taken for Epoch 10:3.52 - F1: 0.6432
2026-02-09 01:07:31 - INFO - Time taken for Epoch 11:3.53 - F1: 0.6433
2026-02-09 01:07:35 - INFO - Time taken for Epoch 12:3.53 - F1: 0.6400
2026-02-09 01:07:38 - INFO - Time taken for Epoch 13:3.52 - F1: 0.6385
2026-02-09 01:07:42 - INFO - Time taken for Epoch 14:3.52 - F1: 0.6385
2026-02-09 01:07:45 - INFO - Time taken for Epoch 15:3.52 - F1: 0.6404
2026-02-09 01:07:45 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 01:07:45 - INFO - Best F1:0.6532 - Best Epoch:4
2026-02-09 01:07:54 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6600, Test ECE: 0.0500
2026-02-09 01:07:54 - INFO - All results: {'f1_macro': 0.659983521594166, 'ece': np.float64(0.05003916813914937)}
2026-02-09 01:07:54 - INFO - 
Total time taken: 1352.76 seconds
2026-02-09 01:07:54 - INFO - Trial 18 finished with value: 0.659983521594166 and parameters: {'learning_rate': 0.00013079251234305632, 'weight_decay': 0.00017511129097440162, 'batch_size': 64, 'co_train_epochs': 14, 'epoch_patience': 9}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 01:07:54 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 01:07:54 - INFO - Devices: cuda:1, cuda:1
2026-02-09 01:07:54 - INFO - Starting log
2026-02-09 01:07:54 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 01:07:54 - INFO - Learning Rate: 0.0004697424366735676
Weight Decay: 0.0004401226422670549
Batch Size: 16
No. Epochs: 17
Epoch Patience: 7
 Accumulation Steps: 4
2026-02-09 01:07:55 - INFO - Generating initial weights
2026-02-09 01:08:30 - INFO - Time taken for Epoch 1:32.13 - F1: 0.0127
2026-02-09 01:09:02 - INFO - Time taken for Epoch 2:32.05 - F1: 0.0127
2026-02-09 01:09:34 - INFO - Time taken for Epoch 3:32.27 - F1: 0.0368
2026-02-09 01:10:07 - INFO - Time taken for Epoch 4:32.26 - F1: 0.0127
2026-02-09 01:10:39 - INFO - Time taken for Epoch 5:31.98 - F1: 0.0481
2026-02-09 01:11:11 - INFO - Time taken for Epoch 6:32.28 - F1: 0.1105
2026-02-09 01:11:43 - INFO - Time taken for Epoch 7:32.17 - F1: 0.0963
2026-02-09 01:12:16 - INFO - Time taken for Epoch 8:32.46 - F1: 0.1948
2026-02-09 01:12:47 - INFO - Time taken for Epoch 9:31.67 - F1: 0.2360
2026-02-09 01:13:19 - INFO - Time taken for Epoch 10:31.90 - F1: 0.2432
2026-02-09 01:13:51 - INFO - Time taken for Epoch 11:31.88 - F1: 0.2364
2026-02-09 01:14:23 - INFO - Time taken for Epoch 12:32.07 - F1: 0.2311
2026-02-09 01:14:56 - INFO - Time taken for Epoch 13:32.44 - F1: 0.2327
2026-02-09 01:15:28 - INFO - Time taken for Epoch 14:32.31 - F1: 0.1960
2026-02-09 01:16:00 - INFO - Time taken for Epoch 15:32.25 - F1: 0.2047
2026-02-09 01:16:32 - INFO - Time taken for Epoch 16:32.31 - F1: 0.2367
2026-02-09 01:17:05 - INFO - Time taken for Epoch 17:32.14 - F1: 0.2401
2026-02-09 01:17:05 - INFO - Best F1:0.2432 - Best Epoch:10
2026-02-09 01:17:06 - INFO - Starting co-training
2026-02-09 01:17:58 - INFO - Time taken for Epoch 1: 51.61s - F1: 0.03710145
2026-02-09 01:18:50 - INFO - Time taken for Epoch 2: 52.38s - F1: 0.03710145
2026-02-09 01:19:42 - INFO - Time taken for Epoch 3: 51.29s - F1: 0.03710145
2026-02-09 01:21:12 - INFO - Time taken for Epoch 4: 90.87s - F1: 0.03710145
2026-02-09 01:23:01 - INFO - Time taken for Epoch 5: 108.90s - F1: 0.03710145
2026-02-09 01:24:50 - INFO - Time taken for Epoch 6: 108.83s - F1: 0.03710145
2026-02-09 01:26:38 - INFO - Time taken for Epoch 7: 108.18s - F1: 0.03710145
2026-02-09 01:28:27 - INFO - Time taken for Epoch 8: 108.33s - F1: 0.03710145
2026-02-09 01:28:27 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-09 01:28:29 - INFO - Fine-tuning models
2026-02-09 01:28:39 - INFO - Time taken for Epoch 1:9.14 - F1: 0.0127
2026-02-09 01:28:49 - INFO - Time taken for Epoch 2:10.33 - F1: 0.0127
2026-02-09 01:28:58 - INFO - Time taken for Epoch 3:9.01 - F1: 0.0127
2026-02-09 01:29:07 - INFO - Time taken for Epoch 4:8.97 - F1: 0.0127
2026-02-09 01:29:16 - INFO - Time taken for Epoch 5:9.01 - F1: 0.0127
2026-02-09 01:29:25 - INFO - Time taken for Epoch 6:8.98 - F1: 0.0127
2026-02-09 01:29:34 - INFO - Time taken for Epoch 7:8.98 - F1: 0.0127
2026-02-09 01:29:43 - INFO - Time taken for Epoch 8:8.98 - F1: 0.0127
2026-02-09 01:29:52 - INFO - Time taken for Epoch 9:8.98 - F1: 0.0127
2026-02-09 01:30:01 - INFO - Time taken for Epoch 10:8.98 - F1: 0.0127
2026-02-09 01:30:10 - INFO - Time taken for Epoch 11:9.01 - F1: 0.0127
2026-02-09 01:30:10 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 01:30:10 - INFO - Best F1:0.0127 - Best Epoch:0
2026-02-09 01:30:28 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0126, Test ECE: 0.5938
2026-02-09 01:30:28 - INFO - All results: {'f1_macro': 0.012608353033884948, 'ece': np.float64(0.5938311549569816)}
2026-02-09 01:30:28 - INFO - 
Total time taken: 1354.06 seconds
2026-02-09 01:30:28 - INFO - Trial 19 finished with value: 0.012608353033884948 and parameters: {'learning_rate': 0.0004697424366735676, 'weight_decay': 0.0004401226422670549, 'batch_size': 16, 'co_train_epochs': 17, 'epoch_patience': 7}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 01:30:28 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 01:30:28 - INFO - Devices: cuda:1, cuda:1
2026-02-09 01:30:28 - INFO - Starting log
2026-02-09 01:30:28 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 01:30:28 - INFO - Learning Rate: 2.4689690494201103e-05
Weight Decay: 0.0025476965083210336
Batch Size: 32
No. Epochs: 9
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-09 01:30:29 - INFO - Generating initial weights
2026-02-09 01:31:21 - INFO - Time taken for Epoch 1:48.32 - F1: 0.0680
2026-02-09 01:32:09 - INFO - Time taken for Epoch 2:48.27 - F1: 0.0886
2026-02-09 01:32:57 - INFO - Time taken for Epoch 3:48.19 - F1: 0.1061
2026-02-09 01:33:45 - INFO - Time taken for Epoch 4:48.10 - F1: 0.1275
2026-02-09 01:34:33 - INFO - Time taken for Epoch 5:47.84 - F1: 0.1603
2026-02-09 01:35:21 - INFO - Time taken for Epoch 6:47.98 - F1: 0.1535
2026-02-09 01:36:09 - INFO - Time taken for Epoch 7:47.87 - F1: 0.1558
2026-02-09 01:36:57 - INFO - Time taken for Epoch 8:48.13 - F1: 0.1581
2026-02-09 01:37:45 - INFO - Time taken for Epoch 9:47.75 - F1: 0.1576
2026-02-09 01:37:45 - INFO - Best F1:0.1603 - Best Epoch:5
2026-02-09 01:37:46 - INFO - Starting co-training
2026-02-09 01:39:28 - INFO - Time taken for Epoch 1: 101.59s - F1: 0.51899417
2026-02-09 01:41:10 - INFO - Time taken for Epoch 2: 102.58s - F1: 0.54105124
2026-02-09 01:42:53 - INFO - Time taken for Epoch 3: 102.32s - F1: 0.58285812
2026-02-09 01:44:35 - INFO - Time taken for Epoch 4: 102.39s - F1: 0.59884736
2026-02-09 01:46:17 - INFO - Time taken for Epoch 5: 102.22s - F1: 0.61983369
2026-02-09 01:48:00 - INFO - Time taken for Epoch 6: 102.54s - F1: 0.60933556
2026-02-09 01:49:41 - INFO - Time taken for Epoch 7: 101.28s - F1: 0.63050705
2026-02-09 01:51:12 - INFO - Time taken for Epoch 8: 91.34s - F1: 0.61674202
2026-02-09 01:52:08 - INFO - Time taken for Epoch 9: 55.89s - F1: 0.61031035
2026-02-09 01:52:11 - INFO - Fine-tuning models
2026-02-09 01:52:15 - INFO - Time taken for Epoch 1:3.90 - F1: 0.6183
2026-02-09 01:52:20 - INFO - Time taken for Epoch 2:4.86 - F1: 0.6209
2026-02-09 01:52:24 - INFO - Time taken for Epoch 3:4.84 - F1: 0.6158
2026-02-09 01:52:28 - INFO - Time taken for Epoch 4:3.85 - F1: 0.6175
2026-02-09 01:52:32 - INFO - Time taken for Epoch 5:3.86 - F1: 0.6125
2026-02-09 01:52:36 - INFO - Time taken for Epoch 6:3.85 - F1: 0.6133
2026-02-09 01:52:40 - INFO - Time taken for Epoch 7:3.85 - F1: 0.6117
2026-02-09 01:52:44 - INFO - Time taken for Epoch 8:3.86 - F1: 0.6085
2026-02-09 01:52:48 - INFO - Time taken for Epoch 9:3.85 - F1: 0.6089
2026-02-09 01:52:51 - INFO - Time taken for Epoch 10:3.87 - F1: 0.6107
2026-02-09 01:52:55 - INFO - Time taken for Epoch 11:3.88 - F1: 0.6122
2026-02-09 01:52:59 - INFO - Time taken for Epoch 12:3.86 - F1: 0.6114
2026-02-09 01:52:59 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 01:52:59 - INFO - Best F1:0.6209 - Best Epoch:1
2026-02-09 01:53:08 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6435, Test ECE: 0.0175
2026-02-09 01:53:08 - INFO - All results: {'f1_macro': 0.6434725659864517, 'ece': np.float64(0.017530257028360502)}
2026-02-09 01:53:08 - INFO - 
Total time taken: 1360.27 seconds
2026-02-09 01:53:08 - INFO - Trial 20 finished with value: 0.6434725659864517 and parameters: {'learning_rate': 2.4689690494201103e-05, 'weight_decay': 0.0025476965083210336, 'batch_size': 32, 'co_train_epochs': 9, 'epoch_patience': 9}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 01:53:08 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 01:53:08 - INFO - Devices: cuda:1, cuda:1
2026-02-09 01:53:08 - INFO - Starting log
2026-02-09 01:53:08 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 01:53:09 - INFO - Learning Rate: 0.00015409262230069756
Weight Decay: 0.0001298599785735877
Batch Size: 64
No. Epochs: 14
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-09 01:53:10 - INFO - Generating initial weights
2026-02-09 01:53:38 - INFO - Time taken for Epoch 1:25.49 - F1: 0.1349
2026-02-09 01:54:04 - INFO - Time taken for Epoch 2:25.61 - F1: 0.0613
2026-02-09 01:54:29 - INFO - Time taken for Epoch 3:25.62 - F1: 0.1611
2026-02-09 01:54:55 - INFO - Time taken for Epoch 4:25.70 - F1: 0.2582
2026-02-09 01:55:20 - INFO - Time taken for Epoch 5:25.52 - F1: 0.3298
2026-02-09 01:55:46 - INFO - Time taken for Epoch 6:25.65 - F1: 0.3459
2026-02-09 01:56:12 - INFO - Time taken for Epoch 7:25.66 - F1: 0.3505
2026-02-09 01:56:37 - INFO - Time taken for Epoch 8:25.53 - F1: 0.3582
2026-02-09 01:57:03 - INFO - Time taken for Epoch 9:25.74 - F1: 0.3460
2026-02-09 01:57:28 - INFO - Time taken for Epoch 10:25.41 - F1: 0.3495
2026-02-09 01:57:54 - INFO - Time taken for Epoch 11:25.43 - F1: 0.3510
2026-02-09 01:58:19 - INFO - Time taken for Epoch 12:25.59 - F1: 0.3526
2026-02-09 01:58:45 - INFO - Time taken for Epoch 13:25.66 - F1: 0.3566
2026-02-09 01:59:11 - INFO - Time taken for Epoch 14:25.57 - F1: 0.3545
2026-02-09 01:59:11 - INFO - Best F1:0.3582 - Best Epoch:8
2026-02-09 01:59:12 - INFO - Starting co-training
2026-02-09 02:00:22 - INFO - Time taken for Epoch 1: 69.40s - F1: 0.61744705
2026-02-09 02:01:32 - INFO - Time taken for Epoch 2: 70.36s - F1: 0.61276251
2026-02-09 02:02:41 - INFO - Time taken for Epoch 3: 69.34s - F1: 0.60114470
2026-02-09 02:03:51 - INFO - Time taken for Epoch 4: 69.36s - F1: 0.59351516
2026-02-09 02:05:00 - INFO - Time taken for Epoch 5: 69.45s - F1: 0.60930145
2026-02-09 02:06:09 - INFO - Time taken for Epoch 6: 69.13s - F1: 0.62156578
2026-02-09 02:07:20 - INFO - Time taken for Epoch 7: 70.36s - F1: 0.60334476
2026-02-09 02:08:29 - INFO - Time taken for Epoch 8: 69.68s - F1: 0.62264010
2026-02-09 02:09:40 - INFO - Time taken for Epoch 9: 70.48s - F1: 0.59625801
2026-02-09 02:10:49 - INFO - Time taken for Epoch 10: 69.57s - F1: 0.61420305
2026-02-09 02:11:59 - INFO - Time taken for Epoch 11: 69.42s - F1: 0.59314878
2026-02-09 02:13:08 - INFO - Time taken for Epoch 12: 69.31s - F1: 0.59659884
2026-02-09 02:14:17 - INFO - Time taken for Epoch 13: 69.28s - F1: 0.60444727
2026-02-09 02:15:26 - INFO - Time taken for Epoch 14: 69.00s - F1: 0.60435806
2026-02-09 02:15:29 - INFO - Fine-tuning models
2026-02-09 02:15:32 - INFO - Time taken for Epoch 1:3.57 - F1: 0.6327
2026-02-09 02:15:37 - INFO - Time taken for Epoch 2:4.47 - F1: 0.6186
2026-02-09 02:15:40 - INFO - Time taken for Epoch 3:3.52 - F1: 0.6274
2026-02-09 02:15:44 - INFO - Time taken for Epoch 4:3.51 - F1: 0.6254
2026-02-09 02:15:47 - INFO - Time taken for Epoch 5:3.51 - F1: 0.6278
2026-02-09 02:15:51 - INFO - Time taken for Epoch 6:3.51 - F1: 0.6233
2026-02-09 02:15:54 - INFO - Time taken for Epoch 7:3.51 - F1: 0.6153
2026-02-09 02:15:58 - INFO - Time taken for Epoch 8:3.52 - F1: 0.6092
2026-02-09 02:16:02 - INFO - Time taken for Epoch 9:3.52 - F1: 0.6067
2026-02-09 02:16:05 - INFO - Time taken for Epoch 10:3.52 - F1: 0.5910
2026-02-09 02:16:09 - INFO - Time taken for Epoch 11:3.57 - F1: 0.5946
2026-02-09 02:16:09 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 02:16:09 - INFO - Best F1:0.6327 - Best Epoch:0
2026-02-09 02:16:17 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6331, Test ECE: 0.0474
2026-02-09 02:16:17 - INFO - All results: {'f1_macro': 0.6330727519551828, 'ece': np.float64(0.04743509591586095)}
2026-02-09 02:16:17 - INFO - 
Total time taken: 1388.93 seconds
2026-02-09 02:16:17 - INFO - Trial 21 finished with value: 0.6330727519551828 and parameters: {'learning_rate': 0.00015409262230069756, 'weight_decay': 0.0001298599785735877, 'batch_size': 64, 'co_train_epochs': 14, 'epoch_patience': 9}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 02:16:17 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 02:16:17 - INFO - Devices: cuda:1, cuda:1
2026-02-09 02:16:17 - INFO - Starting log
2026-02-09 02:16:17 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 02:16:18 - INFO - Learning Rate: 7.090318559921216e-05
Weight Decay: 0.00014831851149792438
Batch Size: 64
No. Epochs: 14
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-09 02:16:19 - INFO - Generating initial weights
2026-02-09 02:16:47 - INFO - Time taken for Epoch 1:25.61 - F1: 0.0828
2026-02-09 02:17:13 - INFO - Time taken for Epoch 2:25.60 - F1: 0.1549
2026-02-09 02:17:38 - INFO - Time taken for Epoch 3:25.61 - F1: 0.1189
2026-02-09 02:18:04 - INFO - Time taken for Epoch 4:25.44 - F1: 0.1218
2026-02-09 02:18:29 - INFO - Time taken for Epoch 5:25.42 - F1: 0.1416
2026-02-09 02:18:55 - INFO - Time taken for Epoch 6:25.44 - F1: 0.1785
2026-02-09 02:19:20 - INFO - Time taken for Epoch 7:25.58 - F1: 0.2150
2026-02-09 02:19:46 - INFO - Time taken for Epoch 8:25.45 - F1: 0.2531
2026-02-09 02:20:11 - INFO - Time taken for Epoch 9:25.62 - F1: 0.2867
2026-02-09 02:20:37 - INFO - Time taken for Epoch 10:25.71 - F1: 0.3039
2026-02-09 02:21:02 - INFO - Time taken for Epoch 11:25.45 - F1: 0.3113
2026-02-09 02:21:28 - INFO - Time taken for Epoch 12:25.58 - F1: 0.3195
2026-02-09 02:21:53 - INFO - Time taken for Epoch 13:25.44 - F1: 0.3226
2026-02-09 02:22:19 - INFO - Time taken for Epoch 14:25.50 - F1: 0.3199
2026-02-09 02:22:19 - INFO - Best F1:0.3226 - Best Epoch:13
2026-02-09 02:22:20 - INFO - Starting co-training
2026-02-09 02:23:30 - INFO - Time taken for Epoch 1: 69.31s - F1: 0.60721281
2026-02-09 02:24:40 - INFO - Time taken for Epoch 2: 70.26s - F1: 0.60781414
2026-02-09 02:25:50 - INFO - Time taken for Epoch 3: 70.21s - F1: 0.61254637
2026-02-09 02:27:00 - INFO - Time taken for Epoch 4: 70.08s - F1: 0.59269187
2026-02-09 02:28:09 - INFO - Time taken for Epoch 5: 69.04s - F1: 0.59886299
2026-02-09 02:29:19 - INFO - Time taken for Epoch 6: 69.45s - F1: 0.59169824
2026-02-09 02:30:28 - INFO - Time taken for Epoch 7: 69.03s - F1: 0.58937494
2026-02-09 02:31:37 - INFO - Time taken for Epoch 8: 69.01s - F1: 0.58701606
2026-02-09 02:32:46 - INFO - Time taken for Epoch 9: 68.92s - F1: 0.60634299
2026-02-09 02:33:55 - INFO - Time taken for Epoch 10: 69.46s - F1: 0.61184954
2026-02-09 02:35:05 - INFO - Time taken for Epoch 11: 69.47s - F1: 0.61369813
2026-02-09 02:36:15 - INFO - Time taken for Epoch 12: 70.57s - F1: 0.58686129
2026-02-09 02:37:25 - INFO - Time taken for Epoch 13: 69.37s - F1: 0.60298252
2026-02-09 02:38:34 - INFO - Time taken for Epoch 14: 68.99s - F1: 0.60664100
2026-02-09 02:38:36 - INFO - Fine-tuning models
2026-02-09 02:38:40 - INFO - Time taken for Epoch 1:3.56 - F1: 0.5927
2026-02-09 02:38:44 - INFO - Time taken for Epoch 2:4.30 - F1: 0.5925
2026-02-09 02:38:48 - INFO - Time taken for Epoch 3:3.51 - F1: 0.5980
2026-02-09 02:38:52 - INFO - Time taken for Epoch 4:4.43 - F1: 0.5990
2026-02-09 02:38:56 - INFO - Time taken for Epoch 5:4.40 - F1: 0.6052
2026-02-09 02:39:01 - INFO - Time taken for Epoch 6:4.37 - F1: 0.6080
2026-02-09 02:39:05 - INFO - Time taken for Epoch 7:4.35 - F1: 0.6058
2026-02-09 02:39:09 - INFO - Time taken for Epoch 8:3.51 - F1: 0.6051
2026-02-09 02:39:12 - INFO - Time taken for Epoch 9:3.51 - F1: 0.6066
2026-02-09 02:39:16 - INFO - Time taken for Epoch 10:3.52 - F1: 0.6083
2026-02-09 02:39:27 - INFO - Time taken for Epoch 11:11.52 - F1: 0.6087
2026-02-09 02:39:32 - INFO - Time taken for Epoch 12:4.46 - F1: 0.6054
2026-02-09 02:39:35 - INFO - Time taken for Epoch 13:3.52 - F1: 0.6074
2026-02-09 02:39:39 - INFO - Time taken for Epoch 14:3.52 - F1: 0.6066
2026-02-09 02:39:42 - INFO - Time taken for Epoch 15:3.52 - F1: 0.6060
2026-02-09 02:39:46 - INFO - Time taken for Epoch 16:3.51 - F1: 0.6043
2026-02-09 02:39:49 - INFO - Time taken for Epoch 17:3.51 - F1: 0.6052
2026-02-09 02:39:53 - INFO - Time taken for Epoch 18:3.51 - F1: 0.6052
2026-02-09 02:39:56 - INFO - Time taken for Epoch 19:3.52 - F1: 0.6048
2026-02-09 02:40:00 - INFO - Time taken for Epoch 20:3.52 - F1: 0.6051
2026-02-09 02:40:03 - INFO - Time taken for Epoch 21:3.52 - F1: 0.6038
2026-02-09 02:40:03 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 02:40:03 - INFO - Best F1:0.6087 - Best Epoch:10
2026-02-09 02:40:12 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6295, Test ECE: 0.0516
2026-02-09 02:40:12 - INFO - All results: {'f1_macro': 0.6294698751090466, 'ece': np.float64(0.051632108640978086)}
2026-02-09 02:40:12 - INFO - 
Total time taken: 1434.53 seconds
2026-02-09 02:40:12 - INFO - Trial 22 finished with value: 0.6294698751090466 and parameters: {'learning_rate': 7.090318559921216e-05, 'weight_decay': 0.00014831851149792438, 'batch_size': 64, 'co_train_epochs': 14, 'epoch_patience': 10}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 02:40:12 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 02:40:12 - INFO - Devices: cuda:1, cuda:1
2026-02-09 02:40:12 - INFO - Starting log
2026-02-09 02:40:12 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 02:40:12 - INFO - Learning Rate: 0.00017621690602134192
Weight Decay: 4.0614391178765935e-05
Batch Size: 64
No. Epochs: 13
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-09 02:40:13 - INFO - Generating initial weights
2026-02-09 02:40:42 - INFO - Time taken for Epoch 1:25.71 - F1: 0.1258
2026-02-09 02:41:07 - INFO - Time taken for Epoch 2:25.61 - F1: 0.0528
2026-02-09 02:41:33 - INFO - Time taken for Epoch 3:25.45 - F1: 0.1841
2026-02-09 02:41:58 - INFO - Time taken for Epoch 4:25.55 - F1: 0.2905
2026-02-09 02:42:24 - INFO - Time taken for Epoch 5:25.61 - F1: 0.3522
2026-02-09 02:42:49 - INFO - Time taken for Epoch 6:25.55 - F1: 0.3688
2026-02-09 02:43:15 - INFO - Time taken for Epoch 7:25.56 - F1: 0.3584
2026-02-09 02:43:41 - INFO - Time taken for Epoch 8:25.47 - F1: 0.3498
2026-02-09 02:44:06 - INFO - Time taken for Epoch 9:25.66 - F1: 0.3466
2026-02-09 02:44:32 - INFO - Time taken for Epoch 10:25.40 - F1: 0.3552
2026-02-09 02:44:57 - INFO - Time taken for Epoch 11:25.63 - F1: 0.3583
2026-02-09 02:45:23 - INFO - Time taken for Epoch 12:25.49 - F1: 0.3628
2026-02-09 02:45:48 - INFO - Time taken for Epoch 13:25.60 - F1: 0.3638
2026-02-09 02:45:48 - INFO - Best F1:0.3688 - Best Epoch:6
2026-02-09 02:45:50 - INFO - Starting co-training
2026-02-09 02:46:59 - INFO - Time taken for Epoch 1: 69.32s - F1: 0.61720144
2026-02-09 02:48:09 - INFO - Time taken for Epoch 2: 70.08s - F1: 0.64956289
2026-02-09 02:49:20 - INFO - Time taken for Epoch 3: 70.56s - F1: 0.60721819
2026-02-09 02:50:29 - INFO - Time taken for Epoch 4: 69.38s - F1: 0.60871699
2026-02-09 02:51:38 - INFO - Time taken for Epoch 5: 69.21s - F1: 0.60286105
2026-02-09 02:52:48 - INFO - Time taken for Epoch 6: 69.15s - F1: 0.58356870
2026-02-09 02:53:57 - INFO - Time taken for Epoch 7: 69.27s - F1: 0.59926164
2026-02-09 02:55:06 - INFO - Time taken for Epoch 8: 68.92s - F1: 0.61630984
2026-02-09 02:56:15 - INFO - Time taken for Epoch 9: 69.12s - F1: 0.58879554
2026-02-09 02:57:24 - INFO - Time taken for Epoch 10: 68.95s - F1: 0.59886257
2026-02-09 02:58:33 - INFO - Time taken for Epoch 11: 69.40s - F1: 0.60702621
2026-02-09 02:58:33 - INFO - Performance not improving for 9 consecutive epochs.
2026-02-09 02:58:36 - INFO - Fine-tuning models
2026-02-09 02:58:39 - INFO - Time taken for Epoch 1:3.57 - F1: 0.6016
2026-02-09 02:58:44 - INFO - Time taken for Epoch 2:4.52 - F1: 0.6237
2026-02-09 02:58:48 - INFO - Time taken for Epoch 3:4.56 - F1: 0.6371
2026-02-09 02:58:53 - INFO - Time taken for Epoch 4:4.56 - F1: 0.6401
2026-02-09 02:58:58 - INFO - Time taken for Epoch 5:4.58 - F1: 0.6470
2026-02-09 02:59:02 - INFO - Time taken for Epoch 6:4.57 - F1: 0.6369
2026-02-09 02:59:06 - INFO - Time taken for Epoch 7:3.53 - F1: 0.6338
2026-02-09 02:59:09 - INFO - Time taken for Epoch 8:3.53 - F1: 0.6310
2026-02-09 02:59:13 - INFO - Time taken for Epoch 9:3.54 - F1: 0.6290
2026-02-09 02:59:16 - INFO - Time taken for Epoch 10:3.53 - F1: 0.6361
2026-02-09 02:59:20 - INFO - Time taken for Epoch 11:3.53 - F1: 0.6441
2026-02-09 02:59:23 - INFO - Time taken for Epoch 12:3.53 - F1: 0.6447
2026-02-09 02:59:27 - INFO - Time taken for Epoch 13:3.52 - F1: 0.6463
2026-02-09 02:59:30 - INFO - Time taken for Epoch 14:3.53 - F1: 0.6542
2026-02-09 02:59:35 - INFO - Time taken for Epoch 15:4.56 - F1: 0.6543
2026-02-09 02:59:40 - INFO - Time taken for Epoch 16:4.56 - F1: 0.6530
2026-02-09 02:59:43 - INFO - Time taken for Epoch 17:3.51 - F1: 0.6533
2026-02-09 02:59:47 - INFO - Time taken for Epoch 18:3.52 - F1: 0.6527
2026-02-09 02:59:50 - INFO - Time taken for Epoch 19:3.52 - F1: 0.6565
2026-02-09 02:59:55 - INFO - Time taken for Epoch 20:4.64 - F1: 0.6547
2026-02-09 02:59:58 - INFO - Time taken for Epoch 21:3.52 - F1: 0.6536
2026-02-09 03:00:02 - INFO - Time taken for Epoch 22:3.52 - F1: 0.6561
2026-02-09 03:00:05 - INFO - Time taken for Epoch 23:3.53 - F1: 0.6552
2026-02-09 03:00:09 - INFO - Time taken for Epoch 24:3.52 - F1: 0.6537
2026-02-09 03:00:12 - INFO - Time taken for Epoch 25:3.53 - F1: 0.6524
2026-02-09 03:00:16 - INFO - Time taken for Epoch 26:3.52 - F1: 0.6522
2026-02-09 03:00:19 - INFO - Time taken for Epoch 27:3.52 - F1: 0.6522
2026-02-09 03:00:23 - INFO - Time taken for Epoch 28:3.51 - F1: 0.6517
2026-02-09 03:00:26 - INFO - Time taken for Epoch 29:3.51 - F1: 0.6532
2026-02-09 03:00:26 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 03:00:26 - INFO - Best F1:0.6565 - Best Epoch:18
2026-02-09 03:00:35 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6412, Test ECE: 0.0784
2026-02-09 03:00:35 - INFO - All results: {'f1_macro': 0.6411851269020262, 'ece': np.float64(0.07838246315941774)}
2026-02-09 03:00:35 - INFO - 
Total time taken: 1223.24 seconds
2026-02-09 03:00:35 - INFO - Trial 23 finished with value: 0.6411851269020262 and parameters: {'learning_rate': 0.00017621690602134192, 'weight_decay': 4.0614391178765935e-05, 'batch_size': 64, 'co_train_epochs': 13, 'epoch_patience': 9}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 03:00:35 - INFO - Using devices: cuda:1, cuda:1
2026-02-09 03:00:35 - INFO - Devices: cuda:1, cuda:1
2026-02-09 03:00:35 - INFO - Starting log
2026-02-09 03:00:35 - INFO - Dataset: humanitarian9, Event: hurricane_irma_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-09 03:00:35 - INFO - Learning Rate: 0.00010325011503481509
Weight Decay: 0.00034591241507138273
Batch Size: 64
No. Epochs: 16
Epoch Patience: 7
 Accumulation Steps: 1
2026-02-09 03:00:36 - INFO - Generating initial weights
2026-02-09 03:01:05 - INFO - Time taken for Epoch 1:25.52 - F1: 0.1296
2026-02-09 03:01:30 - INFO - Time taken for Epoch 2:25.43 - F1: 0.1015
2026-02-09 03:01:56 - INFO - Time taken for Epoch 3:25.51 - F1: 0.1171
2026-02-09 03:02:21 - INFO - Time taken for Epoch 4:25.54 - F1: 0.1539
2026-02-09 03:02:47 - INFO - Time taken for Epoch 5:25.42 - F1: 0.2172
2026-02-09 03:03:12 - INFO - Time taken for Epoch 6:25.37 - F1: 0.3055
2026-02-09 03:03:38 - INFO - Time taken for Epoch 7:25.65 - F1: 0.3245
2026-02-09 03:04:03 - INFO - Time taken for Epoch 8:25.54 - F1: 0.3314
2026-02-09 03:04:29 - INFO - Time taken for Epoch 9:25.58 - F1: 0.3327
2026-02-09 03:04:54 - INFO - Time taken for Epoch 10:25.55 - F1: 0.3390
2026-02-09 03:05:20 - INFO - Time taken for Epoch 11:25.70 - F1: 0.3396
2026-02-09 03:05:46 - INFO - Time taken for Epoch 12:25.52 - F1: 0.3473
2026-02-09 03:06:11 - INFO - Time taken for Epoch 13:25.49 - F1: 0.3504
2026-02-09 03:06:37 - INFO - Time taken for Epoch 14:25.53 - F1: 0.3537
2026-02-09 03:07:02 - INFO - Time taken for Epoch 15:25.42 - F1: 0.3539
2026-02-09 03:07:28 - INFO - Time taken for Epoch 16:25.59 - F1: 0.3519
2026-02-09 03:07:28 - INFO - Best F1:0.3539 - Best Epoch:15
2026-02-09 03:07:29 - INFO - Starting co-training
2026-02-09 03:08:38 - INFO - Time taken for Epoch 1: 69.19s - F1: 0.61431989
2026-02-09 03:09:49 - INFO - Time taken for Epoch 2: 70.28s - F1: 0.59611851
2026-02-09 03:10:58 - INFO - Time taken for Epoch 3: 69.21s - F1: 0.62258520
2026-02-09 03:12:08 - INFO - Time taken for Epoch 4: 70.04s - F1: 0.60405051
2026-02-09 03:13:17 - INFO - Time taken for Epoch 5: 69.07s - F1: 0.60244753
2026-02-09 03:14:26 - INFO - Time taken for Epoch 6: 69.24s - F1: 0.58468041
2026-02-09 03:15:36 - INFO - Time taken for Epoch 7: 69.57s - F1: 0.60143944
2026-02-09 03:16:45 - INFO - Time taken for Epoch 8: 69.58s - F1: 0.60989290
2026-02-09 03:17:55 - INFO - Time taken for Epoch 9: 69.24s - F1: 0.60148493
2026-02-09 03:19:04 - INFO - Time taken for Epoch 10: 69.08s - F1: 0.60940498
2026-02-09 03:19:04 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-09 03:19:06 - INFO - Fine-tuning models
2026-02-09 03:19:10 - INFO - Time taken for Epoch 1:3.56 - F1: 0.6326
2026-02-09 03:19:14 - INFO - Time taken for Epoch 2:4.44 - F1: 0.6404
2026-02-09 03:19:19 - INFO - Time taken for Epoch 3:4.50 - F1: 0.6470
2026-02-09 03:19:23 - INFO - Time taken for Epoch 4:4.51 - F1: 0.6386
2026-02-09 03:19:27 - INFO - Time taken for Epoch 5:3.51 - F1: 0.6379
2026-02-09 03:19:30 - INFO - Time taken for Epoch 6:3.51 - F1: 0.6377
2026-02-09 03:19:34 - INFO - Time taken for Epoch 7:3.52 - F1: 0.6350
2026-02-09 03:19:37 - INFO - Time taken for Epoch 8:3.52 - F1: 0.6349
2026-02-09 03:19:41 - INFO - Time taken for Epoch 9:3.53 - F1: 0.6338
2026-02-09 03:19:44 - INFO - Time taken for Epoch 10:3.58 - F1: 0.6303
2026-02-09 03:19:48 - INFO - Time taken for Epoch 11:3.57 - F1: 0.6259
2026-02-09 03:19:51 - INFO - Time taken for Epoch 12:3.54 - F1: 0.6212
2026-02-09 03:19:55 - INFO - Time taken for Epoch 13:3.52 - F1: 0.6188
2026-02-09 03:19:55 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-09 03:19:55 - INFO - Best F1:0.6470 - Best Epoch:2
2026-02-09 03:20:03 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6455, Test ECE: 0.0468
2026-02-09 03:20:03 - INFO - All results: {'f1_macro': 0.6454674063466582, 'ece': np.float64(0.04677450295165582)}
2026-02-09 03:20:03 - INFO - 
Total time taken: 1168.38 seconds
2026-02-09 03:20:03 - INFO - Trial 24 finished with value: 0.6454674063466582 and parameters: {'learning_rate': 0.00010325011503481509, 'weight_decay': 0.00034591241507138273, 'batch_size': 64, 'co_train_epochs': 16, 'epoch_patience': 7}. Best is trial 14 with value: 0.6690158334893085.
2026-02-09 03:20:03 - INFO - 
[BEST TRIAL RESULTS]
2026-02-09 03:20:03 - INFO - F1 Score: 0.6690
2026-02-09 03:20:03 - INFO - Params: {'learning_rate': 3.0633867483279684e-05, 'weight_decay': 0.00010400931529602136, 'batch_size': 64, 'co_train_epochs': 8, 'epoch_patience': 10}
2026-02-09 03:20:03 - INFO -   learning_rate: 3.0633867483279684e-05
2026-02-09 03:20:03 - INFO -   weight_decay: 0.00010400931529602136
2026-02-09 03:20:03 - INFO -   batch_size: 64
2026-02-09 03:20:03 - INFO -   co_train_epochs: 8
2026-02-09 03:20:03 - INFO -   epoch_patience: 10
2026-02-09 03:20:03 - INFO - 
Total time taken: 33339.33 seconds
