2026-02-12 13:15:42 - INFO - 
[Optuna] Starting hyperparameter search with 14 trials.
2026-02-12 13:15:42 - INFO - A new study created in memory with name: study_humanitarian8_canada_wildfires_2016
2026-02-12 13:15:42 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 13:15:42 - INFO - Devices: cuda:1, cuda:1
2026-02-12 13:15:42 - INFO - Starting log
2026-02-12 13:15:42 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:15:43 - INFO - Learning Rate: 2.473901800273487e-05
Weight Decay: 0.00010503193951806903
Batch Size: 64
No. Epochs: 8
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-12 13:15:44 - INFO - Generating initial weights
2026-02-12 13:15:52 - INFO - Time taken for Epoch 1:6.53 - F1: 0.0099
2026-02-12 13:15:58 - INFO - Time taken for Epoch 2:6.18 - F1: 0.0197
2026-02-12 13:16:04 - INFO - Time taken for Epoch 3:6.18 - F1: 0.0187
2026-02-12 13:16:10 - INFO - Time taken for Epoch 4:6.24 - F1: 0.0462
2026-02-12 13:16:17 - INFO - Time taken for Epoch 5:6.19 - F1: 0.0824
2026-02-12 13:16:23 - INFO - Time taken for Epoch 6:6.21 - F1: 0.1234
2026-02-12 13:16:29 - INFO - Time taken for Epoch 7:6.20 - F1: 0.1300
2026-02-12 13:16:35 - INFO - Time taken for Epoch 8:6.24 - F1: 0.1364
2026-02-12 13:16:35 - INFO - Best F1:0.1364 - Best Epoch:8
2026-02-12 13:16:36 - INFO - Starting co-training
2026-02-12 13:16:53 - INFO - Time taken for Epoch 1: 16.45s - F1: 0.15896556
2026-02-12 13:17:10 - INFO - Time taken for Epoch 2: 17.37s - F1: 0.30441454
2026-02-12 13:17:28 - INFO - Time taken for Epoch 3: 17.32s - F1: 0.43705068
2026-02-12 13:17:54 - INFO - Time taken for Epoch 4: 25.82s - F1: 0.48625795
2026-02-12 13:18:11 - INFO - Time taken for Epoch 5: 17.32s - F1: 0.49842148
2026-02-12 13:18:38 - INFO - Time taken for Epoch 6: 27.54s - F1: 0.47967678
2026-02-12 13:18:55 - INFO - Time taken for Epoch 7: 16.30s - F1: 0.48566069
2026-02-12 13:19:11 - INFO - Time taken for Epoch 8: 16.38s - F1: 0.50204036
2026-02-12 13:19:17 - INFO - Fine-tuning models
2026-02-12 13:19:18 - INFO - Time taken for Epoch 1:1.10 - F1: 0.4949
2026-02-12 13:19:20 - INFO - Time taken for Epoch 2:2.01 - F1: 0.5046
2026-02-12 13:19:22 - INFO - Time taken for Epoch 3:2.03 - F1: 0.5041
2026-02-12 13:19:23 - INFO - Time taken for Epoch 4:1.06 - F1: 0.5040
2026-02-12 13:19:24 - INFO - Time taken for Epoch 5:1.06 - F1: 0.5020
2026-02-12 13:19:25 - INFO - Time taken for Epoch 6:1.07 - F1: 0.4981
2026-02-12 13:19:26 - INFO - Time taken for Epoch 7:1.06 - F1: 0.4960
2026-02-12 13:19:27 - INFO - Time taken for Epoch 8:1.06 - F1: 0.5067
2026-02-12 13:19:29 - INFO - Time taken for Epoch 9:2.04 - F1: 0.5150
2026-02-12 13:19:31 - INFO - Time taken for Epoch 10:2.07 - F1: 0.5115
2026-02-12 13:19:32 - INFO - Time taken for Epoch 11:1.06 - F1: 0.5099
2026-02-12 13:19:33 - INFO - Time taken for Epoch 12:1.06 - F1: 0.5038
2026-02-12 13:19:34 - INFO - Time taken for Epoch 13:1.06 - F1: 0.5172
2026-02-12 13:19:36 - INFO - Time taken for Epoch 14:2.05 - F1: 0.5263
2026-02-12 13:19:39 - INFO - Time taken for Epoch 15:2.08 - F1: 0.5246
2026-02-12 13:19:40 - INFO - Time taken for Epoch 16:1.06 - F1: 0.5186
2026-02-12 13:19:41 - INFO - Time taken for Epoch 17:1.06 - F1: 0.5154
2026-02-12 13:19:42 - INFO - Time taken for Epoch 18:1.06 - F1: 0.5099
2026-02-12 13:19:43 - INFO - Time taken for Epoch 19:1.06 - F1: 0.5099
2026-02-12 13:19:44 - INFO - Time taken for Epoch 20:1.06 - F1: 0.5071
2026-02-12 13:19:45 - INFO - Time taken for Epoch 21:1.06 - F1: 0.5086
2026-02-12 13:19:46 - INFO - Time taken for Epoch 22:1.06 - F1: 0.5187
2026-02-12 13:19:47 - INFO - Time taken for Epoch 23:1.07 - F1: 0.5268
2026-02-12 13:19:49 - INFO - Time taken for Epoch 24:2.07 - F1: 0.5276
2026-02-12 13:19:51 - INFO - Time taken for Epoch 25:2.25 - F1: 0.5299
2026-02-12 13:20:12 - INFO - Time taken for Epoch 26:20.63 - F1: 0.5299
2026-02-12 13:20:13 - INFO - Time taken for Epoch 27:1.06 - F1: 0.5299
2026-02-12 13:20:14 - INFO - Time taken for Epoch 28:1.06 - F1: 0.5299
2026-02-12 13:20:15 - INFO - Time taken for Epoch 29:1.06 - F1: 0.5299
2026-02-12 13:20:16 - INFO - Time taken for Epoch 30:1.06 - F1: 0.5302
2026-02-12 13:20:18 - INFO - Time taken for Epoch 31:2.06 - F1: 0.5302
2026-02-12 13:20:19 - INFO - Time taken for Epoch 32:1.05 - F1: 0.5302
2026-02-12 13:20:20 - INFO - Time taken for Epoch 33:1.05 - F1: 0.5302
2026-02-12 13:20:21 - INFO - Time taken for Epoch 34:1.06 - F1: 0.5302
2026-02-12 13:20:23 - INFO - Time taken for Epoch 35:1.05 - F1: 0.5319
2026-02-12 13:20:25 - INFO - Time taken for Epoch 36:2.07 - F1: 0.5319
2026-02-12 13:20:26 - INFO - Time taken for Epoch 37:1.05 - F1: 0.5319
2026-02-12 13:20:27 - INFO - Time taken for Epoch 38:1.06 - F1: 0.5319
2026-02-12 13:20:28 - INFO - Time taken for Epoch 39:1.06 - F1: 0.5319
2026-02-12 13:20:29 - INFO - Time taken for Epoch 40:1.06 - F1: 0.5319
2026-02-12 13:20:30 - INFO - Time taken for Epoch 41:1.06 - F1: 0.5319
2026-02-12 13:20:31 - INFO - Time taken for Epoch 42:1.06 - F1: 0.5319
2026-02-12 13:20:32 - INFO - Time taken for Epoch 43:1.06 - F1: 0.5319
2026-02-12 13:20:33 - INFO - Time taken for Epoch 44:1.06 - F1: 0.5319
2026-02-12 13:20:34 - INFO - Time taken for Epoch 45:1.06 - F1: 0.5333
2026-02-12 13:20:36 - INFO - Time taken for Epoch 46:2.07 - F1: 0.5333
2026-02-12 13:20:37 - INFO - Time taken for Epoch 47:1.06 - F1: 0.5333
2026-02-12 13:20:38 - INFO - Time taken for Epoch 48:1.06 - F1: 0.5330
2026-02-12 13:20:39 - INFO - Time taken for Epoch 49:1.07 - F1: 0.5330
2026-02-12 13:20:40 - INFO - Time taken for Epoch 50:1.06 - F1: 0.5281
2026-02-12 13:20:42 - INFO - Time taken for Epoch 51:1.06 - F1: 0.5266
2026-02-12 13:20:43 - INFO - Time taken for Epoch 52:1.06 - F1: 0.5316
2026-02-12 13:20:44 - INFO - Time taken for Epoch 53:1.06 - F1: 0.5316
2026-02-12 13:20:45 - INFO - Time taken for Epoch 54:1.07 - F1: 0.5316
2026-02-12 13:20:46 - INFO - Time taken for Epoch 55:1.07 - F1: 0.5266
2026-02-12 13:20:46 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 13:20:46 - INFO - Best F1:0.5333 - Best Epoch:44
2026-02-12 13:20:50 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6030, Test ECE: 0.0481
2026-02-12 13:20:50 - INFO - All results: {'f1_macro': 0.6029784764951207, 'ece': np.float64(0.04805998360173086)}
2026-02-12 13:20:50 - INFO - 
Total time taken: 307.66 seconds
2026-02-12 13:20:50 - INFO - Trial 0 finished with value: 0.6029784764951207 and parameters: {'learning_rate': 2.473901800273487e-05, 'weight_decay': 0.00010503193951806903, 'batch_size': 64, 'co_train_epochs': 8, 'epoch_patience': 9}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 13:20:50 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 13:20:50 - INFO - Devices: cuda:1, cuda:1
2026-02-12 13:20:50 - INFO - Starting log
2026-02-12 13:20:50 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:20:50 - INFO - Learning Rate: 6.0013538625319384e-05
Weight Decay: 0.003855058718440453
Batch Size: 16
No. Epochs: 18
Epoch Patience: 5
 Accumulation Steps: 4
2026-02-12 13:20:51 - INFO - Generating initial weights
2026-02-12 13:21:00 - INFO - Time taken for Epoch 1:7.73 - F1: 0.0715
2026-02-12 13:21:07 - INFO - Time taken for Epoch 2:7.70 - F1: 0.1122
2026-02-12 13:21:15 - INFO - Time taken for Epoch 3:7.73 - F1: 0.1224
2026-02-12 13:21:23 - INFO - Time taken for Epoch 4:7.78 - F1: 0.1057
2026-02-12 13:21:30 - INFO - Time taken for Epoch 5:7.72 - F1: 0.0975
2026-02-12 13:21:38 - INFO - Time taken for Epoch 6:7.67 - F1: 0.1296
2026-02-12 13:21:46 - INFO - Time taken for Epoch 7:7.41 - F1: 0.1178
2026-02-12 13:21:53 - INFO - Time taken for Epoch 8:7.52 - F1: 0.1241
2026-02-12 13:22:01 - INFO - Time taken for Epoch 9:7.46 - F1: 0.1112
2026-02-12 13:22:08 - INFO - Time taken for Epoch 10:7.48 - F1: 0.1200
2026-02-12 13:22:16 - INFO - Time taken for Epoch 11:7.53 - F1: 0.1352
2026-02-12 13:22:23 - INFO - Time taken for Epoch 12:7.72 - F1: 0.1380
2026-02-12 13:22:31 - INFO - Time taken for Epoch 13:7.76 - F1: 0.1435
2026-02-12 13:22:39 - INFO - Time taken for Epoch 14:7.70 - F1: 0.1457
2026-02-12 13:22:46 - INFO - Time taken for Epoch 15:7.56 - F1: 0.1474
2026-02-12 13:22:54 - INFO - Time taken for Epoch 16:7.73 - F1: 0.1535
2026-02-12 13:23:02 - INFO - Time taken for Epoch 17:7.73 - F1: 0.1559
2026-02-12 13:23:09 - INFO - Time taken for Epoch 18:7.73 - F1: 0.1630
2026-02-12 13:23:09 - INFO - Best F1:0.1630 - Best Epoch:18
2026-02-12 13:23:11 - INFO - Starting co-training
2026-02-12 13:23:23 - INFO - Time taken for Epoch 1: 12.04s - F1: 0.25917351
2026-02-12 13:23:36 - INFO - Time taken for Epoch 2: 12.87s - F1: 0.34449106
2026-02-12 13:23:49 - INFO - Time taken for Epoch 3: 13.22s - F1: 0.45528762
2026-02-12 13:24:08 - INFO - Time taken for Epoch 4: 19.26s - F1: 0.48888678
2026-02-12 13:24:21 - INFO - Time taken for Epoch 5: 13.10s - F1: 0.49212309
2026-02-12 13:24:34 - INFO - Time taken for Epoch 6: 13.20s - F1: 0.49155569
2026-02-12 13:24:46 - INFO - Time taken for Epoch 7: 12.03s - F1: 0.48461870
2026-02-12 13:24:59 - INFO - Time taken for Epoch 8: 12.09s - F1: 0.49596648
2026-02-12 13:25:12 - INFO - Time taken for Epoch 9: 12.98s - F1: 0.57325586
2026-02-12 13:25:26 - INFO - Time taken for Epoch 10: 14.14s - F1: 0.50872481
2026-02-12 13:25:38 - INFO - Time taken for Epoch 11: 11.87s - F1: 0.50144522
2026-02-12 13:25:50 - INFO - Time taken for Epoch 12: 11.95s - F1: 0.49384348
2026-02-12 13:26:02 - INFO - Time taken for Epoch 13: 12.01s - F1: 0.54188756
2026-02-12 13:26:14 - INFO - Time taken for Epoch 14: 11.97s - F1: 0.48572354
2026-02-12 13:26:14 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-12 13:26:16 - INFO - Fine-tuning models
2026-02-12 13:26:17 - INFO - Time taken for Epoch 1:1.24 - F1: 0.5491
2026-02-12 13:26:19 - INFO - Time taken for Epoch 2:2.21 - F1: 0.5636
2026-02-12 13:26:21 - INFO - Time taken for Epoch 3:2.31 - F1: 0.5618
2026-02-12 13:26:23 - INFO - Time taken for Epoch 4:1.20 - F1: 0.5357
2026-02-12 13:26:24 - INFO - Time taken for Epoch 5:1.19 - F1: 0.5041
2026-02-12 13:26:25 - INFO - Time taken for Epoch 6:1.19 - F1: 0.4880
2026-02-12 13:26:26 - INFO - Time taken for Epoch 7:1.20 - F1: 0.4863
2026-02-12 13:26:27 - INFO - Time taken for Epoch 8:1.20 - F1: 0.5482
2026-02-12 13:26:29 - INFO - Time taken for Epoch 9:1.20 - F1: 0.5490
2026-02-12 13:26:30 - INFO - Time taken for Epoch 10:1.20 - F1: 0.5354
2026-02-12 13:26:31 - INFO - Time taken for Epoch 11:1.20 - F1: 0.5699
2026-02-12 13:26:33 - INFO - Time taken for Epoch 12:2.20 - F1: 0.5719
2026-02-12 13:26:35 - INFO - Time taken for Epoch 13:2.19 - F1: 0.5897
2026-02-12 13:26:38 - INFO - Time taken for Epoch 14:2.18 - F1: 0.5893
2026-02-12 13:26:39 - INFO - Time taken for Epoch 15:1.19 - F1: 0.5893
2026-02-12 13:26:40 - INFO - Time taken for Epoch 16:1.19 - F1: 0.5868
2026-02-12 13:26:41 - INFO - Time taken for Epoch 17:1.19 - F1: 0.6137
2026-02-12 13:26:43 - INFO - Time taken for Epoch 18:2.20 - F1: 0.6236
2026-02-12 13:26:46 - INFO - Time taken for Epoch 19:2.21 - F1: 0.6236
2026-02-12 13:26:47 - INFO - Time taken for Epoch 20:1.20 - F1: 0.6297
2026-02-12 13:26:49 - INFO - Time taken for Epoch 21:2.20 - F1: 0.6297
2026-02-12 13:26:50 - INFO - Time taken for Epoch 22:1.19 - F1: 0.5876
2026-02-12 13:26:51 - INFO - Time taken for Epoch 23:1.22 - F1: 0.5930
2026-02-12 13:26:53 - INFO - Time taken for Epoch 24:1.25 - F1: 0.5930
2026-02-12 13:26:54 - INFO - Time taken for Epoch 25:1.25 - F1: 0.5995
2026-02-12 13:26:55 - INFO - Time taken for Epoch 26:1.25 - F1: 0.5995
2026-02-12 13:26:56 - INFO - Time taken for Epoch 27:1.24 - F1: 0.5156
2026-02-12 13:27:00 - INFO - Time taken for Epoch 28:3.16 - F1: 0.5995
2026-02-12 13:27:01 - INFO - Time taken for Epoch 29:1.21 - F1: 0.5995
2026-02-12 13:27:02 - INFO - Time taken for Epoch 30:1.20 - F1: 0.5995
2026-02-12 13:27:02 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 13:27:02 - INFO - Best F1:0.6297 - Best Epoch:19
2026-02-12 13:27:06 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5891, Test ECE: 0.0591
2026-02-12 13:27:06 - INFO - All results: {'f1_macro': 0.5890990866571812, 'ece': np.float64(0.05911443782656381)}
2026-02-12 13:27:06 - INFO - 
Total time taken: 376.39 seconds
2026-02-12 13:27:06 - INFO - Trial 1 finished with value: 0.5890990866571812 and parameters: {'learning_rate': 6.0013538625319384e-05, 'weight_decay': 0.003855058718440453, 'batch_size': 16, 'co_train_epochs': 18, 'epoch_patience': 5}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 13:27:06 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 13:27:06 - INFO - Devices: cuda:1, cuda:1
2026-02-12 13:27:06 - INFO - Starting log
2026-02-12 13:27:06 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:27:06 - INFO - Learning Rate: 7.401658705877731e-05
Weight Decay: 0.001011523046759711
Batch Size: 32
No. Epochs: 17
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-12 13:27:07 - INFO - Generating initial weights
2026-02-12 13:27:15 - INFO - Time taken for Epoch 1:7.01 - F1: 0.0634
2026-02-12 13:27:22 - INFO - Time taken for Epoch 2:6.96 - F1: 0.0722
2026-02-12 13:27:29 - INFO - Time taken for Epoch 3:6.93 - F1: 0.1965
2026-02-12 13:27:36 - INFO - Time taken for Epoch 4:6.95 - F1: 0.2469
2026-02-12 13:27:43 - INFO - Time taken for Epoch 5:6.89 - F1: 0.2919
2026-02-12 13:27:50 - INFO - Time taken for Epoch 6:6.89 - F1: 0.2858
2026-02-12 13:27:57 - INFO - Time taken for Epoch 7:6.89 - F1: 0.3067
2026-02-12 13:28:04 - INFO - Time taken for Epoch 8:6.80 - F1: 0.2877
2026-02-12 13:28:10 - INFO - Time taken for Epoch 9:6.88 - F1: 0.3042
2026-02-12 13:28:17 - INFO - Time taken for Epoch 10:6.95 - F1: 0.3032
2026-02-12 13:28:24 - INFO - Time taken for Epoch 11:6.97 - F1: 0.2858
2026-02-12 13:28:31 - INFO - Time taken for Epoch 12:6.99 - F1: 0.2930
2026-02-12 13:28:38 - INFO - Time taken for Epoch 13:6.92 - F1: 0.2904
2026-02-12 13:28:45 - INFO - Time taken for Epoch 14:6.89 - F1: 0.2941
2026-02-12 13:28:52 - INFO - Time taken for Epoch 15:6.94 - F1: 0.2969
2026-02-12 13:28:59 - INFO - Time taken for Epoch 16:6.90 - F1: 0.2969
2026-02-12 13:29:06 - INFO - Time taken for Epoch 17:6.89 - F1: 0.2969
2026-02-12 13:29:06 - INFO - Best F1:0.3067 - Best Epoch:7
2026-02-12 13:29:07 - INFO - Starting co-training
2026-02-12 13:29:20 - INFO - Time taken for Epoch 1: 13.19s - F1: 0.36042096
2026-02-12 13:29:34 - INFO - Time taken for Epoch 2: 14.23s - F1: 0.45487696
2026-02-12 13:30:02 - INFO - Time taken for Epoch 3: 27.48s - F1: 0.46673570
2026-02-12 13:30:16 - INFO - Time taken for Epoch 4: 14.12s - F1: 0.47414518
2026-02-12 13:30:30 - INFO - Time taken for Epoch 5: 14.34s - F1: 0.51059740
2026-02-12 13:30:45 - INFO - Time taken for Epoch 6: 14.27s - F1: 0.51555326
2026-02-12 13:30:59 - INFO - Time taken for Epoch 7: 14.19s - F1: 0.50243357
2026-02-12 13:31:12 - INFO - Time taken for Epoch 8: 13.23s - F1: 0.53332727
2026-02-12 13:31:26 - INFO - Time taken for Epoch 9: 14.24s - F1: 0.55279812
2026-02-12 13:31:41 - INFO - Time taken for Epoch 10: 14.19s - F1: 0.52761076
2026-02-12 13:31:54 - INFO - Time taken for Epoch 11: 13.18s - F1: 0.62014623
2026-02-12 13:32:08 - INFO - Time taken for Epoch 12: 14.52s - F1: 0.50491105
2026-02-12 13:32:22 - INFO - Time taken for Epoch 13: 13.36s - F1: 0.53873284
2026-02-12 13:32:35 - INFO - Time taken for Epoch 14: 13.33s - F1: 0.55087233
2026-02-12 13:32:48 - INFO - Time taken for Epoch 15: 13.16s - F1: 0.53167732
2026-02-12 13:33:01 - INFO - Time taken for Epoch 16: 13.22s - F1: 0.49115736
2026-02-12 13:33:15 - INFO - Time taken for Epoch 17: 13.22s - F1: 0.60021198
2026-02-12 13:33:17 - INFO - Fine-tuning models
2026-02-12 13:33:18 - INFO - Time taken for Epoch 1:1.19 - F1: 0.5073
2026-02-12 13:33:20 - INFO - Time taken for Epoch 2:2.10 - F1: 0.5776
2026-02-12 13:33:25 - INFO - Time taken for Epoch 3:4.82 - F1: 0.5248
2026-02-12 13:33:26 - INFO - Time taken for Epoch 4:1.17 - F1: 0.5385
2026-02-12 13:33:27 - INFO - Time taken for Epoch 5:1.15 - F1: 0.6310
2026-02-12 13:33:32 - INFO - Time taken for Epoch 6:4.36 - F1: 0.6219
2026-02-12 13:33:33 - INFO - Time taken for Epoch 7:1.15 - F1: 0.6152
2026-02-12 13:33:34 - INFO - Time taken for Epoch 8:1.17 - F1: 0.6125
2026-02-12 13:33:35 - INFO - Time taken for Epoch 9:1.18 - F1: 0.6134
2026-02-12 13:33:36 - INFO - Time taken for Epoch 10:1.17 - F1: 0.6124
2026-02-12 13:33:37 - INFO - Time taken for Epoch 11:1.15 - F1: 0.6142
2026-02-12 13:33:39 - INFO - Time taken for Epoch 12:1.18 - F1: 0.5998
2026-02-12 13:33:40 - INFO - Time taken for Epoch 13:1.16 - F1: 0.5983
2026-02-12 13:33:41 - INFO - Time taken for Epoch 14:1.17 - F1: 0.5917
2026-02-12 13:33:42 - INFO - Time taken for Epoch 15:1.15 - F1: 0.5908
2026-02-12 13:33:42 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 13:33:42 - INFO - Best F1:0.6310 - Best Epoch:4
2026-02-12 13:33:46 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5888, Test ECE: 0.0566
2026-02-12 13:33:46 - INFO - All results: {'f1_macro': 0.5887790259253163, 'ece': np.float64(0.05656795849960841)}
2026-02-12 13:33:46 - INFO - 
Total time taken: 400.00 seconds
2026-02-12 13:33:46 - INFO - Trial 2 finished with value: 0.5887790259253163 and parameters: {'learning_rate': 7.401658705877731e-05, 'weight_decay': 0.001011523046759711, 'batch_size': 32, 'co_train_epochs': 17, 'epoch_patience': 10}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 13:33:46 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 13:33:46 - INFO - Devices: cuda:1, cuda:1
2026-02-12 13:33:46 - INFO - Starting log
2026-02-12 13:33:46 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:33:46 - INFO - Learning Rate: 0.0008510064877230954
Weight Decay: 0.0008829861976804371
Batch Size: 8
No. Epochs: 10
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-12 13:33:48 - INFO - Generating initial weights
2026-02-12 13:33:58 - INFO - Time taken for Epoch 1:10.01 - F1: 0.0300
2026-02-12 13:34:08 - INFO - Time taken for Epoch 2:9.96 - F1: 0.0365
2026-02-12 13:34:18 - INFO - Time taken for Epoch 3:9.81 - F1: 0.0069
2026-02-12 13:34:28 - INFO - Time taken for Epoch 4:9.96 - F1: 0.0085
2026-02-12 13:34:38 - INFO - Time taken for Epoch 5:9.86 - F1: 0.0247
2026-02-12 13:34:48 - INFO - Time taken for Epoch 6:9.86 - F1: 0.0085
2026-02-12 13:34:57 - INFO - Time taken for Epoch 7:9.62 - F1: 0.0085
2026-02-12 13:35:07 - INFO - Time taken for Epoch 8:9.67 - F1: 0.0365
2026-02-12 13:35:17 - INFO - Time taken for Epoch 9:9.73 - F1: 0.0365
2026-02-12 13:35:27 - INFO - Time taken for Epoch 10:9.81 - F1: 0.0365
2026-02-12 13:35:27 - INFO - Best F1:0.0365 - Best Epoch:2
2026-02-12 13:35:28 - INFO - Starting co-training
2026-02-12 13:35:40 - INFO - Time taken for Epoch 1: 12.45s - F1: 0.07352941
2026-02-12 13:35:54 - INFO - Time taken for Epoch 2: 13.40s - F1: 0.07352941
2026-02-12 13:36:06 - INFO - Time taken for Epoch 3: 12.45s - F1: 0.03651685
2026-02-12 13:36:19 - INFO - Time taken for Epoch 4: 12.49s - F1: 0.03651685
2026-02-12 13:36:31 - INFO - Time taken for Epoch 5: 12.47s - F1: 0.03651685
2026-02-12 13:36:31 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-12 13:36:33 - INFO - Fine-tuning models
2026-02-12 13:36:35 - INFO - Time taken for Epoch 1:1.53 - F1: 0.0115
2026-02-12 13:36:38 - INFO - Time taken for Epoch 2:2.47 - F1: 0.0308
2026-02-12 13:36:40 - INFO - Time taken for Epoch 3:2.52 - F1: 0.0085
2026-02-12 13:36:41 - INFO - Time taken for Epoch 4:1.47 - F1: 0.0247
2026-02-12 13:36:43 - INFO - Time taken for Epoch 5:1.47 - F1: 0.0247
2026-02-12 13:36:44 - INFO - Time taken for Epoch 6:1.48 - F1: 0.0247
2026-02-12 13:36:46 - INFO - Time taken for Epoch 7:1.47 - F1: 0.0115
2026-02-12 13:36:47 - INFO - Time taken for Epoch 8:1.47 - F1: 0.0115
2026-02-12 13:36:49 - INFO - Time taken for Epoch 9:1.49 - F1: 0.0365
2026-02-12 13:36:59 - INFO - Time taken for Epoch 10:9.77 - F1: 0.0365
2026-02-12 13:37:00 - INFO - Time taken for Epoch 11:1.46 - F1: 0.0365
2026-02-12 13:37:02 - INFO - Time taken for Epoch 12:1.46 - F1: 0.0365
2026-02-12 13:37:03 - INFO - Time taken for Epoch 13:1.45 - F1: 0.0085
2026-02-12 13:37:04 - INFO - Time taken for Epoch 14:1.46 - F1: 0.0085
2026-02-12 13:37:06 - INFO - Time taken for Epoch 15:1.47 - F1: 0.0085
2026-02-12 13:37:07 - INFO - Time taken for Epoch 16:1.46 - F1: 0.0308
2026-02-12 13:37:09 - INFO - Time taken for Epoch 17:1.47 - F1: 0.0308
2026-02-12 13:37:10 - INFO - Time taken for Epoch 18:1.46 - F1: 0.0308
2026-02-12 13:37:12 - INFO - Time taken for Epoch 19:1.47 - F1: 0.0247
2026-02-12 13:37:12 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 13:37:12 - INFO - Best F1:0.0365 - Best Epoch:8
2026-02-12 13:37:16 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0361, Test ECE: 0.1924
2026-02-12 13:37:16 - INFO - All results: {'f1_macro': 0.036057692307692304, 'ece': np.float64(0.19237390207440666)}
2026-02-12 13:37:16 - INFO - 
Total time taken: 210.26 seconds
2026-02-12 13:37:16 - INFO - Trial 3 finished with value: 0.036057692307692304 and parameters: {'learning_rate': 0.0008510064877230954, 'weight_decay': 0.0008829861976804371, 'batch_size': 8, 'co_train_epochs': 10, 'epoch_patience': 4}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 13:37:16 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 13:37:16 - INFO - Devices: cuda:1, cuda:1
2026-02-12 13:37:16 - INFO - Starting log
2026-02-12 13:37:16 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:37:17 - INFO - Learning Rate: 0.00010881154498307519
Weight Decay: 0.0004075495400872856
Batch Size: 16
No. Epochs: 14
Epoch Patience: 9
 Accumulation Steps: 4
2026-02-12 13:37:18 - INFO - Generating initial weights
2026-02-12 13:37:26 - INFO - Time taken for Epoch 1:7.77 - F1: 0.0933
2026-02-12 13:37:34 - INFO - Time taken for Epoch 2:7.78 - F1: 0.0947
2026-02-12 13:37:42 - INFO - Time taken for Epoch 3:7.83 - F1: 0.0295
2026-02-12 13:37:50 - INFO - Time taken for Epoch 4:7.85 - F1: 0.1300
2026-02-12 13:37:58 - INFO - Time taken for Epoch 5:7.84 - F1: 0.1041
2026-02-12 13:38:05 - INFO - Time taken for Epoch 6:7.78 - F1: 0.1184
2026-02-12 13:38:13 - INFO - Time taken for Epoch 7:7.78 - F1: 0.1287
2026-02-12 13:38:21 - INFO - Time taken for Epoch 8:7.80 - F1: 0.1523
2026-02-12 13:38:29 - INFO - Time taken for Epoch 9:7.86 - F1: 0.1592
2026-02-12 13:38:37 - INFO - Time taken for Epoch 10:7.87 - F1: 0.1542
2026-02-12 13:38:45 - INFO - Time taken for Epoch 11:7.80 - F1: 0.1424
2026-02-12 13:38:52 - INFO - Time taken for Epoch 12:7.85 - F1: 0.1420
2026-02-12 13:39:00 - INFO - Time taken for Epoch 13:7.80 - F1: 0.1473
2026-02-12 13:39:08 - INFO - Time taken for Epoch 14:7.85 - F1: 0.1668
2026-02-12 13:39:08 - INFO - Best F1:0.1668 - Best Epoch:14
2026-02-12 13:39:09 - INFO - Starting co-training
2026-02-12 13:39:21 - INFO - Time taken for Epoch 1: 12.15s - F1: 0.26041858
2026-02-12 13:39:34 - INFO - Time taken for Epoch 2: 13.00s - F1: 0.42409152
2026-02-12 13:39:47 - INFO - Time taken for Epoch 3: 12.91s - F1: 0.48395762
2026-02-12 13:40:04 - INFO - Time taken for Epoch 4: 16.82s - F1: 0.41715575
2026-02-12 13:40:16 - INFO - Time taken for Epoch 5: 11.97s - F1: 0.48171853
2026-02-12 13:40:28 - INFO - Time taken for Epoch 6: 11.98s - F1: 0.43987564
2026-02-12 13:40:40 - INFO - Time taken for Epoch 7: 11.97s - F1: 0.45072583
2026-02-12 13:40:52 - INFO - Time taken for Epoch 8: 11.98s - F1: 0.50043953
2026-02-12 13:41:05 - INFO - Time taken for Epoch 9: 12.85s - F1: 0.52158369
2026-02-12 13:41:18 - INFO - Time taken for Epoch 10: 12.95s - F1: 0.49311983
2026-02-12 13:41:30 - INFO - Time taken for Epoch 11: 11.94s - F1: 0.49896509
2026-02-12 13:41:42 - INFO - Time taken for Epoch 12: 11.91s - F1: 0.48284311
2026-02-12 13:41:54 - INFO - Time taken for Epoch 13: 12.03s - F1: 0.46753944
2026-02-12 13:42:23 - INFO - Time taken for Epoch 14: 28.97s - F1: 0.50543538
2026-02-12 13:42:25 - INFO - Fine-tuning models
2026-02-12 13:42:26 - INFO - Time taken for Epoch 1:1.29 - F1: 0.5027
2026-02-12 13:42:29 - INFO - Time taken for Epoch 2:2.34 - F1: 0.5212
2026-02-12 13:42:31 - INFO - Time taken for Epoch 3:2.27 - F1: 0.5369
2026-02-12 13:42:33 - INFO - Time taken for Epoch 4:2.24 - F1: 0.4998
2026-02-12 13:42:34 - INFO - Time taken for Epoch 5:1.20 - F1: 0.4966
2026-02-12 13:42:36 - INFO - Time taken for Epoch 6:1.20 - F1: 0.4885
2026-02-12 13:42:37 - INFO - Time taken for Epoch 7:1.20 - F1: 0.4923
2026-02-12 13:42:38 - INFO - Time taken for Epoch 8:1.20 - F1: 0.4945
2026-02-12 13:42:39 - INFO - Time taken for Epoch 9:1.20 - F1: 0.5520
2026-02-12 13:42:42 - INFO - Time taken for Epoch 10:2.75 - F1: 0.5357
2026-02-12 13:42:43 - INFO - Time taken for Epoch 11:1.20 - F1: 0.5366
2026-02-12 13:42:44 - INFO - Time taken for Epoch 12:1.20 - F1: 0.5423
2026-02-12 13:42:45 - INFO - Time taken for Epoch 13:1.20 - F1: 0.5531
2026-02-12 13:42:48 - INFO - Time taken for Epoch 14:2.54 - F1: 0.5477
2026-02-12 13:42:49 - INFO - Time taken for Epoch 15:1.20 - F1: 0.5446
2026-02-12 13:42:50 - INFO - Time taken for Epoch 16:1.20 - F1: 0.5639
2026-02-12 13:42:53 - INFO - Time taken for Epoch 17:2.24 - F1: 0.5577
2026-02-12 13:42:54 - INFO - Time taken for Epoch 18:1.20 - F1: 0.5575
2026-02-12 13:42:55 - INFO - Time taken for Epoch 19:1.21 - F1: 0.5662
2026-02-12 13:42:57 - INFO - Time taken for Epoch 20:2.23 - F1: 0.5639
2026-02-12 13:42:59 - INFO - Time taken for Epoch 21:1.21 - F1: 0.5599
2026-02-12 13:43:00 - INFO - Time taken for Epoch 22:1.20 - F1: 0.5608
2026-02-12 13:43:01 - INFO - Time taken for Epoch 23:1.20 - F1: 0.5585
2026-02-12 13:43:02 - INFO - Time taken for Epoch 24:1.21 - F1: 0.5585
2026-02-12 13:43:03 - INFO - Time taken for Epoch 25:1.20 - F1: 0.5585
2026-02-12 13:43:05 - INFO - Time taken for Epoch 26:1.20 - F1: 0.5570
2026-02-12 13:43:06 - INFO - Time taken for Epoch 27:1.20 - F1: 0.5689
2026-02-12 13:43:18 - INFO - Time taken for Epoch 28:12.59 - F1: 0.5655
2026-02-12 13:43:20 - INFO - Time taken for Epoch 29:1.21 - F1: 0.5668
2026-02-12 13:43:21 - INFO - Time taken for Epoch 30:1.20 - F1: 0.5668
2026-02-12 13:43:22 - INFO - Time taken for Epoch 31:1.20 - F1: 0.5673
2026-02-12 13:43:23 - INFO - Time taken for Epoch 32:1.21 - F1: 0.5673
2026-02-12 13:43:24 - INFO - Time taken for Epoch 33:1.20 - F1: 0.5688
2026-02-12 13:43:26 - INFO - Time taken for Epoch 34:1.20 - F1: 0.5692
2026-02-12 13:43:28 - INFO - Time taken for Epoch 35:2.22 - F1: 0.5692
2026-02-12 13:43:29 - INFO - Time taken for Epoch 36:1.19 - F1: 0.5669
2026-02-12 13:43:30 - INFO - Time taken for Epoch 37:1.20 - F1: 0.5669
2026-02-12 13:43:31 - INFO - Time taken for Epoch 38:1.20 - F1: 0.5614
2026-02-12 13:43:33 - INFO - Time taken for Epoch 39:1.20 - F1: 0.5666
2026-02-12 13:43:34 - INFO - Time taken for Epoch 40:1.20 - F1: 0.5613
2026-02-12 13:43:35 - INFO - Time taken for Epoch 41:1.20 - F1: 0.5613
2026-02-12 13:43:36 - INFO - Time taken for Epoch 42:1.20 - F1: 0.5613
2026-02-12 13:43:37 - INFO - Time taken for Epoch 43:1.20 - F1: 0.5613
2026-02-12 13:43:39 - INFO - Time taken for Epoch 44:1.19 - F1: 0.5663
2026-02-12 13:43:39 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 13:43:39 - INFO - Best F1:0.5692 - Best Epoch:33
2026-02-12 13:43:43 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5455, Test ECE: 0.1254
2026-02-12 13:43:43 - INFO - All results: {'f1_macro': 0.5455381328800173, 'ece': np.float64(0.12537045331483476)}
2026-02-12 13:43:43 - INFO - 
Total time taken: 386.33 seconds
2026-02-12 13:43:43 - INFO - Trial 4 finished with value: 0.5455381328800173 and parameters: {'learning_rate': 0.00010881154498307519, 'weight_decay': 0.0004075495400872856, 'batch_size': 16, 'co_train_epochs': 14, 'epoch_patience': 9}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 13:43:43 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 13:43:43 - INFO - Devices: cuda:1, cuda:1
2026-02-12 13:43:43 - INFO - Starting log
2026-02-12 13:43:43 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:43:43 - INFO - Learning Rate: 0.0001814567482521381
Weight Decay: 1.2662683193187517e-05
Batch Size: 64
No. Epochs: 14
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-12 13:43:44 - INFO - Generating initial weights
2026-02-12 13:43:51 - INFO - Time taken for Epoch 1:6.33 - F1: 0.0776
2026-02-12 13:43:58 - INFO - Time taken for Epoch 2:6.28 - F1: 0.1726
2026-02-12 13:44:04 - INFO - Time taken for Epoch 3:6.26 - F1: 0.3518
2026-02-12 13:44:10 - INFO - Time taken for Epoch 4:6.29 - F1: 0.3084
2026-02-12 13:44:16 - INFO - Time taken for Epoch 5:6.27 - F1: 0.2994
2026-02-12 13:44:23 - INFO - Time taken for Epoch 6:6.24 - F1: 0.3354
2026-02-12 13:44:29 - INFO - Time taken for Epoch 7:6.21 - F1: 0.3106
2026-02-12 13:44:35 - INFO - Time taken for Epoch 8:6.28 - F1: 0.2862
2026-02-12 13:44:41 - INFO - Time taken for Epoch 9:6.30 - F1: 0.3038
2026-02-12 13:44:48 - INFO - Time taken for Epoch 10:6.27 - F1: 0.3037
2026-02-12 13:44:54 - INFO - Time taken for Epoch 11:6.28 - F1: 0.3031
2026-02-12 13:45:00 - INFO - Time taken for Epoch 12:6.28 - F1: 0.3102
2026-02-12 13:45:07 - INFO - Time taken for Epoch 13:6.28 - F1: 0.3084
2026-02-12 13:45:13 - INFO - Time taken for Epoch 14:6.28 - F1: 0.2838
2026-02-12 13:45:13 - INFO - Best F1:0.3518 - Best Epoch:3
2026-02-12 13:45:14 - INFO - Starting co-training
2026-02-12 13:45:30 - INFO - Time taken for Epoch 1: 16.35s - F1: 0.47799364
2026-02-12 13:45:47 - INFO - Time taken for Epoch 2: 17.05s - F1: 0.45002816
2026-02-12 13:46:04 - INFO - Time taken for Epoch 3: 16.35s - F1: 0.50837899
2026-02-12 13:46:21 - INFO - Time taken for Epoch 4: 17.34s - F1: 0.50179367
2026-02-12 13:46:38 - INFO - Time taken for Epoch 5: 16.41s - F1: 0.53343135
2026-02-12 13:47:01 - INFO - Time taken for Epoch 6: 23.65s - F1: 0.50114906
2026-02-12 13:47:18 - INFO - Time taken for Epoch 7: 16.44s - F1: 0.54776913
2026-02-12 13:47:50 - INFO - Time taken for Epoch 8: 32.81s - F1: 0.50060257
2026-02-12 13:48:07 - INFO - Time taken for Epoch 9: 16.29s - F1: 0.51563722
2026-02-12 13:48:23 - INFO - Time taken for Epoch 10: 16.34s - F1: 0.52765261
2026-02-12 13:48:39 - INFO - Time taken for Epoch 11: 16.31s - F1: 0.51298623
2026-02-12 13:48:56 - INFO - Time taken for Epoch 12: 16.42s - F1: 0.48235260
2026-02-12 13:49:12 - INFO - Time taken for Epoch 13: 16.37s - F1: 0.51180914
2026-02-12 13:49:29 - INFO - Time taken for Epoch 14: 16.39s - F1: 0.57474910
2026-02-12 13:49:32 - INFO - Fine-tuning models
2026-02-12 13:49:33 - INFO - Time taken for Epoch 1:1.10 - F1: 0.5501
2026-02-12 13:49:35 - INFO - Time taken for Epoch 2:2.13 - F1: 0.5410
2026-02-12 13:49:36 - INFO - Time taken for Epoch 3:1.08 - F1: 0.5383
2026-02-12 13:49:37 - INFO - Time taken for Epoch 4:1.07 - F1: 0.5346
2026-02-12 13:49:39 - INFO - Time taken for Epoch 5:1.08 - F1: 0.5301
2026-02-12 13:49:40 - INFO - Time taken for Epoch 6:1.09 - F1: 0.5241
2026-02-12 13:49:41 - INFO - Time taken for Epoch 7:1.08 - F1: 0.5633
2026-02-12 13:50:04 - INFO - Time taken for Epoch 8:23.21 - F1: 0.5577
2026-02-12 13:50:05 - INFO - Time taken for Epoch 9:1.06 - F1: 0.5611
2026-02-12 13:50:06 - INFO - Time taken for Epoch 10:1.05 - F1: 0.5789
2026-02-12 13:50:08 - INFO - Time taken for Epoch 11:2.09 - F1: 0.5895
2026-02-12 13:50:10 - INFO - Time taken for Epoch 12:2.26 - F1: 0.5895
2026-02-12 13:50:11 - INFO - Time taken for Epoch 13:1.05 - F1: 0.5895
2026-02-12 13:50:12 - INFO - Time taken for Epoch 14:1.06 - F1: 0.5943
2026-02-12 13:50:15 - INFO - Time taken for Epoch 15:2.19 - F1: 0.5966
2026-02-12 13:50:17 - INFO - Time taken for Epoch 16:2.10 - F1: 0.5966
2026-02-12 13:50:18 - INFO - Time taken for Epoch 17:1.06 - F1: 0.6046
2026-02-12 13:50:20 - INFO - Time taken for Epoch 18:2.18 - F1: 0.5982
2026-02-12 13:50:21 - INFO - Time taken for Epoch 19:1.06 - F1: 0.5856
2026-02-12 13:50:22 - INFO - Time taken for Epoch 20:1.06 - F1: 0.5856
2026-02-12 13:50:23 - INFO - Time taken for Epoch 21:1.06 - F1: 0.6088
2026-02-12 13:50:25 - INFO - Time taken for Epoch 22:2.14 - F1: 0.6107
2026-02-12 13:50:27 - INFO - Time taken for Epoch 23:2.12 - F1: 0.6052
2026-02-12 13:50:29 - INFO - Time taken for Epoch 24:1.06 - F1: 0.6050
2026-02-12 13:50:30 - INFO - Time taken for Epoch 25:1.07 - F1: 0.6050
2026-02-12 13:50:33 - INFO - Time taken for Epoch 26:3.16 - F1: 0.6032
2026-02-12 13:50:34 - INFO - Time taken for Epoch 27:1.06 - F1: 0.6054
2026-02-12 13:50:35 - INFO - Time taken for Epoch 28:1.06 - F1: 0.5919
2026-02-12 13:50:36 - INFO - Time taken for Epoch 29:1.06 - F1: 0.5777
2026-02-12 13:50:37 - INFO - Time taken for Epoch 30:1.06 - F1: 0.5903
2026-02-12 13:50:38 - INFO - Time taken for Epoch 31:1.07 - F1: 0.5859
2026-02-12 13:50:39 - INFO - Time taken for Epoch 32:1.06 - F1: 0.5630
2026-02-12 13:50:39 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 13:50:39 - INFO - Best F1:0.6107 - Best Epoch:21
2026-02-12 13:50:43 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5808, Test ECE: 0.0337
2026-02-12 13:50:43 - INFO - All results: {'f1_macro': 0.5807813324571327, 'ece': np.float64(0.03368324657504478)}
2026-02-12 13:50:43 - INFO - 
Total time taken: 420.08 seconds
2026-02-12 13:50:43 - INFO - Trial 5 finished with value: 0.5807813324571327 and parameters: {'learning_rate': 0.0001814567482521381, 'weight_decay': 1.2662683193187517e-05, 'batch_size': 64, 'co_train_epochs': 14, 'epoch_patience': 10}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 13:50:43 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 13:50:43 - INFO - Devices: cuda:1, cuda:1
2026-02-12 13:50:43 - INFO - Starting log
2026-02-12 13:50:43 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:50:43 - INFO - Learning Rate: 0.0005404857170085565
Weight Decay: 2.0122664609925472e-05
Batch Size: 8
No. Epochs: 14
Epoch Patience: 6
 Accumulation Steps: 8
2026-02-12 13:50:44 - INFO - Generating initial weights
2026-02-12 13:50:55 - INFO - Time taken for Epoch 1:9.79 - F1: 0.1123
2026-02-12 13:51:05 - INFO - Time taken for Epoch 2:9.75 - F1: 0.0254
2026-02-12 13:51:14 - INFO - Time taken for Epoch 3:9.52 - F1: 0.0591
2026-02-12 13:51:24 - INFO - Time taken for Epoch 4:9.73 - F1: 0.1035
2026-02-12 13:51:34 - INFO - Time taken for Epoch 5:9.81 - F1: 0.1650
2026-02-12 13:51:43 - INFO - Time taken for Epoch 6:9.71 - F1: 0.1992
2026-02-12 13:51:53 - INFO - Time taken for Epoch 7:9.73 - F1: 0.1874
2026-02-12 13:52:03 - INFO - Time taken for Epoch 8:9.84 - F1: 0.2468
2026-02-12 13:52:13 - INFO - Time taken for Epoch 9:9.81 - F1: 0.2646
2026-02-12 13:52:23 - INFO - Time taken for Epoch 10:9.69 - F1: 0.2695
2026-02-12 13:52:32 - INFO - Time taken for Epoch 11:9.73 - F1: 0.2562
2026-02-12 13:52:42 - INFO - Time taken for Epoch 12:9.91 - F1: 0.2151
2026-02-12 13:53:05 - INFO - Time taken for Epoch 13:22.48 - F1: 0.2548
2026-02-12 13:53:14 - INFO - Time taken for Epoch 14:9.49 - F1: 0.2410
2026-02-12 13:53:14 - INFO - Best F1:0.2695 - Best Epoch:10
2026-02-12 13:53:15 - INFO - Starting co-training
2026-02-12 13:53:28 - INFO - Time taken for Epoch 1: 12.45s - F1: 0.09274327
2026-02-12 13:53:41 - INFO - Time taken for Epoch 2: 13.29s - F1: 0.07352941
2026-02-12 13:53:54 - INFO - Time taken for Epoch 3: 12.47s - F1: 0.03651685
2026-02-12 13:54:06 - INFO - Time taken for Epoch 4: 12.54s - F1: 0.07352941
2026-02-12 13:54:18 - INFO - Time taken for Epoch 5: 12.42s - F1: 0.07352941
2026-02-12 13:54:31 - INFO - Time taken for Epoch 6: 12.45s - F1: 0.07352941
2026-02-12 13:54:44 - INFO - Time taken for Epoch 7: 12.63s - F1: 0.07352941
2026-02-12 13:54:44 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-12 13:54:46 - INFO - Fine-tuning models
2026-02-12 13:54:47 - INFO - Time taken for Epoch 1:1.53 - F1: 0.0247
2026-02-12 13:54:50 - INFO - Time taken for Epoch 2:2.35 - F1: 0.0115
2026-02-12 13:54:51 - INFO - Time taken for Epoch 3:1.48 - F1: 0.0085
2026-02-12 13:54:52 - INFO - Time taken for Epoch 4:1.47 - F1: 0.0308
2026-02-12 13:55:03 - INFO - Time taken for Epoch 5:10.43 - F1: 0.0308
2026-02-12 13:55:04 - INFO - Time taken for Epoch 6:1.47 - F1: 0.0115
2026-02-12 13:55:06 - INFO - Time taken for Epoch 7:1.47 - F1: 0.0115
2026-02-12 13:55:07 - INFO - Time taken for Epoch 8:1.46 - F1: 0.0115
2026-02-12 13:55:09 - INFO - Time taken for Epoch 9:1.48 - F1: 0.0115
2026-02-12 13:55:10 - INFO - Time taken for Epoch 10:1.48 - F1: 0.0115
2026-02-12 13:55:12 - INFO - Time taken for Epoch 11:1.49 - F1: 0.0085
2026-02-12 13:55:13 - INFO - Time taken for Epoch 12:1.48 - F1: 0.0085
2026-02-12 13:55:15 - INFO - Time taken for Epoch 13:1.47 - F1: 0.0085
2026-02-12 13:55:16 - INFO - Time taken for Epoch 14:1.48 - F1: 0.0085
2026-02-12 13:55:16 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 13:55:16 - INFO - Best F1:0.0308 - Best Epoch:3
2026-02-12 13:55:20 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0301, Test ECE: 0.2415
2026-02-12 13:55:20 - INFO - All results: {'f1_macro': 0.030138339920948616, 'ece': np.float64(0.24147274641508468)}
2026-02-12 13:55:20 - INFO - 
Total time taken: 277.65 seconds
2026-02-12 13:55:20 - INFO - Trial 6 finished with value: 0.030138339920948616 and parameters: {'learning_rate': 0.0005404857170085565, 'weight_decay': 2.0122664609925472e-05, 'batch_size': 8, 'co_train_epochs': 14, 'epoch_patience': 6}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 13:55:20 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 13:55:20 - INFO - Devices: cuda:1, cuda:1
2026-02-12 13:55:20 - INFO - Starting log
2026-02-12 13:55:20 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:55:21 - INFO - Learning Rate: 0.000488328919422565
Weight Decay: 0.008667542141488804
Batch Size: 8
No. Epochs: 17
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-12 13:55:22 - INFO - Generating initial weights
2026-02-12 13:55:32 - INFO - Time taken for Epoch 1:9.78 - F1: 0.1120
2026-02-12 13:55:42 - INFO - Time taken for Epoch 2:9.79 - F1: 0.0277
2026-02-12 13:55:52 - INFO - Time taken for Epoch 3:9.75 - F1: 0.0670
2026-02-12 13:56:02 - INFO - Time taken for Epoch 4:9.99 - F1: 0.1109
2026-02-12 13:56:12 - INFO - Time taken for Epoch 5:9.94 - F1: 0.1863
2026-02-12 13:56:22 - INFO - Time taken for Epoch 6:9.84 - F1: 0.2484
2026-02-12 13:56:32 - INFO - Time taken for Epoch 7:9.78 - F1: 0.2577
2026-02-12 13:56:41 - INFO - Time taken for Epoch 8:9.92 - F1: 0.2266
2026-02-12 13:56:51 - INFO - Time taken for Epoch 9:9.72 - F1: 0.2513
2026-02-12 13:57:01 - INFO - Time taken for Epoch 10:9.73 - F1: 0.2760
2026-02-12 13:57:11 - INFO - Time taken for Epoch 11:9.71 - F1: 0.3395
2026-02-12 13:57:21 - INFO - Time taken for Epoch 12:9.94 - F1: 0.3526
2026-02-12 13:57:30 - INFO - Time taken for Epoch 13:9.81 - F1: 0.3734
2026-02-12 13:57:40 - INFO - Time taken for Epoch 14:9.87 - F1: 0.3666
2026-02-12 13:57:50 - INFO - Time taken for Epoch 15:9.91 - F1: 0.3310
2026-02-12 13:58:00 - INFO - Time taken for Epoch 16:9.81 - F1: 0.3234
2026-02-12 13:58:09 - INFO - Time taken for Epoch 17:9.53 - F1: 0.3199
2026-02-12 13:58:09 - INFO - Best F1:0.3734 - Best Epoch:13
2026-02-12 13:58:11 - INFO - Starting co-training
2026-02-12 13:58:23 - INFO - Time taken for Epoch 1: 12.58s - F1: 0.03651685
2026-02-12 13:58:38 - INFO - Time taken for Epoch 2: 14.56s - F1: 0.03651685
2026-02-12 13:58:50 - INFO - Time taken for Epoch 3: 12.35s - F1: 0.03651685
2026-02-12 13:59:03 - INFO - Time taken for Epoch 4: 12.83s - F1: 0.03651685
2026-02-12 13:59:16 - INFO - Time taken for Epoch 5: 12.82s - F1: 0.03651685
2026-02-12 13:59:16 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-12 13:59:18 - INFO - Fine-tuning models
2026-02-12 13:59:20 - INFO - Time taken for Epoch 1:1.55 - F1: 0.0247
2026-02-12 13:59:22 - INFO - Time taken for Epoch 2:2.61 - F1: 0.0115
2026-02-12 13:59:24 - INFO - Time taken for Epoch 3:1.51 - F1: 0.0115
2026-02-12 13:59:25 - INFO - Time taken for Epoch 4:1.50 - F1: 0.0308
2026-02-12 13:59:28 - INFO - Time taken for Epoch 5:2.64 - F1: 0.0115
2026-02-12 13:59:29 - INFO - Time taken for Epoch 6:1.48 - F1: 0.0115
2026-02-12 13:59:31 - INFO - Time taken for Epoch 7:1.51 - F1: 0.0365
2026-02-12 13:59:44 - INFO - Time taken for Epoch 8:13.00 - F1: 0.0365
2026-02-12 13:59:45 - INFO - Time taken for Epoch 9:1.51 - F1: 0.0365
2026-02-12 13:59:47 - INFO - Time taken for Epoch 10:1.50 - F1: 0.0365
2026-02-12 13:59:48 - INFO - Time taken for Epoch 11:1.48 - F1: 0.0365
2026-02-12 13:59:50 - INFO - Time taken for Epoch 12:1.50 - F1: 0.0365
2026-02-12 13:59:51 - INFO - Time taken for Epoch 13:1.48 - F1: 0.0308
2026-02-12 13:59:53 - INFO - Time taken for Epoch 14:1.46 - F1: 0.0308
2026-02-12 13:59:54 - INFO - Time taken for Epoch 15:1.46 - F1: 0.0308
2026-02-12 13:59:56 - INFO - Time taken for Epoch 16:1.47 - F1: 0.0085
2026-02-12 13:59:57 - INFO - Time taken for Epoch 17:1.47 - F1: 0.0085
2026-02-12 13:59:57 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 13:59:57 - INFO - Best F1:0.0365 - Best Epoch:6
2026-02-12 14:00:01 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0361, Test ECE: 0.1546
2026-02-12 14:00:01 - INFO - All results: {'f1_macro': 0.036057692307692304, 'ece': np.float64(0.15458893307139362)}
2026-02-12 14:00:01 - INFO - 
Total time taken: 280.97 seconds
2026-02-12 14:00:01 - INFO - Trial 7 finished with value: 0.036057692307692304 and parameters: {'learning_rate': 0.000488328919422565, 'weight_decay': 0.008667542141488804, 'batch_size': 8, 'co_train_epochs': 17, 'epoch_patience': 4}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 14:00:01 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 14:00:01 - INFO - Devices: cuda:1, cuda:1
2026-02-12 14:00:01 - INFO - Starting log
2026-02-12 14:00:01 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 14:00:02 - INFO - Learning Rate: 0.0003855909865201637
Weight Decay: 0.003075778056591817
Batch Size: 64
No. Epochs: 9
Epoch Patience: 6
 Accumulation Steps: 1
2026-02-12 14:00:03 - INFO - Generating initial weights
2026-02-12 14:00:10 - INFO - Time taken for Epoch 1:6.38 - F1: 0.1028
2026-02-12 14:00:18 - INFO - Time taken for Epoch 2:8.33 - F1: 0.2311
2026-02-12 14:00:25 - INFO - Time taken for Epoch 3:6.24 - F1: 0.3565
2026-02-12 14:00:31 - INFO - Time taken for Epoch 4:6.24 - F1: 0.2716
2026-02-12 14:00:37 - INFO - Time taken for Epoch 5:6.27 - F1: 0.3056
2026-02-12 14:00:43 - INFO - Time taken for Epoch 6:6.26 - F1: 0.3287
2026-02-12 14:00:50 - INFO - Time taken for Epoch 7:6.26 - F1: 0.2996
2026-02-12 14:00:56 - INFO - Time taken for Epoch 8:6.33 - F1: 0.2788
2026-02-12 14:01:02 - INFO - Time taken for Epoch 9:6.31 - F1: 0.2770
2026-02-12 14:01:02 - INFO - Best F1:0.3565 - Best Epoch:3
2026-02-12 14:01:03 - INFO - Starting co-training
2026-02-12 14:01:20 - INFO - Time taken for Epoch 1: 16.35s - F1: 0.07352941
2026-02-12 14:01:37 - INFO - Time taken for Epoch 2: 17.20s - F1: 0.07352941
2026-02-12 14:01:53 - INFO - Time taken for Epoch 3: 16.42s - F1: 0.07352941
2026-02-12 14:02:10 - INFO - Time taken for Epoch 4: 16.36s - F1: 0.07352941
2026-02-12 14:02:26 - INFO - Time taken for Epoch 5: 16.42s - F1: 0.07352941
2026-02-12 14:02:43 - INFO - Time taken for Epoch 6: 16.42s - F1: 0.07352941
2026-02-12 14:02:59 - INFO - Time taken for Epoch 7: 16.53s - F1: 0.07352941
2026-02-12 14:02:59 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-12 14:03:01 - INFO - Fine-tuning models
2026-02-12 14:03:02 - INFO - Time taken for Epoch 1:1.10 - F1: 0.0500
2026-02-12 14:03:04 - INFO - Time taken for Epoch 2:1.85 - F1: 0.0118
2026-02-12 14:03:05 - INFO - Time taken for Epoch 3:1.07 - F1: 0.0115
2026-02-12 14:03:06 - INFO - Time taken for Epoch 4:1.07 - F1: 0.0022
2026-02-12 14:03:07 - INFO - Time taken for Epoch 5:1.07 - F1: 0.0085
2026-02-12 14:03:09 - INFO - Time taken for Epoch 6:1.06 - F1: 0.0022
2026-02-12 14:03:10 - INFO - Time taken for Epoch 7:1.07 - F1: 0.0735
2026-02-12 14:03:15 - INFO - Time taken for Epoch 8:5.50 - F1: 0.0735
2026-02-12 14:03:16 - INFO - Time taken for Epoch 9:1.06 - F1: 0.0115
2026-02-12 14:03:17 - INFO - Time taken for Epoch 10:1.06 - F1: 0.0115
2026-02-12 14:03:18 - INFO - Time taken for Epoch 11:1.06 - F1: 0.0115
2026-02-12 14:03:19 - INFO - Time taken for Epoch 12:1.06 - F1: 0.0115
2026-02-12 14:03:20 - INFO - Time taken for Epoch 13:1.06 - F1: 0.0115
2026-02-12 14:03:21 - INFO - Time taken for Epoch 14:1.06 - F1: 0.0115
2026-02-12 14:03:23 - INFO - Time taken for Epoch 15:1.06 - F1: 0.0308
2026-02-12 14:03:24 - INFO - Time taken for Epoch 16:1.06 - F1: 0.0308
2026-02-12 14:03:25 - INFO - Time taken for Epoch 17:1.06 - F1: 0.0735
2026-02-12 14:03:25 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 14:03:25 - INFO - Best F1:0.0735 - Best Epoch:6
2026-02-12 14:03:28 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0737, Test ECE: 0.0874
2026-02-12 14:03:28 - INFO - All results: {'f1_macro': 0.07369255150554675, 'ece': np.float64(0.08739623126019253)}
2026-02-12 14:03:28 - INFO - 
Total time taken: 206.78 seconds
2026-02-12 14:03:28 - INFO - Trial 8 finished with value: 0.07369255150554675 and parameters: {'learning_rate': 0.0003855909865201637, 'weight_decay': 0.003075778056591817, 'batch_size': 64, 'co_train_epochs': 9, 'epoch_patience': 6}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 14:03:28 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 14:03:28 - INFO - Devices: cuda:1, cuda:1
2026-02-12 14:03:28 - INFO - Starting log
2026-02-12 14:03:28 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 14:03:29 - INFO - Learning Rate: 4.834400254535544e-05
Weight Decay: 1.3837245005237278e-05
Batch Size: 16
No. Epochs: 17
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-12 14:03:30 - INFO - Generating initial weights
2026-02-12 14:03:38 - INFO - Time taken for Epoch 1:7.84 - F1: 0.1139
2026-02-12 14:03:46 - INFO - Time taken for Epoch 2:7.83 - F1: 0.0894
2026-02-12 14:03:54 - INFO - Time taken for Epoch 3:7.76 - F1: 0.1090
2026-02-12 14:04:02 - INFO - Time taken for Epoch 4:7.78 - F1: 0.1168
2026-02-12 14:04:09 - INFO - Time taken for Epoch 5:7.79 - F1: 0.1235
2026-02-12 14:04:17 - INFO - Time taken for Epoch 6:7.88 - F1: 0.1230
2026-02-12 14:04:25 - INFO - Time taken for Epoch 7:7.77 - F1: 0.1224
2026-02-12 14:04:33 - INFO - Time taken for Epoch 8:7.79 - F1: 0.1215
2026-02-12 14:04:41 - INFO - Time taken for Epoch 9:7.69 - F1: 0.1252
2026-02-12 14:04:48 - INFO - Time taken for Epoch 10:7.77 - F1: 0.1102
2026-02-12 14:04:56 - INFO - Time taken for Epoch 11:7.86 - F1: 0.1099
2026-02-12 14:05:04 - INFO - Time taken for Epoch 12:7.73 - F1: 0.1306
2026-02-12 14:05:12 - INFO - Time taken for Epoch 13:7.80 - F1: 0.1364
2026-02-12 14:05:20 - INFO - Time taken for Epoch 14:7.81 - F1: 0.1359
2026-02-12 14:05:27 - INFO - Time taken for Epoch 15:7.73 - F1: 0.1354
2026-02-12 14:05:35 - INFO - Time taken for Epoch 16:7.82 - F1: 0.1380
2026-02-12 14:05:43 - INFO - Time taken for Epoch 17:7.76 - F1: 0.1456
2026-02-12 14:05:43 - INFO - Best F1:0.1456 - Best Epoch:17
2026-02-12 14:05:44 - INFO - Starting co-training
2026-02-12 14:05:56 - INFO - Time taken for Epoch 1: 12.12s - F1: 0.18250269
2026-02-12 14:06:14 - INFO - Time taken for Epoch 2: 17.78s - F1: 0.25847200
2026-02-12 14:06:27 - INFO - Time taken for Epoch 3: 12.87s - F1: 0.40555776
2026-02-12 14:06:39 - INFO - Time taken for Epoch 4: 12.77s - F1: 0.41802275
2026-02-12 14:06:57 - INFO - Time taken for Epoch 5: 17.14s - F1: 0.50383403
2026-02-12 14:07:09 - INFO - Time taken for Epoch 6: 12.89s - F1: 0.47885487
2026-02-12 14:07:22 - INFO - Time taken for Epoch 7: 12.04s - F1: 0.47620070
2026-02-12 14:07:33 - INFO - Time taken for Epoch 8: 11.89s - F1: 0.45997224
2026-02-12 14:07:45 - INFO - Time taken for Epoch 9: 12.03s - F1: 0.49479217
2026-02-12 14:07:45 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-12 14:07:47 - INFO - Fine-tuning models
2026-02-12 14:07:49 - INFO - Time taken for Epoch 1:1.24 - F1: 0.5030
2026-02-12 14:07:51 - INFO - Time taken for Epoch 2:2.19 - F1: 0.4897
2026-02-12 14:07:52 - INFO - Time taken for Epoch 3:1.22 - F1: 0.4814
2026-02-12 14:07:53 - INFO - Time taken for Epoch 4:1.21 - F1: 0.4899
2026-02-12 14:07:55 - INFO - Time taken for Epoch 5:1.22 - F1: 0.4891
2026-02-12 14:07:56 - INFO - Time taken for Epoch 6:1.20 - F1: 0.4947
2026-02-12 14:07:57 - INFO - Time taken for Epoch 7:1.21 - F1: 0.4969
2026-02-12 14:07:58 - INFO - Time taken for Epoch 8:1.20 - F1: 0.4746
2026-02-12 14:08:00 - INFO - Time taken for Epoch 9:1.20 - F1: 0.4772
2026-02-12 14:08:01 - INFO - Time taken for Epoch 10:1.20 - F1: 0.5519
2026-02-12 14:08:03 - INFO - Time taken for Epoch 11:2.11 - F1: 0.5631
2026-02-12 14:08:05 - INFO - Time taken for Epoch 12:2.06 - F1: 0.5635
2026-02-12 14:08:23 - INFO - Time taken for Epoch 13:17.77 - F1: 0.5825
2026-02-12 14:08:25 - INFO - Time taken for Epoch 14:2.05 - F1: 0.5512
2026-02-12 14:08:26 - INFO - Time taken for Epoch 15:1.20 - F1: 0.5383
2026-02-12 14:08:27 - INFO - Time taken for Epoch 16:1.19 - F1: 0.5473
2026-02-12 14:08:28 - INFO - Time taken for Epoch 17:1.20 - F1: 0.5626
2026-02-12 14:08:30 - INFO - Time taken for Epoch 18:1.20 - F1: 0.5744
2026-02-12 14:08:31 - INFO - Time taken for Epoch 19:1.20 - F1: 0.5744
2026-02-12 14:08:32 - INFO - Time taken for Epoch 20:1.20 - F1: 0.5706
2026-02-12 14:08:33 - INFO - Time taken for Epoch 21:1.20 - F1: 0.5754
2026-02-12 14:08:34 - INFO - Time taken for Epoch 22:1.21 - F1: 0.5750
2026-02-12 14:08:36 - INFO - Time taken for Epoch 23:1.20 - F1: 0.5798
2026-02-12 14:08:36 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 14:08:36 - INFO - Best F1:0.5825 - Best Epoch:12
2026-02-12 14:08:39 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5210, Test ECE: 0.1185
2026-02-12 14:08:39 - INFO - All results: {'f1_macro': 0.521039578763062, 'ece': np.float64(0.11850319658772328)}
2026-02-12 14:08:39 - INFO - 
Total time taken: 311.11 seconds
2026-02-12 14:08:39 - INFO - Trial 9 finished with value: 0.521039578763062 and parameters: {'learning_rate': 4.834400254535544e-05, 'weight_decay': 1.3837245005237278e-05, 'batch_size': 16, 'co_train_epochs': 17, 'epoch_patience': 4}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 14:08:39 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 14:08:39 - INFO - Devices: cuda:1, cuda:1
2026-02-12 14:08:39 - INFO - Starting log
2026-02-12 14:08:39 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 14:08:40 - INFO - Learning Rate: 1.1802795913696346e-05
Weight Decay: 7.970788035794766e-05
Batch Size: 64
No. Epochs: 5
Epoch Patience: 8
 Accumulation Steps: 1
2026-02-12 14:08:41 - INFO - Generating initial weights
2026-02-12 14:08:48 - INFO - Time taken for Epoch 1:6.33 - F1: 0.0023
2026-02-12 14:08:54 - INFO - Time taken for Epoch 2:6.26 - F1: 0.0023
2026-02-12 14:09:00 - INFO - Time taken for Epoch 3:6.26 - F1: 0.0023
2026-02-12 14:09:07 - INFO - Time taken for Epoch 4:6.18 - F1: 0.0023
2026-02-12 14:09:14 - INFO - Time taken for Epoch 5:7.41 - F1: 0.0023
2026-02-12 14:09:14 - INFO - Best F1:0.0023 - Best Epoch:5
2026-02-12 14:09:15 - INFO - Starting co-training
2026-02-12 14:09:32 - INFO - Time taken for Epoch 1: 16.33s - F1: 0.07352941
2026-02-12 14:09:49 - INFO - Time taken for Epoch 2: 17.28s - F1: 0.14627541
2026-02-12 14:10:17 - INFO - Time taken for Epoch 3: 27.73s - F1: 0.16463734
2026-02-12 14:10:34 - INFO - Time taken for Epoch 4: 17.35s - F1: 0.25518091
2026-02-12 14:11:05 - INFO - Time taken for Epoch 5: 31.07s - F1: 0.35826815
2026-02-12 14:11:08 - INFO - Fine-tuning models
2026-02-12 14:11:09 - INFO - Time taken for Epoch 1:1.10 - F1: 0.3837
2026-02-12 14:11:11 - INFO - Time taken for Epoch 2:1.97 - F1: 0.3840
2026-02-12 14:11:13 - INFO - Time taken for Epoch 3:2.06 - F1: 0.3810
2026-02-12 14:11:14 - INFO - Time taken for Epoch 4:1.06 - F1: 0.3848
2026-02-12 14:11:16 - INFO - Time taken for Epoch 5:2.04 - F1: 0.3767
2026-02-12 14:11:18 - INFO - Time taken for Epoch 6:1.07 - F1: 0.3937
2026-02-12 14:11:20 - INFO - Time taken for Epoch 7:2.06 - F1: 0.4055
2026-02-12 14:11:22 - INFO - Time taken for Epoch 8:2.11 - F1: 0.3982
2026-02-12 14:11:23 - INFO - Time taken for Epoch 9:1.07 - F1: 0.4241
2026-02-12 14:11:25 - INFO - Time taken for Epoch 10:2.12 - F1: 0.4286
2026-02-12 14:11:27 - INFO - Time taken for Epoch 11:2.08 - F1: 0.4307
2026-02-12 14:11:29 - INFO - Time taken for Epoch 12:2.10 - F1: 0.4270
2026-02-12 14:11:30 - INFO - Time taken for Epoch 13:1.06 - F1: 0.4270
2026-02-12 14:11:31 - INFO - Time taken for Epoch 14:1.06 - F1: 0.4257
2026-02-12 14:11:32 - INFO - Time taken for Epoch 15:1.06 - F1: 0.4459
2026-02-12 14:11:34 - INFO - Time taken for Epoch 16:2.05 - F1: 0.4528
2026-02-12 14:11:36 - INFO - Time taken for Epoch 17:2.08 - F1: 0.4693
2026-02-12 14:11:39 - INFO - Time taken for Epoch 18:2.15 - F1: 0.4754
2026-02-12 14:11:41 - INFO - Time taken for Epoch 19:2.13 - F1: 0.4757
2026-02-12 14:11:51 - INFO - Time taken for Epoch 20:9.92 - F1: 0.4763
2026-02-12 14:11:53 - INFO - Time taken for Epoch 21:2.07 - F1: 0.4763
2026-02-12 14:11:54 - INFO - Time taken for Epoch 22:1.08 - F1: 0.4757
2026-02-12 14:11:55 - INFO - Time taken for Epoch 23:1.06 - F1: 0.4757
2026-02-12 14:11:56 - INFO - Time taken for Epoch 24:1.06 - F1: 0.4757
2026-02-12 14:11:57 - INFO - Time taken for Epoch 25:1.06 - F1: 0.4748
2026-02-12 14:11:58 - INFO - Time taken for Epoch 26:1.07 - F1: 0.4742
2026-02-12 14:11:59 - INFO - Time taken for Epoch 27:1.08 - F1: 0.4742
2026-02-12 14:12:00 - INFO - Time taken for Epoch 28:1.07 - F1: 0.4742
2026-02-12 14:12:01 - INFO - Time taken for Epoch 29:1.07 - F1: 0.4729
2026-02-12 14:12:02 - INFO - Time taken for Epoch 30:1.07 - F1: 0.4735
2026-02-12 14:12:02 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 14:12:02 - INFO - Best F1:0.4763 - Best Epoch:19
2026-02-12 14:12:06 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5221, Test ECE: 0.0631
2026-02-12 14:12:06 - INFO - All results: {'f1_macro': 0.5221435169994388, 'ece': np.float64(0.06305111780595243)}
2026-02-12 14:12:06 - INFO - 
Total time taken: 206.62 seconds
2026-02-12 14:12:06 - INFO - Trial 10 finished with value: 0.5221435169994388 and parameters: {'learning_rate': 1.1802795913696346e-05, 'weight_decay': 7.970788035794766e-05, 'batch_size': 64, 'co_train_epochs': 5, 'epoch_patience': 8}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 14:12:06 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 14:12:06 - INFO - Devices: cuda:1, cuda:1
2026-02-12 14:12:06 - INFO - Starting log
2026-02-12 14:12:06 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 14:12:07 - INFO - Learning Rate: 2.394495002527454e-05
Weight Decay: 9.913780987344649e-05
Batch Size: 16
No. Epochs: 20
Epoch Patience: 7
 Accumulation Steps: 4
2026-02-12 14:12:08 - INFO - Generating initial weights
2026-02-12 14:12:16 - INFO - Time taken for Epoch 1:7.89 - F1: 0.0509
2026-02-12 14:12:24 - INFO - Time taken for Epoch 2:7.61 - F1: 0.0942
2026-02-12 14:12:32 - INFO - Time taken for Epoch 3:7.71 - F1: 0.0678
2026-02-12 14:12:40 - INFO - Time taken for Epoch 4:7.81 - F1: 0.0869
2026-02-12 14:12:47 - INFO - Time taken for Epoch 5:7.89 - F1: 0.0884
2026-02-12 14:12:55 - INFO - Time taken for Epoch 6:7.83 - F1: 0.0917
2026-02-12 14:13:03 - INFO - Time taken for Epoch 7:7.90 - F1: 0.0948
2026-02-12 14:13:11 - INFO - Time taken for Epoch 8:7.64 - F1: 0.1117
2026-02-12 14:13:18 - INFO - Time taken for Epoch 9:7.64 - F1: 0.1164
2026-02-12 14:13:26 - INFO - Time taken for Epoch 10:7.62 - F1: 0.1202
2026-02-12 14:13:34 - INFO - Time taken for Epoch 11:7.73 - F1: 0.1248
2026-02-12 14:13:42 - INFO - Time taken for Epoch 12:7.88 - F1: 0.1205
2026-02-12 14:13:50 - INFO - Time taken for Epoch 13:7.90 - F1: 0.1213
2026-02-12 14:13:57 - INFO - Time taken for Epoch 14:7.76 - F1: 0.1219
2026-02-12 14:14:05 - INFO - Time taken for Epoch 15:7.82 - F1: 0.1275
2026-02-12 14:14:13 - INFO - Time taken for Epoch 16:7.84 - F1: 0.1325
2026-02-12 14:14:21 - INFO - Time taken for Epoch 17:7.84 - F1: 0.1201
2026-02-12 14:14:29 - INFO - Time taken for Epoch 18:7.92 - F1: 0.1171
2026-02-12 14:14:37 - INFO - Time taken for Epoch 19:7.84 - F1: 0.1205
2026-02-12 14:14:44 - INFO - Time taken for Epoch 20:7.86 - F1: 0.1170
2026-02-12 14:14:44 - INFO - Best F1:0.1325 - Best Epoch:16
2026-02-12 14:14:46 - INFO - Starting co-training
2026-02-12 14:14:58 - INFO - Time taken for Epoch 1: 11.98s - F1: 0.15770397
2026-02-12 14:15:11 - INFO - Time taken for Epoch 2: 13.05s - F1: 0.22392198
2026-02-12 14:15:30 - INFO - Time taken for Epoch 3: 19.05s - F1: 0.27377946
2026-02-12 14:15:43 - INFO - Time taken for Epoch 4: 13.18s - F1: 0.33844682
2026-02-12 14:15:56 - INFO - Time taken for Epoch 5: 13.29s - F1: 0.46204139
2026-02-12 14:16:16 - INFO - Time taken for Epoch 6: 19.87s - F1: 0.45201547
2026-02-12 14:16:28 - INFO - Time taken for Epoch 7: 12.01s - F1: 0.48677437
2026-02-12 14:16:48 - INFO - Time taken for Epoch 8: 20.34s - F1: 0.51640125
2026-02-12 14:17:02 - INFO - Time taken for Epoch 9: 13.09s - F1: 0.51014791
2026-02-12 14:17:14 - INFO - Time taken for Epoch 10: 12.19s - F1: 0.43146228
2026-02-12 14:17:26 - INFO - Time taken for Epoch 11: 12.03s - F1: 0.49375369
2026-02-12 14:17:38 - INFO - Time taken for Epoch 12: 11.97s - F1: 0.49627101
2026-02-12 14:17:50 - INFO - Time taken for Epoch 13: 12.17s - F1: 0.48532517
2026-02-12 14:18:02 - INFO - Time taken for Epoch 14: 12.06s - F1: 0.48792699
2026-02-12 14:18:14 - INFO - Time taken for Epoch 15: 12.14s - F1: 0.50872008
2026-02-12 14:18:14 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-12 14:18:16 - INFO - Fine-tuning models
2026-02-12 14:18:18 - INFO - Time taken for Epoch 1:1.24 - F1: 0.5035
2026-02-12 14:18:22 - INFO - Time taken for Epoch 2:3.74 - F1: 0.4955
2026-02-12 14:18:23 - INFO - Time taken for Epoch 3:1.21 - F1: 0.4918
2026-02-12 14:18:24 - INFO - Time taken for Epoch 4:1.20 - F1: 0.4886
2026-02-12 14:18:25 - INFO - Time taken for Epoch 5:1.20 - F1: 0.4787
2026-02-12 14:18:26 - INFO - Time taken for Epoch 6:1.22 - F1: 0.4802
2026-02-12 14:18:28 - INFO - Time taken for Epoch 7:1.20 - F1: 0.4778
2026-02-12 14:18:29 - INFO - Time taken for Epoch 8:1.21 - F1: 0.4797
2026-02-12 14:18:30 - INFO - Time taken for Epoch 9:1.21 - F1: 0.4858
2026-02-12 14:18:31 - INFO - Time taken for Epoch 10:1.21 - F1: 0.4876
2026-02-12 14:18:32 - INFO - Time taken for Epoch 11:1.21 - F1: 0.4893
2026-02-12 14:18:32 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 14:18:32 - INFO - Best F1:0.5035 - Best Epoch:0
2026-02-12 14:18:36 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5194, Test ECE: 0.0456
2026-02-12 14:18:37 - INFO - All results: {'f1_macro': 0.5194419101879242, 'ece': np.float64(0.045618888233484846)}
2026-02-12 14:18:37 - INFO - 
Total time taken: 390.82 seconds
2026-02-12 14:18:37 - INFO - Trial 11 finished with value: 0.5194419101879242 and parameters: {'learning_rate': 2.394495002527454e-05, 'weight_decay': 9.913780987344649e-05, 'batch_size': 16, 'co_train_epochs': 20, 'epoch_patience': 7}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 14:18:37 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 14:18:37 - INFO - Devices: cuda:1, cuda:1
2026-02-12 14:18:37 - INFO - Starting log
2026-02-12 14:18:37 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 14:18:38 - INFO - Learning Rate: 3.109669913639902e-05
Weight Decay: 0.00010221565740826076
Batch Size: 32
No. Epochs: 5
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-12 14:18:39 - INFO - Generating initial weights
2026-02-12 14:18:46 - INFO - Time taken for Epoch 1:7.02 - F1: 0.0171
2026-02-12 14:18:53 - INFO - Time taken for Epoch 2:7.00 - F1: 0.0162
2026-02-12 14:19:00 - INFO - Time taken for Epoch 3:6.97 - F1: 0.0316
2026-02-12 14:19:07 - INFO - Time taken for Epoch 4:6.89 - F1: 0.0652
2026-02-12 14:19:14 - INFO - Time taken for Epoch 5:7.00 - F1: 0.0821
2026-02-12 14:19:14 - INFO - Best F1:0.0821 - Best Epoch:5
2026-02-12 14:19:15 - INFO - Starting co-training
2026-02-12 14:19:29 - INFO - Time taken for Epoch 1: 13.42s - F1: 0.15875332
2026-02-12 14:19:43 - INFO - Time taken for Epoch 2: 14.36s - F1: 0.28135522
2026-02-12 14:19:58 - INFO - Time taken for Epoch 3: 14.34s - F1: 0.35433152
2026-02-12 14:20:12 - INFO - Time taken for Epoch 4: 14.44s - F1: 0.43137214
2026-02-12 14:20:34 - INFO - Time taken for Epoch 5: 21.88s - F1: 0.45366051
2026-02-12 14:20:37 - INFO - Fine-tuning models
2026-02-12 14:20:38 - INFO - Time taken for Epoch 1:1.18 - F1: 0.4671
2026-02-12 14:20:40 - INFO - Time taken for Epoch 2:2.05 - F1: 0.4646
2026-02-12 14:20:41 - INFO - Time taken for Epoch 3:1.15 - F1: 0.4514
2026-02-12 14:20:43 - INFO - Time taken for Epoch 4:1.14 - F1: 0.4504
2026-02-12 14:20:44 - INFO - Time taken for Epoch 5:1.14 - F1: 0.4517
2026-02-12 14:20:45 - INFO - Time taken for Epoch 6:1.17 - F1: 0.4624
2026-02-12 14:20:46 - INFO - Time taken for Epoch 7:1.14 - F1: 0.4592
2026-02-12 14:20:47 - INFO - Time taken for Epoch 8:1.14 - F1: 0.4650
2026-02-12 14:20:48 - INFO - Time taken for Epoch 9:1.14 - F1: 0.5432
2026-02-12 14:20:51 - INFO - Time taken for Epoch 10:2.16 - F1: 0.5619
2026-02-12 14:20:53 - INFO - Time taken for Epoch 11:2.14 - F1: 0.5560
2026-02-12 14:20:54 - INFO - Time taken for Epoch 12:1.15 - F1: 0.5669
2026-02-12 14:21:20 - INFO - Time taken for Epoch 13:26.42 - F1: 0.5739
2026-02-12 14:21:22 - INFO - Time taken for Epoch 14:2.18 - F1: 0.5759
2026-02-12 14:21:25 - INFO - Time taken for Epoch 15:2.18 - F1: 0.5845
2026-02-12 14:21:27 - INFO - Time taken for Epoch 16:2.19 - F1: 0.5965
2026-02-12 14:21:29 - INFO - Time taken for Epoch 17:2.19 - F1: 0.5973
2026-02-12 14:21:31 - INFO - Time taken for Epoch 18:2.17 - F1: 0.6043
2026-02-12 14:21:33 - INFO - Time taken for Epoch 19:2.19 - F1: 0.5981
2026-02-12 14:21:34 - INFO - Time taken for Epoch 20:1.15 - F1: 0.5837
2026-02-12 14:21:36 - INFO - Time taken for Epoch 21:1.15 - F1: 0.5891
2026-02-12 14:21:37 - INFO - Time taken for Epoch 22:1.14 - F1: 0.5838
2026-02-12 14:21:38 - INFO - Time taken for Epoch 23:1.14 - F1: 0.5850
2026-02-12 14:21:39 - INFO - Time taken for Epoch 24:1.15 - F1: 0.5903
2026-02-12 14:21:40 - INFO - Time taken for Epoch 25:1.14 - F1: 0.5847
2026-02-12 14:21:41 - INFO - Time taken for Epoch 26:1.14 - F1: 0.5825
2026-02-12 14:21:42 - INFO - Time taken for Epoch 27:1.15 - F1: 0.5969
2026-02-12 14:21:44 - INFO - Time taken for Epoch 28:1.15 - F1: 0.5969
2026-02-12 14:21:44 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 14:21:44 - INFO - Best F1:0.6043 - Best Epoch:17
2026-02-12 14:21:47 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5689, Test ECE: 0.1082
2026-02-12 14:21:47 - INFO - All results: {'f1_macro': 0.5688900060428896, 'ece': np.float64(0.10817037713661623)}
2026-02-12 14:21:47 - INFO - 
Total time taken: 190.59 seconds
2026-02-12 14:21:48 - INFO - Trial 12 finished with value: 0.5688900060428896 and parameters: {'learning_rate': 3.109669913639902e-05, 'weight_decay': 0.00010221565740826076, 'batch_size': 32, 'co_train_epochs': 5, 'epoch_patience': 6}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 14:21:48 - INFO - Using devices: cuda:1, cuda:1
2026-02-12 14:21:48 - INFO - Devices: cuda:1, cuda:1
2026-02-12 14:21:48 - INFO - Starting log
2026-02-12 14:21:48 - INFO - Dataset: humanitarian8, Event: canada_wildfires_2016, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 14:21:49 - INFO - Learning Rate: 1.023771425934111e-05
Weight Decay: 0.007243408261697904
Batch Size: 64
No. Epochs: 9
Epoch Patience: 8
 Accumulation Steps: 1
2026-02-12 14:21:50 - INFO - Generating initial weights
2026-02-12 14:21:57 - INFO - Time taken for Epoch 1:6.33 - F1: 0.0022
2026-02-12 14:22:03 - INFO - Time taken for Epoch 2:6.23 - F1: 0.0023
2026-02-12 14:22:09 - INFO - Time taken for Epoch 3:6.25 - F1: 0.0023
2026-02-12 14:22:15 - INFO - Time taken for Epoch 4:6.31 - F1: 0.0023
2026-02-12 14:22:22 - INFO - Time taken for Epoch 5:6.26 - F1: 0.0102
2026-02-12 14:22:28 - INFO - Time taken for Epoch 6:6.26 - F1: 0.0102
2026-02-12 14:22:34 - INFO - Time taken for Epoch 7:6.30 - F1: 0.0103
2026-02-12 14:22:41 - INFO - Time taken for Epoch 8:6.34 - F1: 0.0098
2026-02-12 14:22:47 - INFO - Time taken for Epoch 9:6.31 - F1: 0.0168
2026-02-12 14:22:47 - INFO - Best F1:0.0168 - Best Epoch:9
2026-02-12 14:22:48 - INFO - Starting co-training
2026-02-12 14:23:05 - INFO - Time taken for Epoch 1: 16.51s - F1: 0.07352941
2026-02-12 14:23:22 - INFO - Time taken for Epoch 2: 17.43s - F1: 0.13960584
2026-02-12 14:23:51 - INFO - Time taken for Epoch 3: 29.11s - F1: 0.16292735
2026-02-12 14:24:09 - INFO - Time taken for Epoch 4: 17.47s - F1: 0.27140386
2026-02-12 14:24:35 - INFO - Time taken for Epoch 5: 26.63s - F1: 0.34429772
2026-02-12 14:24:53 - INFO - Time taken for Epoch 6: 17.89s - F1: 0.34205444
2026-02-12 14:25:09 - INFO - Time taken for Epoch 7: 16.27s - F1: 0.41792714
2026-02-12 14:25:27 - INFO - Time taken for Epoch 8: 17.47s - F1: 0.45355024
2026-02-12 14:25:44 - INFO - Time taken for Epoch 9: 17.48s - F1: 0.46775564
2026-02-12 14:25:58 - INFO - Fine-tuning models
2026-02-12 14:26:00 - INFO - Time taken for Epoch 1:1.10 - F1: 0.4833
2026-02-12 14:26:02 - INFO - Time taken for Epoch 2:2.62 - F1: 0.4811
2026-02-12 14:26:03 - INFO - Time taken for Epoch 3:1.07 - F1: 0.4856
2026-02-12 14:26:06 - INFO - Time taken for Epoch 4:2.24 - F1: 0.4856
2026-02-12 14:26:07 - INFO - Time taken for Epoch 5:1.06 - F1: 0.4957
2026-02-12 14:26:09 - INFO - Time taken for Epoch 6:2.37 - F1: 0.4950
2026-02-12 14:26:10 - INFO - Time taken for Epoch 7:1.06 - F1: 0.4950
2026-02-12 14:26:11 - INFO - Time taken for Epoch 8:1.06 - F1: 0.4946
2026-02-12 14:26:12 - INFO - Time taken for Epoch 9:1.06 - F1: 0.4992
2026-02-12 14:26:14 - INFO - Time taken for Epoch 10:2.28 - F1: 0.4938
2026-02-12 14:26:15 - INFO - Time taken for Epoch 11:1.07 - F1: 0.4939
2026-02-12 14:26:17 - INFO - Time taken for Epoch 12:1.09 - F1: 0.4885
2026-02-12 14:26:18 - INFO - Time taken for Epoch 13:1.06 - F1: 0.4871
2026-02-12 14:26:19 - INFO - Time taken for Epoch 14:1.06 - F1: 0.4814
2026-02-12 14:26:20 - INFO - Time taken for Epoch 15:1.05 - F1: 0.4761
2026-02-12 14:26:21 - INFO - Time taken for Epoch 16:1.06 - F1: 0.4774
2026-02-12 14:26:22 - INFO - Time taken for Epoch 17:1.06 - F1: 0.4872
2026-02-12 14:26:23 - INFO - Time taken for Epoch 18:1.06 - F1: 0.4864
2026-02-12 14:26:24 - INFO - Time taken for Epoch 19:1.06 - F1: 0.4909
2026-02-12 14:26:24 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 14:26:24 - INFO - Best F1:0.4992 - Best Epoch:8
2026-02-12 14:26:28 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian8, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5206, Test ECE: 0.1256
2026-02-12 14:26:28 - INFO - All results: {'f1_macro': 0.5206125415719994, 'ece': np.float64(0.12562304554360637)}
2026-02-12 14:26:28 - INFO - 
Total time taken: 280.16 seconds
2026-02-12 14:26:28 - INFO - Trial 13 finished with value: 0.5206125415719994 and parameters: {'learning_rate': 1.023771425934111e-05, 'weight_decay': 0.007243408261697904, 'batch_size': 64, 'co_train_epochs': 9, 'epoch_patience': 8}. Best is trial 0 with value: 0.6029784764951207.
2026-02-12 14:26:28 - INFO - 
[BEST TRIAL RESULTS]
2026-02-12 14:26:28 - INFO - F1 Score: 0.6030
2026-02-12 14:26:28 - INFO - Params: {'learning_rate': 2.473901800273487e-05, 'weight_decay': 0.00010503193951806903, 'batch_size': 64, 'co_train_epochs': 8, 'epoch_patience': 9}
2026-02-12 14:26:28 - INFO -   learning_rate: 2.473901800273487e-05
2026-02-12 14:26:28 - INFO -   weight_decay: 0.00010503193951806903
2026-02-12 14:26:28 - INFO -   batch_size: 64
2026-02-12 14:26:28 - INFO -   co_train_epochs: 8
2026-02-12 14:26:28 - INFO -   epoch_patience: 9
2026-02-12 14:26:28 - INFO - 
Total time taken: 4245.87 seconds
