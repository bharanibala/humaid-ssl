2026-02-13 22:16:24 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 22:16:24 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_maria_2017
2026-02-13 22:16:24 - INFO - Using devices: cuda:1, cuda:1
2026-02-13 22:16:24 - INFO - Devices: cuda:1, cuda:1
2026-02-13 22:16:24 - INFO - Starting log
2026-02-13 22:16:24 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 22:16:25 - INFO - Learning Rate: 0.0002830223099677685
Weight Decay: 1.496014529314776e-05
Batch Size: 32
No. Epochs: 18
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-13 22:16:27 - INFO - Generating initial weights
2026-02-13 22:16:53 - INFO - Time taken for Epoch 1:23.71 - F1: 0.0089
2026-02-13 22:17:16 - INFO - Time taken for Epoch 2:23.35 - F1: 0.0038
2026-02-13 22:17:39 - INFO - Time taken for Epoch 3:23.27 - F1: 0.0189
2026-02-13 22:18:03 - INFO - Time taken for Epoch 4:23.31 - F1: 0.0189
2026-02-13 22:18:26 - INFO - Time taken for Epoch 5:23.49 - F1: 0.0189
2026-02-13 22:18:49 - INFO - Time taken for Epoch 6:23.41 - F1: 0.0189
2026-02-13 22:19:13 - INFO - Time taken for Epoch 7:23.44 - F1: 0.0189
2026-02-13 22:19:36 - INFO - Time taken for Epoch 8:23.28 - F1: 0.0189
2026-02-13 22:20:00 - INFO - Time taken for Epoch 9:23.34 - F1: 0.0189
2026-02-13 22:20:23 - INFO - Time taken for Epoch 10:23.41 - F1: 0.0189
2026-02-13 22:20:46 - INFO - Time taken for Epoch 11:23.44 - F1: 0.0189
2026-02-13 22:21:10 - INFO - Time taken for Epoch 12:23.49 - F1: 0.0189
2026-02-13 22:21:33 - INFO - Time taken for Epoch 13:23.30 - F1: 0.0189
2026-02-13 22:21:57 - INFO - Time taken for Epoch 14:23.36 - F1: 0.0189
2026-02-13 22:22:20 - INFO - Time taken for Epoch 15:23.44 - F1: 0.0189
2026-02-13 22:22:43 - INFO - Time taken for Epoch 16:23.47 - F1: 0.0189
2026-02-13 22:23:07 - INFO - Time taken for Epoch 17:23.44 - F1: 0.0189
2026-02-13 22:23:30 - INFO - Time taken for Epoch 18:23.29 - F1: 0.0189
2026-02-13 22:23:30 - INFO - Best F1:0.0189 - Best Epoch:3
2026-02-13 22:23:32 - INFO - Starting co-training
2026-02-13 22:24:13 - INFO - Time taken for Epoch 1: 41.39s - F1: 0.04755179
2026-02-13 22:24:56 - INFO - Time taken for Epoch 2: 42.54s - F1: 0.04755179
2026-02-13 22:25:37 - INFO - Time taken for Epoch 3: 41.16s - F1: 0.04755179
2026-02-13 22:26:18 - INFO - Time taken for Epoch 4: 41.12s - F1: 0.04755179
2026-02-13 22:26:59 - INFO - Time taken for Epoch 5: 41.34s - F1: 0.04755179
2026-02-13 22:27:40 - INFO - Time taken for Epoch 6: 41.05s - F1: 0.04755179
2026-02-13 22:28:22 - INFO - Time taken for Epoch 7: 41.30s - F1: 0.04755179
2026-02-13 22:29:03 - INFO - Time taken for Epoch 8: 41.53s - F1: 0.04755179
2026-02-13 22:29:45 - INFO - Time taken for Epoch 9: 41.41s - F1: 0.04755179
2026-02-13 22:30:26 - INFO - Time taken for Epoch 10: 41.64s - F1: 0.04755179
2026-02-13 22:30:26 - INFO - Performance not improving for 9 consecutive epochs.
2026-02-13 22:30:29 - INFO - Fine-tuning models
2026-02-13 22:30:35 - INFO - Time taken for Epoch 1:5.68 - F1: 0.0081
2026-02-13 22:30:41 - INFO - Time taken for Epoch 2:6.61 - F1: 0.0089
2026-02-13 22:30:48 - INFO - Time taken for Epoch 3:6.86 - F1: 0.0189
2026-02-13 22:30:55 - INFO - Time taken for Epoch 4:7.04 - F1: 0.0189
2026-02-13 22:31:01 - INFO - Time taken for Epoch 5:5.60 - F1: 0.0189
2026-02-13 22:31:06 - INFO - Time taken for Epoch 6:5.57 - F1: 0.0189
2026-02-13 22:31:12 - INFO - Time taken for Epoch 7:5.68 - F1: 0.0189
2026-02-13 22:31:18 - INFO - Time taken for Epoch 8:5.60 - F1: 0.0189
2026-02-13 22:31:23 - INFO - Time taken for Epoch 9:5.61 - F1: 0.0189
2026-02-13 22:31:29 - INFO - Time taken for Epoch 10:5.61 - F1: 0.0189
2026-02-13 22:31:34 - INFO - Time taken for Epoch 11:5.62 - F1: 0.0189
2026-02-13 22:31:40 - INFO - Time taken for Epoch 12:5.59 - F1: 0.0189
2026-02-13 22:31:46 - INFO - Time taken for Epoch 13:5.59 - F1: 0.0189
2026-02-13 22:31:46 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 22:31:46 - INFO - Best F1:0.0189 - Best Epoch:2
2026-02-13 22:31:54 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.0188, Test ECE: 0.1913
2026-02-13 22:31:54 - INFO - All results: {'f1_macro': 0.018765432098765432, 'ece': np.float64(0.19126115958419487)}
2026-02-13 22:31:54 - INFO - 
Total time taken: 929.58 seconds
2026-02-13 22:31:54 - INFO - Trial 0 finished with value: 0.018765432098765432 and parameters: {'learning_rate': 0.0002830223099677685, 'weight_decay': 1.496014529314776e-05, 'batch_size': 32, 'co_train_epochs': 18, 'epoch_patience': 9}. Best is trial 0 with value: 0.018765432098765432.
2026-02-13 22:31:54 - INFO - Using devices: cuda:1, cuda:1
2026-02-13 22:31:54 - INFO - Devices: cuda:1, cuda:1
2026-02-13 22:31:54 - INFO - Starting log
2026-02-13 22:31:54 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 22:31:54 - INFO - Learning Rate: 0.0003735509411371687
Weight Decay: 0.001594867264168228
Batch Size: 64
No. Epochs: 13
Epoch Patience: 5
 Accumulation Steps: 1
2026-02-13 22:31:55 - INFO - Generating initial weights
2026-02-13 22:32:19 - INFO - Time taken for Epoch 1:21.00 - F1: 0.0344
2026-02-13 22:32:39 - INFO - Time taken for Epoch 2:20.87 - F1: 0.0038
2026-02-13 22:33:00 - INFO - Time taken for Epoch 3:20.85 - F1: 0.0038
2026-02-13 22:33:21 - INFO - Time taken for Epoch 4:20.94 - F1: 0.0189
2026-02-13 22:33:42 - INFO - Time taken for Epoch 5:20.95 - F1: 0.0189
2026-02-13 22:34:03 - INFO - Time taken for Epoch 6:21.08 - F1: 0.0189
2026-02-13 22:34:24 - INFO - Time taken for Epoch 7:20.93 - F1: 0.0189
2026-02-13 22:34:45 - INFO - Time taken for Epoch 8:20.89 - F1: 0.0189
2026-02-13 22:35:06 - INFO - Time taken for Epoch 9:20.89 - F1: 0.0189
2026-02-13 22:35:27 - INFO - Time taken for Epoch 10:20.93 - F1: 0.0189
2026-02-13 22:35:48 - INFO - Time taken for Epoch 11:20.82 - F1: 0.0189
2026-02-13 22:36:08 - INFO - Time taken for Epoch 12:20.83 - F1: 0.0189
2026-02-13 22:36:29 - INFO - Time taken for Epoch 13:20.80 - F1: 0.0189
2026-02-13 22:36:29 - INFO - Best F1:0.0344 - Best Epoch:1
2026-02-13 22:36:30 - INFO - Starting co-training
2026-02-13 22:37:22 - INFO - Time taken for Epoch 1: 50.77s - F1: 0.30733024
2026-02-13 22:38:13 - INFO - Time taken for Epoch 2: 51.69s - F1: 0.04755179
2026-02-13 22:39:04 - INFO - Time taken for Epoch 3: 50.84s - F1: 0.04755179
2026-02-13 22:39:55 - INFO - Time taken for Epoch 4: 50.60s - F1: 0.04755179
2026-02-13 22:40:45 - INFO - Time taken for Epoch 5: 50.64s - F1: 0.04755179
2026-02-13 22:41:36 - INFO - Time taken for Epoch 6: 50.80s - F1: 0.04755179
2026-02-13 22:41:36 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-13 22:41:49 - INFO - Fine-tuning models
2026-02-13 22:41:54 - INFO - Time taken for Epoch 1:5.01 - F1: 0.1356
2026-02-13 22:42:00 - INFO - Time taken for Epoch 2:5.98 - F1: 0.1556
2026-02-13 22:42:06 - INFO - Time taken for Epoch 3:6.18 - F1: 0.1736
2026-02-13 22:42:12 - INFO - Time taken for Epoch 4:6.13 - F1: 0.1394
2026-02-13 22:42:17 - INFO - Time taken for Epoch 5:4.96 - F1: 0.0506
2026-02-13 22:42:22 - INFO - Time taken for Epoch 6:4.94 - F1: 0.0189
2026-02-13 22:42:27 - INFO - Time taken for Epoch 7:4.98 - F1: 0.0189
2026-02-13 22:42:32 - INFO - Time taken for Epoch 8:5.02 - F1: 0.0189
2026-02-13 22:42:37 - INFO - Time taken for Epoch 9:4.97 - F1: 0.0189
2026-02-13 22:42:42 - INFO - Time taken for Epoch 10:5.01 - F1: 0.0189
2026-02-13 22:42:47 - INFO - Time taken for Epoch 11:4.97 - F1: 0.0189
2026-02-13 22:42:52 - INFO - Time taken for Epoch 12:4.96 - F1: 0.0189
2026-02-13 22:42:57 - INFO - Time taken for Epoch 13:4.97 - F1: 0.0189
2026-02-13 22:42:57 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 22:42:57 - INFO - Best F1:0.1736 - Best Epoch:2
2026-02-13 22:43:04 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.1697, Test ECE: 0.0804
2026-02-13 22:43:04 - INFO - All results: {'f1_macro': 0.1697056582828649, 'ece': np.float64(0.08037248365426032)}
2026-02-13 22:43:04 - INFO - 
Total time taken: 670.54 seconds
2026-02-13 22:43:04 - INFO - Trial 1 finished with value: 0.1697056582828649 and parameters: {'learning_rate': 0.0003735509411371687, 'weight_decay': 0.001594867264168228, 'batch_size': 64, 'co_train_epochs': 13, 'epoch_patience': 5}. Best is trial 1 with value: 0.1697056582828649.
2026-02-13 22:43:04 - INFO - Using devices: cuda:1, cuda:1
2026-02-13 22:43:04 - INFO - Devices: cuda:1, cuda:1
2026-02-13 22:43:04 - INFO - Starting log
2026-02-13 22:43:04 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 22:43:05 - INFO - Learning Rate: 0.00023947041632604572
Weight Decay: 0.0003392186780918139
Batch Size: 16
No. Epochs: 5
Epoch Patience: 5
 Accumulation Steps: 4
2026-02-13 22:43:06 - INFO - Generating initial weights
2026-02-13 22:43:35 - INFO - Time taken for Epoch 1:26.95 - F1: 0.0180
2026-02-13 22:44:02 - INFO - Time taken for Epoch 2:26.95 - F1: 0.0089
2026-02-13 22:44:29 - INFO - Time taken for Epoch 3:27.13 - F1: 0.0089
2026-02-13 22:44:56 - INFO - Time taken for Epoch 4:26.67 - F1: 0.0189
2026-02-13 22:45:23 - INFO - Time taken for Epoch 5:26.78 - F1: 0.0189
2026-02-13 22:45:23 - INFO - Best F1:0.0189 - Best Epoch:4
2026-02-13 22:45:24 - INFO - Starting co-training
2026-02-13 22:46:02 - INFO - Time taken for Epoch 1: 38.00s - F1: 0.12202593
2026-02-13 22:46:41 - INFO - Time taken for Epoch 2: 38.57s - F1: 0.04755179
2026-02-13 22:47:19 - INFO - Time taken for Epoch 3: 37.86s - F1: 0.04755179
2026-02-13 22:47:56 - INFO - Time taken for Epoch 4: 37.72s - F1: 0.04755179
2026-02-13 22:48:39 - INFO - Time taken for Epoch 5: 42.32s - F1: 0.04755179
2026-02-13 22:48:41 - INFO - Fine-tuning models
2026-02-13 22:48:48 - INFO - Time taken for Epoch 1:6.50 - F1: 0.0666
2026-02-13 22:48:55 - INFO - Time taken for Epoch 2:7.48 - F1: 0.0425
2026-02-13 22:49:02 - INFO - Time taken for Epoch 3:6.43 - F1: 0.0414
2026-02-13 22:49:08 - INFO - Time taken for Epoch 4:6.39 - F1: 0.0276
2026-02-13 22:49:14 - INFO - Time taken for Epoch 5:6.41 - F1: 0.0471
2026-02-13 22:49:21 - INFO - Time taken for Epoch 6:6.44 - F1: 0.0434
2026-02-13 22:49:27 - INFO - Time taken for Epoch 7:6.39 - F1: 0.0434
2026-02-13 22:49:34 - INFO - Time taken for Epoch 8:6.37 - F1: 0.0830
2026-02-13 22:49:41 - INFO - Time taken for Epoch 9:7.60 - F1: 0.0365
2026-02-13 22:49:47 - INFO - Time taken for Epoch 10:6.34 - F1: 0.0189
2026-02-13 22:49:54 - INFO - Time taken for Epoch 11:6.41 - F1: 0.0189
2026-02-13 22:50:00 - INFO - Time taken for Epoch 12:6.43 - F1: 0.0189
2026-02-13 22:50:07 - INFO - Time taken for Epoch 13:6.41 - F1: 0.0189
2026-02-13 22:50:13 - INFO - Time taken for Epoch 14:6.48 - F1: 0.0189
2026-02-13 22:50:20 - INFO - Time taken for Epoch 15:6.42 - F1: 0.0189
2026-02-13 22:50:26 - INFO - Time taken for Epoch 16:6.41 - F1: 0.0189
2026-02-13 22:50:32 - INFO - Time taken for Epoch 17:6.41 - F1: 0.0189
2026-02-13 22:50:39 - INFO - Time taken for Epoch 18:6.43 - F1: 0.0189
2026-02-13 22:50:39 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 22:50:39 - INFO - Best F1:0.0830 - Best Epoch:7
2026-02-13 22:50:47 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.0750, Test ECE: 0.0971
2026-02-13 22:50:47 - INFO - All results: {'f1_macro': 0.07504611623872087, 'ece': np.float64(0.09705497785348667)}
2026-02-13 22:50:47 - INFO - 
Total time taken: 463.16 seconds
2026-02-13 22:50:47 - INFO - Trial 2 finished with value: 0.07504611623872087 and parameters: {'learning_rate': 0.00023947041632604572, 'weight_decay': 0.0003392186780918139, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 5}. Best is trial 1 with value: 0.1697056582828649.
2026-02-13 22:50:47 - INFO - Using devices: cuda:1, cuda:1
2026-02-13 22:50:47 - INFO - Devices: cuda:1, cuda:1
2026-02-13 22:50:47 - INFO - Starting log
2026-02-13 22:50:47 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 22:50:48 - INFO - Learning Rate: 0.0001305858178576094
Weight Decay: 0.000551684795891728
Batch Size: 64
No. Epochs: 13
Epoch Patience: 5
 Accumulation Steps: 1
2026-02-13 22:50:49 - INFO - Generating initial weights
2026-02-13 22:51:12 - INFO - Time taken for Epoch 1:21.02 - F1: 0.0800
2026-02-13 22:51:33 - INFO - Time taken for Epoch 2:20.99 - F1: 0.1041
2026-02-13 22:51:54 - INFO - Time taken for Epoch 3:20.94 - F1: 0.0762
2026-02-13 22:52:15 - INFO - Time taken for Epoch 4:21.02 - F1: 0.0855
2026-02-13 22:52:36 - INFO - Time taken for Epoch 5:20.88 - F1: 0.1848
2026-02-13 22:52:57 - INFO - Time taken for Epoch 6:20.92 - F1: 0.2938
2026-02-13 22:53:18 - INFO - Time taken for Epoch 7:21.04 - F1: 0.3579
2026-02-13 22:53:39 - INFO - Time taken for Epoch 8:20.94 - F1: 0.4283
2026-02-13 22:54:00 - INFO - Time taken for Epoch 9:20.92 - F1: 0.4639
2026-02-13 22:54:21 - INFO - Time taken for Epoch 10:20.98 - F1: 0.4571
2026-02-13 22:54:42 - INFO - Time taken for Epoch 11:21.01 - F1: 0.5174
2026-02-13 22:55:03 - INFO - Time taken for Epoch 12:21.08 - F1: 0.5159
2026-02-13 22:55:24 - INFO - Time taken for Epoch 13:20.93 - F1: 0.5207
2026-02-13 22:55:24 - INFO - Best F1:0.5207 - Best Epoch:13
2026-02-13 22:55:25 - INFO - Starting co-training
2026-02-13 22:56:17 - INFO - Time taken for Epoch 1: 50.92s - F1: 0.61745861
2026-02-13 22:57:08 - INFO - Time taken for Epoch 2: 51.92s - F1: 0.61583708
2026-02-13 22:57:59 - INFO - Time taken for Epoch 3: 50.80s - F1: 0.63329673
2026-02-13 22:58:51 - INFO - Time taken for Epoch 4: 51.84s - F1: 0.64630746
2026-02-13 22:59:43 - INFO - Time taken for Epoch 5: 51.67s - F1: 0.59926135
2026-02-13 23:00:33 - INFO - Time taken for Epoch 6: 50.71s - F1: 0.63163550
2026-02-13 23:01:24 - INFO - Time taken for Epoch 7: 50.40s - F1: 0.61916925
2026-02-13 23:02:15 - INFO - Time taken for Epoch 8: 50.77s - F1: 0.60753478
2026-02-13 23:03:05 - INFO - Time taken for Epoch 9: 50.62s - F1: 0.63629631
2026-02-13 23:03:05 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-13 23:03:23 - INFO - Fine-tuning models
2026-02-13 23:03:28 - INFO - Time taken for Epoch 1:5.02 - F1: 0.6296
2026-02-13 23:03:35 - INFO - Time taken for Epoch 2:6.62 - F1: 0.6543
2026-02-13 23:03:41 - INFO - Time taken for Epoch 3:6.04 - F1: 0.6659
2026-02-13 23:03:47 - INFO - Time taken for Epoch 4:6.16 - F1: 0.6642
2026-02-13 23:03:52 - INFO - Time taken for Epoch 5:4.97 - F1: 0.6662
2026-02-13 23:03:58 - INFO - Time taken for Epoch 6:6.07 - F1: 0.6637
2026-02-13 23:04:03 - INFO - Time taken for Epoch 7:4.98 - F1: 0.6709
2026-02-13 23:04:18 - INFO - Time taken for Epoch 8:15.24 - F1: 0.6843
2026-02-13 23:04:25 - INFO - Time taken for Epoch 9:6.22 - F1: 0.7139
2026-02-13 23:04:31 - INFO - Time taken for Epoch 10:6.12 - F1: 0.7210
2026-02-13 23:04:37 - INFO - Time taken for Epoch 11:6.18 - F1: 0.7104
2026-02-13 23:04:42 - INFO - Time taken for Epoch 12:4.96 - F1: 0.7005
2026-02-13 23:04:47 - INFO - Time taken for Epoch 13:4.98 - F1: 0.7122
2026-02-13 23:04:52 - INFO - Time taken for Epoch 14:4.99 - F1: 0.7050
2026-02-13 23:04:57 - INFO - Time taken for Epoch 15:5.17 - F1: 0.7053
2026-02-13 23:05:02 - INFO - Time taken for Epoch 16:4.95 - F1: 0.7081
2026-02-13 23:05:07 - INFO - Time taken for Epoch 17:4.97 - F1: 0.7081
2026-02-13 23:05:12 - INFO - Time taken for Epoch 18:4.98 - F1: 0.7092
2026-02-13 23:05:17 - INFO - Time taken for Epoch 19:4.96 - F1: 0.7088
2026-02-13 23:05:22 - INFO - Time taken for Epoch 20:4.94 - F1: 0.7094
2026-02-13 23:05:22 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 23:05:22 - INFO - Best F1:0.7210 - Best Epoch:9
2026-02-13 23:05:29 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.6676, Test ECE: 0.0564
2026-02-13 23:05:29 - INFO - All results: {'f1_macro': 0.6675576528456106, 'ece': np.float64(0.05639395498038992)}
2026-02-13 23:05:29 - INFO - 
Total time taken: 881.89 seconds
2026-02-13 23:05:29 - INFO - Trial 3 finished with value: 0.6675576528456106 and parameters: {'learning_rate': 0.0001305858178576094, 'weight_decay': 0.000551684795891728, 'batch_size': 64, 'co_train_epochs': 13, 'epoch_patience': 5}. Best is trial 3 with value: 0.6675576528456106.
2026-02-13 23:05:29 - INFO - Using devices: cuda:1, cuda:1
2026-02-13 23:05:29 - INFO - Devices: cuda:1, cuda:1
2026-02-13 23:05:29 - INFO - Starting log
2026-02-13 23:05:29 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 23:05:30 - INFO - Learning Rate: 0.0006545981112410041
Weight Decay: 0.0002860734770364091
Batch Size: 8
No. Epochs: 10
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-13 23:05:31 - INFO - Generating initial weights
2026-02-13 23:06:08 - INFO - Time taken for Epoch 1:33.93 - F1: 0.0189
2026-02-13 23:06:41 - INFO - Time taken for Epoch 2:33.88 - F1: 0.0197
2026-02-13 23:07:15 - INFO - Time taken for Epoch 3:33.80 - F1: 0.0189
2026-02-13 23:07:49 - INFO - Time taken for Epoch 4:33.40 - F1: 0.0189
2026-02-13 23:08:22 - INFO - Time taken for Epoch 5:33.27 - F1: 0.0189
2026-02-13 23:08:55 - INFO - Time taken for Epoch 6:33.49 - F1: 0.0189
2026-02-13 23:09:29 - INFO - Time taken for Epoch 7:33.49 - F1: 0.0189
2026-02-13 23:10:02 - INFO - Time taken for Epoch 8:33.48 - F1: 0.0189
2026-02-13 23:10:36 - INFO - Time taken for Epoch 9:33.78 - F1: 0.0189
2026-02-13 23:11:10 - INFO - Time taken for Epoch 10:33.51 - F1: 0.0189
2026-02-13 23:11:10 - INFO - Best F1:0.0197 - Best Epoch:2
2026-02-13 23:11:11 - INFO - Starting co-training
2026-02-13 23:11:51 - INFO - Time taken for Epoch 1: 39.62s - F1: 0.04755179
2026-02-13 23:12:32 - INFO - Time taken for Epoch 2: 40.85s - F1: 0.04755179
2026-02-13 23:13:12 - INFO - Time taken for Epoch 3: 39.77s - F1: 0.04755179
2026-02-13 23:13:51 - INFO - Time taken for Epoch 4: 39.55s - F1: 0.04755179
2026-02-13 23:14:31 - INFO - Time taken for Epoch 5: 39.47s - F1: 0.04755179
2026-02-13 23:15:10 - INFO - Time taken for Epoch 6: 39.34s - F1: 0.04755179
2026-02-13 23:15:50 - INFO - Time taken for Epoch 7: 40.00s - F1: 0.04755179
2026-02-13 23:16:30 - INFO - Time taken for Epoch 8: 39.77s - F1: 0.04755179
2026-02-13 23:16:30 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-13 23:16:32 - INFO - Fine-tuning models
2026-02-13 23:16:40 - INFO - Time taken for Epoch 1:8.09 - F1: 0.0081
2026-02-13 23:16:49 - INFO - Time taken for Epoch 2:9.07 - F1: 0.0089
2026-02-13 23:16:59 - INFO - Time taken for Epoch 3:9.03 - F1: 0.0189
2026-02-13 23:17:20 - INFO - Time taken for Epoch 4:21.24 - F1: 0.0189
2026-02-13 23:17:28 - INFO - Time taken for Epoch 5:8.16 - F1: 0.0189
2026-02-13 23:17:36 - INFO - Time taken for Epoch 6:8.14 - F1: 0.0189
2026-02-13 23:17:44 - INFO - Time taken for Epoch 7:8.05 - F1: 0.0189
2026-02-13 23:17:52 - INFO - Time taken for Epoch 8:8.03 - F1: 0.0189
2026-02-13 23:18:00 - INFO - Time taken for Epoch 9:8.13 - F1: 0.0189
2026-02-13 23:18:08 - INFO - Time taken for Epoch 10:8.11 - F1: 0.0189
2026-02-13 23:18:16 - INFO - Time taken for Epoch 11:8.05 - F1: 0.0189
2026-02-13 23:18:24 - INFO - Time taken for Epoch 12:7.92 - F1: 0.0189
2026-02-13 23:18:32 - INFO - Time taken for Epoch 13:8.01 - F1: 0.0189
2026-02-13 23:18:32 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 23:18:32 - INFO - Best F1:0.0189 - Best Epoch:2
2026-02-13 23:18:48 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.0188, Test ECE: 0.2509
2026-02-13 23:18:48 - INFO - All results: {'f1_macro': 0.018765432098765432, 'ece': np.float64(0.25094687473674754)}
2026-02-13 23:18:48 - INFO - 
Total time taken: 798.25 seconds
2026-02-13 23:18:48 - INFO - Trial 4 finished with value: 0.018765432098765432 and parameters: {'learning_rate': 0.0006545981112410041, 'weight_decay': 0.0002860734770364091, 'batch_size': 8, 'co_train_epochs': 10, 'epoch_patience': 7}. Best is trial 3 with value: 0.6675576528456106.
2026-02-13 23:18:48 - INFO - Using devices: cuda:1, cuda:1
2026-02-13 23:18:48 - INFO - Devices: cuda:1, cuda:1
2026-02-13 23:18:48 - INFO - Starting log
2026-02-13 23:18:48 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 23:18:48 - INFO - Learning Rate: 0.0006686583296209358
Weight Decay: 2.6033703135445345e-05
Batch Size: 16
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 4
2026-02-13 23:18:49 - INFO - Generating initial weights
2026-02-13 23:19:19 - INFO - Time taken for Epoch 1:27.05 - F1: 0.0189
2026-02-13 23:19:45 - INFO - Time taken for Epoch 2:26.55 - F1: 0.0189
2026-02-13 23:20:12 - INFO - Time taken for Epoch 3:26.95 - F1: 0.0189
2026-02-13 23:20:39 - INFO - Time taken for Epoch 4:26.90 - F1: 0.0189
2026-02-13 23:21:06 - INFO - Time taken for Epoch 5:26.64 - F1: 0.0189
2026-02-13 23:21:32 - INFO - Time taken for Epoch 6:26.36 - F1: 0.0189
2026-02-13 23:21:59 - INFO - Time taken for Epoch 7:26.75 - F1: 0.0189
2026-02-13 23:22:26 - INFO - Time taken for Epoch 8:26.81 - F1: 0.0189
2026-02-13 23:22:26 - INFO - Best F1:0.0189 - Best Epoch:1
2026-02-13 23:22:27 - INFO - Starting co-training
2026-02-13 23:23:05 - INFO - Time taken for Epoch 1: 37.56s - F1: 0.04755179
2026-02-13 23:23:44 - INFO - Time taken for Epoch 2: 39.10s - F1: 0.04755179
2026-02-13 23:24:21 - INFO - Time taken for Epoch 3: 37.49s - F1: 0.04755179
2026-02-13 23:24:59 - INFO - Time taken for Epoch 4: 37.58s - F1: 0.04755179
2026-02-13 23:25:37 - INFO - Time taken for Epoch 5: 37.65s - F1: 0.04755179
2026-02-13 23:26:14 - INFO - Time taken for Epoch 6: 37.69s - F1: 0.04755179
2026-02-13 23:26:52 - INFO - Time taken for Epoch 7: 37.74s - F1: 0.04755179
2026-02-13 23:26:52 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-13 23:26:54 - INFO - Fine-tuning models
2026-02-13 23:27:01 - INFO - Time taken for Epoch 1:6.38 - F1: 0.0081
2026-02-13 23:27:09 - INFO - Time taken for Epoch 2:7.53 - F1: 0.0089
2026-02-13 23:27:16 - INFO - Time taken for Epoch 3:7.49 - F1: 0.0189
2026-02-13 23:27:24 - INFO - Time taken for Epoch 4:7.86 - F1: 0.0189
2026-02-13 23:27:30 - INFO - Time taken for Epoch 5:6.39 - F1: 0.0189
2026-02-13 23:27:37 - INFO - Time taken for Epoch 6:6.44 - F1: 0.0189
2026-02-13 23:27:43 - INFO - Time taken for Epoch 7:6.47 - F1: 0.0189
2026-02-13 23:27:50 - INFO - Time taken for Epoch 8:6.41 - F1: 0.0189
2026-02-13 23:27:56 - INFO - Time taken for Epoch 9:6.43 - F1: 0.0189
2026-02-13 23:28:02 - INFO - Time taken for Epoch 10:6.44 - F1: 0.0189
2026-02-13 23:28:09 - INFO - Time taken for Epoch 11:6.38 - F1: 0.0189
2026-02-13 23:28:15 - INFO - Time taken for Epoch 12:6.41 - F1: 0.0189
2026-02-13 23:28:22 - INFO - Time taken for Epoch 13:6.31 - F1: 0.0189
2026-02-13 23:28:22 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 23:28:22 - INFO - Best F1:0.0189 - Best Epoch:2
2026-02-13 23:28:30 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.0188, Test ECE: 0.1985
2026-02-13 23:28:30 - INFO - All results: {'f1_macro': 0.018765432098765432, 'ece': np.float64(0.19847818320402655)}
2026-02-13 23:28:30 - INFO - 
Total time taken: 582.42 seconds
2026-02-13 23:28:30 - INFO - Trial 5 finished with value: 0.018765432098765432 and parameters: {'learning_rate': 0.0006686583296209358, 'weight_decay': 2.6033703135445345e-05, 'batch_size': 16, 'co_train_epochs': 8, 'epoch_patience': 6}. Best is trial 3 with value: 0.6675576528456106.
2026-02-13 23:28:30 - INFO - Using devices: cuda:1, cuda:1
2026-02-13 23:28:30 - INFO - Devices: cuda:1, cuda:1
2026-02-13 23:28:30 - INFO - Starting log
2026-02-13 23:28:30 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 23:28:31 - INFO - Learning Rate: 0.000139533975523812
Weight Decay: 0.00040918170359423154
Batch Size: 8
No. Epochs: 7
Epoch Patience: 5
 Accumulation Steps: 8
2026-02-13 23:28:32 - INFO - Generating initial weights
2026-02-13 23:29:08 - INFO - Time taken for Epoch 1:33.92 - F1: 0.0616
2026-02-13 23:29:41 - INFO - Time taken for Epoch 2:33.50 - F1: 0.0526
2026-02-13 23:30:15 - INFO - Time taken for Epoch 3:33.84 - F1: 0.0532
2026-02-13 23:30:49 - INFO - Time taken for Epoch 4:33.86 - F1: 0.0521
2026-02-13 23:31:23 - INFO - Time taken for Epoch 5:33.68 - F1: 0.1379
2026-02-13 23:31:57 - INFO - Time taken for Epoch 6:33.77 - F1: 0.1800
2026-02-13 23:32:30 - INFO - Time taken for Epoch 7:33.92 - F1: 0.3219
2026-02-13 23:32:30 - INFO - Best F1:0.3219 - Best Epoch:7
2026-02-13 23:32:37 - INFO - Starting co-training
2026-02-13 23:33:17 - INFO - Time taken for Epoch 1: 39.61s - F1: 0.20119176
2026-02-13 23:33:58 - INFO - Time taken for Epoch 2: 41.06s - F1: 0.10298317
2026-02-13 23:34:38 - INFO - Time taken for Epoch 3: 39.78s - F1: 0.21348100
2026-02-13 23:35:19 - INFO - Time taken for Epoch 4: 41.21s - F1: 0.21175699
2026-02-13 23:35:58 - INFO - Time taken for Epoch 5: 39.55s - F1: 0.20737066
2026-02-13 23:36:38 - INFO - Time taken for Epoch 6: 40.00s - F1: 0.21962864
2026-02-13 23:37:19 - INFO - Time taken for Epoch 7: 40.94s - F1: 0.21648803
2026-02-13 23:37:22 - INFO - Fine-tuning models
2026-02-13 23:37:31 - INFO - Time taken for Epoch 1:8.08 - F1: 0.1529
2026-02-13 23:37:40 - INFO - Time taken for Epoch 2:9.13 - F1: 0.2175
2026-02-13 23:37:49 - INFO - Time taken for Epoch 3:9.49 - F1: 0.2707
2026-02-13 23:37:59 - INFO - Time taken for Epoch 4:9.28 - F1: 0.2580
2026-02-13 23:38:07 - INFO - Time taken for Epoch 5:8.12 - F1: 0.1872
2026-02-13 23:38:15 - INFO - Time taken for Epoch 6:8.18 - F1: 0.2983
2026-02-13 23:38:24 - INFO - Time taken for Epoch 7:9.48 - F1: 0.2561
2026-02-13 23:38:33 - INFO - Time taken for Epoch 8:8.18 - F1: 0.2374
2026-02-13 23:38:41 - INFO - Time taken for Epoch 9:8.02 - F1: 0.2762
2026-02-13 23:38:49 - INFO - Time taken for Epoch 10:8.10 - F1: 0.3356
2026-02-13 23:39:04 - INFO - Time taken for Epoch 11:14.93 - F1: 0.3597
2026-02-13 23:39:13 - INFO - Time taken for Epoch 12:9.16 - F1: 0.2623
2026-02-13 23:39:21 - INFO - Time taken for Epoch 13:8.06 - F1: 0.2926
2026-02-13 23:39:29 - INFO - Time taken for Epoch 14:8.10 - F1: 0.2711
2026-02-13 23:39:37 - INFO - Time taken for Epoch 15:8.10 - F1: 0.3502
2026-02-13 23:39:45 - INFO - Time taken for Epoch 16:8.07 - F1: 0.3264
2026-02-13 23:39:53 - INFO - Time taken for Epoch 17:8.00 - F1: 0.3365
2026-02-13 23:40:01 - INFO - Time taken for Epoch 18:8.03 - F1: 0.3900
2026-02-13 23:40:10 - INFO - Time taken for Epoch 19:9.21 - F1: 0.4611
2026-02-13 23:40:20 - INFO - Time taken for Epoch 20:9.23 - F1: 0.3702
2026-02-13 23:40:28 - INFO - Time taken for Epoch 21:7.99 - F1: 0.4774
2026-02-13 23:40:37 - INFO - Time taken for Epoch 22:9.36 - F1: 0.4714
2026-02-13 23:40:45 - INFO - Time taken for Epoch 23:8.11 - F1: 0.4890
2026-02-13 23:40:54 - INFO - Time taken for Epoch 24:9.18 - F1: 0.4381
2026-02-13 23:41:02 - INFO - Time taken for Epoch 25:8.05 - F1: 0.4149
2026-02-13 23:41:10 - INFO - Time taken for Epoch 26:8.11 - F1: 0.5032
2026-02-13 23:41:20 - INFO - Time taken for Epoch 27:9.22 - F1: 0.5119
2026-02-13 23:41:37 - INFO - Time taken for Epoch 28:17.86 - F1: 0.4716
2026-02-13 23:41:46 - INFO - Time taken for Epoch 29:8.09 - F1: 0.4966
2026-02-13 23:41:54 - INFO - Time taken for Epoch 30:8.14 - F1: 0.4706
2026-02-13 23:42:02 - INFO - Time taken for Epoch 31:8.13 - F1: 0.4861
2026-02-13 23:42:10 - INFO - Time taken for Epoch 32:8.12 - F1: 0.4892
2026-02-13 23:42:18 - INFO - Time taken for Epoch 33:8.10 - F1: 0.5123
2026-02-13 23:42:27 - INFO - Time taken for Epoch 34:9.23 - F1: 0.5076
2026-02-13 23:42:35 - INFO - Time taken for Epoch 35:8.10 - F1: 0.4943
2026-02-13 23:42:43 - INFO - Time taken for Epoch 36:8.06 - F1: 0.5482
2026-02-13 23:42:53 - INFO - Time taken for Epoch 37:9.49 - F1: 0.5395
2026-02-13 23:43:01 - INFO - Time taken for Epoch 38:8.07 - F1: 0.5236
2026-02-13 23:43:09 - INFO - Time taken for Epoch 39:8.12 - F1: 0.5240
2026-02-13 23:43:17 - INFO - Time taken for Epoch 40:8.05 - F1: 0.5144
2026-02-13 23:43:25 - INFO - Time taken for Epoch 41:8.09 - F1: 0.5062
2026-02-13 23:43:33 - INFO - Time taken for Epoch 42:8.08 - F1: 0.5267
2026-02-13 23:43:41 - INFO - Time taken for Epoch 43:8.06 - F1: 0.5161
2026-02-13 23:43:49 - INFO - Time taken for Epoch 44:8.04 - F1: 0.5022
2026-02-13 23:43:57 - INFO - Time taken for Epoch 45:8.08 - F1: 0.5231
2026-02-13 23:44:06 - INFO - Time taken for Epoch 46:8.10 - F1: 0.5342
2026-02-13 23:44:06 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 23:44:06 - INFO - Best F1:0.5482 - Best Epoch:35
2026-02-13 23:44:16 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.5192, Test ECE: 0.1023
2026-02-13 23:44:16 - INFO - All results: {'f1_macro': 0.5191666568951795, 'ece': np.float64(0.1022756469853212)}
2026-02-13 23:44:16 - INFO - 
Total time taken: 945.79 seconds
2026-02-13 23:44:16 - INFO - Trial 6 finished with value: 0.5191666568951795 and parameters: {'learning_rate': 0.000139533975523812, 'weight_decay': 0.00040918170359423154, 'batch_size': 8, 'co_train_epochs': 7, 'epoch_patience': 5}. Best is trial 3 with value: 0.6675576528456106.
2026-02-13 23:44:16 - INFO - Using devices: cuda:1, cuda:1
2026-02-13 23:44:16 - INFO - Devices: cuda:1, cuda:1
2026-02-13 23:44:16 - INFO - Starting log
2026-02-13 23:44:16 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-13 23:44:17 - INFO - Learning Rate: 0.00017945089864980065
Weight Decay: 0.004430091508051954
Batch Size: 16
No. Epochs: 13
Epoch Patience: 7
 Accumulation Steps: 4
2026-02-13 23:44:18 - INFO - Generating initial weights
2026-02-13 23:44:47 - INFO - Time taken for Epoch 1:27.03 - F1: 0.0081
2026-02-13 23:45:14 - INFO - Time taken for Epoch 2:27.01 - F1: 0.0363
2026-02-13 23:45:41 - INFO - Time taken for Epoch 3:26.86 - F1: 0.0322
2026-02-13 23:46:08 - INFO - Time taken for Epoch 4:27.00 - F1: 0.0189
2026-02-13 23:46:36 - INFO - Time taken for Epoch 5:27.23 - F1: 0.0189
2026-02-13 23:47:02 - INFO - Time taken for Epoch 6:26.36 - F1: 0.0189
2026-02-13 23:47:29 - INFO - Time taken for Epoch 7:26.95 - F1: 0.0189
2026-02-13 23:47:56 - INFO - Time taken for Epoch 8:26.81 - F1: 0.0189
2026-02-13 23:48:23 - INFO - Time taken for Epoch 9:27.02 - F1: 0.0336
2026-02-13 23:48:50 - INFO - Time taken for Epoch 10:26.86 - F1: 0.0461
2026-02-13 23:49:17 - INFO - Time taken for Epoch 11:27.00 - F1: 0.1211
2026-02-13 23:49:44 - INFO - Time taken for Epoch 12:27.07 - F1: 0.1629
2026-02-13 23:50:10 - INFO - Time taken for Epoch 13:26.53 - F1: 0.2118
2026-02-13 23:50:10 - INFO - Best F1:0.2118 - Best Epoch:13
2026-02-13 23:50:11 - INFO - Starting co-training
2026-02-13 23:50:50 - INFO - Time taken for Epoch 1: 37.92s - F1: 0.34750252
2026-02-13 23:51:29 - INFO - Time taken for Epoch 2: 38.93s - F1: 0.23947413
2026-02-13 23:52:06 - INFO - Time taken for Epoch 3: 37.70s - F1: 0.21528379
2026-02-13 23:52:44 - INFO - Time taken for Epoch 4: 37.60s - F1: 0.04755179
2026-02-13 23:53:22 - INFO - Time taken for Epoch 5: 37.57s - F1: 0.21326420
2026-02-13 23:53:59 - INFO - Time taken for Epoch 6: 37.45s - F1: 0.13145589
2026-02-13 23:54:36 - INFO - Time taken for Epoch 7: 37.51s - F1: 0.04755179
2026-02-13 23:55:14 - INFO - Time taken for Epoch 8: 37.86s - F1: 0.04755179
2026-02-13 23:55:14 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-13 23:55:17 - INFO - Fine-tuning models
2026-02-13 23:55:23 - INFO - Time taken for Epoch 1:6.44 - F1: 0.3383
2026-02-13 23:55:31 - INFO - Time taken for Epoch 2:7.43 - F1: 0.3116
2026-02-13 23:55:37 - INFO - Time taken for Epoch 3:6.34 - F1: 0.3994
2026-02-13 23:55:45 - INFO - Time taken for Epoch 4:7.66 - F1: 0.4682
2026-02-13 23:55:52 - INFO - Time taken for Epoch 5:7.57 - F1: 0.5392
2026-02-13 23:56:00 - INFO - Time taken for Epoch 6:7.66 - F1: 0.5509
2026-02-13 23:56:15 - INFO - Time taken for Epoch 7:14.83 - F1: 0.5496
2026-02-13 23:56:21 - INFO - Time taken for Epoch 8:6.38 - F1: 0.5577
2026-02-13 23:56:29 - INFO - Time taken for Epoch 9:7.54 - F1: 0.5653
2026-02-13 23:56:36 - INFO - Time taken for Epoch 10:7.58 - F1: 0.5716
2026-02-13 23:56:44 - INFO - Time taken for Epoch 11:7.73 - F1: 0.5744
2026-02-13 23:56:56 - INFO - Time taken for Epoch 12:11.86 - F1: 0.5820
2026-02-13 23:57:03 - INFO - Time taken for Epoch 13:7.48 - F1: 0.5714
2026-02-13 23:57:10 - INFO - Time taken for Epoch 14:6.39 - F1: 0.5897
2026-02-13 23:57:17 - INFO - Time taken for Epoch 15:7.48 - F1: 0.5931
2026-02-13 23:57:25 - INFO - Time taken for Epoch 16:7.52 - F1: 0.6028
2026-02-13 23:57:37 - INFO - Time taken for Epoch 17:11.93 - F1: 0.6157
2026-02-13 23:57:44 - INFO - Time taken for Epoch 18:7.50 - F1: 0.5972
2026-02-13 23:57:51 - INFO - Time taken for Epoch 19:6.45 - F1: 0.5733
2026-02-13 23:57:57 - INFO - Time taken for Epoch 20:6.41 - F1: 0.6238
2026-02-13 23:58:11 - INFO - Time taken for Epoch 21:13.49 - F1: 0.6241
2026-02-13 23:58:19 - INFO - Time taken for Epoch 22:7.95 - F1: 0.5911
2026-02-13 23:58:25 - INFO - Time taken for Epoch 23:6.46 - F1: 0.5803
2026-02-13 23:58:31 - INFO - Time taken for Epoch 24:6.26 - F1: 0.5780
2026-02-13 23:58:38 - INFO - Time taken for Epoch 25:6.50 - F1: 0.5861
2026-02-13 23:58:44 - INFO - Time taken for Epoch 26:6.42 - F1: 0.6032
2026-02-13 23:58:51 - INFO - Time taken for Epoch 27:6.44 - F1: 0.6136
2026-02-13 23:58:57 - INFO - Time taken for Epoch 28:6.43 - F1: 0.6182
2026-02-13 23:59:03 - INFO - Time taken for Epoch 29:6.41 - F1: 0.6226
2026-02-13 23:59:10 - INFO - Time taken for Epoch 30:6.41 - F1: 0.6333
2026-02-13 23:59:17 - INFO - Time taken for Epoch 31:7.55 - F1: 0.6265
2026-02-13 23:59:24 - INFO - Time taken for Epoch 32:6.45 - F1: 0.6209
2026-02-13 23:59:30 - INFO - Time taken for Epoch 33:6.46 - F1: 0.6253
2026-02-13 23:59:37 - INFO - Time taken for Epoch 34:6.43 - F1: 0.6165
2026-02-13 23:59:43 - INFO - Time taken for Epoch 35:6.45 - F1: 0.6114
2026-02-13 23:59:50 - INFO - Time taken for Epoch 36:6.45 - F1: 0.6136
2026-02-13 23:59:56 - INFO - Time taken for Epoch 37:6.42 - F1: 0.6172
2026-02-14 00:00:02 - INFO - Time taken for Epoch 38:6.40 - F1: 0.6119
2026-02-14 00:00:09 - INFO - Time taken for Epoch 39:6.41 - F1: 0.6227
2026-02-14 00:00:15 - INFO - Time taken for Epoch 40:6.44 - F1: 0.6105
2026-02-14 00:00:15 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 00:00:15 - INFO - Best F1:0.6333 - Best Epoch:29
2026-02-14 00:00:29 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.6382, Test ECE: 0.0965
2026-02-14 00:00:29 - INFO - All results: {'f1_macro': 0.6381806216950592, 'ece': np.float64(0.09647113837084724)}
2026-02-14 00:00:29 - INFO - 
Total time taken: 972.97 seconds
2026-02-14 00:00:29 - INFO - Trial 7 finished with value: 0.6381806216950592 and parameters: {'learning_rate': 0.00017945089864980065, 'weight_decay': 0.004430091508051954, 'batch_size': 16, 'co_train_epochs': 13, 'epoch_patience': 7}. Best is trial 3 with value: 0.6675576528456106.
2026-02-14 00:00:29 - INFO - Using devices: cuda:1, cuda:1
2026-02-14 00:00:29 - INFO - Devices: cuda:1, cuda:1
2026-02-14 00:00:29 - INFO - Starting log
2026-02-14 00:00:29 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-14 00:00:30 - INFO - Learning Rate: 0.0008904664928401202
Weight Decay: 4.922149069246448e-05
Batch Size: 8
No. Epochs: 18
Epoch Patience: 8
 Accumulation Steps: 8
2026-02-14 00:00:31 - INFO - Generating initial weights
2026-02-14 00:01:07 - INFO - Time taken for Epoch 1:33.51 - F1: 0.0189
2026-02-14 00:01:41 - INFO - Time taken for Epoch 2:33.79 - F1: 0.0394
2026-02-14 00:02:14 - INFO - Time taken for Epoch 3:33.68 - F1: 0.0089
2026-02-14 00:02:48 - INFO - Time taken for Epoch 4:33.62 - F1: 0.0189
2026-02-14 00:03:22 - INFO - Time taken for Epoch 5:33.66 - F1: 0.0189
2026-02-14 00:03:55 - INFO - Time taken for Epoch 6:33.90 - F1: 0.0189
2026-02-14 00:04:29 - INFO - Time taken for Epoch 7:33.33 - F1: 0.0189
2026-02-14 00:05:03 - INFO - Time taken for Epoch 8:33.76 - F1: 0.0189
2026-02-14 00:05:36 - INFO - Time taken for Epoch 9:33.55 - F1: 0.0189
2026-02-14 00:06:10 - INFO - Time taken for Epoch 10:33.70 - F1: 0.0189
2026-02-14 00:06:44 - INFO - Time taken for Epoch 11:33.73 - F1: 0.0189
2026-02-14 00:07:17 - INFO - Time taken for Epoch 12:33.90 - F1: 0.0189
2026-02-14 00:07:51 - INFO - Time taken for Epoch 13:33.64 - F1: 0.0189
2026-02-14 00:08:24 - INFO - Time taken for Epoch 14:33.39 - F1: 0.0189
2026-02-14 00:08:58 - INFO - Time taken for Epoch 15:33.60 - F1: 0.0189
2026-02-14 00:09:32 - INFO - Time taken for Epoch 16:33.75 - F1: 0.0189
2026-02-14 00:10:04 - INFO - Time taken for Epoch 17:32.33 - F1: 0.0189
2026-02-14 00:10:37 - INFO - Time taken for Epoch 18:33.00 - F1: 0.0189
2026-02-14 00:10:37 - INFO - Best F1:0.0394 - Best Epoch:2
2026-02-14 00:10:38 - INFO - Starting co-training
2026-02-14 00:11:18 - INFO - Time taken for Epoch 1: 39.66s - F1: 0.04755179
2026-02-14 00:11:59 - INFO - Time taken for Epoch 2: 40.90s - F1: 0.04755179
2026-02-14 00:12:39 - INFO - Time taken for Epoch 3: 39.75s - F1: 0.04755179
2026-02-14 00:13:19 - INFO - Time taken for Epoch 4: 39.99s - F1: 0.04755179
2026-02-14 00:14:00 - INFO - Time taken for Epoch 5: 40.85s - F1: 0.04755179
2026-02-14 00:14:39 - INFO - Time taken for Epoch 6: 39.59s - F1: 0.04755179
2026-02-14 00:15:20 - INFO - Time taken for Epoch 7: 40.06s - F1: 0.04755179
2026-02-14 00:15:59 - INFO - Time taken for Epoch 8: 39.34s - F1: 0.04755179
2026-02-14 00:16:39 - INFO - Time taken for Epoch 9: 39.94s - F1: 0.04755179
2026-02-14 00:16:39 - INFO - Performance not improving for 8 consecutive epochs.
2026-02-14 00:16:41 - INFO - Fine-tuning models
2026-02-14 00:16:50 - INFO - Time taken for Epoch 1:8.15 - F1: 0.0476
2026-02-14 00:16:59 - INFO - Time taken for Epoch 2:9.11 - F1: 0.0089
2026-02-14 00:17:07 - INFO - Time taken for Epoch 3:8.05 - F1: 0.0189
2026-02-14 00:17:15 - INFO - Time taken for Epoch 4:8.08 - F1: 0.0189
2026-02-14 00:17:23 - INFO - Time taken for Epoch 5:7.83 - F1: 0.0189
2026-02-14 00:17:31 - INFO - Time taken for Epoch 6:8.11 - F1: 0.0189
2026-02-14 00:17:39 - INFO - Time taken for Epoch 7:8.11 - F1: 0.0189
2026-02-14 00:17:47 - INFO - Time taken for Epoch 8:8.09 - F1: 0.0189
2026-02-14 00:17:55 - INFO - Time taken for Epoch 9:8.00 - F1: 0.0189
2026-02-14 00:18:03 - INFO - Time taken for Epoch 10:8.05 - F1: 0.0189
2026-02-14 00:18:11 - INFO - Time taken for Epoch 11:8.04 - F1: 0.0189
2026-02-14 00:18:11 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 00:18:11 - INFO - Best F1:0.0476 - Best Epoch:0
2026-02-14 00:18:21 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0570
2026-02-14 00:18:21 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.05697763877586254)}
2026-02-14 00:18:21 - INFO - 
Total time taken: 1072.07 seconds
2026-02-14 00:18:21 - INFO - Trial 8 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0008904664928401202, 'weight_decay': 4.922149069246448e-05, 'batch_size': 8, 'co_train_epochs': 18, 'epoch_patience': 8}. Best is trial 3 with value: 0.6675576528456106.
2026-02-14 00:18:21 - INFO - Using devices: cuda:1, cuda:1
2026-02-14 00:18:21 - INFO - Devices: cuda:1, cuda:1
2026-02-14 00:18:21 - INFO - Starting log
2026-02-14 00:18:21 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 50, Seed: 1234, HF Model: GPT-4o, NumShots: 50, PLM: bert-tweet
2026-02-14 00:18:22 - INFO - Learning Rate: 3.034971248295048e-05
Weight Decay: 1.9783950477346463e-05
Batch Size: 64
No. Epochs: 15
Epoch Patience: 5
 Accumulation Steps: 1
2026-02-14 00:18:23 - INFO - Generating initial weights
2026-02-14 00:18:46 - INFO - Time taken for Epoch 1:21.09 - F1: 0.0065
2026-02-14 00:19:07 - INFO - Time taken for Epoch 2:20.94 - F1: 0.0198
2026-02-14 00:19:28 - INFO - Time taken for Epoch 3:21.01 - F1: 0.0514
2026-02-14 00:19:49 - INFO - Time taken for Epoch 4:21.02 - F1: 0.1519
2026-02-14 00:20:10 - INFO - Time taken for Epoch 5:21.05 - F1: 0.2638
2026-02-14 00:20:31 - INFO - Time taken for Epoch 6:20.97 - F1: 0.3157
2026-02-14 00:20:52 - INFO - Time taken for Epoch 7:21.09 - F1: 0.3414
2026-02-14 00:21:13 - INFO - Time taken for Epoch 8:21.01 - F1: 0.3729
2026-02-14 00:21:34 - INFO - Time taken for Epoch 9:21.05 - F1: 0.4009
2026-02-14 00:21:55 - INFO - Time taken for Epoch 10:21.03 - F1: 0.4172
2026-02-14 00:22:16 - INFO - Time taken for Epoch 11:21.02 - F1: 0.4320
2026-02-14 00:22:37 - INFO - Time taken for Epoch 12:21.15 - F1: 0.4387
2026-02-14 00:22:59 - INFO - Time taken for Epoch 13:21.08 - F1: 0.4478
2026-02-14 00:23:19 - INFO - Time taken for Epoch 14:20.91 - F1: 0.4540
2026-02-14 00:23:40 - INFO - Time taken for Epoch 15:21.02 - F1: 0.4544
2026-02-14 00:23:40 - INFO - Best F1:0.4544 - Best Epoch:15
2026-02-14 00:23:47 - INFO - Starting co-training
2026-02-14 00:24:38 - INFO - Time taken for Epoch 1: 50.76s - F1: 0.48752077
2026-02-14 00:25:30 - INFO - Time taken for Epoch 2: 52.02s - F1: 0.59707790
2026-02-14 00:26:21 - INFO - Time taken for Epoch 3: 51.65s - F1: 0.60524790
2026-02-14 00:27:13 - INFO - Time taken for Epoch 4: 51.76s - F1: 0.63121281
2026-02-14 00:28:05 - INFO - Time taken for Epoch 5: 51.72s - F1: 0.64796074
2026-02-14 00:28:57 - INFO - Time taken for Epoch 6: 51.80s - F1: 0.64414767
2026-02-14 00:29:48 - INFO - Time taken for Epoch 7: 50.88s - F1: 0.65537749
2026-02-14 00:30:39 - INFO - Time taken for Epoch 8: 51.95s - F1: 0.64881035
2026-02-14 00:31:30 - INFO - Time taken for Epoch 9: 50.80s - F1: 0.63889924
2026-02-14 00:32:21 - INFO - Time taken for Epoch 10: 50.68s - F1: 0.62753060
2026-02-14 00:33:12 - INFO - Time taken for Epoch 11: 50.86s - F1: 0.62987736
2026-02-14 00:34:03 - INFO - Time taken for Epoch 12: 51.05s - F1: 0.64708138
2026-02-14 00:34:03 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-14 00:34:05 - INFO - Fine-tuning models
2026-02-14 00:34:11 - INFO - Time taken for Epoch 1:5.06 - F1: 0.6094
2026-02-14 00:34:17 - INFO - Time taken for Epoch 2:5.97 - F1: 0.6315
2026-02-14 00:34:23 - INFO - Time taken for Epoch 3:6.32 - F1: 0.6633
2026-02-14 00:34:29 - INFO - Time taken for Epoch 4:6.18 - F1: 0.6715
2026-02-14 00:34:35 - INFO - Time taken for Epoch 5:6.11 - F1: 0.6638
2026-02-14 00:34:40 - INFO - Time taken for Epoch 6:4.98 - F1: 0.6686
2026-02-14 00:34:45 - INFO - Time taken for Epoch 7:4.97 - F1: 0.6733
2026-02-14 00:35:00 - INFO - Time taken for Epoch 8:15.19 - F1: 0.6759
2026-02-14 00:35:06 - INFO - Time taken for Epoch 9:6.06 - F1: 0.6751
2026-02-14 00:35:11 - INFO - Time taken for Epoch 10:4.96 - F1: 0.6850
2026-02-14 00:35:17 - INFO - Time taken for Epoch 11:6.12 - F1: 0.6806
2026-02-14 00:35:22 - INFO - Time taken for Epoch 12:4.95 - F1: 0.6765
2026-02-14 00:35:27 - INFO - Time taken for Epoch 13:4.96 - F1: 0.6824
2026-02-14 00:35:32 - INFO - Time taken for Epoch 14:4.99 - F1: 0.6850
2026-02-14 00:35:37 - INFO - Time taken for Epoch 15:4.99 - F1: 0.6890
2026-02-14 00:35:43 - INFO - Time taken for Epoch 16:6.06 - F1: 0.6960
2026-02-14 00:35:50 - INFO - Time taken for Epoch 17:6.13 - F1: 0.6943
2026-02-14 00:35:55 - INFO - Time taken for Epoch 18:4.96 - F1: 0.6853
2026-02-14 00:35:59 - INFO - Time taken for Epoch 19:4.94 - F1: 0.6732
2026-02-14 00:36:04 - INFO - Time taken for Epoch 20:4.95 - F1: 0.6727
2026-02-14 00:36:09 - INFO - Time taken for Epoch 21:5.00 - F1: 0.6740
2026-02-14 00:36:14 - INFO - Time taken for Epoch 22:4.97 - F1: 0.6761
2026-02-14 00:36:19 - INFO - Time taken for Epoch 23:4.98 - F1: 0.6766
2026-02-14 00:36:26 - INFO - Time taken for Epoch 24:6.94 - F1: 0.6741
2026-02-14 00:36:31 - INFO - Time taken for Epoch 25:4.98 - F1: 0.6834
2026-02-14 00:36:36 - INFO - Time taken for Epoch 26:4.99 - F1: 0.6849
2026-02-14 00:36:36 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 00:36:36 - INFO - Best F1:0.6960 - Best Epoch:15
2026-02-14 00:36:48 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 50, N: 50 Test SEED: 1234 F1: 0.6857, Test ECE: 0.0298
2026-02-14 00:36:48 - INFO - All results: {'f1_macro': 0.6857350487372116, 'ece': np.float64(0.029811741078941568)}
2026-02-14 00:36:48 - INFO - 
Total time taken: 1107.03 seconds
2026-02-14 00:36:48 - INFO - Trial 9 finished with value: 0.6857350487372116 and parameters: {'learning_rate': 3.034971248295048e-05, 'weight_decay': 1.9783950477346463e-05, 'batch_size': 64, 'co_train_epochs': 15, 'epoch_patience': 5}. Best is trial 9 with value: 0.6857350487372116.
2026-02-14 00:36:48 - INFO - 
[BEST TRIAL RESULTS]
2026-02-14 00:36:48 - INFO - F1 Score: 0.6857
2026-02-14 00:36:48 - INFO - Params: {'learning_rate': 3.034971248295048e-05, 'weight_decay': 1.9783950477346463e-05, 'batch_size': 64, 'co_train_epochs': 15, 'epoch_patience': 5}
2026-02-14 00:36:48 - INFO -   learning_rate: 3.034971248295048e-05
2026-02-14 00:36:48 - INFO -   weight_decay: 1.9783950477346463e-05
2026-02-14 00:36:48 - INFO -   batch_size: 64
2026-02-14 00:36:48 - INFO -   co_train_epochs: 15
2026-02-14 00:36:48 - INFO -   epoch_patience: 5
2026-02-14 00:36:48 - INFO - 
Total time taken: 8424.35 seconds
