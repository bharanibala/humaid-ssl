2026-02-12 20:52:59 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-12 20:52:59 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_dorian_2019
2026-02-12 20:52:59 - INFO - Using devices: cuda, cuda
2026-02-12 20:52:59 - INFO - Devices: cuda, cuda
2026-02-12 20:52:59 - INFO - Starting log
2026-02-12 20:52:59 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 20:53:01 - INFO - Learning Rate: 1.251568894459705e-05
Weight Decay: 0.0008226443364011134
Batch Size: 24
No. Epochs: 6
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-12 20:53:07 - INFO - Generating initial weights
2026-02-12 20:53:28 - INFO - Time taken for Epoch 1:19.91 - F1: 0.0479
2026-02-12 20:53:42 - INFO - Time taken for Epoch 2:13.96 - F1: 0.0471
2026-02-12 20:53:56 - INFO - Time taken for Epoch 3:14.01 - F1: 0.0523
2026-02-12 20:54:10 - INFO - Time taken for Epoch 4:14.01 - F1: 0.0580
2026-02-12 20:54:25 - INFO - Time taken for Epoch 5:14.03 - F1: 0.0641
2026-02-12 20:54:39 - INFO - Time taken for Epoch 6:14.04 - F1: 0.1045
2026-02-12 20:54:39 - INFO - Best F1:0.1045 - Best Epoch:6
2026-02-12 20:54:44 - INFO - Starting co-training
2026-02-12 20:55:15 - INFO - Time taken for Epoch 1: 30.46s - F1: 0.36972918
2026-02-12 20:55:47 - INFO - Time taken for Epoch 2: 31.76s - F1: 0.41223907
2026-02-12 20:56:27 - INFO - Time taken for Epoch 3: 40.19s - F1: 0.43472101
2026-02-12 20:56:58 - INFO - Time taken for Epoch 4: 30.95s - F1: 0.44888582
2026-02-12 20:57:29 - INFO - Time taken for Epoch 5: 31.03s - F1: 0.45762014
2026-02-12 20:58:02 - INFO - Time taken for Epoch 6: 33.06s - F1: 0.45600162
2026-02-12 20:58:13 - INFO - Fine-tuning models
2026-02-12 20:58:15 - INFO - Time taken for Epoch 1:2.14 - F1: 0.4601
2026-02-12 20:58:18 - INFO - Time taken for Epoch 2:3.02 - F1: 0.4602
2026-02-12 20:58:21 - INFO - Time taken for Epoch 3:2.85 - F1: 0.4610
2026-02-12 20:58:25 - INFO - Time taken for Epoch 4:3.32 - F1: 0.4597
2026-02-12 20:58:27 - INFO - Time taken for Epoch 5:2.13 - F1: 0.4569
2026-02-12 20:58:29 - INFO - Time taken for Epoch 6:2.14 - F1: 0.4587
2026-02-12 20:58:31 - INFO - Time taken for Epoch 7:2.13 - F1: 0.4644
2026-02-12 20:58:34 - INFO - Time taken for Epoch 8:3.24 - F1: 0.4664
2026-02-12 20:58:37 - INFO - Time taken for Epoch 9:3.22 - F1: 0.4659
2026-02-12 20:58:40 - INFO - Time taken for Epoch 10:2.14 - F1: 0.4665
2026-02-12 20:58:42 - INFO - Time taken for Epoch 11:2.89 - F1: 0.4576
2026-02-12 20:58:45 - INFO - Time taken for Epoch 12:2.13 - F1: 0.4594
2026-02-12 20:58:47 - INFO - Time taken for Epoch 13:2.13 - F1: 0.4542
2026-02-12 20:58:49 - INFO - Time taken for Epoch 14:2.14 - F1: 0.4545
2026-02-12 20:58:51 - INFO - Time taken for Epoch 15:2.14 - F1: 0.4545
2026-02-12 20:58:53 - INFO - Time taken for Epoch 16:2.14 - F1: 0.4552
2026-02-12 20:58:55 - INFO - Time taken for Epoch 17:2.14 - F1: 0.4584
2026-02-12 20:58:57 - INFO - Time taken for Epoch 18:2.14 - F1: 0.4617
2026-02-12 20:59:00 - INFO - Time taken for Epoch 19:2.14 - F1: 0.4696
2026-02-12 20:59:03 - INFO - Time taken for Epoch 20:3.18 - F1: 0.4683
2026-02-12 20:59:05 - INFO - Time taken for Epoch 21:2.13 - F1: 0.4735
2026-02-12 20:59:08 - INFO - Time taken for Epoch 22:2.86 - F1: 0.4747
2026-02-12 20:59:11 - INFO - Time taken for Epoch 23:2.91 - F1: 0.4756
2026-02-12 20:59:13 - INFO - Time taken for Epoch 24:2.86 - F1: 0.4840
2026-02-12 20:59:16 - INFO - Time taken for Epoch 25:2.84 - F1: 0.4851
2026-02-12 20:59:19 - INFO - Time taken for Epoch 26:2.94 - F1: 0.4875
2026-02-12 20:59:22 - INFO - Time taken for Epoch 27:2.80 - F1: 0.4867
2026-02-12 20:59:24 - INFO - Time taken for Epoch 28:2.13 - F1: 0.4878
2026-02-12 20:59:27 - INFO - Time taken for Epoch 29:3.01 - F1: 0.4872
2026-02-12 20:59:29 - INFO - Time taken for Epoch 30:2.13 - F1: 0.5180
2026-02-12 20:59:32 - INFO - Time taken for Epoch 31:2.90 - F1: 0.5093
2026-02-12 20:59:34 - INFO - Time taken for Epoch 32:2.14 - F1: 0.5066
2026-02-12 20:59:37 - INFO - Time taken for Epoch 33:2.14 - F1: 0.5344
2026-02-12 20:59:44 - INFO - Time taken for Epoch 34:7.58 - F1: 0.5330
2026-02-12 20:59:46 - INFO - Time taken for Epoch 35:2.13 - F1: 0.5267
2026-02-12 20:59:48 - INFO - Time taken for Epoch 36:2.13 - F1: 0.5252
2026-02-12 20:59:50 - INFO - Time taken for Epoch 37:2.13 - F1: 0.5288
2026-02-12 20:59:53 - INFO - Time taken for Epoch 38:2.13 - F1: 0.5368
2026-02-12 20:59:56 - INFO - Time taken for Epoch 39:3.27 - F1: 0.5452
2026-02-12 20:59:59 - INFO - Time taken for Epoch 40:3.37 - F1: 0.5445
2026-02-12 21:00:01 - INFO - Time taken for Epoch 41:2.13 - F1: 0.5477
2026-02-12 21:00:05 - INFO - Time taken for Epoch 42:3.53 - F1: 0.5472
2026-02-12 21:00:07 - INFO - Time taken for Epoch 43:2.13 - F1: 0.5451
2026-02-12 21:00:09 - INFO - Time taken for Epoch 44:2.13 - F1: 0.5445
2026-02-12 21:00:11 - INFO - Time taken for Epoch 45:2.14 - F1: 0.5452
2026-02-12 21:00:13 - INFO - Time taken for Epoch 46:2.14 - F1: 0.5521
2026-02-12 21:00:25 - INFO - Time taken for Epoch 47:11.88 - F1: 0.5553
2026-02-12 21:00:28 - INFO - Time taken for Epoch 48:2.77 - F1: 0.5552
2026-02-12 21:00:30 - INFO - Time taken for Epoch 49:2.13 - F1: 0.5570
2026-02-12 21:00:33 - INFO - Time taken for Epoch 50:3.21 - F1: 0.5632
2026-02-12 21:00:36 - INFO - Time taken for Epoch 51:3.03 - F1: 0.5630
2026-02-12 21:00:39 - INFO - Time taken for Epoch 52:2.13 - F1: 0.5629
2026-02-12 21:00:41 - INFO - Time taken for Epoch 53:2.13 - F1: 0.5601
2026-02-12 21:00:43 - INFO - Time taken for Epoch 54:2.13 - F1: 0.5560
2026-02-12 21:00:45 - INFO - Time taken for Epoch 55:2.13 - F1: 0.5562
2026-02-12 21:00:47 - INFO - Time taken for Epoch 56:2.13 - F1: 0.5596
2026-02-12 21:00:49 - INFO - Time taken for Epoch 57:2.13 - F1: 0.5533
2026-02-12 21:00:51 - INFO - Time taken for Epoch 58:2.14 - F1: 0.5539
2026-02-12 21:00:54 - INFO - Time taken for Epoch 59:2.14 - F1: 0.5531
2026-02-12 21:00:56 - INFO - Time taken for Epoch 60:2.14 - F1: 0.5514
2026-02-12 21:00:56 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 21:00:56 - INFO - Best F1:0.5632 - Best Epoch:49
2026-02-12 21:01:23 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5265, Test ECE: 0.0930
2026-02-12 21:01:23 - INFO - All results: {'f1_macro': 0.5264939698265727, 'ece': np.float64(0.09298789621743347)}
2026-02-12 21:01:23 - INFO - 
Total time taken: 504.50 seconds
2026-02-12 21:01:24 - INFO - Trial 0 finished with value: 0.5264939698265727 and parameters: {'learning_rate': 1.251568894459705e-05, 'weight_decay': 0.0008226443364011134, 'batch_size': 24, 'co_train_epochs': 6, 'epoch_patience': 9}. Best is trial 0 with value: 0.5264939698265727.
2026-02-12 21:01:24 - INFO - Using devices: cuda, cuda
2026-02-12 21:01:24 - INFO - Devices: cuda, cuda
2026-02-12 21:01:24 - INFO - Starting log
2026-02-12 21:01:24 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 21:01:25 - INFO - Learning Rate: 0.0002925874326306994
Weight Decay: 7.485747170365081e-05
Batch Size: 24
No. Epochs: 20
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-12 21:01:30 - INFO - Generating initial weights
2026-02-12 21:01:45 - INFO - Time taken for Epoch 1:14.06 - F1: 0.0362
2026-02-12 21:01:59 - INFO - Time taken for Epoch 2:14.08 - F1: 0.1749
2026-02-12 21:02:13 - INFO - Time taken for Epoch 3:14.06 - F1: 0.3146
2026-02-12 21:02:27 - INFO - Time taken for Epoch 4:14.06 - F1: 0.3594
2026-02-12 21:02:42 - INFO - Time taken for Epoch 5:14.07 - F1: 0.3448
2026-02-12 21:02:56 - INFO - Time taken for Epoch 6:14.07 - F1: 0.3747
2026-02-12 21:03:10 - INFO - Time taken for Epoch 7:14.08 - F1: 0.3745
2026-02-12 21:03:24 - INFO - Time taken for Epoch 8:14.05 - F1: 0.3898
2026-02-12 21:03:38 - INFO - Time taken for Epoch 9:14.06 - F1: 0.4134
2026-02-12 21:03:52 - INFO - Time taken for Epoch 10:14.04 - F1: 0.4186
2026-02-12 21:04:06 - INFO - Time taken for Epoch 11:14.08 - F1: 0.4196
2026-02-12 21:04:20 - INFO - Time taken for Epoch 12:14.07 - F1: 0.4190
2026-02-12 21:04:34 - INFO - Time taken for Epoch 13:14.07 - F1: 0.4055
2026-02-12 21:04:48 - INFO - Time taken for Epoch 14:14.09 - F1: 0.4105
2026-02-12 21:05:02 - INFO - Time taken for Epoch 15:14.03 - F1: 0.4114
2026-02-12 21:05:16 - INFO - Time taken for Epoch 16:14.07 - F1: 0.4137
2026-02-12 21:05:30 - INFO - Time taken for Epoch 17:14.07 - F1: 0.4165
2026-02-12 21:05:44 - INFO - Time taken for Epoch 18:14.07 - F1: 0.4318
2026-02-12 21:05:58 - INFO - Time taken for Epoch 19:14.07 - F1: 0.4345
2026-02-12 21:06:13 - INFO - Time taken for Epoch 20:14.07 - F1: 0.4355
2026-02-12 21:06:13 - INFO - Best F1:0.4355 - Best Epoch:20
2026-02-12 21:08:46 - INFO - Starting co-training
2026-02-12 21:09:16 - INFO - Time taken for Epoch 1: 30.00s - F1: 0.06985171
2026-02-12 21:09:47 - INFO - Time taken for Epoch 2: 30.87s - F1: 0.03396410
2026-02-12 21:10:17 - INFO - Time taken for Epoch 3: 30.18s - F1: 0.03396410
2026-02-12 21:10:47 - INFO - Time taken for Epoch 4: 30.19s - F1: 0.03396410
2026-02-12 21:11:17 - INFO - Time taken for Epoch 5: 30.15s - F1: 0.03396410
2026-02-12 21:11:48 - INFO - Time taken for Epoch 6: 30.22s - F1: 0.03396410
2026-02-12 21:12:18 - INFO - Time taken for Epoch 7: 30.22s - F1: 0.03396410
2026-02-12 21:12:48 - INFO - Time taken for Epoch 8: 30.21s - F1: 0.03396410
2026-02-12 21:13:18 - INFO - Time taken for Epoch 9: 30.19s - F1: 0.03396410
2026-02-12 21:13:48 - INFO - Time taken for Epoch 10: 30.19s - F1: 0.03396410
2026-02-12 21:13:48 - INFO - Performance not improving for 9 consecutive epochs.
2026-02-12 21:13:50 - INFO - Fine-tuning models
2026-02-12 21:13:52 - INFO - Time taken for Epoch 1:2.15 - F1: 0.0725
2026-02-12 21:13:55 - INFO - Time taken for Epoch 2:3.08 - F1: 0.0830
2026-02-12 21:13:58 - INFO - Time taken for Epoch 3:2.88 - F1: 0.0904
2026-02-12 21:14:01 - INFO - Time taken for Epoch 4:2.77 - F1: 0.1000
2026-02-12 21:14:04 - INFO - Time taken for Epoch 5:3.18 - F1: 0.0931
2026-02-12 21:14:06 - INFO - Time taken for Epoch 6:2.12 - F1: 0.1044
2026-02-12 21:14:09 - INFO - Time taken for Epoch 7:2.83 - F1: 0.0313
2026-02-12 21:14:11 - INFO - Time taken for Epoch 8:2.12 - F1: 0.0050
2026-02-12 21:14:13 - INFO - Time taken for Epoch 9:2.13 - F1: 0.0051
2026-02-12 21:14:15 - INFO - Time taken for Epoch 10:2.12 - F1: 0.0099
2026-02-12 21:14:18 - INFO - Time taken for Epoch 11:2.12 - F1: 0.0017
2026-02-12 21:14:20 - INFO - Time taken for Epoch 12:2.12 - F1: 0.0017
2026-02-12 21:14:22 - INFO - Time taken for Epoch 13:2.12 - F1: 0.0017
2026-02-12 21:14:24 - INFO - Time taken for Epoch 14:2.13 - F1: 0.0017
2026-02-12 21:14:26 - INFO - Time taken for Epoch 15:2.13 - F1: 0.0212
2026-02-12 21:14:28 - INFO - Time taken for Epoch 16:2.13 - F1: 0.0212
2026-02-12 21:14:28 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 21:14:28 - INFO - Best F1:0.1044 - Best Epoch:5
2026-02-12 21:14:33 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0899, Test ECE: 0.1100
2026-02-12 21:14:33 - INFO - All results: {'f1_macro': 0.08986874159216193, 'ece': np.float64(0.1099781591435326)}
2026-02-12 21:14:33 - INFO - 
Total time taken: 789.68 seconds
2026-02-12 21:14:33 - INFO - Trial 1 finished with value: 0.08986874159216193 and parameters: {'learning_rate': 0.0002925874326306994, 'weight_decay': 7.485747170365081e-05, 'batch_size': 24, 'co_train_epochs': 20, 'epoch_patience': 9}. Best is trial 0 with value: 0.5264939698265727.
2026-02-12 21:14:33 - INFO - Using devices: cuda, cuda
2026-02-12 21:14:33 - INFO - Devices: cuda, cuda
2026-02-12 21:14:33 - INFO - Starting log
2026-02-12 21:14:33 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 21:14:34 - INFO - Learning Rate: 0.0004125474172013552
Weight Decay: 3.9087476534316687e-05
Batch Size: 16
No. Epochs: 17
Epoch Patience: 6
 Accumulation Steps: 4
2026-02-12 21:14:34 - INFO - Generating initial weights
2026-02-12 21:14:51 - INFO - Time taken for Epoch 1:15.19 - F1: 0.0746
2026-02-12 21:15:06 - INFO - Time taken for Epoch 2:15.17 - F1: 0.1590
2026-02-12 21:15:21 - INFO - Time taken for Epoch 3:15.16 - F1: 0.1512
2026-02-12 21:15:37 - INFO - Time taken for Epoch 4:15.16 - F1: 0.2565
2026-02-12 21:15:52 - INFO - Time taken for Epoch 5:15.17 - F1: 0.2959
2026-02-12 21:16:07 - INFO - Time taken for Epoch 6:15.17 - F1: 0.3304
2026-02-12 21:16:22 - INFO - Time taken for Epoch 7:15.16 - F1: 0.4265
2026-02-12 21:16:37 - INFO - Time taken for Epoch 8:15.19 - F1: 0.4267
2026-02-12 21:16:52 - INFO - Time taken for Epoch 9:15.16 - F1: 0.3936
2026-02-12 21:17:08 - INFO - Time taken for Epoch 10:15.18 - F1: 0.4082
2026-02-12 21:17:23 - INFO - Time taken for Epoch 11:15.16 - F1: 0.4338
2026-02-12 21:17:38 - INFO - Time taken for Epoch 12:15.16 - F1: 0.4575
2026-02-12 21:17:53 - INFO - Time taken for Epoch 13:15.16 - F1: 0.4674
2026-02-12 21:18:08 - INFO - Time taken for Epoch 14:15.17 - F1: 0.4574
2026-02-12 21:18:23 - INFO - Time taken for Epoch 15:15.16 - F1: 0.4417
2026-02-12 21:18:39 - INFO - Time taken for Epoch 16:15.17 - F1: 0.4418
2026-02-12 21:18:54 - INFO - Time taken for Epoch 17:15.16 - F1: 0.4475
2026-02-12 21:18:54 - INFO - Best F1:0.4674 - Best Epoch:13
2026-02-12 21:18:54 - INFO - Starting co-training
2026-02-12 21:19:20 - INFO - Time taken for Epoch 1: 25.25s - F1: 0.02286448
2026-02-12 21:19:46 - INFO - Time taken for Epoch 2: 25.83s - F1: 0.03396410
2026-02-12 21:20:12 - INFO - Time taken for Epoch 3: 25.89s - F1: 0.03396410
2026-02-12 21:20:37 - INFO - Time taken for Epoch 4: 25.13s - F1: 0.03396410
2026-02-12 21:21:02 - INFO - Time taken for Epoch 5: 25.21s - F1: 0.03396410
2026-02-12 21:21:27 - INFO - Time taken for Epoch 6: 25.19s - F1: 0.03396410
2026-02-12 21:21:52 - INFO - Time taken for Epoch 7: 25.19s - F1: 0.03396410
2026-02-12 21:22:18 - INFO - Time taken for Epoch 8: 25.16s - F1: 0.03396410
2026-02-12 21:22:18 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-12 21:22:19 - INFO - Fine-tuning models
2026-02-12 21:22:21 - INFO - Time taken for Epoch 1:2.32 - F1: 0.0340
2026-02-12 21:22:25 - INFO - Time taken for Epoch 2:3.39 - F1: 0.0340
2026-02-12 21:22:27 - INFO - Time taken for Epoch 3:2.31 - F1: 0.0340
2026-02-12 21:22:29 - INFO - Time taken for Epoch 4:2.31 - F1: 0.0050
2026-02-12 21:22:32 - INFO - Time taken for Epoch 5:2.31 - F1: 0.0050
2026-02-12 21:22:34 - INFO - Time taken for Epoch 6:2.31 - F1: 0.0050
2026-02-12 21:22:36 - INFO - Time taken for Epoch 7:2.31 - F1: 0.0276
2026-02-12 21:22:39 - INFO - Time taken for Epoch 8:2.31 - F1: 0.0276
2026-02-12 21:22:41 - INFO - Time taken for Epoch 9:2.31 - F1: 0.0276
2026-02-12 21:22:43 - INFO - Time taken for Epoch 10:2.31 - F1: 0.0276
2026-02-12 21:22:46 - INFO - Time taken for Epoch 11:2.31 - F1: 0.0276
2026-02-12 21:22:46 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 21:22:46 - INFO - Best F1:0.0340 - Best Epoch:0
2026-02-12 21:22:51 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0339, Test ECE: 0.3078
2026-02-12 21:22:51 - INFO - All results: {'f1_macro': 0.03385172693773031, 'ece': np.float64(0.3078411268539707)}
2026-02-12 21:22:51 - INFO - 
Total time taken: 497.67 seconds
2026-02-12 21:22:51 - INFO - Trial 2 finished with value: 0.03385172693773031 and parameters: {'learning_rate': 0.0004125474172013552, 'weight_decay': 3.9087476534316687e-05, 'batch_size': 16, 'co_train_epochs': 17, 'epoch_patience': 6}. Best is trial 0 with value: 0.5264939698265727.
2026-02-12 21:22:51 - INFO - Using devices: cuda, cuda
2026-02-12 21:22:51 - INFO - Devices: cuda, cuda
2026-02-12 21:22:51 - INFO - Starting log
2026-02-12 21:22:51 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 21:22:52 - INFO - Learning Rate: 5.0950272217643695e-05
Weight Decay: 1.8247097295536594e-05
Batch Size: 24
No. Epochs: 15
Epoch Patience: 8
 Accumulation Steps: 2
2026-02-12 21:22:52 - INFO - Generating initial weights
2026-02-12 21:23:08 - INFO - Time taken for Epoch 1:14.12 - F1: 0.0220
2026-02-12 21:23:22 - INFO - Time taken for Epoch 2:14.09 - F1: 0.0843
2026-02-12 21:23:36 - INFO - Time taken for Epoch 3:14.06 - F1: 0.1548
2026-02-12 21:23:50 - INFO - Time taken for Epoch 4:14.10 - F1: 0.2024
2026-02-12 21:24:04 - INFO - Time taken for Epoch 5:14.10 - F1: 0.2536
2026-02-12 21:24:18 - INFO - Time taken for Epoch 6:14.04 - F1: 0.2908
2026-02-12 21:24:32 - INFO - Time taken for Epoch 7:14.09 - F1: 0.3029
2026-02-12 21:24:46 - INFO - Time taken for Epoch 8:14.11 - F1: 0.3172
2026-02-12 21:25:00 - INFO - Time taken for Epoch 9:14.12 - F1: 0.3527
2026-02-12 21:25:14 - INFO - Time taken for Epoch 10:14.09 - F1: 0.3655
2026-02-12 21:25:28 - INFO - Time taken for Epoch 11:14.06 - F1: 0.3739
2026-02-12 21:25:43 - INFO - Time taken for Epoch 12:14.10 - F1: 0.3905
2026-02-12 21:25:57 - INFO - Time taken for Epoch 13:14.10 - F1: 0.3929
2026-02-12 21:26:11 - INFO - Time taken for Epoch 14:14.13 - F1: 0.3985
2026-02-12 21:26:25 - INFO - Time taken for Epoch 15:14.06 - F1: 0.3949
2026-02-12 21:26:25 - INFO - Best F1:0.3985 - Best Epoch:14
2026-02-12 21:26:25 - INFO - Starting co-training
2026-02-12 21:26:56 - INFO - Time taken for Epoch 1: 30.27s - F1: 0.43010844
2026-02-12 21:27:27 - INFO - Time taken for Epoch 2: 30.76s - F1: 0.43916006
2026-02-12 21:28:08 - INFO - Time taken for Epoch 3: 40.97s - F1: 0.42138167
2026-02-12 21:28:38 - INFO - Time taken for Epoch 4: 30.21s - F1: 0.43140041
2026-02-12 21:29:08 - INFO - Time taken for Epoch 5: 30.27s - F1: 0.43831592
2026-02-12 21:29:38 - INFO - Time taken for Epoch 6: 30.24s - F1: 0.45778418
2026-02-12 21:30:09 - INFO - Time taken for Epoch 7: 30.86s - F1: 0.52253718
2026-02-12 21:30:41 - INFO - Time taken for Epoch 8: 31.43s - F1: 0.53162542
2026-02-12 21:31:11 - INFO - Time taken for Epoch 9: 30.83s - F1: 0.54141511
2026-02-12 21:31:52 - INFO - Time taken for Epoch 10: 40.16s - F1: 0.54815845
2026-02-12 21:32:22 - INFO - Time taken for Epoch 11: 30.83s - F1: 0.55177373
2026-02-12 21:32:53 - INFO - Time taken for Epoch 12: 30.91s - F1: 0.56020364
2026-02-12 21:33:24 - INFO - Time taken for Epoch 13: 30.78s - F1: 0.56896580
2026-02-12 21:33:55 - INFO - Time taken for Epoch 14: 30.88s - F1: 0.53714905
2026-02-12 21:34:25 - INFO - Time taken for Epoch 15: 30.21s - F1: 0.55090417
2026-02-12 21:34:27 - INFO - Fine-tuning models
2026-02-12 21:34:29 - INFO - Time taken for Epoch 1:2.15 - F1: 0.5761
2026-02-12 21:34:32 - INFO - Time taken for Epoch 2:3.10 - F1: 0.6001
2026-02-12 21:34:35 - INFO - Time taken for Epoch 3:3.15 - F1: 0.5909
2026-02-12 21:34:37 - INFO - Time taken for Epoch 4:2.13 - F1: 0.5901
2026-02-12 21:34:40 - INFO - Time taken for Epoch 5:2.14 - F1: 0.5967
2026-02-12 21:34:42 - INFO - Time taken for Epoch 6:2.13 - F1: 0.5887
2026-02-12 21:34:44 - INFO - Time taken for Epoch 7:2.13 - F1: 0.5795
2026-02-12 21:34:46 - INFO - Time taken for Epoch 8:2.14 - F1: 0.5746
2026-02-12 21:34:48 - INFO - Time taken for Epoch 9:2.13 - F1: 0.5694
2026-02-12 21:34:50 - INFO - Time taken for Epoch 10:2.14 - F1: 0.5734
2026-02-12 21:34:52 - INFO - Time taken for Epoch 11:2.14 - F1: 0.5671
2026-02-12 21:34:54 - INFO - Time taken for Epoch 12:2.14 - F1: 0.5684
2026-02-12 21:34:54 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 21:34:54 - INFO - Best F1:0.6001 - Best Epoch:1
2026-02-12 21:34:59 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5942, Test ECE: 0.0477
2026-02-12 21:34:59 - INFO - All results: {'f1_macro': 0.5941872592376813, 'ece': np.float64(0.047678764800810375)}
2026-02-12 21:34:59 - INFO - 
Total time taken: 728.46 seconds
2026-02-12 21:34:59 - INFO - Trial 3 finished with value: 0.5941872592376813 and parameters: {'learning_rate': 5.0950272217643695e-05, 'weight_decay': 1.8247097295536594e-05, 'batch_size': 24, 'co_train_epochs': 15, 'epoch_patience': 8}. Best is trial 3 with value: 0.5941872592376813.
2026-02-12 21:34:59 - INFO - Using devices: cuda, cuda
2026-02-12 21:34:59 - INFO - Devices: cuda, cuda
2026-02-12 21:34:59 - INFO - Starting log
2026-02-12 21:34:59 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 21:35:00 - INFO - Learning Rate: 0.0003726374571300888
Weight Decay: 0.0008458144843699992
Batch Size: 8
No. Epochs: 8
Epoch Patience: 5
 Accumulation Steps: 8
2026-02-12 21:35:00 - INFO - Generating initial weights
2026-02-12 21:35:19 - INFO - Time taken for Epoch 1:17.38 - F1: 0.0276
2026-02-12 21:35:36 - INFO - Time taken for Epoch 2:17.30 - F1: 0.0276
2026-02-12 21:35:54 - INFO - Time taken for Epoch 3:17.28 - F1: 0.0303
2026-02-12 21:36:11 - INFO - Time taken for Epoch 4:17.29 - F1: 0.0276
2026-02-12 21:36:28 - INFO - Time taken for Epoch 5:17.26 - F1: 0.0647
2026-02-12 21:36:46 - INFO - Time taken for Epoch 6:17.25 - F1: 0.1060
2026-02-12 21:37:03 - INFO - Time taken for Epoch 7:17.25 - F1: 0.2400
2026-02-12 21:37:20 - INFO - Time taken for Epoch 8:17.26 - F1: 0.3501
2026-02-12 21:37:20 - INFO - Best F1:0.3501 - Best Epoch:8
2026-02-12 21:37:21 - INFO - Starting co-training
2026-02-12 21:37:46 - INFO - Time taken for Epoch 1: 24.91s - F1: 0.02286448
2026-02-12 21:38:11 - INFO - Time taken for Epoch 2: 25.74s - F1: 0.03396410
2026-02-12 21:38:37 - INFO - Time taken for Epoch 3: 25.84s - F1: 0.02286448
2026-02-12 21:39:02 - INFO - Time taken for Epoch 4: 24.88s - F1: 0.02286448
2026-02-12 21:39:27 - INFO - Time taken for Epoch 5: 24.90s - F1: 0.02286448
2026-02-12 21:39:52 - INFO - Time taken for Epoch 6: 24.87s - F1: 0.02286448
2026-02-12 21:40:17 - INFO - Time taken for Epoch 7: 24.87s - F1: 0.02286448
2026-02-12 21:40:17 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-12 21:40:18 - INFO - Fine-tuning models
2026-02-12 21:40:21 - INFO - Time taken for Epoch 1:2.70 - F1: 0.0340
2026-02-12 21:40:24 - INFO - Time taken for Epoch 2:3.38 - F1: 0.0340
2026-02-12 21:40:27 - INFO - Time taken for Epoch 3:2.67 - F1: 0.0340
2026-02-12 21:40:30 - INFO - Time taken for Epoch 4:2.68 - F1: 0.0050
2026-02-12 21:40:32 - INFO - Time taken for Epoch 5:2.68 - F1: 0.0017
2026-02-12 21:40:35 - INFO - Time taken for Epoch 6:2.68 - F1: 0.0017
2026-02-12 21:40:38 - INFO - Time taken for Epoch 7:2.68 - F1: 0.0276
2026-02-12 21:40:40 - INFO - Time taken for Epoch 8:2.69 - F1: 0.0276
2026-02-12 21:40:43 - INFO - Time taken for Epoch 9:2.68 - F1: 0.0276
2026-02-12 21:40:46 - INFO - Time taken for Epoch 10:2.68 - F1: 0.0276
2026-02-12 21:40:48 - INFO - Time taken for Epoch 11:2.68 - F1: 0.0276
2026-02-12 21:40:48 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 21:40:48 - INFO - Best F1:0.0340 - Best Epoch:0
2026-02-12 21:40:54 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0339, Test ECE: 0.4048
2026-02-12 21:40:54 - INFO - All results: {'f1_macro': 0.03385172693773031, 'ece': np.float64(0.4048477135776525)}
2026-02-12 21:40:54 - INFO - 
Total time taken: 354.87 seconds
2026-02-12 21:40:54 - INFO - Trial 4 finished with value: 0.03385172693773031 and parameters: {'learning_rate': 0.0003726374571300888, 'weight_decay': 0.0008458144843699992, 'batch_size': 8, 'co_train_epochs': 8, 'epoch_patience': 5}. Best is trial 3 with value: 0.5941872592376813.
2026-02-12 21:40:54 - INFO - Using devices: cuda, cuda
2026-02-12 21:40:54 - INFO - Devices: cuda, cuda
2026-02-12 21:40:54 - INFO - Starting log
2026-02-12 21:40:54 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 21:40:55 - INFO - Learning Rate: 4.212202361991735e-05
Weight Decay: 0.0007655069908625201
Batch Size: 24
No. Epochs: 10
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-12 21:40:56 - INFO - Generating initial weights
2026-02-12 21:41:11 - INFO - Time taken for Epoch 1:14.13 - F1: 0.0258
2026-02-12 21:41:25 - INFO - Time taken for Epoch 2:14.10 - F1: 0.0749
2026-02-12 21:41:39 - INFO - Time taken for Epoch 3:14.10 - F1: 0.1152
2026-02-12 21:41:54 - INFO - Time taken for Epoch 4:14.09 - F1: 0.1588
2026-02-12 21:42:08 - INFO - Time taken for Epoch 5:14.08 - F1: 0.2087
2026-02-12 21:42:22 - INFO - Time taken for Epoch 6:14.08 - F1: 0.2522
2026-02-12 21:42:36 - INFO - Time taken for Epoch 7:14.09 - F1: 0.2657
2026-02-12 21:42:50 - INFO - Time taken for Epoch 8:14.10 - F1: 0.2756
2026-02-12 21:43:04 - INFO - Time taken for Epoch 9:14.08 - F1: 0.3063
2026-02-12 21:43:18 - INFO - Time taken for Epoch 10:14.06 - F1: 0.3177
2026-02-12 21:43:18 - INFO - Best F1:0.3177 - Best Epoch:10
2026-02-12 21:43:19 - INFO - Starting co-training
2026-02-12 21:43:49 - INFO - Time taken for Epoch 1: 30.16s - F1: 0.43665430
2026-02-12 21:44:20 - INFO - Time taken for Epoch 2: 30.74s - F1: 0.43446670
2026-02-12 21:44:50 - INFO - Time taken for Epoch 3: 30.22s - F1: 0.44660796
2026-02-12 21:45:21 - INFO - Time taken for Epoch 4: 30.73s - F1: 0.45015360
2026-02-12 21:45:52 - INFO - Time taken for Epoch 5: 31.01s - F1: 0.46764202
2026-02-12 21:46:22 - INFO - Time taken for Epoch 6: 30.71s - F1: 0.54092441
2026-02-12 21:46:53 - INFO - Time taken for Epoch 7: 30.88s - F1: 0.53336521
2026-02-12 21:47:23 - INFO - Time taken for Epoch 8: 30.20s - F1: 0.53796496
2026-02-12 21:47:54 - INFO - Time taken for Epoch 9: 30.14s - F1: 0.52874423
2026-02-12 21:48:24 - INFO - Time taken for Epoch 10: 30.20s - F1: 0.53837542
2026-02-12 21:48:25 - INFO - Fine-tuning models
2026-02-12 21:48:27 - INFO - Time taken for Epoch 1:2.15 - F1: 0.5434
2026-02-12 21:48:30 - INFO - Time taken for Epoch 2:2.92 - F1: 0.5479
2026-02-12 21:48:33 - INFO - Time taken for Epoch 3:2.76 - F1: 0.5719
2026-02-12 21:48:36 - INFO - Time taken for Epoch 4:2.79 - F1: 0.5978
2026-02-12 21:48:39 - INFO - Time taken for Epoch 5:2.83 - F1: 0.6134
2026-02-12 21:48:41 - INFO - Time taken for Epoch 6:2.75 - F1: 0.5961
2026-02-12 21:48:44 - INFO - Time taken for Epoch 7:2.13 - F1: 0.5818
2026-02-12 21:48:46 - INFO - Time taken for Epoch 8:2.13 - F1: 0.5755
2026-02-12 21:48:48 - INFO - Time taken for Epoch 9:2.14 - F1: 0.5652
2026-02-12 21:48:50 - INFO - Time taken for Epoch 10:2.13 - F1: 0.5710
2026-02-12 21:48:52 - INFO - Time taken for Epoch 11:2.14 - F1: 0.5979
2026-02-12 21:48:54 - INFO - Time taken for Epoch 12:2.14 - F1: 0.5928
2026-02-12 21:48:56 - INFO - Time taken for Epoch 13:2.14 - F1: 0.5840
2026-02-12 21:48:59 - INFO - Time taken for Epoch 14:2.14 - F1: 0.5761
2026-02-12 21:49:01 - INFO - Time taken for Epoch 15:2.14 - F1: 0.5889
2026-02-12 21:49:01 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 21:49:01 - INFO - Best F1:0.6134 - Best Epoch:4
2026-02-12 21:49:06 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6089, Test ECE: 0.0955
2026-02-12 21:49:06 - INFO - All results: {'f1_macro': 0.6088884469930232, 'ece': np.float64(0.09549461585773397)}
2026-02-12 21:49:06 - INFO - 
Total time taken: 491.64 seconds
2026-02-12 21:49:06 - INFO - Trial 5 finished with value: 0.6088884469930232 and parameters: {'learning_rate': 4.212202361991735e-05, 'weight_decay': 0.0007655069908625201, 'batch_size': 24, 'co_train_epochs': 10, 'epoch_patience': 4}. Best is trial 5 with value: 0.6088884469930232.
2026-02-12 21:49:06 - INFO - Using devices: cuda, cuda
2026-02-12 21:49:06 - INFO - Devices: cuda, cuda
2026-02-12 21:49:06 - INFO - Starting log
2026-02-12 21:49:06 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 21:49:06 - INFO - Learning Rate: 0.0006815027568637131
Weight Decay: 7.123139660504409e-05
Batch Size: 24
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-12 21:49:07 - INFO - Generating initial weights
2026-02-12 21:49:22 - INFO - Time taken for Epoch 1:14.12 - F1: 0.0276
2026-02-12 21:49:36 - INFO - Time taken for Epoch 2:14.06 - F1: 0.0354
2026-02-12 21:49:51 - INFO - Time taken for Epoch 3:14.09 - F1: 0.0837
2026-02-12 21:50:05 - INFO - Time taken for Epoch 4:14.10 - F1: 0.0107
2026-02-12 21:50:19 - INFO - Time taken for Epoch 5:14.10 - F1: 0.0769
2026-02-12 21:50:33 - INFO - Time taken for Epoch 6:14.07 - F1: 0.0485
2026-02-12 21:50:47 - INFO - Time taken for Epoch 7:14.09 - F1: 0.1531
2026-02-12 21:51:01 - INFO - Time taken for Epoch 8:14.08 - F1: 0.1526
2026-02-12 21:51:15 - INFO - Time taken for Epoch 9:14.09 - F1: 0.1946
2026-02-12 21:51:29 - INFO - Time taken for Epoch 10:14.08 - F1: 0.1626
2026-02-12 21:51:43 - INFO - Time taken for Epoch 11:14.08 - F1: 0.0993
2026-02-12 21:51:57 - INFO - Time taken for Epoch 12:14.08 - F1: 0.1156
2026-02-12 21:52:11 - INFO - Time taken for Epoch 13:14.07 - F1: 0.1117
2026-02-12 21:52:25 - INFO - Time taken for Epoch 14:14.07 - F1: 0.0816
2026-02-12 21:52:40 - INFO - Time taken for Epoch 15:14.07 - F1: 0.0769
2026-02-12 21:52:54 - INFO - Time taken for Epoch 16:14.06 - F1: 0.1354
2026-02-12 21:53:08 - INFO - Time taken for Epoch 17:14.01 - F1: 0.0776
2026-02-12 21:53:22 - INFO - Time taken for Epoch 18:14.06 - F1: 0.0899
2026-02-12 21:53:36 - INFO - Time taken for Epoch 19:14.03 - F1: 0.1131
2026-02-12 21:53:36 - INFO - Best F1:0.1946 - Best Epoch:9
2026-02-12 21:53:36 - INFO - Starting co-training
2026-02-12 21:54:07 - INFO - Time taken for Epoch 1: 30.29s - F1: 0.03396410
2026-02-12 21:54:38 - INFO - Time taken for Epoch 2: 30.92s - F1: 0.03396410
2026-02-12 21:55:08 - INFO - Time taken for Epoch 3: 30.28s - F1: 0.03396410
2026-02-12 21:55:38 - INFO - Time taken for Epoch 4: 30.32s - F1: 0.03396410
2026-02-12 21:56:09 - INFO - Time taken for Epoch 5: 30.56s - F1: 0.03396410
2026-02-12 21:56:39 - INFO - Time taken for Epoch 6: 30.40s - F1: 0.03396410
2026-02-12 21:57:10 - INFO - Time taken for Epoch 7: 30.24s - F1: 0.03396410
2026-02-12 21:57:40 - INFO - Time taken for Epoch 8: 30.17s - F1: 0.03396410
2026-02-12 21:58:10 - INFO - Time taken for Epoch 9: 30.26s - F1: 0.03396410
2026-02-12 21:58:40 - INFO - Time taken for Epoch 10: 30.27s - F1: 0.03396410
2026-02-12 21:59:11 - INFO - Time taken for Epoch 11: 30.20s - F1: 0.03396410
2026-02-12 21:59:11 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 21:59:12 - INFO - Fine-tuning models
2026-02-12 21:59:14 - INFO - Time taken for Epoch 1:2.14 - F1: 0.0229
2026-02-12 21:59:17 - INFO - Time taken for Epoch 2:2.73 - F1: 0.0256
2026-02-12 21:59:20 - INFO - Time taken for Epoch 3:2.82 - F1: 0.0017
2026-02-12 21:59:22 - INFO - Time taken for Epoch 4:2.13 - F1: 0.0276
2026-02-12 21:59:25 - INFO - Time taken for Epoch 5:2.76 - F1: 0.0105
2026-02-12 21:59:27 - INFO - Time taken for Epoch 6:2.13 - F1: 0.0354
2026-02-12 21:59:30 - INFO - Time taken for Epoch 7:2.97 - F1: 0.0276
2026-02-12 21:59:32 - INFO - Time taken for Epoch 8:2.13 - F1: 0.0276
2026-02-12 21:59:34 - INFO - Time taken for Epoch 9:2.13 - F1: 0.0340
2026-02-12 21:59:36 - INFO - Time taken for Epoch 10:2.13 - F1: 0.0340
2026-02-12 21:59:38 - INFO - Time taken for Epoch 11:2.13 - F1: 0.0340
2026-02-12 21:59:40 - INFO - Time taken for Epoch 12:2.13 - F1: 0.0050
2026-02-12 21:59:43 - INFO - Time taken for Epoch 13:2.13 - F1: 0.0050
2026-02-12 21:59:45 - INFO - Time taken for Epoch 14:2.13 - F1: 0.0050
2026-02-12 21:59:47 - INFO - Time taken for Epoch 15:2.13 - F1: 0.0050
2026-02-12 21:59:49 - INFO - Time taken for Epoch 16:2.14 - F1: 0.0256
2026-02-12 21:59:49 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 21:59:49 - INFO - Best F1:0.0354 - Best Epoch:5
2026-02-12 21:59:54 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0354, Test ECE: 0.0931
2026-02-12 21:59:54 - INFO - All results: {'f1_macro': 0.03542673107890499, 'ece': np.float64(0.09311091482876466)}
2026-02-12 21:59:54 - INFO - 
Total time taken: 648.07 seconds
2026-02-12 21:59:54 - INFO - Trial 6 finished with value: 0.03542673107890499 and parameters: {'learning_rate': 0.0006815027568637131, 'weight_decay': 7.123139660504409e-05, 'batch_size': 24, 'co_train_epochs': 19, 'epoch_patience': 10}. Best is trial 5 with value: 0.6088884469930232.
2026-02-12 21:59:54 - INFO - Using devices: cuda, cuda
2026-02-12 21:59:54 - INFO - Devices: cuda, cuda
2026-02-12 21:59:54 - INFO - Starting log
2026-02-12 21:59:54 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 21:59:54 - INFO - Learning Rate: 0.00034447516786781005
Weight Decay: 1.5276039322402143e-05
Batch Size: 8
No. Epochs: 20
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-12 21:59:55 - INFO - Generating initial weights
2026-02-12 22:00:14 - INFO - Time taken for Epoch 1:17.39 - F1: 0.0276
2026-02-12 22:00:31 - INFO - Time taken for Epoch 2:17.32 - F1: 0.0276
2026-02-12 22:00:48 - INFO - Time taken for Epoch 3:17.30 - F1: 0.0493
2026-02-12 22:01:06 - INFO - Time taken for Epoch 4:17.29 - F1: 0.1455
2026-02-12 22:01:23 - INFO - Time taken for Epoch 5:17.30 - F1: 0.1558
2026-02-12 22:01:40 - INFO - Time taken for Epoch 6:17.31 - F1: 0.2356
2026-02-12 22:01:58 - INFO - Time taken for Epoch 7:17.30 - F1: 0.3468
2026-02-12 22:02:15 - INFO - Time taken for Epoch 8:17.27 - F1: 0.3384
2026-02-12 22:02:32 - INFO - Time taken for Epoch 9:17.29 - F1: 0.3395
2026-02-12 22:02:49 - INFO - Time taken for Epoch 10:17.28 - F1: 0.3452
2026-02-12 22:03:07 - INFO - Time taken for Epoch 11:17.28 - F1: 0.3604
2026-02-12 22:03:24 - INFO - Time taken for Epoch 12:17.26 - F1: 0.3806
2026-02-12 22:03:41 - INFO - Time taken for Epoch 13:17.27 - F1: 0.3844
2026-02-12 22:03:58 - INFO - Time taken for Epoch 14:17.26 - F1: 0.3769
2026-02-12 22:04:16 - INFO - Time taken for Epoch 15:17.28 - F1: 0.3684
2026-02-12 22:04:33 - INFO - Time taken for Epoch 16:17.26 - F1: 0.3747
2026-02-12 22:04:50 - INFO - Time taken for Epoch 17:17.28 - F1: 0.3816
2026-02-12 22:05:08 - INFO - Time taken for Epoch 18:17.28 - F1: 0.3823
2026-02-12 22:05:25 - INFO - Time taken for Epoch 19:17.25 - F1: 0.3907
2026-02-12 22:05:42 - INFO - Time taken for Epoch 20:17.29 - F1: 0.3975
2026-02-12 22:05:42 - INFO - Best F1:0.3975 - Best Epoch:20
2026-02-12 22:05:43 - INFO - Starting co-training
2026-02-12 22:06:08 - INFO - Time taken for Epoch 1: 24.92s - F1: 0.02758967
2026-02-12 22:06:34 - INFO - Time taken for Epoch 2: 26.03s - F1: 0.03396410
2026-02-12 22:07:00 - INFO - Time taken for Epoch 3: 25.65s - F1: 0.02286448
2026-02-12 22:07:25 - INFO - Time taken for Epoch 4: 24.88s - F1: 0.02286448
2026-02-12 22:07:49 - INFO - Time taken for Epoch 5: 24.90s - F1: 0.02286448
2026-02-12 22:08:14 - INFO - Time taken for Epoch 6: 24.94s - F1: 0.02286448
2026-02-12 22:08:39 - INFO - Time taken for Epoch 7: 24.91s - F1: 0.02286448
2026-02-12 22:09:04 - INFO - Time taken for Epoch 8: 24.93s - F1: 0.02286448
2026-02-12 22:09:29 - INFO - Time taken for Epoch 9: 25.03s - F1: 0.02286448
2026-02-12 22:09:29 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-12 22:09:31 - INFO - Fine-tuning models
2026-02-12 22:09:34 - INFO - Time taken for Epoch 1:2.70 - F1: 0.0340
2026-02-12 22:09:37 - INFO - Time taken for Epoch 2:3.31 - F1: 0.0340
2026-02-12 22:09:40 - INFO - Time taken for Epoch 3:2.69 - F1: 0.0215
2026-02-12 22:09:42 - INFO - Time taken for Epoch 4:2.68 - F1: 0.0017
2026-02-12 22:09:45 - INFO - Time taken for Epoch 5:2.69 - F1: 0.0017
2026-02-12 22:09:48 - INFO - Time taken for Epoch 6:2.68 - F1: 0.0276
2026-02-12 22:09:50 - INFO - Time taken for Epoch 7:2.68 - F1: 0.0276
2026-02-12 22:09:53 - INFO - Time taken for Epoch 8:2.69 - F1: 0.0276
2026-02-12 22:09:56 - INFO - Time taken for Epoch 9:2.69 - F1: 0.0276
2026-02-12 22:09:58 - INFO - Time taken for Epoch 10:2.68 - F1: 0.0276
2026-02-12 22:10:01 - INFO - Time taken for Epoch 11:2.73 - F1: 0.0276
2026-02-12 22:10:01 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 22:10:01 - INFO - Best F1:0.0340 - Best Epoch:0
2026-02-12 22:10:07 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0339, Test ECE: 0.4176
2026-02-12 22:10:07 - INFO - All results: {'f1_macro': 0.03385172693773031, 'ece': np.float64(0.4175942496295316)}
2026-02-12 22:10:07 - INFO - 
Total time taken: 613.42 seconds
2026-02-12 22:10:07 - INFO - Trial 7 finished with value: 0.03385172693773031 and parameters: {'learning_rate': 0.00034447516786781005, 'weight_decay': 1.5276039322402143e-05, 'batch_size': 8, 'co_train_epochs': 20, 'epoch_patience': 7}. Best is trial 5 with value: 0.6088884469930232.
2026-02-12 22:10:07 - INFO - Using devices: cuda, cuda
2026-02-12 22:10:07 - INFO - Devices: cuda, cuda
2026-02-12 22:10:07 - INFO - Starting log
2026-02-12 22:10:07 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 22:10:08 - INFO - Learning Rate: 8.105870829471011e-05
Weight Decay: 0.008573528723405159
Batch Size: 8
No. Epochs: 19
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-12 22:10:09 - INFO - Generating initial weights
2026-02-12 22:10:27 - INFO - Time taken for Epoch 1:17.36 - F1: 0.0339
2026-02-12 22:10:45 - INFO - Time taken for Epoch 2:17.30 - F1: 0.0276
2026-02-12 22:11:02 - INFO - Time taken for Epoch 3:17.33 - F1: 0.0479
2026-02-12 22:11:19 - INFO - Time taken for Epoch 4:17.32 - F1: 0.0925
2026-02-12 22:11:37 - INFO - Time taken for Epoch 5:17.31 - F1: 0.2325
2026-02-12 22:11:54 - INFO - Time taken for Epoch 6:17.35 - F1: 0.3307
2026-02-12 22:12:11 - INFO - Time taken for Epoch 7:17.32 - F1: 0.3677
2026-02-12 22:12:29 - INFO - Time taken for Epoch 8:17.35 - F1: 0.3724
2026-02-12 22:12:46 - INFO - Time taken for Epoch 9:17.31 - F1: 0.3636
2026-02-12 22:13:03 - INFO - Time taken for Epoch 10:17.31 - F1: 0.3662
2026-02-12 22:13:21 - INFO - Time taken for Epoch 11:17.33 - F1: 0.3869
2026-02-12 22:13:38 - INFO - Time taken for Epoch 12:17.31 - F1: 0.3955
2026-02-12 22:13:55 - INFO - Time taken for Epoch 13:17.33 - F1: 0.3892
2026-02-12 22:14:13 - INFO - Time taken for Epoch 14:17.31 - F1: 0.3951
2026-02-12 22:14:30 - INFO - Time taken for Epoch 15:17.34 - F1: 0.3984
2026-02-12 22:14:47 - INFO - Time taken for Epoch 16:17.33 - F1: 0.3937
2026-02-12 22:15:05 - INFO - Time taken for Epoch 17:17.34 - F1: 0.3889
2026-02-12 22:15:22 - INFO - Time taken for Epoch 18:17.29 - F1: 0.3868
2026-02-12 22:15:39 - INFO - Time taken for Epoch 19:17.34 - F1: 0.4039
2026-02-12 22:15:39 - INFO - Best F1:0.4039 - Best Epoch:19
2026-02-12 22:15:40 - INFO - Starting co-training
2026-02-12 22:16:05 - INFO - Time taken for Epoch 1: 24.96s - F1: 0.37292929
2026-02-12 22:16:31 - INFO - Time taken for Epoch 2: 25.59s - F1: 0.38012641
2026-02-12 22:16:56 - INFO - Time taken for Epoch 3: 25.68s - F1: 0.42208458
2026-02-12 22:17:22 - INFO - Time taken for Epoch 4: 25.65s - F1: 0.38004136
2026-02-12 22:17:47 - INFO - Time taken for Epoch 5: 24.94s - F1: 0.41678610
2026-02-12 22:18:12 - INFO - Time taken for Epoch 6: 24.92s - F1: 0.43484631
2026-02-12 22:18:37 - INFO - Time taken for Epoch 7: 25.53s - F1: 0.42822075
2026-02-12 22:19:02 - INFO - Time taken for Epoch 8: 24.94s - F1: 0.44122869
2026-02-12 22:19:28 - INFO - Time taken for Epoch 9: 25.65s - F1: 0.42189401
2026-02-12 22:19:53 - INFO - Time taken for Epoch 10: 24.96s - F1: 0.44321204
2026-02-12 22:20:19 - INFO - Time taken for Epoch 11: 25.62s - F1: 0.44448498
2026-02-12 22:20:44 - INFO - Time taken for Epoch 12: 25.67s - F1: 0.45539378
2026-02-12 22:21:10 - INFO - Time taken for Epoch 13: 25.55s - F1: 0.49779838
2026-02-12 22:21:36 - INFO - Time taken for Epoch 14: 25.77s - F1: 0.46000707
2026-02-12 22:22:00 - INFO - Time taken for Epoch 15: 24.90s - F1: 0.43308023
2026-02-12 22:22:25 - INFO - Time taken for Epoch 16: 24.90s - F1: 0.45797140
2026-02-12 22:22:50 - INFO - Time taken for Epoch 17: 24.94s - F1: 0.41953805
2026-02-12 22:22:50 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-12 22:22:52 - INFO - Fine-tuning models
2026-02-12 22:22:55 - INFO - Time taken for Epoch 1:2.71 - F1: 0.4827
2026-02-12 22:22:58 - INFO - Time taken for Epoch 2:3.33 - F1: 0.4913
2026-02-12 22:23:02 - INFO - Time taken for Epoch 3:3.88 - F1: 0.5019
2026-02-12 22:23:05 - INFO - Time taken for Epoch 4:3.45 - F1: 0.5023
2026-02-12 22:23:09 - INFO - Time taken for Epoch 5:3.71 - F1: 0.5093
2026-02-12 22:23:13 - INFO - Time taken for Epoch 6:3.39 - F1: 0.4957
2026-02-12 22:23:15 - INFO - Time taken for Epoch 7:2.69 - F1: 0.4828
2026-02-12 22:23:18 - INFO - Time taken for Epoch 8:2.69 - F1: 0.4744
2026-02-12 22:23:21 - INFO - Time taken for Epoch 9:2.69 - F1: 0.4606
2026-02-12 22:23:23 - INFO - Time taken for Epoch 10:2.69 - F1: 0.4493
2026-02-12 22:23:26 - INFO - Time taken for Epoch 11:2.69 - F1: 0.4648
2026-02-12 22:23:29 - INFO - Time taken for Epoch 12:2.69 - F1: 0.4715
2026-02-12 22:23:31 - INFO - Time taken for Epoch 13:2.69 - F1: 0.4783
2026-02-12 22:23:34 - INFO - Time taken for Epoch 14:2.69 - F1: 0.4810
2026-02-12 22:23:37 - INFO - Time taken for Epoch 15:2.69 - F1: 0.4923
2026-02-12 22:23:37 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 22:23:37 - INFO - Best F1:0.5093 - Best Epoch:4
2026-02-12 22:23:43 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5025, Test ECE: 0.1251
2026-02-12 22:23:43 - INFO - All results: {'f1_macro': 0.5024926686556125, 'ece': np.float64(0.12507836793398666)}
2026-02-12 22:23:43 - INFO - 
Total time taken: 815.13 seconds
2026-02-12 22:23:43 - INFO - Trial 8 finished with value: 0.5024926686556125 and parameters: {'learning_rate': 8.105870829471011e-05, 'weight_decay': 0.008573528723405159, 'batch_size': 8, 'co_train_epochs': 19, 'epoch_patience': 4}. Best is trial 5 with value: 0.6088884469930232.
2026-02-12 22:23:43 - INFO - Using devices: cuda, cuda
2026-02-12 22:23:43 - INFO - Devices: cuda, cuda
2026-02-12 22:23:43 - INFO - Starting log
2026-02-12 22:23:43 - INFO - Dataset: humanitarian9, Event: hurricane_dorian_2019, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-12 22:23:43 - INFO - Learning Rate: 0.00022861413279054232
Weight Decay: 0.0011314112187709407
Batch Size: 24
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-12 22:23:44 - INFO - Generating initial weights
2026-02-12 22:23:59 - INFO - Time taken for Epoch 1:14.10 - F1: 0.0468
2026-02-12 22:24:13 - INFO - Time taken for Epoch 2:14.09 - F1: 0.2060
2026-02-12 22:24:27 - INFO - Time taken for Epoch 3:14.04 - F1: 0.2984
2026-02-12 22:24:41 - INFO - Time taken for Epoch 4:14.09 - F1: 0.3326
2026-02-12 22:24:55 - INFO - Time taken for Epoch 5:14.11 - F1: 0.3777
2026-02-12 22:25:09 - INFO - Time taken for Epoch 6:14.10 - F1: 0.3577
2026-02-12 22:25:24 - INFO - Time taken for Epoch 7:14.09 - F1: 0.3769
2026-02-12 22:25:38 - INFO - Time taken for Epoch 8:14.08 - F1: 0.4001
2026-02-12 22:25:52 - INFO - Time taken for Epoch 9:14.11 - F1: 0.4150
2026-02-12 22:26:06 - INFO - Time taken for Epoch 10:14.10 - F1: 0.3917
2026-02-12 22:26:20 - INFO - Time taken for Epoch 11:14.11 - F1: 0.4108
2026-02-12 22:26:34 - INFO - Time taken for Epoch 12:14.09 - F1: 0.3890
2026-02-12 22:26:48 - INFO - Time taken for Epoch 13:14.09 - F1: 0.3942
2026-02-12 22:27:02 - INFO - Time taken for Epoch 14:14.06 - F1: 0.4008
2026-02-12 22:27:16 - INFO - Time taken for Epoch 15:14.09 - F1: 0.3947
2026-02-12 22:27:30 - INFO - Time taken for Epoch 16:14.10 - F1: 0.3968
2026-02-12 22:27:44 - INFO - Time taken for Epoch 17:14.10 - F1: 0.3980
2026-02-12 22:27:59 - INFO - Time taken for Epoch 18:14.09 - F1: 0.3991
2026-02-12 22:28:13 - INFO - Time taken for Epoch 19:14.12 - F1: 0.4003
2026-02-12 22:28:13 - INFO - Best F1:0.4150 - Best Epoch:9
2026-02-12 22:28:13 - INFO - Starting co-training
2026-02-12 22:28:44 - INFO - Time taken for Epoch 1: 30.23s - F1: 0.37532780
2026-02-12 22:29:15 - INFO - Time taken for Epoch 2: 30.81s - F1: 0.29665339
2026-02-12 22:29:45 - INFO - Time taken for Epoch 3: 30.24s - F1: 0.19198948
2026-02-12 22:30:15 - INFO - Time taken for Epoch 4: 30.21s - F1: 0.11020143
2026-02-12 22:30:45 - INFO - Time taken for Epoch 5: 30.21s - F1: 0.03396410
2026-02-12 22:31:15 - INFO - Time taken for Epoch 6: 30.18s - F1: 0.03396410
2026-02-12 22:31:46 - INFO - Time taken for Epoch 7: 30.15s - F1: 0.03396410
2026-02-12 22:32:16 - INFO - Time taken for Epoch 8: 30.21s - F1: 0.03396410
2026-02-12 22:32:46 - INFO - Time taken for Epoch 9: 30.20s - F1: 0.03396410
2026-02-12 22:33:16 - INFO - Time taken for Epoch 10: 30.28s - F1: 0.03396410
2026-02-12 22:33:47 - INFO - Time taken for Epoch 11: 30.23s - F1: 0.03396410
2026-02-12 22:33:47 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 22:33:48 - INFO - Fine-tuning models
2026-02-12 22:33:50 - INFO - Time taken for Epoch 1:2.15 - F1: 0.3801
2026-02-12 22:33:53 - INFO - Time taken for Epoch 2:2.85 - F1: 0.3769
2026-02-12 22:33:55 - INFO - Time taken for Epoch 3:2.14 - F1: 0.4438
2026-02-12 22:33:58 - INFO - Time taken for Epoch 4:2.74 - F1: 0.4698
2026-02-12 22:34:01 - INFO - Time taken for Epoch 5:2.76 - F1: 0.4718
2026-02-12 22:34:04 - INFO - Time taken for Epoch 6:2.79 - F1: 0.4359
2026-02-12 22:34:06 - INFO - Time taken for Epoch 7:2.14 - F1: 0.4768
2026-02-12 22:34:09 - INFO - Time taken for Epoch 8:2.86 - F1: 0.4465
2026-02-12 22:34:11 - INFO - Time taken for Epoch 9:2.13 - F1: 0.4208
2026-02-12 22:34:13 - INFO - Time taken for Epoch 10:2.14 - F1: 0.4309
2026-02-12 22:34:15 - INFO - Time taken for Epoch 11:2.14 - F1: 0.4198
2026-02-12 22:34:17 - INFO - Time taken for Epoch 12:2.14 - F1: 0.4358
2026-02-12 22:34:19 - INFO - Time taken for Epoch 13:2.14 - F1: 0.4497
2026-02-12 22:34:21 - INFO - Time taken for Epoch 14:2.14 - F1: 0.4398
2026-02-12 22:34:24 - INFO - Time taken for Epoch 15:2.14 - F1: 0.4427
2026-02-12 22:34:26 - INFO - Time taken for Epoch 16:2.14 - F1: 0.4660
2026-02-12 22:34:28 - INFO - Time taken for Epoch 17:2.15 - F1: 0.4613
2026-02-12 22:34:28 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-12 22:34:28 - INFO - Best F1:0.4768 - Best Epoch:6
2026-02-12 22:34:33 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.4828, Test ECE: 0.1889
2026-02-12 22:34:33 - INFO - All results: {'f1_macro': 0.4828046643823336, 'ece': np.float64(0.1888866572584018)}
2026-02-12 22:34:33 - INFO - 
Total time taken: 650.14 seconds
2026-02-12 22:34:33 - INFO - Trial 9 finished with value: 0.4828046643823336 and parameters: {'learning_rate': 0.00022861413279054232, 'weight_decay': 0.0011314112187709407, 'batch_size': 24, 'co_train_epochs': 19, 'epoch_patience': 10}. Best is trial 5 with value: 0.6088884469930232.
2026-02-12 22:34:33 - INFO - 
[BEST TRIAL RESULTS]
2026-02-12 22:34:33 - INFO - F1 Score: 0.6089
2026-02-12 22:34:33 - INFO - Params: {'learning_rate': 4.212202361991735e-05, 'weight_decay': 0.0007655069908625201, 'batch_size': 24, 'co_train_epochs': 10, 'epoch_patience': 4}
2026-02-12 22:34:33 - INFO -   learning_rate: 4.212202361991735e-05
2026-02-12 22:34:33 - INFO -   weight_decay: 0.0007655069908625201
2026-02-12 22:34:33 - INFO -   batch_size: 24
2026-02-12 22:34:33 - INFO -   co_train_epochs: 10
2026-02-12 22:34:33 - INFO -   epoch_patience: 4
2026-02-12 22:34:33 - INFO - 
Total time taken: 6093.74 seconds
