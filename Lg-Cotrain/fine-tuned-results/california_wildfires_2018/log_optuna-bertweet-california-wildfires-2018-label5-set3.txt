2026-02-13 11:54:01 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 11:54:01 - INFO - A new study created in memory with name: study_humanitarian10_california_wildfires_2018
2026-02-13 11:54:01 - INFO - Using devices: cuda, cuda
2026-02-13 11:54:01 - INFO - Devices: cuda, cuda
2026-02-13 11:54:01 - INFO - Starting log
2026-02-13 11:54:01 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 11:54:02 - INFO - Learning Rate: 0.00011125742730953695
Weight Decay: 0.0003109810477491913
Batch Size: 8
No. Epochs: 12
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-13 11:54:02 - INFO - Generating initial weights
2026-02-13 11:54:21 - INFO - Time taken for Epoch 1:16.86 - F1: 0.0186
2026-02-13 11:54:37 - INFO - Time taken for Epoch 2:16.73 - F1: 0.0338
2026-02-13 11:54:54 - INFO - Time taken for Epoch 3:16.74 - F1: 0.0956
2026-02-13 11:55:11 - INFO - Time taken for Epoch 4:16.72 - F1: 0.1777
2026-02-13 11:55:28 - INFO - Time taken for Epoch 5:16.77 - F1: 0.2342
2026-02-13 11:55:44 - INFO - Time taken for Epoch 6:16.73 - F1: 0.2413
2026-02-13 11:56:01 - INFO - Time taken for Epoch 7:16.76 - F1: 0.2528
2026-02-13 11:56:18 - INFO - Time taken for Epoch 8:16.92 - F1: 0.2525
2026-02-13 11:56:35 - INFO - Time taken for Epoch 9:16.86 - F1: 0.2699
2026-02-13 11:56:52 - INFO - Time taken for Epoch 10:16.73 - F1: 0.2674
2026-02-13 11:57:08 - INFO - Time taken for Epoch 11:16.78 - F1: 0.2766
2026-02-13 11:57:25 - INFO - Time taken for Epoch 12:16.94 - F1: 0.2929
2026-02-13 11:57:25 - INFO - Best F1:0.2929 - Best Epoch:12
2026-02-13 11:57:26 - INFO - Starting co-training
2026-02-13 11:57:51 - INFO - Time taken for Epoch 1: 24.86s - F1: 0.28894971
2026-02-13 11:58:16 - INFO - Time taken for Epoch 2: 25.14s - F1: 0.26658256
2026-02-13 11:58:41 - INFO - Time taken for Epoch 3: 24.76s - F1: 0.29123877
2026-02-13 11:59:07 - INFO - Time taken for Epoch 4: 25.38s - F1: 0.24932584
2026-02-13 11:59:31 - INFO - Time taken for Epoch 5: 24.67s - F1: 0.28417344
2026-02-13 11:59:56 - INFO - Time taken for Epoch 6: 24.68s - F1: 0.13213101
2026-02-13 12:00:21 - INFO - Time taken for Epoch 7: 24.70s - F1: 0.28577043
2026-02-13 12:00:21 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-13 12:00:22 - INFO - Fine-tuning models
2026-02-13 12:00:25 - INFO - Time taken for Epoch 1:2.48 - F1: 0.2893
2026-02-13 12:00:28 - INFO - Time taken for Epoch 2:2.98 - F1: 0.2871
2026-02-13 12:00:30 - INFO - Time taken for Epoch 3:2.44 - F1: 0.2761
2026-02-13 12:00:33 - INFO - Time taken for Epoch 4:2.43 - F1: 0.2516
2026-02-13 12:00:35 - INFO - Time taken for Epoch 5:2.42 - F1: 0.2277
2026-02-13 12:00:37 - INFO - Time taken for Epoch 6:2.56 - F1: 0.2303
2026-02-13 12:00:40 - INFO - Time taken for Epoch 7:2.63 - F1: 0.2468
2026-02-13 12:00:43 - INFO - Time taken for Epoch 8:2.54 - F1: 0.2907
2026-02-13 12:00:46 - INFO - Time taken for Epoch 9:3.06 - F1: 0.2995
2026-02-13 12:00:49 - INFO - Time taken for Epoch 10:3.21 - F1: 0.2935
2026-02-13 12:00:51 - INFO - Time taken for Epoch 11:2.41 - F1: 0.2503
2026-02-13 12:00:54 - INFO - Time taken for Epoch 12:2.41 - F1: 0.2505
2026-02-13 12:00:56 - INFO - Time taken for Epoch 13:2.42 - F1: 0.2861
2026-02-13 12:00:59 - INFO - Time taken for Epoch 14:2.41 - F1: 0.3064
2026-02-13 12:01:08 - INFO - Time taken for Epoch 15:9.00 - F1: 0.3101
2026-02-13 12:01:11 - INFO - Time taken for Epoch 16:3.02 - F1: 0.3149
2026-02-13 12:01:14 - INFO - Time taken for Epoch 17:3.05 - F1: 0.3190
2026-02-13 12:01:17 - INFO - Time taken for Epoch 18:3.09 - F1: 0.3302
2026-02-13 12:01:20 - INFO - Time taken for Epoch 19:3.05 - F1: 0.3377
2026-02-13 12:01:23 - INFO - Time taken for Epoch 20:3.05 - F1: 0.3540
2026-02-13 12:01:26 - INFO - Time taken for Epoch 21:3.10 - F1: 0.3479
2026-02-13 12:01:28 - INFO - Time taken for Epoch 22:2.40 - F1: 0.3456
2026-02-13 12:01:31 - INFO - Time taken for Epoch 23:2.40 - F1: 0.3483
2026-02-13 12:01:33 - INFO - Time taken for Epoch 24:2.40 - F1: 0.3252
2026-02-13 12:01:36 - INFO - Time taken for Epoch 25:2.40 - F1: 0.3390
2026-02-13 12:01:38 - INFO - Time taken for Epoch 26:2.40 - F1: 0.3465
2026-02-13 12:01:41 - INFO - Time taken for Epoch 27:2.71 - F1: 0.3533
2026-02-13 12:01:43 - INFO - Time taken for Epoch 28:2.66 - F1: 0.3487
2026-02-13 12:01:46 - INFO - Time taken for Epoch 29:2.58 - F1: 0.3169
2026-02-13 12:01:48 - INFO - Time taken for Epoch 30:2.61 - F1: 0.3203
2026-02-13 12:01:48 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 12:01:48 - INFO - Best F1:0.3540 - Best Epoch:19
2026-02-13 12:01:55 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.3585, Test ECE: 0.1514
2026-02-13 12:01:55 - INFO - All results: {'f1_macro': 0.3584570793288712, 'ece': np.float64(0.15141454853071568)}
2026-02-13 12:01:55 - INFO - 
Total time taken: 473.73 seconds
2026-02-13 12:01:55 - INFO - Trial 0 finished with value: 0.3584570793288712 and parameters: {'learning_rate': 0.00011125742730953695, 'weight_decay': 0.0003109810477491913, 'batch_size': 8, 'co_train_epochs': 12, 'epoch_patience': 4}. Best is trial 0 with value: 0.3584570793288712.
2026-02-13 12:01:55 - INFO - Using devices: cuda, cuda
2026-02-13 12:01:55 - INFO - Devices: cuda, cuda
2026-02-13 12:01:55 - INFO - Starting log
2026-02-13 12:01:55 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 12:01:55 - INFO - Learning Rate: 0.0004150530519378089
Weight Decay: 0.00028233642406367204
Batch Size: 8
No. Epochs: 15
Epoch Patience: 8
 Accumulation Steps: 8
2026-02-13 12:01:56 - INFO - Generating initial weights
2026-02-13 12:02:14 - INFO - Time taken for Epoch 1:17.00 - F1: 0.0132
2026-02-13 12:02:31 - INFO - Time taken for Epoch 2:16.72 - F1: 0.0372
2026-02-13 12:02:48 - INFO - Time taken for Epoch 3:16.70 - F1: 0.0807
2026-02-13 12:03:04 - INFO - Time taken for Epoch 4:16.69 - F1: 0.0925
2026-02-13 12:03:21 - INFO - Time taken for Epoch 5:16.72 - F1: 0.1072
2026-02-13 12:03:38 - INFO - Time taken for Epoch 6:16.68 - F1: 0.1518
2026-02-13 12:03:54 - INFO - Time taken for Epoch 7:16.67 - F1: 0.2483
2026-02-13 12:04:11 - INFO - Time taken for Epoch 8:16.70 - F1: 0.2608
2026-02-13 12:04:28 - INFO - Time taken for Epoch 9:16.68 - F1: 0.2707
2026-02-13 12:04:44 - INFO - Time taken for Epoch 10:16.69 - F1: 0.2811
2026-02-13 12:05:01 - INFO - Time taken for Epoch 11:16.86 - F1: 0.2465
2026-02-13 12:05:18 - INFO - Time taken for Epoch 12:16.67 - F1: 0.2470
2026-02-13 12:05:35 - INFO - Time taken for Epoch 13:17.43 - F1: 0.2624
2026-02-13 12:05:53 - INFO - Time taken for Epoch 14:17.67 - F1: 0.2606
2026-02-13 12:06:11 - INFO - Time taken for Epoch 15:17.55 - F1: 0.2744
2026-02-13 12:06:11 - INFO - Best F1:0.2811 - Best Epoch:10
2026-02-13 12:06:11 - INFO - Starting co-training
2026-02-13 12:06:37 - INFO - Time taken for Epoch 1: 24.92s - F1: 0.04185068
2026-02-13 12:07:03 - INFO - Time taken for Epoch 2: 26.15s - F1: 0.03024831
2026-02-13 12:07:27 - INFO - Time taken for Epoch 3: 24.55s - F1: 0.03024831
2026-02-13 12:07:52 - INFO - Time taken for Epoch 4: 24.34s - F1: 0.04185068
2026-02-13 12:08:16 - INFO - Time taken for Epoch 5: 24.43s - F1: 0.04185068
2026-02-13 12:08:41 - INFO - Time taken for Epoch 6: 25.00s - F1: 0.04185068
2026-02-13 12:09:07 - INFO - Time taken for Epoch 7: 25.51s - F1: 0.04185068
2026-02-13 12:09:31 - INFO - Time taken for Epoch 8: 24.79s - F1: 0.04185068
2026-02-13 12:09:57 - INFO - Time taken for Epoch 9: 25.17s - F1: 0.04185068
2026-02-13 12:09:57 - INFO - Performance not improving for 8 consecutive epochs.
2026-02-13 12:09:58 - INFO - Fine-tuning models
2026-02-13 12:10:01 - INFO - Time taken for Epoch 1:2.63 - F1: 0.0419
2026-02-13 12:10:04 - INFO - Time taken for Epoch 2:3.11 - F1: 0.0021
2026-02-13 12:10:06 - INFO - Time taken for Epoch 3:2.40 - F1: 0.0021
2026-02-13 12:10:09 - INFO - Time taken for Epoch 4:2.40 - F1: 0.0047
2026-02-13 12:10:11 - INFO - Time taken for Epoch 5:2.50 - F1: 0.0247
2026-02-13 12:10:14 - INFO - Time taken for Epoch 6:2.55 - F1: 0.0247
2026-02-13 12:10:16 - INFO - Time taken for Epoch 7:2.47 - F1: 0.0321
2026-02-13 12:10:19 - INFO - Time taken for Epoch 8:2.43 - F1: 0.0321
2026-02-13 12:10:21 - INFO - Time taken for Epoch 9:2.52 - F1: 0.0321
2026-02-13 12:10:24 - INFO - Time taken for Epoch 10:2.54 - F1: 0.0021
2026-02-13 12:10:26 - INFO - Time taken for Epoch 11:2.47 - F1: 0.0021
2026-02-13 12:10:26 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 12:10:26 - INFO - Best F1:0.0419 - Best Epoch:0
2026-02-13 12:10:32 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0417, Test ECE: 0.0425
2026-02-13 12:10:32 - INFO - All results: {'f1_macro': 0.04171180931744312, 'ece': np.float64(0.04247439211562104)}
2026-02-13 12:10:32 - INFO - 
Total time taken: 517.49 seconds
2026-02-13 12:10:32 - INFO - Trial 1 finished with value: 0.04171180931744312 and parameters: {'learning_rate': 0.0004150530519378089, 'weight_decay': 0.00028233642406367204, 'batch_size': 8, 'co_train_epochs': 15, 'epoch_patience': 8}. Best is trial 0 with value: 0.3584570793288712.
2026-02-13 12:10:32 - INFO - Using devices: cuda, cuda
2026-02-13 12:10:32 - INFO - Devices: cuda, cuda
2026-02-13 12:10:32 - INFO - Starting log
2026-02-13 12:10:32 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 12:10:33 - INFO - Learning Rate: 0.0003230957251695424
Weight Decay: 1.1020799222747614e-05
Batch Size: 24
No. Epochs: 15
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-13 12:10:33 - INFO - Generating initial weights
2026-02-13 12:10:49 - INFO - Time taken for Epoch 1:13.95 - F1: 0.0127
2026-02-13 12:11:03 - INFO - Time taken for Epoch 2:13.88 - F1: 0.1727
2026-02-13 12:11:16 - INFO - Time taken for Epoch 3:13.88 - F1: 0.2066
2026-02-13 12:11:30 - INFO - Time taken for Epoch 4:13.85 - F1: 0.2211
2026-02-13 12:11:44 - INFO - Time taken for Epoch 5:13.70 - F1: 0.2520
2026-02-13 12:11:58 - INFO - Time taken for Epoch 6:13.67 - F1: 0.2771
2026-02-13 12:12:11 - INFO - Time taken for Epoch 7:13.66 - F1: 0.2427
2026-02-13 12:12:25 - INFO - Time taken for Epoch 8:13.66 - F1: 0.2533
2026-02-13 12:12:39 - INFO - Time taken for Epoch 9:13.64 - F1: 0.2655
2026-02-13 12:12:52 - INFO - Time taken for Epoch 10:13.65 - F1: 0.2805
2026-02-13 12:13:06 - INFO - Time taken for Epoch 11:13.63 - F1: 0.2777
2026-02-13 12:13:20 - INFO - Time taken for Epoch 12:13.66 - F1: 0.2656
2026-02-13 12:13:33 - INFO - Time taken for Epoch 13:13.65 - F1: 0.2576
2026-02-13 12:13:47 - INFO - Time taken for Epoch 14:13.64 - F1: 0.2477
2026-02-13 12:14:00 - INFO - Time taken for Epoch 15:13.64 - F1: 0.2573
2026-02-13 12:14:00 - INFO - Best F1:0.2805 - Best Epoch:10
2026-02-13 12:14:01 - INFO - Starting co-training
2026-02-13 12:14:31 - INFO - Time taken for Epoch 1: 29.70s - F1: 0.04185068
2026-02-13 12:15:02 - INFO - Time taken for Epoch 2: 30.34s - F1: 0.04185068
2026-02-13 12:15:31 - INFO - Time taken for Epoch 3: 29.57s - F1: 0.03024831
2026-02-13 12:16:01 - INFO - Time taken for Epoch 4: 29.66s - F1: 0.03024831
2026-02-13 12:16:31 - INFO - Time taken for Epoch 5: 29.73s - F1: 0.04185068
2026-02-13 12:17:00 - INFO - Time taken for Epoch 6: 29.68s - F1: 0.04185068
2026-02-13 12:17:30 - INFO - Time taken for Epoch 7: 29.54s - F1: 0.04185068
2026-02-13 12:17:59 - INFO - Time taken for Epoch 8: 29.53s - F1: 0.04185068
2026-02-13 12:17:59 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-13 12:18:01 - INFO - Fine-tuning models
2026-02-13 12:18:03 - INFO - Time taken for Epoch 1:1.93 - F1: 0.0302
2026-02-13 12:18:05 - INFO - Time taken for Epoch 2:2.61 - F1: 0.0302
2026-02-13 12:18:07 - INFO - Time taken for Epoch 3:1.92 - F1: 0.0247
2026-02-13 12:18:09 - INFO - Time taken for Epoch 4:1.92 - F1: 0.0247
2026-02-13 12:18:11 - INFO - Time taken for Epoch 5:1.92 - F1: 0.0037
2026-02-13 12:18:13 - INFO - Time taken for Epoch 6:1.92 - F1: 0.0037
2026-02-13 12:18:15 - INFO - Time taken for Epoch 7:1.92 - F1: 0.0037
2026-02-13 12:18:17 - INFO - Time taken for Epoch 8:1.93 - F1: 0.0037
2026-02-13 12:18:19 - INFO - Time taken for Epoch 9:1.92 - F1: 0.0037
2026-02-13 12:18:21 - INFO - Time taken for Epoch 10:1.92 - F1: 0.0037
2026-02-13 12:18:23 - INFO - Time taken for Epoch 11:1.92 - F1: 0.0096
2026-02-13 12:18:23 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 12:18:23 - INFO - Best F1:0.0302 - Best Epoch:0
2026-02-13 12:18:28 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0303, Test ECE: 0.2844
2026-02-13 12:18:28 - INFO - All results: {'f1_macro': 0.030313588850174218, 'ece': np.float64(0.28436018448019584)}
2026-02-13 12:18:28 - INFO - 
Total time taken: 475.42 seconds
2026-02-13 12:18:28 - INFO - Trial 2 finished with value: 0.030313588850174218 and parameters: {'learning_rate': 0.0003230957251695424, 'weight_decay': 1.1020799222747614e-05, 'batch_size': 24, 'co_train_epochs': 15, 'epoch_patience': 7}. Best is trial 0 with value: 0.3584570793288712.
2026-02-13 12:18:28 - INFO - Using devices: cuda, cuda
2026-02-13 12:18:28 - INFO - Devices: cuda, cuda
2026-02-13 12:18:28 - INFO - Starting log
2026-02-13 12:18:28 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 12:18:28 - INFO - Learning Rate: 4.424333916890828e-05
Weight Decay: 3.87698482059888e-05
Batch Size: 8
No. Epochs: 7
Epoch Patience: 9
 Accumulation Steps: 8
2026-02-13 12:18:29 - INFO - Generating initial weights
2026-02-13 12:18:47 - INFO - Time taken for Epoch 1:16.77 - F1: 0.0739
2026-02-13 12:19:04 - INFO - Time taken for Epoch 2:16.74 - F1: 0.0350
2026-02-13 12:19:20 - INFO - Time taken for Epoch 3:16.73 - F1: 0.0309
2026-02-13 12:19:37 - INFO - Time taken for Epoch 4:16.73 - F1: 0.0402
2026-02-13 12:19:54 - INFO - Time taken for Epoch 5:16.77 - F1: 0.0417
2026-02-13 12:20:11 - INFO - Time taken for Epoch 6:16.77 - F1: 0.0748
2026-02-13 12:20:27 - INFO - Time taken for Epoch 7:16.73 - F1: 0.0986
2026-02-13 12:20:27 - INFO - Best F1:0.0986 - Best Epoch:7
2026-02-13 12:20:28 - INFO - Starting co-training
2026-02-13 12:20:53 - INFO - Time taken for Epoch 1: 24.48s - F1: 0.29588497
2026-02-13 12:21:18 - INFO - Time taken for Epoch 2: 25.10s - F1: 0.30329195
2026-02-13 12:21:43 - INFO - Time taken for Epoch 3: 25.06s - F1: 0.35506660
2026-02-13 12:22:08 - INFO - Time taken for Epoch 4: 25.16s - F1: 0.39039315
2026-02-13 12:22:33 - INFO - Time taken for Epoch 5: 25.12s - F1: 0.51485070
2026-02-13 12:22:58 - INFO - Time taken for Epoch 6: 25.04s - F1: 0.47916688
2026-02-13 12:23:23 - INFO - Time taken for Epoch 7: 24.45s - F1: 0.54911514
2026-02-13 12:23:25 - INFO - Fine-tuning models
2026-02-13 12:23:27 - INFO - Time taken for Epoch 1:2.43 - F1: 0.5370
2026-02-13 12:23:30 - INFO - Time taken for Epoch 2:2.99 - F1: 0.5272
2026-02-13 12:23:33 - INFO - Time taken for Epoch 3:2.41 - F1: 0.5353
2026-02-13 12:23:35 - INFO - Time taken for Epoch 4:2.42 - F1: 0.5374
2026-02-13 12:23:38 - INFO - Time taken for Epoch 5:3.02 - F1: 0.5416
2026-02-13 12:23:41 - INFO - Time taken for Epoch 6:3.04 - F1: 0.5353
2026-02-13 12:23:44 - INFO - Time taken for Epoch 7:2.43 - F1: 0.5436
2026-02-13 12:23:47 - INFO - Time taken for Epoch 8:3.05 - F1: 0.5490
2026-02-13 12:23:50 - INFO - Time taken for Epoch 9:3.11 - F1: 0.5568
2026-02-13 12:23:53 - INFO - Time taken for Epoch 10:3.03 - F1: 0.5732
2026-02-13 12:23:56 - INFO - Time taken for Epoch 11:3.05 - F1: 0.5802
2026-02-13 12:23:59 - INFO - Time taken for Epoch 12:3.05 - F1: 0.5782
2026-02-13 12:24:02 - INFO - Time taken for Epoch 13:2.43 - F1: 0.5891
2026-02-13 12:24:05 - INFO - Time taken for Epoch 14:3.33 - F1: 0.5915
2026-02-13 12:24:19 - INFO - Time taken for Epoch 15:14.24 - F1: 0.5842
2026-02-13 12:24:21 - INFO - Time taken for Epoch 16:2.41 - F1: 0.5769
2026-02-13 12:24:24 - INFO - Time taken for Epoch 17:2.41 - F1: 0.5820
2026-02-13 12:24:26 - INFO - Time taken for Epoch 18:2.41 - F1: 0.5836
2026-02-13 12:24:29 - INFO - Time taken for Epoch 19:2.41 - F1: 0.5828
2026-02-13 12:24:31 - INFO - Time taken for Epoch 20:2.41 - F1: 0.5814
2026-02-13 12:24:34 - INFO - Time taken for Epoch 21:2.42 - F1: 0.5875
2026-02-13 12:24:36 - INFO - Time taken for Epoch 22:2.43 - F1: 0.5875
2026-02-13 12:24:38 - INFO - Time taken for Epoch 23:2.42 - F1: 0.5861
2026-02-13 12:24:41 - INFO - Time taken for Epoch 24:2.42 - F1: 0.5898
2026-02-13 12:24:41 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 12:24:41 - INFO - Best F1:0.5915 - Best Epoch:13
2026-02-13 12:24:47 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5824, Test ECE: 0.0905
2026-02-13 12:24:47 - INFO - All results: {'f1_macro': 0.5824052367597672, 'ece': np.float64(0.09051971505717983)}
2026-02-13 12:24:47 - INFO - 
Total time taken: 378.95 seconds
2026-02-13 12:24:47 - INFO - Trial 3 finished with value: 0.5824052367597672 and parameters: {'learning_rate': 4.424333916890828e-05, 'weight_decay': 3.87698482059888e-05, 'batch_size': 8, 'co_train_epochs': 7, 'epoch_patience': 9}. Best is trial 3 with value: 0.5824052367597672.
2026-02-13 12:24:47 - INFO - Using devices: cuda, cuda
2026-02-13 12:24:47 - INFO - Devices: cuda, cuda
2026-02-13 12:24:47 - INFO - Starting log
2026-02-13 12:24:47 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 12:24:47 - INFO - Learning Rate: 0.00027248542920794305
Weight Decay: 0.00017374031981644568
Batch Size: 16
No. Epochs: 15
Epoch Patience: 5
 Accumulation Steps: 4
2026-02-13 12:24:48 - INFO - Generating initial weights
2026-02-13 12:25:04 - INFO - Time taken for Epoch 1:14.73 - F1: 0.0120
2026-02-13 12:25:19 - INFO - Time taken for Epoch 2:14.73 - F1: 0.0325
2026-02-13 12:25:33 - INFO - Time taken for Epoch 3:14.70 - F1: 0.0314
2026-02-13 12:25:48 - INFO - Time taken for Epoch 4:14.81 - F1: 0.0729
2026-02-13 12:26:03 - INFO - Time taken for Epoch 5:14.83 - F1: 0.1299
2026-02-13 12:26:18 - INFO - Time taken for Epoch 6:14.80 - F1: 0.3058
2026-02-13 12:26:32 - INFO - Time taken for Epoch 7:14.75 - F1: 0.3224
2026-02-13 12:26:47 - INFO - Time taken for Epoch 8:14.72 - F1: 0.3215
2026-02-13 12:27:02 - INFO - Time taken for Epoch 9:14.71 - F1: 0.3031
2026-02-13 12:27:17 - INFO - Time taken for Epoch 10:14.85 - F1: 0.3009
2026-02-13 12:27:32 - INFO - Time taken for Epoch 11:14.87 - F1: 0.3356
2026-02-13 12:27:46 - INFO - Time taken for Epoch 12:14.80 - F1: 0.3344
2026-02-13 12:28:01 - INFO - Time taken for Epoch 13:15.02 - F1: 0.3279
2026-02-13 12:28:16 - INFO - Time taken for Epoch 14:15.08 - F1: 0.3353
2026-02-13 12:28:31 - INFO - Time taken for Epoch 15:14.68 - F1: 0.3500
2026-02-13 12:28:31 - INFO - Best F1:0.3500 - Best Epoch:15
2026-02-13 12:28:32 - INFO - Starting co-training
2026-02-13 12:28:57 - INFO - Time taken for Epoch 1: 24.67s - F1: 0.03214286
2026-02-13 12:29:22 - INFO - Time taken for Epoch 2: 25.27s - F1: 0.03024831
2026-02-13 12:29:47 - INFO - Time taken for Epoch 3: 24.63s - F1: 0.03024831
2026-02-13 12:30:11 - INFO - Time taken for Epoch 4: 24.60s - F1: 0.04185068
2026-02-13 12:30:37 - INFO - Time taken for Epoch 5: 25.29s - F1: 0.04185068
2026-02-13 12:31:01 - INFO - Time taken for Epoch 6: 24.65s - F1: 0.04185068
2026-02-13 12:31:26 - INFO - Time taken for Epoch 7: 24.58s - F1: 0.04185068
2026-02-13 12:31:50 - INFO - Time taken for Epoch 8: 24.65s - F1: 0.04185068
2026-02-13 12:32:15 - INFO - Time taken for Epoch 9: 24.66s - F1: 0.04185068
2026-02-13 12:32:15 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-13 12:32:17 - INFO - Fine-tuning models
2026-02-13 12:32:19 - INFO - Time taken for Epoch 1:2.13 - F1: 0.0302
2026-02-13 12:32:22 - INFO - Time taken for Epoch 2:2.70 - F1: 0.0302
2026-02-13 12:32:24 - INFO - Time taken for Epoch 3:2.11 - F1: 0.0302
2026-02-13 12:32:26 - INFO - Time taken for Epoch 4:2.10 - F1: 0.0302
2026-02-13 12:32:28 - INFO - Time taken for Epoch 5:2.11 - F1: 0.0120
2026-02-13 12:32:30 - INFO - Time taken for Epoch 6:2.10 - F1: 0.0120
2026-02-13 12:32:32 - INFO - Time taken for Epoch 7:2.11 - F1: 0.0120
2026-02-13 12:32:34 - INFO - Time taken for Epoch 8:2.11 - F1: 0.0120
2026-02-13 12:32:36 - INFO - Time taken for Epoch 9:2.11 - F1: 0.0120
2026-02-13 12:32:38 - INFO - Time taken for Epoch 10:2.10 - F1: 0.0120
2026-02-13 12:32:40 - INFO - Time taken for Epoch 11:2.10 - F1: 0.0120
2026-02-13 12:32:40 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 12:32:40 - INFO - Best F1:0.0302 - Best Epoch:0
2026-02-13 12:32:46 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0303, Test ECE: 0.3416
2026-02-13 12:32:46 - INFO - All results: {'f1_macro': 0.030313588850174218, 'ece': np.float64(0.34162332810089246)}
2026-02-13 12:32:46 - INFO - 
Total time taken: 478.96 seconds
2026-02-13 12:32:46 - INFO - Trial 4 finished with value: 0.030313588850174218 and parameters: {'learning_rate': 0.00027248542920794305, 'weight_decay': 0.00017374031981644568, 'batch_size': 16, 'co_train_epochs': 15, 'epoch_patience': 5}. Best is trial 3 with value: 0.5824052367597672.
2026-02-13 12:32:46 - INFO - Using devices: cuda, cuda
2026-02-13 12:32:46 - INFO - Devices: cuda, cuda
2026-02-13 12:32:46 - INFO - Starting log
2026-02-13 12:32:46 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 12:32:46 - INFO - Learning Rate: 4.581682004490164e-05
Weight Decay: 0.0005665927353746237
Batch Size: 16
No. Epochs: 16
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-13 12:32:47 - INFO - Generating initial weights
2026-02-13 12:33:03 - INFO - Time taken for Epoch 1:14.73 - F1: 0.0120
2026-02-13 12:33:17 - INFO - Time taken for Epoch 2:14.69 - F1: 0.0120
2026-02-13 12:33:32 - INFO - Time taken for Epoch 3:14.68 - F1: 0.0120
2026-02-13 12:33:47 - INFO - Time taken for Epoch 4:14.69 - F1: 0.0120
2026-02-13 12:34:01 - INFO - Time taken for Epoch 5:14.66 - F1: 0.0199
2026-02-13 12:34:16 - INFO - Time taken for Epoch 6:14.67 - F1: 0.0409
2026-02-13 12:34:31 - INFO - Time taken for Epoch 7:14.67 - F1: 0.0555
2026-02-13 12:34:45 - INFO - Time taken for Epoch 8:14.69 - F1: 0.0590
2026-02-13 12:35:00 - INFO - Time taken for Epoch 9:14.65 - F1: 0.0609
2026-02-13 12:35:15 - INFO - Time taken for Epoch 10:14.66 - F1: 0.0609
2026-02-13 12:35:30 - INFO - Time taken for Epoch 11:14.98 - F1: 0.0612
2026-02-13 12:35:44 - INFO - Time taken for Epoch 12:14.65 - F1: 0.0760
2026-02-13 12:35:59 - INFO - Time taken for Epoch 13:14.97 - F1: 0.1081
2026-02-13 12:36:14 - INFO - Time taken for Epoch 14:15.08 - F1: 0.1182
2026-02-13 12:36:29 - INFO - Time taken for Epoch 15:14.99 - F1: 0.1491
2026-02-13 12:36:44 - INFO - Time taken for Epoch 16:15.05 - F1: 0.1841
2026-02-13 12:36:44 - INFO - Best F1:0.1841 - Best Epoch:16
2026-02-13 12:36:45 - INFO - Starting co-training
2026-02-13 12:37:11 - INFO - Time taken for Epoch 1: 25.38s - F1: 0.32052929
2026-02-13 12:37:37 - INFO - Time taken for Epoch 2: 26.36s - F1: 0.37008519
2026-02-13 12:38:03 - INFO - Time taken for Epoch 3: 26.46s - F1: 0.51304083
2026-02-13 12:38:30 - INFO - Time taken for Epoch 4: 26.18s - F1: 0.46689670
2026-02-13 12:38:55 - INFO - Time taken for Epoch 5: 25.04s - F1: 0.52471335
2026-02-13 12:39:20 - INFO - Time taken for Epoch 6: 25.52s - F1: 0.56502577
2026-02-13 12:39:46 - INFO - Time taken for Epoch 7: 25.37s - F1: 0.57650608
2026-02-13 12:40:11 - INFO - Time taken for Epoch 8: 25.39s - F1: 0.56188335
2026-02-13 12:40:36 - INFO - Time taken for Epoch 9: 24.66s - F1: 0.58292397
2026-02-13 12:41:01 - INFO - Time taken for Epoch 10: 25.51s - F1: 0.59271246
2026-02-13 12:41:26 - INFO - Time taken for Epoch 11: 25.36s - F1: 0.57498632
2026-02-13 12:41:51 - INFO - Time taken for Epoch 12: 24.71s - F1: 0.56160472
2026-02-13 12:42:16 - INFO - Time taken for Epoch 13: 24.65s - F1: 0.56767741
2026-02-13 12:42:40 - INFO - Time taken for Epoch 14: 24.66s - F1: 0.59076839
2026-02-13 12:42:40 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-13 12:42:42 - INFO - Fine-tuning models
2026-02-13 12:42:44 - INFO - Time taken for Epoch 1:2.13 - F1: 0.5972
2026-02-13 12:42:47 - INFO - Time taken for Epoch 2:2.70 - F1: 0.5858
2026-02-13 12:42:49 - INFO - Time taken for Epoch 3:2.11 - F1: 0.5669
2026-02-13 12:42:51 - INFO - Time taken for Epoch 4:2.11 - F1: 0.5768
2026-02-13 12:42:53 - INFO - Time taken for Epoch 5:2.11 - F1: 0.5654
2026-02-13 12:42:55 - INFO - Time taken for Epoch 6:2.11 - F1: 0.5609
2026-02-13 12:42:57 - INFO - Time taken for Epoch 7:2.11 - F1: 0.5638
2026-02-13 12:43:00 - INFO - Time taken for Epoch 8:2.11 - F1: 0.5780
2026-02-13 12:43:02 - INFO - Time taken for Epoch 9:2.11 - F1: 0.5786
2026-02-13 12:43:04 - INFO - Time taken for Epoch 10:2.11 - F1: 0.5800
2026-02-13 12:43:06 - INFO - Time taken for Epoch 11:2.12 - F1: 0.5787
2026-02-13 12:43:06 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 12:43:06 - INFO - Best F1:0.5972 - Best Epoch:0
2026-02-13 12:43:11 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5893, Test ECE: 0.0314
2026-02-13 12:43:11 - INFO - All results: {'f1_macro': 0.5892575586368285, 'ece': np.float64(0.03141159467873387)}
2026-02-13 12:43:11 - INFO - 
Total time taken: 625.41 seconds
2026-02-13 12:43:11 - INFO - Trial 5 finished with value: 0.5892575586368285 and parameters: {'learning_rate': 4.581682004490164e-05, 'weight_decay': 0.0005665927353746237, 'batch_size': 16, 'co_train_epochs': 16, 'epoch_patience': 4}. Best is trial 5 with value: 0.5892575586368285.
2026-02-13 12:43:11 - INFO - Using devices: cuda, cuda
2026-02-13 12:43:11 - INFO - Devices: cuda, cuda
2026-02-13 12:43:11 - INFO - Starting log
2026-02-13 12:43:11 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 12:43:11 - INFO - Learning Rate: 0.0006120017767069921
Weight Decay: 0.00037511736564421885
Batch Size: 8
No. Epochs: 19
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-13 12:43:12 - INFO - Generating initial weights
2026-02-13 12:43:30 - INFO - Time taken for Epoch 1:16.77 - F1: 0.0265
2026-02-13 12:43:47 - INFO - Time taken for Epoch 2:16.70 - F1: 0.0464
2026-02-13 12:44:03 - INFO - Time taken for Epoch 3:16.71 - F1: 0.0130
2026-02-13 12:44:21 - INFO - Time taken for Epoch 4:17.38 - F1: 0.0304
2026-02-13 12:44:38 - INFO - Time taken for Epoch 5:17.44 - F1: 0.0227
2026-02-13 12:44:56 - INFO - Time taken for Epoch 6:17.72 - F1: 0.0042
2026-02-13 12:45:13 - INFO - Time taken for Epoch 7:17.25 - F1: 0.0046
2026-02-13 12:45:30 - INFO - Time taken for Epoch 8:16.71 - F1: 0.0103
2026-02-13 12:45:47 - INFO - Time taken for Epoch 9:16.84 - F1: 0.0164
2026-02-13 12:46:04 - INFO - Time taken for Epoch 10:17.10 - F1: 0.0037
2026-02-13 12:46:21 - INFO - Time taken for Epoch 11:17.19 - F1: 0.0050
2026-02-13 12:46:38 - INFO - Time taken for Epoch 12:16.73 - F1: 0.0624
2026-02-13 12:46:54 - INFO - Time taken for Epoch 13:16.66 - F1: 0.0640
2026-02-13 12:47:11 - INFO - Time taken for Epoch 14:16.68 - F1: 0.0743
2026-02-13 12:47:28 - INFO - Time taken for Epoch 15:16.67 - F1: 0.0505
2026-02-13 12:47:44 - INFO - Time taken for Epoch 16:16.66 - F1: 0.0416
2026-02-13 12:48:01 - INFO - Time taken for Epoch 17:16.68 - F1: 0.0137
2026-02-13 12:48:18 - INFO - Time taken for Epoch 18:16.72 - F1: 0.0164
2026-02-13 12:48:35 - INFO - Time taken for Epoch 19:16.68 - F1: 0.0125
2026-02-13 12:48:35 - INFO - Best F1:0.0743 - Best Epoch:14
2026-02-13 12:48:35 - INFO - Starting co-training
2026-02-13 12:49:00 - INFO - Time taken for Epoch 1: 24.42s - F1: 0.04185068
2026-02-13 12:49:25 - INFO - Time taken for Epoch 2: 25.41s - F1: 0.04185068
2026-02-13 12:49:50 - INFO - Time taken for Epoch 3: 24.34s - F1: 0.04185068
2026-02-13 12:50:14 - INFO - Time taken for Epoch 4: 24.31s - F1: 0.04185068
2026-02-13 12:50:38 - INFO - Time taken for Epoch 5: 24.32s - F1: 0.04185068
2026-02-13 12:50:38 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-13 12:50:40 - INFO - Fine-tuning models
2026-02-13 12:50:42 - INFO - Time taken for Epoch 1:2.42 - F1: 0.0419
2026-02-13 12:50:45 - INFO - Time taken for Epoch 2:3.08 - F1: 0.0021
2026-02-13 12:50:48 - INFO - Time taken for Epoch 3:2.39 - F1: 0.0021
2026-02-13 12:50:50 - INFO - Time taken for Epoch 4:2.40 - F1: 0.0021
2026-02-13 12:50:52 - INFO - Time taken for Epoch 5:2.40 - F1: 0.0047
2026-02-13 12:50:55 - INFO - Time taken for Epoch 6:2.40 - F1: 0.0247
2026-02-13 12:50:57 - INFO - Time taken for Epoch 7:2.40 - F1: 0.0096
2026-02-13 12:51:00 - INFO - Time taken for Epoch 8:2.40 - F1: 0.0302
2026-02-13 12:51:02 - INFO - Time taken for Epoch 9:2.40 - F1: 0.0302
2026-02-13 12:51:04 - INFO - Time taken for Epoch 10:2.39 - F1: 0.0037
2026-02-13 12:51:07 - INFO - Time taken for Epoch 11:2.41 - F1: 0.0419
2026-02-13 12:51:07 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 12:51:07 - INFO - Best F1:0.0419 - Best Epoch:0
2026-02-13 12:51:12 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0417, Test ECE: 0.0371
2026-02-13 12:51:12 - INFO - All results: {'f1_macro': 0.04171180931744312, 'ece': np.float64(0.03706694750815204)}
2026-02-13 12:51:12 - INFO - 
Total time taken: 481.33 seconds
2026-02-13 12:51:12 - INFO - Trial 6 finished with value: 0.04171180931744312 and parameters: {'learning_rate': 0.0006120017767069921, 'weight_decay': 0.00037511736564421885, 'batch_size': 8, 'co_train_epochs': 19, 'epoch_patience': 4}. Best is trial 5 with value: 0.5892575586368285.
2026-02-13 12:51:12 - INFO - Using devices: cuda, cuda
2026-02-13 12:51:12 - INFO - Devices: cuda, cuda
2026-02-13 12:51:12 - INFO - Starting log
2026-02-13 12:51:12 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 12:51:13 - INFO - Learning Rate: 0.0007211796439595445
Weight Decay: 1.7782138777997422e-05
Batch Size: 24
No. Epochs: 13
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-13 12:51:13 - INFO - Generating initial weights
2026-02-13 12:51:28 - INFO - Time taken for Epoch 1:13.61 - F1: 0.0108
2026-02-13 12:51:42 - INFO - Time taken for Epoch 2:13.57 - F1: 0.0673
2026-02-13 12:51:55 - INFO - Time taken for Epoch 3:13.54 - F1: 0.1089
2026-02-13 12:52:09 - INFO - Time taken for Epoch 4:13.55 - F1: 0.1272
2026-02-13 12:52:23 - INFO - Time taken for Epoch 5:13.53 - F1: 0.2142
2026-02-13 12:52:36 - INFO - Time taken for Epoch 6:13.69 - F1: 0.2033
2026-02-13 12:52:50 - INFO - Time taken for Epoch 7:13.64 - F1: 0.1858
2026-02-13 12:53:03 - INFO - Time taken for Epoch 8:13.59 - F1: 0.2278
2026-02-13 12:53:17 - INFO - Time taken for Epoch 9:13.93 - F1: 0.2264
2026-02-13 12:53:31 - INFO - Time taken for Epoch 10:13.80 - F1: 0.2319
2026-02-13 12:53:45 - INFO - Time taken for Epoch 11:13.96 - F1: 0.2422
2026-02-13 12:53:59 - INFO - Time taken for Epoch 12:13.92 - F1: 0.2454
2026-02-13 12:54:13 - INFO - Time taken for Epoch 13:13.90 - F1: 0.2346
2026-02-13 12:54:13 - INFO - Best F1:0.2454 - Best Epoch:12
2026-02-13 12:54:14 - INFO - Starting co-training
2026-02-13 12:54:44 - INFO - Time taken for Epoch 1: 30.24s - F1: 0.03024831
2026-02-13 12:55:15 - INFO - Time taken for Epoch 2: 30.74s - F1: 0.03024831
2026-02-13 12:55:44 - INFO - Time taken for Epoch 3: 29.47s - F1: 0.03024831
2026-02-13 12:56:14 - INFO - Time taken for Epoch 4: 29.41s - F1: 0.03024831
2026-02-13 12:56:44 - INFO - Time taken for Epoch 5: 30.05s - F1: 0.03024831
2026-02-13 12:56:44 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-13 12:56:45 - INFO - Fine-tuning models
2026-02-13 12:56:47 - INFO - Time taken for Epoch 1:2.04 - F1: 0.0302
2026-02-13 12:56:50 - INFO - Time taken for Epoch 2:2.62 - F1: 0.0247
2026-02-13 12:56:52 - INFO - Time taken for Epoch 3:1.91 - F1: 0.0021
2026-02-13 12:56:54 - INFO - Time taken for Epoch 4:1.91 - F1: 0.0037
2026-02-13 12:56:56 - INFO - Time taken for Epoch 5:1.91 - F1: 0.0037
2026-02-13 12:56:58 - INFO - Time taken for Epoch 6:1.91 - F1: 0.0108
2026-02-13 12:57:00 - INFO - Time taken for Epoch 7:1.93 - F1: 0.0108
2026-02-13 12:57:02 - INFO - Time taken for Epoch 8:1.91 - F1: 0.0096
2026-02-13 12:57:03 - INFO - Time taken for Epoch 9:1.91 - F1: 0.0096
2026-02-13 12:57:05 - INFO - Time taken for Epoch 10:1.91 - F1: 0.0047
2026-02-13 12:57:07 - INFO - Time taken for Epoch 11:1.91 - F1: 0.0047
2026-02-13 12:57:07 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 12:57:07 - INFO - Best F1:0.0302 - Best Epoch:0
2026-02-13 12:57:12 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0303, Test ECE: 0.2141
2026-02-13 12:57:12 - INFO - All results: {'f1_macro': 0.030313588850174218, 'ece': np.float64(0.21414851440622903)}
2026-02-13 12:57:12 - INFO - 
Total time taken: 359.70 seconds
2026-02-13 12:57:12 - INFO - Trial 7 finished with value: 0.030313588850174218 and parameters: {'learning_rate': 0.0007211796439595445, 'weight_decay': 1.7782138777997422e-05, 'batch_size': 24, 'co_train_epochs': 13, 'epoch_patience': 4}. Best is trial 5 with value: 0.5892575586368285.
2026-02-13 12:57:12 - INFO - Using devices: cuda, cuda
2026-02-13 12:57:12 - INFO - Devices: cuda, cuda
2026-02-13 12:57:12 - INFO - Starting log
2026-02-13 12:57:12 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 12:57:12 - INFO - Learning Rate: 3.492349261937366e-05
Weight Decay: 0.001100500490298213
Batch Size: 24
No. Epochs: 11
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-13 12:57:13 - INFO - Generating initial weights
2026-02-13 12:57:28 - INFO - Time taken for Epoch 1:13.71 - F1: 0.1144
2026-02-13 12:57:43 - INFO - Time taken for Epoch 2:14.18 - F1: 0.1264
2026-02-13 12:57:57 - INFO - Time taken for Epoch 3:14.08 - F1: 0.1080
2026-02-13 12:58:10 - INFO - Time taken for Epoch 4:13.85 - F1: 0.1008
2026-02-13 12:58:24 - INFO - Time taken for Epoch 5:14.06 - F1: 0.0692
2026-02-13 12:58:38 - INFO - Time taken for Epoch 6:13.85 - F1: 0.0712
2026-02-13 12:58:52 - INFO - Time taken for Epoch 7:14.08 - F1: 0.0873
2026-02-13 12:59:07 - INFO - Time taken for Epoch 8:14.16 - F1: 0.1335
2026-02-13 12:59:20 - INFO - Time taken for Epoch 9:13.82 - F1: 0.1492
2026-02-13 12:59:34 - INFO - Time taken for Epoch 10:14.00 - F1: 0.1557
2026-02-13 12:59:48 - INFO - Time taken for Epoch 11:13.75 - F1: 0.1703
2026-02-13 12:59:48 - INFO - Best F1:0.1703 - Best Epoch:11
2026-02-13 12:59:49 - INFO - Starting co-training
2026-02-13 13:00:19 - INFO - Time taken for Epoch 1: 29.91s - F1: 0.45496591
2026-02-13 13:00:50 - INFO - Time taken for Epoch 2: 30.65s - F1: 0.47838607
2026-02-13 13:01:25 - INFO - Time taken for Epoch 3: 35.75s - F1: 0.50679017
2026-02-13 13:02:04 - INFO - Time taken for Epoch 4: 38.91s - F1: 0.57295493
2026-02-13 13:02:35 - INFO - Time taken for Epoch 5: 30.29s - F1: 0.56424652
2026-02-13 13:03:04 - INFO - Time taken for Epoch 6: 29.52s - F1: 0.55410730
2026-02-13 13:03:34 - INFO - Time taken for Epoch 7: 29.52s - F1: 0.57202539
2026-02-13 13:04:03 - INFO - Time taken for Epoch 8: 29.49s - F1: 0.60937673
2026-02-13 13:04:33 - INFO - Time taken for Epoch 9: 30.11s - F1: 0.59134413
2026-02-13 13:05:03 - INFO - Time taken for Epoch 10: 29.54s - F1: 0.59291665
2026-02-13 13:05:32 - INFO - Time taken for Epoch 11: 29.61s - F1: 0.60715468
2026-02-13 13:05:34 - INFO - Fine-tuning models
2026-02-13 13:05:36 - INFO - Time taken for Epoch 1:1.98 - F1: 0.6020
2026-02-13 13:05:39 - INFO - Time taken for Epoch 2:2.67 - F1: 0.5966
2026-02-13 13:05:41 - INFO - Time taken for Epoch 3:1.98 - F1: 0.5737
2026-02-13 13:05:43 - INFO - Time taken for Epoch 4:1.97 - F1: 0.5611
2026-02-13 13:05:45 - INFO - Time taken for Epoch 5:1.98 - F1: 0.5455
2026-02-13 13:05:47 - INFO - Time taken for Epoch 6:1.97 - F1: 0.5506
2026-02-13 13:05:49 - INFO - Time taken for Epoch 7:1.98 - F1: 0.5497
2026-02-13 13:05:51 - INFO - Time taken for Epoch 8:1.98 - F1: 0.5531
2026-02-13 13:05:53 - INFO - Time taken for Epoch 9:1.98 - F1: 0.5564
2026-02-13 13:05:55 - INFO - Time taken for Epoch 10:1.98 - F1: 0.5711
2026-02-13 13:05:57 - INFO - Time taken for Epoch 11:1.98 - F1: 0.5807
2026-02-13 13:05:57 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 13:05:57 - INFO - Best F1:0.6020 - Best Epoch:0
2026-02-13 13:06:02 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5928, Test ECE: 0.0433
2026-02-13 13:06:02 - INFO - All results: {'f1_macro': 0.5928475934554178, 'ece': np.float64(0.043266851880793544)}
2026-02-13 13:06:02 - INFO - 
Total time taken: 529.37 seconds
2026-02-13 13:06:02 - INFO - Trial 8 finished with value: 0.5928475934554178 and parameters: {'learning_rate': 3.492349261937366e-05, 'weight_decay': 0.001100500490298213, 'batch_size': 24, 'co_train_epochs': 11, 'epoch_patience': 6}. Best is trial 8 with value: 0.5928475934554178.
2026-02-13 13:06:02 - INFO - Using devices: cuda, cuda
2026-02-13 13:06:02 - INFO - Devices: cuda, cuda
2026-02-13 13:06:02 - INFO - Starting log
2026-02-13 13:06:02 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 13:06:02 - INFO - Learning Rate: 1.633055583211017e-05
Weight Decay: 0.00044586671357210473
Batch Size: 24
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-13 13:06:02 - INFO - Generating initial weights
2026-02-13 13:06:17 - INFO - Time taken for Epoch 1:13.66 - F1: 0.0919
2026-02-13 13:06:31 - INFO - Time taken for Epoch 2:13.63 - F1: 0.1042
2026-02-13 13:06:45 - INFO - Time taken for Epoch 3:13.65 - F1: 0.1195
2026-02-13 13:06:58 - INFO - Time taken for Epoch 4:13.65 - F1: 0.1300
2026-02-13 13:07:12 - INFO - Time taken for Epoch 5:13.72 - F1: 0.1291
2026-02-13 13:07:26 - INFO - Time taken for Epoch 6:13.94 - F1: 0.1210
2026-02-13 13:07:40 - INFO - Time taken for Epoch 7:13.97 - F1: 0.1131
2026-02-13 13:07:54 - INFO - Time taken for Epoch 8:14.09 - F1: 0.1080
2026-02-13 13:08:08 - INFO - Time taken for Epoch 9:13.70 - F1: 0.1054
2026-02-13 13:08:21 - INFO - Time taken for Epoch 10:13.66 - F1: 0.1053
2026-02-13 13:08:35 - INFO - Time taken for Epoch 11:13.62 - F1: 0.1046
2026-02-13 13:08:49 - INFO - Time taken for Epoch 12:13.62 - F1: 0.1069
2026-02-13 13:09:02 - INFO - Time taken for Epoch 13:13.63 - F1: 0.1063
2026-02-13 13:09:16 - INFO - Time taken for Epoch 14:13.66 - F1: 0.1080
2026-02-13 13:09:30 - INFO - Time taken for Epoch 15:13.66 - F1: 0.1228
2026-02-13 13:09:43 - INFO - Time taken for Epoch 16:13.65 - F1: 0.1281
2026-02-13 13:09:57 - INFO - Time taken for Epoch 17:13.61 - F1: 0.1289
2026-02-13 13:10:11 - INFO - Time taken for Epoch 18:13.68 - F1: 0.1313
2026-02-13 13:10:24 - INFO - Time taken for Epoch 19:13.67 - F1: 0.1346
2026-02-13 13:10:24 - INFO - Best F1:0.1346 - Best Epoch:19
2026-02-13 13:10:25 - INFO - Starting co-training
2026-02-13 13:10:55 - INFO - Time taken for Epoch 1: 29.51s - F1: 0.38679983
2026-02-13 13:11:25 - INFO - Time taken for Epoch 2: 30.08s - F1: 0.44204007
2026-02-13 13:11:55 - INFO - Time taken for Epoch 3: 30.17s - F1: 0.44400344
2026-02-13 13:12:25 - INFO - Time taken for Epoch 4: 30.12s - F1: 0.46960047
2026-02-13 13:12:55 - INFO - Time taken for Epoch 5: 30.15s - F1: 0.51622212
2026-02-13 13:13:29 - INFO - Time taken for Epoch 6: 33.96s - F1: 0.55952486
2026-02-13 13:14:06 - INFO - Time taken for Epoch 7: 37.26s - F1: 0.57257193
2026-02-13 13:14:38 - INFO - Time taken for Epoch 8: 31.17s - F1: 0.56743715
2026-02-13 13:15:07 - INFO - Time taken for Epoch 9: 29.52s - F1: 0.56686696
2026-02-13 13:15:37 - INFO - Time taken for Epoch 10: 29.51s - F1: 0.59718731
2026-02-13 13:16:07 - INFO - Time taken for Epoch 11: 30.25s - F1: 0.57979897
2026-02-13 13:16:36 - INFO - Time taken for Epoch 12: 29.54s - F1: 0.59705566
2026-02-13 13:17:06 - INFO - Time taken for Epoch 13: 29.47s - F1: 0.61650338
2026-02-13 13:17:36 - INFO - Time taken for Epoch 14: 30.27s - F1: 0.62129892
2026-02-13 13:18:06 - INFO - Time taken for Epoch 15: 30.20s - F1: 0.59155713
2026-02-13 13:18:36 - INFO - Time taken for Epoch 16: 29.51s - F1: 0.61878960
2026-02-13 13:19:05 - INFO - Time taken for Epoch 17: 29.49s - F1: 0.61159599
2026-02-13 13:19:35 - INFO - Time taken for Epoch 18: 29.49s - F1: 0.59491890
2026-02-13 13:20:04 - INFO - Time taken for Epoch 19: 29.48s - F1: 0.60682559
2026-02-13 13:20:06 - INFO - Fine-tuning models
2026-02-13 13:20:08 - INFO - Time taken for Epoch 1:1.98 - F1: 0.6260
2026-02-13 13:20:11 - INFO - Time taken for Epoch 2:2.60 - F1: 0.6084
2026-02-13 13:20:13 - INFO - Time taken for Epoch 3:1.97 - F1: 0.6128
2026-02-13 13:20:15 - INFO - Time taken for Epoch 4:1.97 - F1: 0.6028
2026-02-13 13:20:17 - INFO - Time taken for Epoch 5:1.97 - F1: 0.5949
2026-02-13 13:20:19 - INFO - Time taken for Epoch 6:1.97 - F1: 0.5842
2026-02-13 13:20:21 - INFO - Time taken for Epoch 7:1.97 - F1: 0.5766
2026-02-13 13:20:22 - INFO - Time taken for Epoch 8:1.97 - F1: 0.5766
2026-02-13 13:20:24 - INFO - Time taken for Epoch 9:1.98 - F1: 0.5808
2026-02-13 13:20:26 - INFO - Time taken for Epoch 10:1.97 - F1: 0.5788
2026-02-13 13:20:28 - INFO - Time taken for Epoch 11:1.97 - F1: 0.5748
2026-02-13 13:20:28 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 13:20:28 - INFO - Best F1:0.6260 - Best Epoch:0
2026-02-13 13:20:33 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6068, Test ECE: 0.0240
2026-02-13 13:20:33 - INFO - All results: {'f1_macro': 0.6068095651086741, 'ece': np.float64(0.02402529314231742)}
2026-02-13 13:20:33 - INFO - 
Total time taken: 871.76 seconds
2026-02-13 13:20:33 - INFO - Trial 9 finished with value: 0.6068095651086741 and parameters: {'learning_rate': 1.633055583211017e-05, 'weight_decay': 0.00044586671357210473, 'batch_size': 24, 'co_train_epochs': 19, 'epoch_patience': 10}. Best is trial 9 with value: 0.6068095651086741.
2026-02-13 13:20:33 - INFO - 
[BEST TRIAL RESULTS]
2026-02-13 13:20:33 - INFO - F1 Score: 0.6068
2026-02-13 13:20:33 - INFO - Params: {'learning_rate': 1.633055583211017e-05, 'weight_decay': 0.00044586671357210473, 'batch_size': 24, 'co_train_epochs': 19, 'epoch_patience': 10}
2026-02-13 13:20:33 - INFO -   learning_rate: 1.633055583211017e-05
2026-02-13 13:20:33 - INFO -   weight_decay: 0.00044586671357210473
2026-02-13 13:20:33 - INFO -   batch_size: 24
2026-02-13 13:20:33 - INFO -   co_train_epochs: 19
2026-02-13 13:20:33 - INFO -   epoch_patience: 10
2026-02-13 13:20:33 - INFO - 
Total time taken: 5192.48 seconds
