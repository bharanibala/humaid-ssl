2026-02-14 14:30:24 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-14 14:30:24 - INFO - A new study created in memory with name: study_humanitarian10_california_wildfires_2018
2026-02-14 14:30:24 - INFO - Using devices: cuda, cuda
2026-02-14 14:30:24 - INFO - Devices: cuda, cuda
2026-02-14 14:30:24 - INFO - Starting log
2026-02-14 14:30:24 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 14:30:24 - INFO - Learning Rate: 0.00039105294187820344
Weight Decay: 0.000585688050160275
Batch Size: 24
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-14 14:30:25 - INFO - Generating initial weights
2026-02-14 14:30:40 - INFO - Time taken for Epoch 1:13.84 - F1: 0.0091
2026-02-14 14:30:54 - INFO - Time taken for Epoch 2:13.68 - F1: 0.0282
2026-02-14 14:31:08 - INFO - Time taken for Epoch 3:13.68 - F1: 0.0403
2026-02-14 14:31:21 - INFO - Time taken for Epoch 4:13.72 - F1: 0.0321
2026-02-14 14:31:35 - INFO - Time taken for Epoch 5:13.72 - F1: 0.0321
2026-02-14 14:31:49 - INFO - Time taken for Epoch 6:13.71 - F1: 0.0302
2026-02-14 14:32:03 - INFO - Time taken for Epoch 7:13.72 - F1: 0.0048
2026-02-14 14:32:16 - INFO - Time taken for Epoch 8:13.73 - F1: 0.0021
2026-02-14 14:32:30 - INFO - Time taken for Epoch 9:13.74 - F1: 0.0021
2026-02-14 14:32:44 - INFO - Time taken for Epoch 10:13.74 - F1: 0.0096
2026-02-14 14:32:58 - INFO - Time taken for Epoch 11:13.75 - F1: 0.0096
2026-02-14 14:33:11 - INFO - Time taken for Epoch 12:13.70 - F1: 0.0037
2026-02-14 14:33:25 - INFO - Time taken for Epoch 13:13.78 - F1: 0.0037
2026-02-14 14:33:39 - INFO - Time taken for Epoch 14:13.73 - F1: 0.0037
2026-02-14 14:33:53 - INFO - Time taken for Epoch 15:13.77 - F1: 0.0037
2026-02-14 14:34:06 - INFO - Time taken for Epoch 16:13.77 - F1: 0.0037
2026-02-14 14:34:20 - INFO - Time taken for Epoch 17:13.77 - F1: 0.0096
2026-02-14 14:34:34 - INFO - Time taken for Epoch 18:13.76 - F1: 0.0096
2026-02-14 14:34:48 - INFO - Time taken for Epoch 19:13.77 - F1: 0.0096
2026-02-14 14:34:48 - INFO - Best F1:0.0403 - Best Epoch:3
2026-02-14 14:34:49 - INFO - Starting co-training
2026-02-14 14:35:17 - INFO - Time taken for Epoch 1: 28.40s - F1: 0.03024831
2026-02-14 14:35:46 - INFO - Time taken for Epoch 2: 28.94s - F1: 0.04185068
2026-02-14 14:36:15 - INFO - Time taken for Epoch 3: 28.99s - F1: 0.03024831
2026-02-14 14:36:43 - INFO - Time taken for Epoch 4: 28.39s - F1: 0.03024831
2026-02-14 14:37:12 - INFO - Time taken for Epoch 5: 28.47s - F1: 0.03024831
2026-02-14 14:37:40 - INFO - Time taken for Epoch 6: 28.35s - F1: 0.03024831
2026-02-14 14:38:09 - INFO - Time taken for Epoch 7: 28.38s - F1: 0.03024831
2026-02-14 14:38:37 - INFO - Time taken for Epoch 8: 28.38s - F1: 0.03024831
2026-02-14 14:39:05 - INFO - Time taken for Epoch 9: 28.42s - F1: 0.03024831
2026-02-14 14:39:34 - INFO - Time taken for Epoch 10: 28.36s - F1: 0.04185068
2026-02-14 14:40:02 - INFO - Time taken for Epoch 11: 28.42s - F1: 0.04185068
2026-02-14 14:40:31 - INFO - Time taken for Epoch 12: 28.42s - F1: 0.04185068
2026-02-14 14:40:31 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 14:40:32 - INFO - Fine-tuning models
2026-02-14 14:40:35 - INFO - Time taken for Epoch 1:2.68 - F1: 0.0419
2026-02-14 14:40:38 - INFO - Time taken for Epoch 2:3.23 - F1: 0.0037
2026-02-14 14:40:41 - INFO - Time taken for Epoch 3:2.67 - F1: 0.0021
2026-02-14 14:40:44 - INFO - Time taken for Epoch 4:2.66 - F1: 0.0021
2026-02-14 14:40:46 - INFO - Time taken for Epoch 5:2.67 - F1: 0.0419
2026-02-14 14:40:49 - INFO - Time taken for Epoch 6:2.67 - F1: 0.0302
2026-02-14 14:40:52 - INFO - Time taken for Epoch 7:2.66 - F1: 0.0321
2026-02-14 14:40:54 - INFO - Time taken for Epoch 8:2.67 - F1: 0.0321
2026-02-14 14:40:57 - INFO - Time taken for Epoch 9:2.67 - F1: 0.0321
2026-02-14 14:41:00 - INFO - Time taken for Epoch 10:2.67 - F1: 0.0321
2026-02-14 14:41:02 - INFO - Time taken for Epoch 11:2.67 - F1: 0.0321
2026-02-14 14:41:02 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 14:41:02 - INFO - Best F1:0.0419 - Best Epoch:0
2026-02-14 14:41:07 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0417, Test ECE: 0.3065
2026-02-14 14:41:07 - INFO - All results: {'f1_macro': 0.04171180931744312, 'ece': np.float64(0.30649213334781356)}
2026-02-14 14:41:07 - INFO - 
Total time taken: 643.44 seconds
2026-02-14 14:41:07 - INFO - Trial 0 finished with value: 0.04171180931744312 and parameters: {'learning_rate': 0.00039105294187820344, 'weight_decay': 0.000585688050160275, 'batch_size': 24, 'co_train_epochs': 19, 'epoch_patience': 10}. Best is trial 0 with value: 0.04171180931744312.
2026-02-14 14:41:07 - INFO - Using devices: cuda, cuda
2026-02-14 14:41:07 - INFO - Devices: cuda, cuda
2026-02-14 14:41:07 - INFO - Starting log
2026-02-14 14:41:07 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 14:41:08 - INFO - Learning Rate: 7.512146807326007e-05
Weight Decay: 0.008458550146438523
Batch Size: 16
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 4
2026-02-14 14:41:08 - INFO - Generating initial weights
2026-02-14 14:41:24 - INFO - Time taken for Epoch 1:15.00 - F1: 0.0653
2026-02-14 14:41:39 - INFO - Time taken for Epoch 2:14.94 - F1: 0.0978
2026-02-14 14:41:54 - INFO - Time taken for Epoch 3:14.94 - F1: 0.1189
2026-02-14 14:42:09 - INFO - Time taken for Epoch 4:14.95 - F1: 0.1622
2026-02-14 14:42:24 - INFO - Time taken for Epoch 5:14.94 - F1: 0.3175
2026-02-14 14:42:39 - INFO - Time taken for Epoch 6:14.95 - F1: 0.4214
2026-02-14 14:42:54 - INFO - Time taken for Epoch 7:14.95 - F1: 0.4754
2026-02-14 14:43:09 - INFO - Time taken for Epoch 8:14.95 - F1: 0.4746
2026-02-14 14:43:24 - INFO - Time taken for Epoch 9:14.96 - F1: 0.4878
2026-02-14 14:43:39 - INFO - Time taken for Epoch 10:14.94 - F1: 0.5093
2026-02-14 14:43:54 - INFO - Time taken for Epoch 11:14.94 - F1: 0.5104
2026-02-14 14:44:09 - INFO - Time taken for Epoch 12:14.94 - F1: 0.4976
2026-02-14 14:44:24 - INFO - Time taken for Epoch 13:14.95 - F1: 0.5061
2026-02-14 14:44:39 - INFO - Time taken for Epoch 14:14.94 - F1: 0.5181
2026-02-14 14:44:54 - INFO - Time taken for Epoch 15:14.94 - F1: 0.5067
2026-02-14 14:45:09 - INFO - Time taken for Epoch 16:14.96 - F1: 0.5117
2026-02-14 14:45:23 - INFO - Time taken for Epoch 17:14.94 - F1: 0.5238
2026-02-14 14:45:38 - INFO - Time taken for Epoch 18:14.95 - F1: 0.5287
2026-02-14 14:45:53 - INFO - Time taken for Epoch 19:14.94 - F1: 0.5145
2026-02-14 14:45:53 - INFO - Best F1:0.5287 - Best Epoch:18
2026-02-14 14:45:54 - INFO - Starting co-training
2026-02-14 14:46:18 - INFO - Time taken for Epoch 1: 23.71s - F1: 0.40726809
2026-02-14 14:46:42 - INFO - Time taken for Epoch 2: 24.43s - F1: 0.48995153
2026-02-14 14:47:07 - INFO - Time taken for Epoch 3: 24.39s - F1: 0.45028335
2026-02-14 14:47:30 - INFO - Time taken for Epoch 4: 23.73s - F1: 0.49458515
2026-02-14 14:47:55 - INFO - Time taken for Epoch 5: 24.37s - F1: 0.55138222
2026-02-14 14:48:19 - INFO - Time taken for Epoch 6: 24.27s - F1: 0.54767246
2026-02-14 14:48:43 - INFO - Time taken for Epoch 7: 23.72s - F1: 0.56510026
2026-02-14 14:49:07 - INFO - Time taken for Epoch 8: 24.29s - F1: 0.59238526
2026-02-14 14:49:31 - INFO - Time taken for Epoch 9: 24.38s - F1: 0.57247233
2026-02-14 14:49:55 - INFO - Time taken for Epoch 10: 23.70s - F1: 0.56674592
2026-02-14 14:50:19 - INFO - Time taken for Epoch 11: 23.71s - F1: 0.55696496
2026-02-14 14:50:43 - INFO - Time taken for Epoch 12: 23.83s - F1: 0.56768023
2026-02-14 14:51:06 - INFO - Time taken for Epoch 13: 23.72s - F1: 0.58374550
2026-02-14 14:51:30 - INFO - Time taken for Epoch 14: 23.70s - F1: 0.57178152
2026-02-14 14:51:54 - INFO - Time taken for Epoch 15: 23.72s - F1: 0.56448728
2026-02-14 14:52:18 - INFO - Time taken for Epoch 16: 23.71s - F1: 0.55204241
2026-02-14 14:52:41 - INFO - Time taken for Epoch 17: 23.70s - F1: 0.55701906
2026-02-14 14:53:05 - INFO - Time taken for Epoch 18: 23.74s - F1: 0.56945747
2026-02-14 14:53:05 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 14:53:06 - INFO - Fine-tuning models
2026-02-14 14:53:09 - INFO - Time taken for Epoch 1:2.93 - F1: 0.5572
2026-02-14 14:53:13 - INFO - Time taken for Epoch 2:3.53 - F1: 0.5156
2026-02-14 14:53:16 - INFO - Time taken for Epoch 3:2.91 - F1: 0.5090
2026-02-14 14:53:19 - INFO - Time taken for Epoch 4:2.91 - F1: 0.5383
2026-02-14 14:53:22 - INFO - Time taken for Epoch 5:2.91 - F1: 0.5594
2026-02-14 14:53:25 - INFO - Time taken for Epoch 6:3.55 - F1: 0.5579
2026-02-14 14:53:28 - INFO - Time taken for Epoch 7:2.91 - F1: 0.5676
2026-02-14 14:53:32 - INFO - Time taken for Epoch 8:3.54 - F1: 0.5691
2026-02-14 14:53:35 - INFO - Time taken for Epoch 9:3.58 - F1: 0.5748
2026-02-14 14:53:39 - INFO - Time taken for Epoch 10:3.54 - F1: 0.5885
2026-02-14 14:53:42 - INFO - Time taken for Epoch 11:3.55 - F1: 0.6000
2026-02-14 14:53:53 - INFO - Time taken for Epoch 12:11.00 - F1: 0.5916
2026-02-14 14:53:56 - INFO - Time taken for Epoch 13:2.90 - F1: 0.5956
2026-02-14 14:53:59 - INFO - Time taken for Epoch 14:2.91 - F1: 0.6086
2026-02-14 14:54:03 - INFO - Time taken for Epoch 15:3.55 - F1: 0.6105
2026-02-14 14:54:06 - INFO - Time taken for Epoch 16:3.59 - F1: 0.6054
2026-02-14 14:54:09 - INFO - Time taken for Epoch 17:2.91 - F1: 0.6058
2026-02-14 14:54:12 - INFO - Time taken for Epoch 18:2.90 - F1: 0.6102
2026-02-14 14:54:15 - INFO - Time taken for Epoch 19:2.91 - F1: 0.6142
2026-02-14 14:54:19 - INFO - Time taken for Epoch 20:3.57 - F1: 0.6135
2026-02-14 14:54:21 - INFO - Time taken for Epoch 21:2.92 - F1: 0.6148
2026-02-14 14:54:34 - INFO - Time taken for Epoch 22:12.71 - F1: 0.6128
2026-02-14 14:54:37 - INFO - Time taken for Epoch 23:2.90 - F1: 0.6134
2026-02-14 14:54:40 - INFO - Time taken for Epoch 24:2.91 - F1: 0.6119
2026-02-14 14:54:43 - INFO - Time taken for Epoch 25:2.91 - F1: 0.6112
2026-02-14 14:54:46 - INFO - Time taken for Epoch 26:2.91 - F1: 0.6094
2026-02-14 14:54:49 - INFO - Time taken for Epoch 27:2.90 - F1: 0.6094
2026-02-14 14:54:52 - INFO - Time taken for Epoch 28:2.91 - F1: 0.6086
2026-02-14 14:54:55 - INFO - Time taken for Epoch 29:2.92 - F1: 0.6065
2026-02-14 14:54:57 - INFO - Time taken for Epoch 30:2.92 - F1: 0.6037
2026-02-14 14:55:00 - INFO - Time taken for Epoch 31:2.91 - F1: 0.6037
2026-02-14 14:55:00 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 14:55:00 - INFO - Best F1:0.6148 - Best Epoch:20
2026-02-14 14:55:05 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6252, Test ECE: 0.0880
2026-02-14 14:55:05 - INFO - All results: {'f1_macro': 0.6252164814887116, 'ece': np.float64(0.08800980646589045)}
2026-02-14 14:55:05 - INFO - 
Total time taken: 838.18 seconds
2026-02-14 14:55:05 - INFO - Trial 1 finished with value: 0.6252164814887116 and parameters: {'learning_rate': 7.512146807326007e-05, 'weight_decay': 0.008458550146438523, 'batch_size': 16, 'co_train_epochs': 19, 'epoch_patience': 10}. Best is trial 1 with value: 0.6252164814887116.
2026-02-14 14:55:05 - INFO - Using devices: cuda, cuda
2026-02-14 14:55:05 - INFO - Devices: cuda, cuda
2026-02-14 14:55:05 - INFO - Starting log
2026-02-14 14:55:05 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 14:55:06 - INFO - Learning Rate: 3.071600944097072e-05
Weight Decay: 0.006510015998609281
Batch Size: 16
No. Epochs: 18
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-14 14:55:06 - INFO - Generating initial weights
2026-02-14 14:55:23 - INFO - Time taken for Epoch 1:14.99 - F1: 0.0578
2026-02-14 14:55:38 - INFO - Time taken for Epoch 2:14.96 - F1: 0.0683
2026-02-14 14:55:52 - INFO - Time taken for Epoch 3:14.95 - F1: 0.0837
2026-02-14 14:56:07 - INFO - Time taken for Epoch 4:14.96 - F1: 0.1291
2026-02-14 14:56:22 - INFO - Time taken for Epoch 5:14.97 - F1: 0.1634
2026-02-14 14:56:37 - INFO - Time taken for Epoch 6:14.96 - F1: 0.1807
2026-02-14 14:56:52 - INFO - Time taken for Epoch 7:15.02 - F1: 0.2246
2026-02-14 14:57:07 - INFO - Time taken for Epoch 8:14.96 - F1: 0.3087
2026-02-14 14:57:22 - INFO - Time taken for Epoch 9:14.98 - F1: 0.3764
2026-02-14 14:57:37 - INFO - Time taken for Epoch 10:14.94 - F1: 0.4170
2026-02-14 14:57:52 - INFO - Time taken for Epoch 11:14.96 - F1: 0.4587
2026-02-14 14:58:07 - INFO - Time taken for Epoch 12:14.95 - F1: 0.4652
2026-02-14 14:58:22 - INFO - Time taken for Epoch 13:14.94 - F1: 0.4850
2026-02-14 14:58:37 - INFO - Time taken for Epoch 14:14.96 - F1: 0.4926
2026-02-14 14:58:52 - INFO - Time taken for Epoch 15:14.95 - F1: 0.4942
2026-02-14 14:59:07 - INFO - Time taken for Epoch 16:14.97 - F1: 0.4960
2026-02-14 14:59:22 - INFO - Time taken for Epoch 17:15.00 - F1: 0.4968
2026-02-14 14:59:37 - INFO - Time taken for Epoch 18:14.96 - F1: 0.5010
2026-02-14 14:59:37 - INFO - Best F1:0.5010 - Best Epoch:18
2026-02-14 14:59:38 - INFO - Starting co-training
2026-02-14 15:00:02 - INFO - Time taken for Epoch 1: 23.75s - F1: 0.31956945
2026-02-14 15:00:26 - INFO - Time taken for Epoch 2: 24.64s - F1: 0.36635976
2026-02-14 15:00:51 - INFO - Time taken for Epoch 3: 24.39s - F1: 0.42564230
2026-02-14 15:01:15 - INFO - Time taken for Epoch 4: 24.25s - F1: 0.49745581
2026-02-14 15:01:39 - INFO - Time taken for Epoch 5: 24.28s - F1: 0.51467693
2026-02-14 15:02:03 - INFO - Time taken for Epoch 6: 24.23s - F1: 0.57128670
2026-02-14 15:02:28 - INFO - Time taken for Epoch 7: 24.33s - F1: 0.57227859
2026-02-14 15:02:52 - INFO - Time taken for Epoch 8: 24.31s - F1: 0.57766463
2026-02-14 15:03:16 - INFO - Time taken for Epoch 9: 24.37s - F1: 0.55255632
2026-02-14 15:03:40 - INFO - Time taken for Epoch 10: 23.67s - F1: 0.58116901
2026-02-14 15:04:04 - INFO - Time taken for Epoch 11: 24.24s - F1: 0.59399177
2026-02-14 15:04:29 - INFO - Time taken for Epoch 12: 24.42s - F1: 0.58043511
2026-02-14 15:04:52 - INFO - Time taken for Epoch 13: 23.71s - F1: 0.58475509
2026-02-14 15:05:16 - INFO - Time taken for Epoch 14: 23.76s - F1: 0.56381402
2026-02-14 15:05:40 - INFO - Time taken for Epoch 15: 23.71s - F1: 0.59207974
2026-02-14 15:05:40 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-14 15:05:41 - INFO - Fine-tuning models
2026-02-14 15:05:44 - INFO - Time taken for Epoch 1:2.93 - F1: 0.5675
2026-02-14 15:05:48 - INFO - Time taken for Epoch 2:3.48 - F1: 0.5735
2026-02-14 15:05:51 - INFO - Time taken for Epoch 3:3.51 - F1: 0.5598
2026-02-14 15:05:54 - INFO - Time taken for Epoch 4:2.91 - F1: 0.5503
2026-02-14 15:05:57 - INFO - Time taken for Epoch 5:2.92 - F1: 0.5368
2026-02-14 15:06:00 - INFO - Time taken for Epoch 6:2.93 - F1: 0.5346
2026-02-14 15:06:03 - INFO - Time taken for Epoch 7:2.91 - F1: 0.5460
2026-02-14 15:06:06 - INFO - Time taken for Epoch 8:2.92 - F1: 0.5543
2026-02-14 15:06:09 - INFO - Time taken for Epoch 9:2.91 - F1: 0.5669
2026-02-14 15:06:12 - INFO - Time taken for Epoch 10:2.91 - F1: 0.5689
2026-02-14 15:06:14 - INFO - Time taken for Epoch 11:2.92 - F1: 0.5834
2026-02-14 15:06:18 - INFO - Time taken for Epoch 12:3.52 - F1: 0.5853
2026-02-14 15:06:31 - INFO - Time taken for Epoch 13:12.85 - F1: 0.5824
2026-02-14 15:06:34 - INFO - Time taken for Epoch 14:2.91 - F1: 0.5850
2026-02-14 15:06:37 - INFO - Time taken for Epoch 15:2.91 - F1: 0.5916
2026-02-14 15:06:40 - INFO - Time taken for Epoch 16:3.49 - F1: 0.5880
2026-02-14 15:06:43 - INFO - Time taken for Epoch 17:2.91 - F1: 0.5893
2026-02-14 15:06:46 - INFO - Time taken for Epoch 18:2.91 - F1: 0.5929
2026-02-14 15:06:49 - INFO - Time taken for Epoch 19:3.51 - F1: 0.5980
2026-02-14 15:06:53 - INFO - Time taken for Epoch 20:3.50 - F1: 0.5953
2026-02-14 15:06:56 - INFO - Time taken for Epoch 21:2.92 - F1: 0.5922
2026-02-14 15:06:59 - INFO - Time taken for Epoch 22:2.91 - F1: 0.5965
2026-02-14 15:07:02 - INFO - Time taken for Epoch 23:2.93 - F1: 0.6115
2026-02-14 15:07:07 - INFO - Time taken for Epoch 24:5.30 - F1: 0.6086
2026-02-14 15:07:10 - INFO - Time taken for Epoch 25:2.92 - F1: 0.6157
2026-02-14 15:07:18 - INFO - Time taken for Epoch 26:7.68 - F1: 0.6185
2026-02-14 15:07:21 - INFO - Time taken for Epoch 27:3.50 - F1: 0.6216
2026-02-14 15:07:25 - INFO - Time taken for Epoch 28:3.49 - F1: 0.6216
2026-02-14 15:07:28 - INFO - Time taken for Epoch 29:2.91 - F1: 0.6197
2026-02-14 15:07:30 - INFO - Time taken for Epoch 30:2.91 - F1: 0.6173
2026-02-14 15:07:33 - INFO - Time taken for Epoch 31:2.91 - F1: 0.6160
2026-02-14 15:07:36 - INFO - Time taken for Epoch 32:2.91 - F1: 0.6144
2026-02-14 15:07:39 - INFO - Time taken for Epoch 33:2.92 - F1: 0.6144
2026-02-14 15:07:42 - INFO - Time taken for Epoch 34:2.93 - F1: 0.6124
2026-02-14 15:07:45 - INFO - Time taken for Epoch 35:2.92 - F1: 0.6123
2026-02-14 15:07:48 - INFO - Time taken for Epoch 36:2.93 - F1: 0.6146
2026-02-14 15:07:51 - INFO - Time taken for Epoch 37:2.92 - F1: 0.6141
2026-02-14 15:07:51 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 15:07:51 - INFO - Best F1:0.6216 - Best Epoch:26
2026-02-14 15:07:56 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6445, Test ECE: 0.0440
2026-02-14 15:07:56 - INFO - All results: {'f1_macro': 0.6445369536291542, 'ece': np.float64(0.044046600455702536)}
2026-02-14 15:07:56 - INFO - 
Total time taken: 770.50 seconds
2026-02-14 15:07:56 - INFO - Trial 2 finished with value: 0.6445369536291542 and parameters: {'learning_rate': 3.071600944097072e-05, 'weight_decay': 0.006510015998609281, 'batch_size': 16, 'co_train_epochs': 18, 'epoch_patience': 4}. Best is trial 2 with value: 0.6445369536291542.
2026-02-14 15:07:56 - INFO - Using devices: cuda, cuda
2026-02-14 15:07:56 - INFO - Devices: cuda, cuda
2026-02-14 15:07:56 - INFO - Starting log
2026-02-14 15:07:56 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 15:07:56 - INFO - Learning Rate: 0.00021250156413983622
Weight Decay: 0.0009847267559005952
Batch Size: 8
No. Epochs: 17
Epoch Patience: 6
 Accumulation Steps: 8
2026-02-14 15:07:57 - INFO - Generating initial weights
2026-02-14 15:08:15 - INFO - Time taken for Epoch 1:17.14 - F1: 0.0318
2026-02-14 15:08:32 - INFO - Time taken for Epoch 2:17.07 - F1: 0.0596
2026-02-14 15:08:49 - INFO - Time taken for Epoch 3:17.08 - F1: 0.0829
2026-02-14 15:09:06 - INFO - Time taken for Epoch 4:17.10 - F1: 0.1187
2026-02-14 15:09:24 - INFO - Time taken for Epoch 5:17.09 - F1: 0.2181
2026-02-14 15:09:41 - INFO - Time taken for Epoch 6:17.09 - F1: 0.2828
2026-02-14 15:09:58 - INFO - Time taken for Epoch 7:17.09 - F1: 0.3915
2026-02-14 15:10:15 - INFO - Time taken for Epoch 8:17.09 - F1: 0.4520
2026-02-14 15:10:32 - INFO - Time taken for Epoch 9:17.09 - F1: 0.4643
2026-02-14 15:10:49 - INFO - Time taken for Epoch 10:17.07 - F1: 0.5216
2026-02-14 15:11:06 - INFO - Time taken for Epoch 11:17.10 - F1: 0.4935
2026-02-14 15:11:23 - INFO - Time taken for Epoch 12:17.08 - F1: 0.4893
2026-02-14 15:11:40 - INFO - Time taken for Epoch 13:17.10 - F1: 0.5205
2026-02-14 15:11:57 - INFO - Time taken for Epoch 14:17.10 - F1: 0.5197
2026-02-14 15:12:14 - INFO - Time taken for Epoch 15:17.09 - F1: 0.5289
2026-02-14 15:12:32 - INFO - Time taken for Epoch 16:17.11 - F1: 0.5356
2026-02-14 15:12:49 - INFO - Time taken for Epoch 17:17.08 - F1: 0.5288
2026-02-14 15:12:49 - INFO - Best F1:0.5356 - Best Epoch:16
2026-02-14 15:12:49 - INFO - Starting co-training
2026-02-14 15:13:13 - INFO - Time taken for Epoch 1: 23.45s - F1: 0.03214286
2026-02-14 15:13:37 - INFO - Time taken for Epoch 2: 23.90s - F1: 0.03024831
2026-02-14 15:14:00 - INFO - Time taken for Epoch 3: 23.47s - F1: 0.03024831
2026-02-14 15:14:24 - INFO - Time taken for Epoch 4: 23.44s - F1: 0.04185068
2026-02-14 15:14:48 - INFO - Time taken for Epoch 5: 23.96s - F1: 0.04185068
2026-02-14 15:15:11 - INFO - Time taken for Epoch 6: 23.49s - F1: 0.04185068
2026-02-14 15:15:35 - INFO - Time taken for Epoch 7: 23.42s - F1: 0.03024831
2026-02-14 15:15:58 - INFO - Time taken for Epoch 8: 23.41s - F1: 0.03024831
2026-02-14 15:16:21 - INFO - Time taken for Epoch 9: 23.53s - F1: 0.03024831
2026-02-14 15:16:45 - INFO - Time taken for Epoch 10: 23.46s - F1: 0.03024831
2026-02-14 15:16:45 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-14 15:16:55 - WARNING - '(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a8af26be-57dc-4bc2-8851-a133bcc5dc2e)')' thrown while requesting HEAD https://huggingface.co/vinai/bertweet-base/resolve/main/config.json
2026-02-14 15:16:55 - WARNING - Retrying in 1s [Retry 1/5].
2026-02-14 15:16:57 - INFO - Fine-tuning models
2026-02-14 15:17:01 - INFO - Time taken for Epoch 1:3.37 - F1: 0.0419
2026-02-14 15:17:05 - INFO - Time taken for Epoch 2:3.94 - F1: 0.0302
2026-02-14 15:17:08 - INFO - Time taken for Epoch 3:3.36 - F1: 0.0021
2026-02-14 15:17:11 - INFO - Time taken for Epoch 4:3.35 - F1: 0.0021
2026-02-14 15:17:15 - INFO - Time taken for Epoch 5:3.34 - F1: 0.0021
2026-02-14 15:17:18 - INFO - Time taken for Epoch 6:3.35 - F1: 0.0037
2026-02-14 15:17:21 - INFO - Time taken for Epoch 7:3.36 - F1: 0.0037
2026-02-14 15:17:25 - INFO - Time taken for Epoch 8:3.35 - F1: 0.0037
2026-02-14 15:17:28 - INFO - Time taken for Epoch 9:3.35 - F1: 0.0096
2026-02-14 15:17:31 - INFO - Time taken for Epoch 10:3.35 - F1: 0.0096
2026-02-14 15:17:35 - INFO - Time taken for Epoch 11:3.35 - F1: 0.0096
2026-02-14 15:17:35 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 15:17:35 - INFO - Best F1:0.0419 - Best Epoch:0
2026-02-14 15:17:40 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0417, Test ECE: 0.1584
2026-02-14 15:17:40 - INFO - All results: {'f1_macro': 0.04171180931744312, 'ece': np.float64(0.15840476469827136)}
2026-02-14 15:17:40 - INFO - 
Total time taken: 584.56 seconds
2026-02-14 15:17:40 - INFO - Trial 3 finished with value: 0.04171180931744312 and parameters: {'learning_rate': 0.00021250156413983622, 'weight_decay': 0.0009847267559005952, 'batch_size': 8, 'co_train_epochs': 17, 'epoch_patience': 6}. Best is trial 2 with value: 0.6445369536291542.
2026-02-14 15:17:40 - INFO - Using devices: cuda, cuda
2026-02-14 15:17:40 - INFO - Devices: cuda, cuda
2026-02-14 15:17:40 - INFO - Starting log
2026-02-14 15:17:40 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 15:17:41 - INFO - Learning Rate: 0.00010313889866297136
Weight Decay: 2.0200747251786086e-05
Batch Size: 8
No. Epochs: 10
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-14 15:17:51 - INFO - Generating initial weights
2026-02-14 15:18:10 - INFO - Time taken for Epoch 1:17.09 - F1: 0.0587
2026-02-14 15:18:27 - INFO - Time taken for Epoch 2:17.09 - F1: 0.0969
2026-02-14 15:18:44 - INFO - Time taken for Epoch 3:17.10 - F1: 0.1194
2026-02-14 15:19:01 - INFO - Time taken for Epoch 4:17.10 - F1: 0.2014
2026-02-14 15:19:18 - INFO - Time taken for Epoch 5:17.08 - F1: 0.3169
2026-02-14 15:19:35 - INFO - Time taken for Epoch 6:17.09 - F1: 0.3837
2026-02-14 15:19:52 - INFO - Time taken for Epoch 7:17.07 - F1: 0.4264
2026-02-14 15:20:09 - INFO - Time taken for Epoch 8:17.08 - F1: 0.4766
2026-02-14 15:20:26 - INFO - Time taken for Epoch 9:17.08 - F1: 0.4960
2026-02-14 15:20:44 - INFO - Time taken for Epoch 10:17.09 - F1: 0.5088
2026-02-14 15:20:44 - INFO - Best F1:0.5088 - Best Epoch:10
2026-02-14 15:20:44 - INFO - Starting co-training
2026-02-14 15:21:08 - INFO - Time taken for Epoch 1: 23.46s - F1: 0.29492021
2026-02-14 15:21:32 - INFO - Time taken for Epoch 2: 24.02s - F1: 0.30723674
2026-02-14 15:21:56 - INFO - Time taken for Epoch 3: 24.09s - F1: 0.28188841
2026-02-14 15:22:19 - INFO - Time taken for Epoch 4: 23.45s - F1: 0.26430464
2026-02-14 15:22:43 - INFO - Time taken for Epoch 5: 23.43s - F1: 0.30824783
2026-02-14 15:23:07 - INFO - Time taken for Epoch 6: 24.11s - F1: 0.40322462
2026-02-14 15:23:31 - INFO - Time taken for Epoch 7: 24.09s - F1: 0.39430887
2026-02-14 15:23:54 - INFO - Time taken for Epoch 8: 23.47s - F1: 0.42305724
2026-02-14 15:24:19 - INFO - Time taken for Epoch 9: 24.37s - F1: 0.43013731
2026-02-14 15:24:43 - INFO - Time taken for Epoch 10: 24.06s - F1: 0.50731510
2026-02-14 15:24:45 - INFO - Fine-tuning models
2026-02-14 15:24:48 - INFO - Time taken for Epoch 1:3.39 - F1: 0.5009
2026-02-14 15:24:52 - INFO - Time taken for Epoch 2:3.94 - F1: 0.4860
2026-02-14 15:24:55 - INFO - Time taken for Epoch 3:3.36 - F1: 0.5210
2026-02-14 15:24:59 - INFO - Time taken for Epoch 4:3.98 - F1: 0.5370
2026-02-14 15:25:03 - INFO - Time taken for Epoch 5:3.98 - F1: 0.5289
2026-02-14 15:25:07 - INFO - Time taken for Epoch 6:3.35 - F1: 0.5334
2026-02-14 15:25:10 - INFO - Time taken for Epoch 7:3.35 - F1: 0.5306
2026-02-14 15:25:13 - INFO - Time taken for Epoch 8:3.36 - F1: 0.5411
2026-02-14 15:25:18 - INFO - Time taken for Epoch 9:4.07 - F1: 0.5542
2026-02-14 15:25:22 - INFO - Time taken for Epoch 10:4.17 - F1: 0.5426
2026-02-14 15:25:25 - INFO - Time taken for Epoch 11:3.36 - F1: 0.5256
2026-02-14 15:25:37 - INFO - Time taken for Epoch 12:11.45 - F1: 0.5198
2026-02-14 15:25:40 - INFO - Time taken for Epoch 13:3.36 - F1: 0.5212
2026-02-14 15:25:43 - INFO - Time taken for Epoch 14:3.35 - F1: 0.5217
2026-02-14 15:25:47 - INFO - Time taken for Epoch 15:3.37 - F1: 0.5333
2026-02-14 15:25:50 - INFO - Time taken for Epoch 16:3.35 - F1: 0.5269
2026-02-14 15:25:53 - INFO - Time taken for Epoch 17:3.36 - F1: 0.5325
2026-02-14 15:25:57 - INFO - Time taken for Epoch 18:3.36 - F1: 0.5402
2026-02-14 15:26:00 - INFO - Time taken for Epoch 19:3.36 - F1: 0.5546
2026-02-14 15:26:04 - INFO - Time taken for Epoch 20:4.03 - F1: 0.5513
2026-02-14 15:26:07 - INFO - Time taken for Epoch 21:3.36 - F1: 0.5430
2026-02-14 15:26:11 - INFO - Time taken for Epoch 22:3.36 - F1: 0.5522
2026-02-14 15:26:14 - INFO - Time taken for Epoch 23:3.37 - F1: 0.5588
2026-02-14 15:26:18 - INFO - Time taken for Epoch 24:4.02 - F1: 0.5536
2026-02-14 15:26:22 - INFO - Time taken for Epoch 25:3.36 - F1: 0.5574
2026-02-14 15:26:25 - INFO - Time taken for Epoch 26:3.36 - F1: 0.5554
2026-02-14 15:26:28 - INFO - Time taken for Epoch 27:3.38 - F1: 0.5481
2026-02-14 15:26:32 - INFO - Time taken for Epoch 28:3.36 - F1: 0.5446
2026-02-14 15:26:35 - INFO - Time taken for Epoch 29:3.36 - F1: 0.5396
2026-02-14 15:26:38 - INFO - Time taken for Epoch 30:3.37 - F1: 0.5382
2026-02-14 15:26:42 - INFO - Time taken for Epoch 31:3.36 - F1: 0.5330
2026-02-14 15:26:45 - INFO - Time taken for Epoch 32:3.37 - F1: 0.5405
2026-02-14 15:26:48 - INFO - Time taken for Epoch 33:3.36 - F1: 0.5452
2026-02-14 15:26:48 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 15:26:48 - INFO - Best F1:0.5588 - Best Epoch:22
2026-02-14 15:26:54 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.5648, Test ECE: 0.1253
2026-02-14 15:26:54 - INFO - All results: {'f1_macro': 0.5648165958730532, 'ece': np.float64(0.12533194095936645)}
2026-02-14 15:26:54 - INFO - 
Total time taken: 553.77 seconds
2026-02-14 15:26:54 - INFO - Trial 4 finished with value: 0.5648165958730532 and parameters: {'learning_rate': 0.00010313889866297136, 'weight_decay': 2.0200747251786086e-05, 'batch_size': 8, 'co_train_epochs': 10, 'epoch_patience': 10}. Best is trial 2 with value: 0.6445369536291542.
2026-02-14 15:26:54 - INFO - Using devices: cuda, cuda
2026-02-14 15:26:54 - INFO - Devices: cuda, cuda
2026-02-14 15:26:54 - INFO - Starting log
2026-02-14 15:26:54 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 15:26:55 - INFO - Learning Rate: 0.00023209450278109133
Weight Decay: 0.0003014145807383822
Batch Size: 8
No. Epochs: 6
Epoch Patience: 5
 Accumulation Steps: 8
2026-02-14 15:26:55 - INFO - Generating initial weights
2026-02-14 15:27:14 - INFO - Time taken for Epoch 1:17.14 - F1: 0.0109
2026-02-14 15:27:31 - INFO - Time taken for Epoch 2:17.09 - F1: 0.0166
2026-02-14 15:27:48 - INFO - Time taken for Epoch 3:17.08 - F1: 0.0627
2026-02-14 15:28:05 - INFO - Time taken for Epoch 4:17.08 - F1: 0.0727
2026-02-14 15:28:22 - INFO - Time taken for Epoch 5:17.09 - F1: 0.0948
2026-02-14 15:28:39 - INFO - Time taken for Epoch 6:17.09 - F1: 0.2056
2026-02-14 15:28:39 - INFO - Best F1:0.2056 - Best Epoch:6
2026-02-14 15:28:40 - INFO - Starting co-training
2026-02-14 15:29:03 - INFO - Time taken for Epoch 1: 23.48s - F1: 0.03024831
2026-02-14 15:29:27 - INFO - Time taken for Epoch 2: 24.03s - F1: 0.03024831
2026-02-14 15:29:51 - INFO - Time taken for Epoch 3: 23.45s - F1: 0.03024831
2026-02-14 15:30:14 - INFO - Time taken for Epoch 4: 23.43s - F1: 0.03024831
2026-02-14 15:30:38 - INFO - Time taken for Epoch 5: 23.42s - F1: 0.04185068
2026-02-14 15:31:02 - INFO - Time taken for Epoch 6: 24.20s - F1: 0.04185068
2026-02-14 15:31:03 - INFO - Fine-tuning models
2026-02-14 15:31:07 - INFO - Time taken for Epoch 1:3.37 - F1: 0.0419
2026-02-14 15:31:11 - INFO - Time taken for Epoch 2:3.95 - F1: 0.0419
2026-02-14 15:31:14 - INFO - Time taken for Epoch 3:3.36 - F1: 0.0419
2026-02-14 15:31:17 - INFO - Time taken for Epoch 4:3.39 - F1: 0.0021
2026-02-14 15:31:21 - INFO - Time taken for Epoch 5:3.36 - F1: 0.0021
2026-02-14 15:31:24 - INFO - Time taken for Epoch 6:3.36 - F1: 0.0021
2026-02-14 15:31:28 - INFO - Time taken for Epoch 7:3.36 - F1: 0.0247
2026-02-14 15:31:31 - INFO - Time taken for Epoch 8:3.35 - F1: 0.0247
2026-02-14 15:31:34 - INFO - Time taken for Epoch 9:3.36 - F1: 0.0419
2026-02-14 15:31:38 - INFO - Time taken for Epoch 10:3.36 - F1: 0.0419
2026-02-14 15:31:41 - INFO - Time taken for Epoch 11:3.36 - F1: 0.0108
2026-02-14 15:31:41 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 15:31:41 - INFO - Best F1:0.0419 - Best Epoch:0
2026-02-14 15:31:47 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0417, Test ECE: 0.3938
2026-02-14 15:31:47 - INFO - All results: {'f1_macro': 0.04171180931744312, 'ece': np.float64(0.3937770290622803)}
2026-02-14 15:31:47 - INFO - 
Total time taken: 292.38 seconds
2026-02-14 15:31:47 - INFO - Trial 5 finished with value: 0.04171180931744312 and parameters: {'learning_rate': 0.00023209450278109133, 'weight_decay': 0.0003014145807383822, 'batch_size': 8, 'co_train_epochs': 6, 'epoch_patience': 5}. Best is trial 2 with value: 0.6445369536291542.
2026-02-14 15:31:47 - INFO - Using devices: cuda, cuda
2026-02-14 15:31:47 - INFO - Devices: cuda, cuda
2026-02-14 15:31:47 - INFO - Starting log
2026-02-14 15:31:47 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 15:31:47 - INFO - Learning Rate: 2.1181943652026412e-05
Weight Decay: 0.0002648225760549852
Batch Size: 24
No. Epochs: 6
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-14 15:31:48 - INFO - Generating initial weights
2026-02-14 15:32:03 - INFO - Time taken for Epoch 1:13.91 - F1: 0.0514
2026-02-14 15:32:17 - INFO - Time taken for Epoch 2:13.88 - F1: 0.0619
2026-02-14 15:32:30 - INFO - Time taken for Epoch 3:13.89 - F1: 0.0683
2026-02-14 15:32:44 - INFO - Time taken for Epoch 4:13.86 - F1: 0.0860
2026-02-14 15:32:58 - INFO - Time taken for Epoch 5:13.83 - F1: 0.0978
2026-02-14 15:33:12 - INFO - Time taken for Epoch 6:13.87 - F1: 0.1327
2026-02-14 15:33:12 - INFO - Best F1:0.1327 - Best Epoch:6
2026-02-14 15:33:13 - INFO - Starting co-training
2026-02-14 15:33:41 - INFO - Time taken for Epoch 1: 28.43s - F1: 0.36043038
2026-02-14 15:34:10 - INFO - Time taken for Epoch 2: 28.93s - F1: 0.43958038
2026-02-14 15:34:39 - INFO - Time taken for Epoch 3: 29.02s - F1: 0.46315760
2026-02-14 15:35:08 - INFO - Time taken for Epoch 4: 29.05s - F1: 0.52749173
2026-02-14 15:35:37 - INFO - Time taken for Epoch 5: 28.96s - F1: 0.55344101
2026-02-14 15:36:06 - INFO - Time taken for Epoch 6: 28.99s - F1: 0.58200368
2026-02-14 15:36:08 - INFO - Fine-tuning models
2026-02-14 15:36:11 - INFO - Time taken for Epoch 1:2.70 - F1: 0.5751
2026-02-14 15:36:14 - INFO - Time taken for Epoch 2:3.29 - F1: 0.5766
2026-02-14 15:36:18 - INFO - Time taken for Epoch 3:3.56 - F1: 0.5755
2026-02-14 15:36:20 - INFO - Time taken for Epoch 4:2.69 - F1: 0.5806
2026-02-14 15:36:24 - INFO - Time taken for Epoch 5:3.32 - F1: 0.5839
2026-02-14 15:36:27 - INFO - Time taken for Epoch 6:3.32 - F1: 0.5967
2026-02-14 15:36:30 - INFO - Time taken for Epoch 7:3.30 - F1: 0.5878
2026-02-14 15:36:33 - INFO - Time taken for Epoch 8:2.68 - F1: 0.5825
2026-02-14 15:36:36 - INFO - Time taken for Epoch 9:2.68 - F1: 0.5786
2026-02-14 15:36:38 - INFO - Time taken for Epoch 10:2.68 - F1: 0.5864
2026-02-14 15:36:41 - INFO - Time taken for Epoch 11:2.68 - F1: 0.5930
2026-02-14 15:36:44 - INFO - Time taken for Epoch 12:2.69 - F1: 0.5983
2026-02-14 15:36:49 - INFO - Time taken for Epoch 13:5.37 - F1: 0.5942
2026-02-14 15:36:52 - INFO - Time taken for Epoch 14:2.68 - F1: 0.5956
2026-02-14 15:36:54 - INFO - Time taken for Epoch 15:2.68 - F1: 0.5922
2026-02-14 15:36:57 - INFO - Time taken for Epoch 16:2.68 - F1: 0.6042
2026-02-14 15:37:00 - INFO - Time taken for Epoch 17:3.32 - F1: 0.6123
2026-02-14 15:37:04 - INFO - Time taken for Epoch 18:3.34 - F1: 0.6147
2026-02-14 15:37:07 - INFO - Time taken for Epoch 19:3.32 - F1: 0.6178
2026-02-14 15:37:10 - INFO - Time taken for Epoch 20:3.31 - F1: 0.6096
2026-02-14 15:37:13 - INFO - Time taken for Epoch 21:2.68 - F1: 0.6064
2026-02-14 15:37:16 - INFO - Time taken for Epoch 22:2.68 - F1: 0.6104
2026-02-14 15:37:18 - INFO - Time taken for Epoch 23:2.69 - F1: 0.6119
2026-02-14 15:37:21 - INFO - Time taken for Epoch 24:2.69 - F1: 0.6063
2026-02-14 15:37:24 - INFO - Time taken for Epoch 25:2.69 - F1: 0.6040
2026-02-14 15:37:26 - INFO - Time taken for Epoch 26:2.69 - F1: 0.6058
2026-02-14 15:37:29 - INFO - Time taken for Epoch 27:2.68 - F1: 0.6083
2026-02-14 15:37:32 - INFO - Time taken for Epoch 28:2.68 - F1: 0.6103
2026-02-14 15:37:34 - INFO - Time taken for Epoch 29:2.68 - F1: 0.6230
2026-02-14 15:37:38 - INFO - Time taken for Epoch 30:3.35 - F1: 0.6173
2026-02-14 15:37:41 - INFO - Time taken for Epoch 31:2.68 - F1: 0.6141
2026-02-14 15:37:43 - INFO - Time taken for Epoch 32:2.68 - F1: 0.6142
2026-02-14 15:37:46 - INFO - Time taken for Epoch 33:2.68 - F1: 0.6146
2026-02-14 15:37:49 - INFO - Time taken for Epoch 34:2.68 - F1: 0.6168
2026-02-14 15:37:51 - INFO - Time taken for Epoch 35:2.68 - F1: 0.6141
2026-02-14 15:37:54 - INFO - Time taken for Epoch 36:2.68 - F1: 0.6150
2026-02-14 15:37:57 - INFO - Time taken for Epoch 37:2.68 - F1: 0.6176
2026-02-14 15:37:59 - INFO - Time taken for Epoch 38:2.68 - F1: 0.6167
2026-02-14 15:38:02 - INFO - Time taken for Epoch 39:2.69 - F1: 0.6148
2026-02-14 15:38:02 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 15:38:02 - INFO - Best F1:0.6230 - Best Epoch:28
2026-02-14 15:38:07 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6507, Test ECE: 0.0474
2026-02-14 15:38:07 - INFO - All results: {'f1_macro': 0.6506950004293586, 'ece': np.float64(0.04739751785606003)}
2026-02-14 15:38:07 - INFO - 
Total time taken: 380.11 seconds
2026-02-14 15:38:07 - INFO - Trial 6 finished with value: 0.6506950004293586 and parameters: {'learning_rate': 2.1181943652026412e-05, 'weight_decay': 0.0002648225760549852, 'batch_size': 24, 'co_train_epochs': 6, 'epoch_patience': 10}. Best is trial 6 with value: 0.6506950004293586.
2026-02-14 15:38:07 - INFO - Using devices: cuda, cuda
2026-02-14 15:38:07 - INFO - Devices: cuda, cuda
2026-02-14 15:38:07 - INFO - Starting log
2026-02-14 15:38:07 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 15:38:07 - INFO - Learning Rate: 0.000237838946453756
Weight Decay: 3.998681437429393e-05
Batch Size: 16
No. Epochs: 8
Epoch Patience: 10
 Accumulation Steps: 4
2026-02-14 15:38:08 - INFO - Generating initial weights
2026-02-14 15:38:24 - INFO - Time taken for Epoch 1:15.05 - F1: 0.0497
2026-02-14 15:38:39 - INFO - Time taken for Epoch 2:14.99 - F1: 0.0416
2026-02-14 15:38:54 - INFO - Time taken for Epoch 3:14.98 - F1: 0.0469
2026-02-14 15:39:09 - INFO - Time taken for Epoch 4:14.98 - F1: 0.0419
2026-02-14 15:39:24 - INFO - Time taken for Epoch 5:14.96 - F1: 0.1142
2026-02-14 15:39:39 - INFO - Time taken for Epoch 6:14.95 - F1: 0.1467
2026-02-14 15:39:54 - INFO - Time taken for Epoch 7:14.96 - F1: 0.1821
2026-02-14 15:40:09 - INFO - Time taken for Epoch 8:14.98 - F1: 0.2651
2026-02-14 15:40:09 - INFO - Best F1:0.2651 - Best Epoch:8
2026-02-14 15:40:09 - INFO - Starting co-training
2026-02-14 15:40:33 - INFO - Time taken for Epoch 1: 23.69s - F1: 0.03599511
2026-02-14 15:40:57 - INFO - Time taken for Epoch 2: 24.21s - F1: 0.03024831
2026-02-14 15:41:21 - INFO - Time taken for Epoch 3: 23.74s - F1: 0.03024831
2026-02-14 15:41:45 - INFO - Time taken for Epoch 4: 23.67s - F1: 0.03024831
2026-02-14 15:42:08 - INFO - Time taken for Epoch 5: 23.69s - F1: 0.04185068
2026-02-14 15:42:33 - INFO - Time taken for Epoch 6: 24.27s - F1: 0.04185068
2026-02-14 15:42:56 - INFO - Time taken for Epoch 7: 23.73s - F1: 0.03024831
2026-02-14 15:43:20 - INFO - Time taken for Epoch 8: 23.68s - F1: 0.03024831
2026-02-14 15:43:21 - INFO - Fine-tuning models
2026-02-14 15:43:24 - INFO - Time taken for Epoch 1:2.92 - F1: 0.0419
2026-02-14 15:43:28 - INFO - Time taken for Epoch 2:3.47 - F1: 0.0419
2026-02-14 15:43:31 - INFO - Time taken for Epoch 3:2.90 - F1: 0.0419
2026-02-14 15:43:34 - INFO - Time taken for Epoch 4:2.90 - F1: 0.0021
2026-02-14 15:43:37 - INFO - Time taken for Epoch 5:2.90 - F1: 0.0021
2026-02-14 15:43:39 - INFO - Time taken for Epoch 6:2.90 - F1: 0.0021
2026-02-14 15:43:42 - INFO - Time taken for Epoch 7:2.90 - F1: 0.0037
2026-02-14 15:43:45 - INFO - Time taken for Epoch 8:2.90 - F1: 0.0037
2026-02-14 15:43:48 - INFO - Time taken for Epoch 9:2.90 - F1: 0.0120
2026-02-14 15:43:51 - INFO - Time taken for Epoch 10:2.90 - F1: 0.0120
2026-02-14 15:43:54 - INFO - Time taken for Epoch 11:2.90 - F1: 0.0120
2026-02-14 15:43:54 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 15:43:54 - INFO - Best F1:0.0419 - Best Epoch:0
2026-02-14 15:43:59 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0417, Test ECE: 0.4074
2026-02-14 15:43:59 - INFO - All results: {'f1_macro': 0.04171180931744312, 'ece': np.float64(0.4073765666251473)}
2026-02-14 15:43:59 - INFO - 
Total time taken: 352.31 seconds
2026-02-14 15:43:59 - INFO - Trial 7 finished with value: 0.04171180931744312 and parameters: {'learning_rate': 0.000237838946453756, 'weight_decay': 3.998681437429393e-05, 'batch_size': 16, 'co_train_epochs': 8, 'epoch_patience': 10}. Best is trial 6 with value: 0.6506950004293586.
2026-02-14 15:43:59 - INFO - Using devices: cuda, cuda
2026-02-14 15:43:59 - INFO - Devices: cuda, cuda
2026-02-14 15:43:59 - INFO - Starting log
2026-02-14 15:43:59 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 15:43:59 - INFO - Learning Rate: 0.00010411066931047
Weight Decay: 0.002065733815117087
Batch Size: 8
No. Epochs: 16
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-14 15:44:00 - INFO - Generating initial weights
2026-02-14 15:44:18 - INFO - Time taken for Epoch 1:17.17 - F1: 0.0462
2026-02-14 15:44:36 - INFO - Time taken for Epoch 2:17.10 - F1: 0.0669
2026-02-14 15:44:53 - INFO - Time taken for Epoch 3:17.11 - F1: 0.0479
2026-02-14 15:45:10 - INFO - Time taken for Epoch 4:17.12 - F1: 0.0644
2026-02-14 15:45:27 - INFO - Time taken for Epoch 5:17.09 - F1: 0.0829
2026-02-14 15:45:44 - INFO - Time taken for Epoch 6:17.16 - F1: 0.1079
2026-02-14 15:46:02 - INFO - Time taken for Epoch 7:17.51 - F1: 0.1728
2026-02-14 15:46:19 - INFO - Time taken for Epoch 8:17.44 - F1: 0.2089
2026-02-14 15:46:36 - INFO - Time taken for Epoch 9:17.43 - F1: 0.2235
2026-02-14 15:46:54 - INFO - Time taken for Epoch 10:17.42 - F1: 0.2776
2026-02-14 15:47:11 - INFO - Time taken for Epoch 11:17.40 - F1: 0.3804
2026-02-14 15:47:29 - INFO - Time taken for Epoch 12:17.44 - F1: 0.4528
2026-02-14 15:47:46 - INFO - Time taken for Epoch 13:17.41 - F1: 0.4523
2026-02-14 15:48:03 - INFO - Time taken for Epoch 14:17.42 - F1: 0.4598
2026-02-14 15:48:21 - INFO - Time taken for Epoch 15:17.45 - F1: 0.4701
2026-02-14 15:48:38 - INFO - Time taken for Epoch 16:17.42 - F1: 0.5179
2026-02-14 15:48:38 - INFO - Best F1:0.5179 - Best Epoch:16
2026-02-14 15:48:39 - INFO - Starting co-training
2026-02-14 15:49:03 - INFO - Time taken for Epoch 1: 23.83s - F1: 0.04715766
2026-02-14 15:49:27 - INFO - Time taken for Epoch 2: 24.47s - F1: 0.03024831
2026-02-14 15:49:51 - INFO - Time taken for Epoch 3: 23.77s - F1: 0.03024831
2026-02-14 15:50:15 - INFO - Time taken for Epoch 4: 23.75s - F1: 0.04185068
2026-02-14 15:50:39 - INFO - Time taken for Epoch 5: 23.80s - F1: 0.04185068
2026-02-14 15:51:03 - INFO - Time taken for Epoch 6: 23.79s - F1: 0.04185068
2026-02-14 15:51:26 - INFO - Time taken for Epoch 7: 23.80s - F1: 0.04185068
2026-02-14 15:51:50 - INFO - Time taken for Epoch 8: 23.77s - F1: 0.04185068
2026-02-14 15:52:14 - INFO - Time taken for Epoch 9: 23.91s - F1: 0.04185068
2026-02-14 15:52:38 - INFO - Time taken for Epoch 10: 23.84s - F1: 0.04185068
2026-02-14 15:53:02 - INFO - Time taken for Epoch 11: 23.78s - F1: 0.04185068
2026-02-14 15:53:02 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 15:53:03 - INFO - Fine-tuning models
2026-02-14 15:53:07 - INFO - Time taken for Epoch 1:3.47 - F1: 0.0395
2026-02-14 15:53:10 - INFO - Time taken for Epoch 2:3.97 - F1: 0.0422
2026-02-14 15:53:15 - INFO - Time taken for Epoch 3:4.01 - F1: 0.0108
2026-02-14 15:53:18 - INFO - Time taken for Epoch 4:3.46 - F1: 0.0108
2026-02-14 15:53:21 - INFO - Time taken for Epoch 5:3.45 - F1: 0.0247
2026-02-14 15:53:25 - INFO - Time taken for Epoch 6:3.45 - F1: 0.0247
2026-02-14 15:53:28 - INFO - Time taken for Epoch 7:3.45 - F1: 0.0247
2026-02-14 15:53:32 - INFO - Time taken for Epoch 8:3.45 - F1: 0.0247
2026-02-14 15:53:35 - INFO - Time taken for Epoch 9:3.45 - F1: 0.0021
2026-02-14 15:53:39 - INFO - Time taken for Epoch 10:3.45 - F1: 0.0021
2026-02-14 15:53:42 - INFO - Time taken for Epoch 11:3.46 - F1: 0.0021
2026-02-14 15:53:46 - INFO - Time taken for Epoch 12:3.47 - F1: 0.0021
2026-02-14 15:53:46 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 15:53:46 - INFO - Best F1:0.0422 - Best Epoch:1
2026-02-14 15:53:51 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0437, Test ECE: 0.1651
2026-02-14 15:53:51 - INFO - All results: {'f1_macro': 0.04365561694290976, 'ece': np.float64(0.16507431607716383)}
2026-02-14 15:53:51 - INFO - 
Total time taken: 592.21 seconds
2026-02-14 15:53:51 - INFO - Trial 8 finished with value: 0.04365561694290976 and parameters: {'learning_rate': 0.00010411066931047, 'weight_decay': 0.002065733815117087, 'batch_size': 8, 'co_train_epochs': 16, 'epoch_patience': 10}. Best is trial 6 with value: 0.6506950004293586.
2026-02-14 15:53:51 - INFO - Using devices: cuda, cuda
2026-02-14 15:53:51 - INFO - Devices: cuda, cuda
2026-02-14 15:53:51 - INFO - Starting log
2026-02-14 15:53:51 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 15:53:52 - INFO - Learning Rate: 0.0002374756018577719
Weight Decay: 0.009529709665997208
Batch Size: 24
No. Epochs: 10
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-14 15:53:52 - INFO - Generating initial weights
2026-02-14 15:54:07 - INFO - Time taken for Epoch 1:13.95 - F1: 0.0152
2026-02-14 15:54:21 - INFO - Time taken for Epoch 2:13.95 - F1: 0.0096
2026-02-14 15:54:35 - INFO - Time taken for Epoch 3:13.86 - F1: 0.0429
2026-02-14 15:54:49 - INFO - Time taken for Epoch 4:13.90 - F1: 0.0321
2026-02-14 15:55:03 - INFO - Time taken for Epoch 5:13.90 - F1: 0.0321
2026-02-14 15:55:17 - INFO - Time taken for Epoch 6:13.91 - F1: 0.0321
2026-02-14 15:55:31 - INFO - Time taken for Epoch 7:13.86 - F1: 0.0321
2026-02-14 15:55:45 - INFO - Time taken for Epoch 8:13.86 - F1: 0.0321
2026-02-14 15:55:59 - INFO - Time taken for Epoch 9:13.91 - F1: 0.0320
2026-02-14 15:56:12 - INFO - Time taken for Epoch 10:13.90 - F1: 0.0999
2026-02-14 15:56:12 - INFO - Best F1:0.0999 - Best Epoch:10
2026-02-14 15:56:13 - INFO - Starting co-training
2026-02-14 15:56:42 - INFO - Time taken for Epoch 1: 28.64s - F1: 0.17222918
2026-02-14 15:57:11 - INFO - Time taken for Epoch 2: 29.19s - F1: 0.04185068
2026-02-14 15:57:40 - INFO - Time taken for Epoch 3: 28.63s - F1: 0.04185068
2026-02-14 15:58:08 - INFO - Time taken for Epoch 4: 28.61s - F1: 0.04185068
2026-02-14 15:58:37 - INFO - Time taken for Epoch 5: 28.61s - F1: 0.04185068
2026-02-14 15:59:06 - INFO - Time taken for Epoch 6: 28.60s - F1: 0.04185068
2026-02-14 15:59:34 - INFO - Time taken for Epoch 7: 28.68s - F1: 0.04185068
2026-02-14 16:00:03 - INFO - Time taken for Epoch 8: 28.61s - F1: 0.04185068
2026-02-14 16:00:31 - INFO - Time taken for Epoch 9: 28.58s - F1: 0.04185068
2026-02-14 16:01:00 - INFO - Time taken for Epoch 10: 28.62s - F1: 0.04185068
2026-02-14 16:01:01 - INFO - Fine-tuning models
2026-02-14 16:01:04 - INFO - Time taken for Epoch 1:2.72 - F1: 0.1481
2026-02-14 16:01:08 - INFO - Time taken for Epoch 2:3.39 - F1: 0.1476
2026-02-14 16:01:10 - INFO - Time taken for Epoch 3:2.69 - F1: 0.1546
2026-02-14 16:01:14 - INFO - Time taken for Epoch 4:3.44 - F1: 0.1300
2026-02-14 16:01:16 - INFO - Time taken for Epoch 5:2.70 - F1: 0.1553
2026-02-14 16:01:20 - INFO - Time taken for Epoch 6:3.77 - F1: 0.1556
2026-02-14 16:01:24 - INFO - Time taken for Epoch 7:3.36 - F1: 0.1508
2026-02-14 16:01:26 - INFO - Time taken for Epoch 8:2.69 - F1: 0.1430
2026-02-14 16:01:29 - INFO - Time taken for Epoch 9:2.69 - F1: 0.1562
2026-02-14 16:01:32 - INFO - Time taken for Epoch 10:3.42 - F1: 0.1047
2026-02-14 16:01:35 - INFO - Time taken for Epoch 11:2.69 - F1: 0.1024
2026-02-14 16:01:38 - INFO - Time taken for Epoch 12:2.70 - F1: 0.1519
2026-02-14 16:01:41 - INFO - Time taken for Epoch 13:2.70 - F1: 0.1650
2026-02-14 16:01:49 - INFO - Time taken for Epoch 14:7.99 - F1: 0.1552
2026-02-14 16:01:51 - INFO - Time taken for Epoch 15:2.69 - F1: 0.1524
2026-02-14 16:01:54 - INFO - Time taken for Epoch 16:2.69 - F1: 0.1326
2026-02-14 16:01:57 - INFO - Time taken for Epoch 17:2.70 - F1: 0.0170
2026-02-14 16:01:59 - INFO - Time taken for Epoch 18:2.69 - F1: 0.0021
2026-02-14 16:02:02 - INFO - Time taken for Epoch 19:2.69 - F1: 0.0321
2026-02-14 16:02:05 - INFO - Time taken for Epoch 20:2.69 - F1: 0.0411
2026-02-14 16:02:07 - INFO - Time taken for Epoch 21:2.69 - F1: 0.0375
2026-02-14 16:02:10 - INFO - Time taken for Epoch 22:2.69 - F1: 0.0719
2026-02-14 16:02:13 - INFO - Time taken for Epoch 23:2.69 - F1: 0.0839
2026-02-14 16:02:13 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-14 16:02:13 - INFO - Best F1:0.1650 - Best Epoch:12
2026-02-14 16:02:18 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.1703, Test ECE: 0.1515
2026-02-14 16:02:18 - INFO - All results: {'f1_macro': 0.1702596551154765, 'ece': np.float64(0.15153856409240637)}
2026-02-14 16:02:18 - INFO - 
Total time taken: 506.35 seconds
2026-02-14 16:02:18 - INFO - Trial 9 finished with value: 0.1702596551154765 and parameters: {'learning_rate': 0.0002374756018577719, 'weight_decay': 0.009529709665997208, 'batch_size': 24, 'co_train_epochs': 10, 'epoch_patience': 9}. Best is trial 6 with value: 0.6506950004293586.
2026-02-14 16:02:18 - INFO - 
[BEST TRIAL RESULTS]
2026-02-14 16:02:18 - INFO - F1 Score: 0.6507
2026-02-14 16:02:18 - INFO - Params: {'learning_rate': 2.1181943652026412e-05, 'weight_decay': 0.0002648225760549852, 'batch_size': 24, 'co_train_epochs': 6, 'epoch_patience': 10}
2026-02-14 16:02:18 - INFO -   learning_rate: 2.1181943652026412e-05
2026-02-14 16:02:18 - INFO -   weight_decay: 0.0002648225760549852
2026-02-14 16:02:18 - INFO -   batch_size: 24
2026-02-14 16:02:18 - INFO -   co_train_epochs: 6
2026-02-14 16:02:18 - INFO -   epoch_patience: 10
2026-02-14 16:02:18 - INFO - 
Total time taken: 5513.97 seconds
