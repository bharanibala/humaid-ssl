2026-02-13 14:43:58 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 14:43:58 - INFO - A new study created in memory with name: study_humanitarian10_california_wildfires_2018
2026-02-13 14:43:59 - INFO - Using devices: cuda, cuda
2026-02-13 14:43:59 - INFO - Devices: cuda, cuda
2026-02-13 14:43:59 - INFO - Starting log
2026-02-13 14:43:59 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 14:43:59 - INFO - Learning Rate: 6.098944067755963e-05
Weight Decay: 5.563704688269024e-05
Batch Size: 24
No. Epochs: 20
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-13 14:44:00 - INFO - Generating initial weights
2026-02-13 14:44:15 - INFO - Time taken for Epoch 1:13.99 - F1: 0.1295
2026-02-13 14:44:29 - INFO - Time taken for Epoch 2:14.06 - F1: 0.0120
2026-02-13 14:44:43 - INFO - Time taken for Epoch 3:13.85 - F1: 0.0120
2026-02-13 14:44:57 - INFO - Time taken for Epoch 4:13.85 - F1: 0.0120
2026-02-13 14:45:11 - INFO - Time taken for Epoch 5:13.76 - F1: 0.0120
2026-02-13 14:45:25 - INFO - Time taken for Epoch 6:13.68 - F1: 0.0452
2026-02-13 14:45:38 - INFO - Time taken for Epoch 7:13.68 - F1: 0.1139
2026-02-13 14:45:52 - INFO - Time taken for Epoch 8:13.69 - F1: 0.2005
2026-02-13 14:46:06 - INFO - Time taken for Epoch 9:13.78 - F1: 0.2560
2026-02-13 14:46:19 - INFO - Time taken for Epoch 10:13.69 - F1: 0.3039
2026-02-13 14:46:33 - INFO - Time taken for Epoch 11:14.03 - F1: 0.3309
2026-02-13 14:46:48 - INFO - Time taken for Epoch 12:14.24 - F1: 0.3499
2026-02-13 14:47:02 - INFO - Time taken for Epoch 13:14.12 - F1: 0.3843
2026-02-13 14:47:16 - INFO - Time taken for Epoch 14:14.15 - F1: 0.3945
2026-02-13 14:47:30 - INFO - Time taken for Epoch 15:14.09 - F1: 0.4058
2026-02-13 14:47:44 - INFO - Time taken for Epoch 16:13.97 - F1: 0.4290
2026-02-13 14:47:58 - INFO - Time taken for Epoch 17:13.80 - F1: 0.4249
2026-02-13 14:48:12 - INFO - Time taken for Epoch 18:13.94 - F1: 0.4210
2026-02-13 14:48:26 - INFO - Time taken for Epoch 19:13.88 - F1: 0.4222
2026-02-13 14:48:40 - INFO - Time taken for Epoch 20:14.20 - F1: 0.4342
2026-02-13 14:48:40 - INFO - Best F1:0.4342 - Best Epoch:20
2026-02-13 14:48:41 - INFO - Starting co-training
2026-02-13 14:49:12 - INFO - Time taken for Epoch 1: 30.71s - F1: 0.43127271
2026-02-13 14:49:42 - INFO - Time taken for Epoch 2: 30.29s - F1: 0.43767834
2026-02-13 14:50:13 - INFO - Time taken for Epoch 3: 31.19s - F1: 0.54402731
2026-02-13 14:50:44 - INFO - Time taken for Epoch 4: 30.36s - F1: 0.57539288
2026-02-13 14:51:14 - INFO - Time taken for Epoch 5: 30.57s - F1: 0.55223334
2026-02-13 14:51:44 - INFO - Time taken for Epoch 6: 29.81s - F1: 0.55404648
2026-02-13 14:52:14 - INFO - Time taken for Epoch 7: 29.57s - F1: 0.58313502
2026-02-13 14:52:44 - INFO - Time taken for Epoch 8: 30.14s - F1: 0.56587769
2026-02-13 14:53:13 - INFO - Time taken for Epoch 9: 29.78s - F1: 0.54467367
2026-02-13 14:53:43 - INFO - Time taken for Epoch 10: 29.66s - F1: 0.58675219
2026-02-13 14:54:13 - INFO - Time taken for Epoch 11: 30.35s - F1: 0.55485694
2026-02-13 14:54:43 - INFO - Time taken for Epoch 12: 29.61s - F1: 0.59111602
2026-02-13 14:55:13 - INFO - Time taken for Epoch 13: 30.28s - F1: 0.58309949
2026-02-13 14:55:43 - INFO - Time taken for Epoch 14: 29.58s - F1: 0.57996062
2026-02-13 14:56:12 - INFO - Time taken for Epoch 15: 29.38s - F1: 0.59056392
2026-02-13 14:56:42 - INFO - Time taken for Epoch 16: 29.59s - F1: 0.58875381
2026-02-13 14:57:11 - INFO - Time taken for Epoch 17: 29.41s - F1: 0.58456589
2026-02-13 14:57:41 - INFO - Time taken for Epoch 18: 29.37s - F1: 0.57412462
2026-02-13 14:57:41 - INFO - Performance not improving for 6 consecutive epochs.
2026-02-13 14:57:42 - INFO - Fine-tuning models
2026-02-13 14:57:45 - INFO - Time taken for Epoch 1:2.20 - F1: 0.5986
2026-02-13 14:57:47 - INFO - Time taken for Epoch 2:2.88 - F1: 0.5934
2026-02-13 14:57:50 - INFO - Time taken for Epoch 3:2.24 - F1: 0.5672
2026-02-13 14:57:52 - INFO - Time taken for Epoch 4:2.23 - F1: 0.5577
2026-02-13 14:57:54 - INFO - Time taken for Epoch 5:2.29 - F1: 0.5206
2026-02-13 14:57:57 - INFO - Time taken for Epoch 6:2.24 - F1: 0.4593
2026-02-13 14:57:59 - INFO - Time taken for Epoch 7:2.23 - F1: 0.5111
2026-02-13 14:58:01 - INFO - Time taken for Epoch 8:2.28 - F1: 0.5720
2026-02-13 14:58:03 - INFO - Time taken for Epoch 9:2.24 - F1: 0.5816
2026-02-13 14:58:05 - INFO - Time taken for Epoch 10:2.24 - F1: 0.5837
2026-02-13 14:58:08 - INFO - Time taken for Epoch 11:2.28 - F1: 0.5840
2026-02-13 14:58:08 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 14:58:08 - INFO - Best F1:0.5986 - Best Epoch:0
2026-02-13 14:58:13 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5906, Test ECE: 0.0575
2026-02-13 14:58:13 - INFO - All results: {'f1_macro': 0.5906326876920462, 'ece': np.float64(0.057477979281436574)}
2026-02-13 14:58:13 - INFO - 
Total time taken: 854.15 seconds
2026-02-13 14:58:13 - INFO - Trial 0 finished with value: 0.5906326876920462 and parameters: {'learning_rate': 6.098944067755963e-05, 'weight_decay': 5.563704688269024e-05, 'batch_size': 24, 'co_train_epochs': 20, 'epoch_patience': 6}. Best is trial 0 with value: 0.5906326876920462.
2026-02-13 14:58:13 - INFO - Using devices: cuda, cuda
2026-02-13 14:58:13 - INFO - Devices: cuda, cuda
2026-02-13 14:58:13 - INFO - Starting log
2026-02-13 14:58:13 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 14:58:13 - INFO - Learning Rate: 3.754097230653442e-05
Weight Decay: 2.079498633737805e-05
Batch Size: 8
No. Epochs: 6
Epoch Patience: 8
 Accumulation Steps: 8
2026-02-13 14:58:14 - INFO - Generating initial weights
2026-02-13 14:58:32 - INFO - Time taken for Epoch 1:16.92 - F1: 0.0120
2026-02-13 14:58:49 - INFO - Time taken for Epoch 2:16.87 - F1: 0.0120
2026-02-13 14:59:06 - INFO - Time taken for Epoch 3:16.88 - F1: 0.0120
2026-02-13 14:59:23 - INFO - Time taken for Epoch 4:16.86 - F1: 0.0120
2026-02-13 14:59:39 - INFO - Time taken for Epoch 5:16.86 - F1: 0.0120
2026-02-13 14:59:56 - INFO - Time taken for Epoch 6:16.87 - F1: 0.0120
2026-02-13 14:59:56 - INFO - Best F1:0.0120 - Best Epoch:1
2026-02-13 14:59:57 - INFO - Starting co-training
2026-02-13 15:00:21 - INFO - Time taken for Epoch 1: 24.30s - F1: 0.29045725
2026-02-13 15:00:46 - INFO - Time taken for Epoch 2: 24.80s - F1: 0.32987146
2026-02-13 15:01:11 - INFO - Time taken for Epoch 3: 24.85s - F1: 0.34816426
2026-02-13 15:01:36 - INFO - Time taken for Epoch 4: 24.81s - F1: 0.38051474
2026-02-13 15:02:01 - INFO - Time taken for Epoch 5: 24.84s - F1: 0.42441050
2026-02-13 15:02:25 - INFO - Time taken for Epoch 6: 24.80s - F1: 0.46973968
2026-02-13 15:02:28 - INFO - Fine-tuning models
2026-02-13 15:02:30 - INFO - Time taken for Epoch 1:2.71 - F1: 0.4809
2026-02-13 15:02:34 - INFO - Time taken for Epoch 2:3.56 - F1: 0.5133
2026-02-13 15:02:37 - INFO - Time taken for Epoch 3:3.61 - F1: 0.5036
2026-02-13 15:02:40 - INFO - Time taken for Epoch 4:2.71 - F1: 0.5284
2026-02-13 15:02:44 - INFO - Time taken for Epoch 5:3.32 - F1: 0.5351
2026-02-13 15:02:47 - INFO - Time taken for Epoch 6:3.41 - F1: 0.5405
2026-02-13 15:02:50 - INFO - Time taken for Epoch 7:3.34 - F1: 0.5370
2026-02-13 15:02:53 - INFO - Time taken for Epoch 8:2.68 - F1: 0.5480
2026-02-13 15:02:56 - INFO - Time taken for Epoch 9:3.30 - F1: 0.5694
2026-02-13 15:03:00 - INFO - Time taken for Epoch 10:3.33 - F1: 0.5801
2026-02-13 15:03:03 - INFO - Time taken for Epoch 11:3.32 - F1: 0.5633
2026-02-13 15:03:06 - INFO - Time taken for Epoch 12:2.68 - F1: 0.5567
2026-02-13 15:03:08 - INFO - Time taken for Epoch 13:2.68 - F1: 0.5539
2026-02-13 15:03:14 - INFO - Time taken for Epoch 14:5.64 - F1: 0.5531
2026-02-13 15:03:17 - INFO - Time taken for Epoch 15:2.68 - F1: 0.5529
2026-02-13 15:03:19 - INFO - Time taken for Epoch 16:2.68 - F1: 0.5519
2026-02-13 15:03:22 - INFO - Time taken for Epoch 17:2.67 - F1: 0.5555
2026-02-13 15:03:25 - INFO - Time taken for Epoch 18:2.68 - F1: 0.5580
2026-02-13 15:03:27 - INFO - Time taken for Epoch 19:2.68 - F1: 0.5640
2026-02-13 15:03:30 - INFO - Time taken for Epoch 20:2.68 - F1: 0.5718
2026-02-13 15:03:30 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 15:03:30 - INFO - Best F1:0.5801 - Best Epoch:9
2026-02-13 15:03:36 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5578, Test ECE: 0.1221
2026-02-13 15:03:36 - INFO - All results: {'f1_macro': 0.5578097572818372, 'ece': np.float64(0.12209701662517264)}
2026-02-13 15:03:36 - INFO - 
Total time taken: 323.01 seconds
2026-02-13 15:03:36 - INFO - Trial 1 finished with value: 0.5578097572818372 and parameters: {'learning_rate': 3.754097230653442e-05, 'weight_decay': 2.079498633737805e-05, 'batch_size': 8, 'co_train_epochs': 6, 'epoch_patience': 8}. Best is trial 0 with value: 0.5906326876920462.
2026-02-13 15:03:36 - INFO - Using devices: cuda, cuda
2026-02-13 15:03:36 - INFO - Devices: cuda, cuda
2026-02-13 15:03:36 - INFO - Starting log
2026-02-13 15:03:36 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 15:03:36 - INFO - Learning Rate: 7.661960619247316e-05
Weight Decay: 8.381672112842573e-05
Batch Size: 16
No. Epochs: 20
Epoch Patience: 10
 Accumulation Steps: 4
2026-02-13 15:03:37 - INFO - Generating initial weights
2026-02-13 15:03:53 - INFO - Time taken for Epoch 1:14.75 - F1: 0.0120
2026-02-13 15:04:08 - INFO - Time taken for Epoch 2:14.73 - F1: 0.0120
2026-02-13 15:04:22 - INFO - Time taken for Epoch 3:14.73 - F1: 0.0120
2026-02-13 15:04:37 - INFO - Time taken for Epoch 4:14.73 - F1: 0.0120
2026-02-13 15:04:52 - INFO - Time taken for Epoch 5:14.75 - F1: 0.0120
2026-02-13 15:05:06 - INFO - Time taken for Epoch 6:14.74 - F1: 0.0120
2026-02-13 15:05:21 - INFO - Time taken for Epoch 7:14.74 - F1: 0.0135
2026-02-13 15:05:36 - INFO - Time taken for Epoch 8:14.73 - F1: 0.0580
2026-02-13 15:05:51 - INFO - Time taken for Epoch 9:14.74 - F1: 0.1248
2026-02-13 15:06:05 - INFO - Time taken for Epoch 10:14.72 - F1: 0.2124
2026-02-13 15:06:20 - INFO - Time taken for Epoch 11:14.73 - F1: 0.2484
2026-02-13 15:06:35 - INFO - Time taken for Epoch 12:14.75 - F1: 0.2794
2026-02-13 15:06:50 - INFO - Time taken for Epoch 13:14.73 - F1: 0.3316
2026-02-13 15:07:04 - INFO - Time taken for Epoch 14:14.74 - F1: 0.3496
2026-02-13 15:07:19 - INFO - Time taken for Epoch 15:14.75 - F1: 0.3757
2026-02-13 15:07:34 - INFO - Time taken for Epoch 16:14.73 - F1: 0.4054
2026-02-13 15:07:49 - INFO - Time taken for Epoch 17:14.74 - F1: 0.4210
2026-02-13 15:08:03 - INFO - Time taken for Epoch 18:14.73 - F1: 0.4247
2026-02-13 15:08:18 - INFO - Time taken for Epoch 19:14.76 - F1: 0.4265
2026-02-13 15:08:33 - INFO - Time taken for Epoch 20:14.74 - F1: 0.4203
2026-02-13 15:08:33 - INFO - Best F1:0.4265 - Best Epoch:19
2026-02-13 15:08:33 - INFO - Starting co-training
2026-02-13 15:08:58 - INFO - Time taken for Epoch 1: 24.49s - F1: 0.34156872
2026-02-13 15:09:23 - INFO - Time taken for Epoch 2: 25.26s - F1: 0.37489292
2026-02-13 15:09:49 - INFO - Time taken for Epoch 3: 25.10s - F1: 0.43989007
2026-02-13 15:10:14 - INFO - Time taken for Epoch 4: 25.07s - F1: 0.44416393
2026-02-13 15:10:39 - INFO - Time taken for Epoch 5: 25.10s - F1: 0.43442511
2026-02-13 15:11:03 - INFO - Time taken for Epoch 6: 24.46s - F1: 0.50445356
2026-02-13 15:11:28 - INFO - Time taken for Epoch 7: 25.07s - F1: 0.50059935
2026-02-13 15:11:53 - INFO - Time taken for Epoch 8: 24.50s - F1: 0.57166896
2026-02-13 15:12:18 - INFO - Time taken for Epoch 9: 25.08s - F1: 0.55917676
2026-02-13 15:12:42 - INFO - Time taken for Epoch 10: 24.49s - F1: 0.57421324
2026-02-13 15:13:07 - INFO - Time taken for Epoch 11: 24.99s - F1: 0.56171634
2026-02-13 15:13:32 - INFO - Time taken for Epoch 12: 24.63s - F1: 0.58221238
2026-02-13 15:13:57 - INFO - Time taken for Epoch 13: 25.05s - F1: 0.50215093
2026-02-13 15:14:21 - INFO - Time taken for Epoch 14: 24.55s - F1: 0.57738982
2026-02-13 15:14:46 - INFO - Time taken for Epoch 15: 24.49s - F1: 0.58842696
2026-02-13 15:15:11 - INFO - Time taken for Epoch 16: 25.09s - F1: 0.58466777
2026-02-13 15:15:36 - INFO - Time taken for Epoch 17: 24.50s - F1: 0.57904628
2026-02-13 15:16:00 - INFO - Time taken for Epoch 18: 24.52s - F1: 0.55483928
2026-02-13 15:16:25 - INFO - Time taken for Epoch 19: 24.46s - F1: 0.55054407
2026-02-13 15:16:49 - INFO - Time taken for Epoch 20: 24.48s - F1: 0.55857013
2026-02-13 15:16:51 - INFO - Fine-tuning models
2026-02-13 15:16:53 - INFO - Time taken for Epoch 1:2.33 - F1: 0.6107
2026-02-13 15:16:56 - INFO - Time taken for Epoch 2:2.92 - F1: 0.6005
2026-02-13 15:16:58 - INFO - Time taken for Epoch 3:2.31 - F1: 0.5923
2026-02-13 15:17:01 - INFO - Time taken for Epoch 4:2.31 - F1: 0.5867
2026-02-13 15:17:03 - INFO - Time taken for Epoch 5:2.31 - F1: 0.5666
2026-02-13 15:17:05 - INFO - Time taken for Epoch 6:2.31 - F1: 0.5566
2026-02-13 15:17:08 - INFO - Time taken for Epoch 7:2.32 - F1: 0.5570
2026-02-13 15:17:10 - INFO - Time taken for Epoch 8:2.31 - F1: 0.5544
2026-02-13 15:17:12 - INFO - Time taken for Epoch 9:2.31 - F1: 0.5767
2026-02-13 15:17:14 - INFO - Time taken for Epoch 10:2.31 - F1: 0.5793
2026-02-13 15:17:17 - INFO - Time taken for Epoch 11:2.32 - F1: 0.5775
2026-02-13 15:17:17 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 15:17:17 - INFO - Best F1:0.6107 - Best Epoch:0
2026-02-13 15:17:22 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6251, Test ECE: 0.0437
2026-02-13 15:17:22 - INFO - All results: {'f1_macro': 0.6250979690211119, 'ece': np.float64(0.0436986119758585)}
2026-02-13 15:17:22 - INFO - 
Total time taken: 826.24 seconds
2026-02-13 15:17:22 - INFO - Trial 2 finished with value: 0.6250979690211119 and parameters: {'learning_rate': 7.661960619247316e-05, 'weight_decay': 8.381672112842573e-05, 'batch_size': 16, 'co_train_epochs': 20, 'epoch_patience': 10}. Best is trial 2 with value: 0.6250979690211119.
2026-02-13 15:17:22 - INFO - Using devices: cuda, cuda
2026-02-13 15:17:22 - INFO - Devices: cuda, cuda
2026-02-13 15:17:22 - INFO - Starting log
2026-02-13 15:17:22 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 15:17:22 - INFO - Learning Rate: 3.060450973117294e-05
Weight Decay: 0.001817970014124171
Batch Size: 8
No. Epochs: 11
Epoch Patience: 5
 Accumulation Steps: 8
2026-02-13 15:17:23 - INFO - Generating initial weights
2026-02-13 15:17:41 - INFO - Time taken for Epoch 1:16.83 - F1: 0.0120
2026-02-13 15:17:58 - INFO - Time taken for Epoch 2:16.80 - F1: 0.0120
2026-02-13 15:18:15 - INFO - Time taken for Epoch 3:16.79 - F1: 0.0120
2026-02-13 15:18:31 - INFO - Time taken for Epoch 4:16.84 - F1: 0.0120
2026-02-13 15:18:48 - INFO - Time taken for Epoch 5:16.80 - F1: 0.0120
2026-02-13 15:19:05 - INFO - Time taken for Epoch 6:16.81 - F1: 0.0120
2026-02-13 15:19:22 - INFO - Time taken for Epoch 7:16.81 - F1: 0.0120
2026-02-13 15:19:39 - INFO - Time taken for Epoch 8:16.78 - F1: 0.0120
2026-02-13 15:19:55 - INFO - Time taken for Epoch 9:16.77 - F1: 0.0120
2026-02-13 15:20:12 - INFO - Time taken for Epoch 10:16.80 - F1: 0.0120
2026-02-13 15:20:29 - INFO - Time taken for Epoch 11:16.80 - F1: 0.0120
2026-02-13 15:20:29 - INFO - Best F1:0.0120 - Best Epoch:11
2026-02-13 15:20:30 - INFO - Starting co-training
2026-02-13 15:20:54 - INFO - Time taken for Epoch 1: 24.17s - F1: 0.29900427
2026-02-13 15:21:19 - INFO - Time taken for Epoch 2: 24.72s - F1: 0.28933013
2026-02-13 15:21:43 - INFO - Time taken for Epoch 3: 24.17s - F1: 0.34756009
2026-02-13 15:22:08 - INFO - Time taken for Epoch 4: 24.75s - F1: 0.43168912
2026-02-13 15:22:33 - INFO - Time taken for Epoch 5: 25.14s - F1: 0.46155599
2026-02-13 15:22:59 - INFO - Time taken for Epoch 6: 25.94s - F1: 0.46861066
2026-02-13 15:23:25 - INFO - Time taken for Epoch 7: 26.25s - F1: 0.49702899
2026-02-13 15:23:50 - INFO - Time taken for Epoch 8: 24.81s - F1: 0.54352141
2026-02-13 15:24:15 - INFO - Time taken for Epoch 9: 24.93s - F1: 0.54951263
2026-02-13 15:24:39 - INFO - Time taken for Epoch 10: 24.79s - F1: 0.56689626
2026-02-13 15:25:04 - INFO - Time taken for Epoch 11: 24.86s - F1: 0.56287921
2026-02-13 15:25:06 - INFO - Fine-tuning models
2026-02-13 15:25:08 - INFO - Time taken for Epoch 1:2.70 - F1: 0.5807
2026-02-13 15:25:12 - INFO - Time taken for Epoch 2:3.30 - F1: 0.5866
2026-02-13 15:25:15 - INFO - Time taken for Epoch 3:3.32 - F1: 0.5682
2026-02-13 15:25:18 - INFO - Time taken for Epoch 4:2.69 - F1: 0.5588
2026-02-13 15:25:20 - INFO - Time taken for Epoch 5:2.68 - F1: 0.5545
2026-02-13 15:25:23 - INFO - Time taken for Epoch 6:2.67 - F1: 0.5486
2026-02-13 15:25:26 - INFO - Time taken for Epoch 7:2.68 - F1: 0.5579
2026-02-13 15:25:28 - INFO - Time taken for Epoch 8:2.68 - F1: 0.5660
2026-02-13 15:25:31 - INFO - Time taken for Epoch 9:2.68 - F1: 0.5672
2026-02-13 15:25:34 - INFO - Time taken for Epoch 10:2.68 - F1: 0.5642
2026-02-13 15:25:37 - INFO - Time taken for Epoch 11:2.68 - F1: 0.5655
2026-02-13 15:25:39 - INFO - Time taken for Epoch 12:2.68 - F1: 0.5609
2026-02-13 15:25:39 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 15:25:39 - INFO - Best F1:0.5866 - Best Epoch:1
2026-02-13 15:25:45 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5944, Test ECE: 0.0652
2026-02-13 15:25:45 - INFO - All results: {'f1_macro': 0.594381401973561, 'ece': np.float64(0.0651771969822969)}
2026-02-13 15:25:45 - INFO - 
Total time taken: 503.00 seconds
2026-02-13 15:25:45 - INFO - Trial 3 finished with value: 0.594381401973561 and parameters: {'learning_rate': 3.060450973117294e-05, 'weight_decay': 0.001817970014124171, 'batch_size': 8, 'co_train_epochs': 11, 'epoch_patience': 5}. Best is trial 2 with value: 0.6250979690211119.
2026-02-13 15:25:45 - INFO - Using devices: cuda, cuda
2026-02-13 15:25:45 - INFO - Devices: cuda, cuda
2026-02-13 15:25:45 - INFO - Starting log
2026-02-13 15:25:45 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 15:25:45 - INFO - Learning Rate: 4.578374022881057e-05
Weight Decay: 2.4109028839065426e-05
Batch Size: 24
No. Epochs: 14
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-13 15:25:46 - INFO - Generating initial weights
2026-02-13 15:26:01 - INFO - Time taken for Epoch 1:13.73 - F1: 0.1504
2026-02-13 15:26:15 - INFO - Time taken for Epoch 2:13.72 - F1: 0.0120
2026-02-13 15:26:28 - INFO - Time taken for Epoch 3:13.70 - F1: 0.0120
2026-02-13 15:26:42 - INFO - Time taken for Epoch 4:13.71 - F1: 0.0120
2026-02-13 15:26:56 - INFO - Time taken for Epoch 5:13.71 - F1: 0.0120
2026-02-13 15:27:09 - INFO - Time taken for Epoch 6:13.70 - F1: 0.0120
2026-02-13 15:27:23 - INFO - Time taken for Epoch 7:13.71 - F1: 0.0120
2026-02-13 15:27:37 - INFO - Time taken for Epoch 8:13.71 - F1: 0.0465
2026-02-13 15:27:51 - INFO - Time taken for Epoch 9:13.70 - F1: 0.0694
2026-02-13 15:28:04 - INFO - Time taken for Epoch 10:13.70 - F1: 0.1603
2026-02-13 15:28:18 - INFO - Time taken for Epoch 11:13.72 - F1: 0.1847
2026-02-13 15:28:32 - INFO - Time taken for Epoch 12:13.69 - F1: 0.2244
2026-02-13 15:28:45 - INFO - Time taken for Epoch 13:13.70 - F1: 0.2712
2026-02-13 15:28:59 - INFO - Time taken for Epoch 14:13.70 - F1: 0.2950
2026-02-13 15:28:59 - INFO - Best F1:0.2950 - Best Epoch:14
2026-02-13 15:29:00 - INFO - Starting co-training
2026-02-13 15:29:29 - INFO - Time taken for Epoch 1: 29.23s - F1: 0.42349919
2026-02-13 15:29:59 - INFO - Time taken for Epoch 2: 29.85s - F1: 0.43927444
2026-02-13 15:30:29 - INFO - Time taken for Epoch 3: 29.84s - F1: 0.47971932
2026-02-13 15:30:59 - INFO - Time taken for Epoch 4: 30.68s - F1: 0.51802180
2026-02-13 15:31:34 - INFO - Time taken for Epoch 5: 34.61s - F1: 0.56653389
2026-02-13 15:32:04 - INFO - Time taken for Epoch 6: 30.06s - F1: 0.57225745
2026-02-13 15:32:34 - INFO - Time taken for Epoch 7: 29.78s - F1: 0.56769453
2026-02-13 15:33:03 - INFO - Time taken for Epoch 8: 29.21s - F1: 0.58759959
2026-02-13 15:33:33 - INFO - Time taken for Epoch 9: 30.26s - F1: 0.55577155
2026-02-13 15:34:03 - INFO - Time taken for Epoch 10: 29.24s - F1: 0.58324230
2026-02-13 15:34:32 - INFO - Time taken for Epoch 11: 29.31s - F1: 0.57610920
2026-02-13 15:35:01 - INFO - Time taken for Epoch 12: 29.23s - F1: 0.58892759
2026-02-13 15:35:31 - INFO - Time taken for Epoch 13: 29.83s - F1: 0.58889320
2026-02-13 15:36:00 - INFO - Time taken for Epoch 14: 29.40s - F1: 0.56703016
2026-02-13 15:36:02 - INFO - Fine-tuning models
2026-02-13 15:36:04 - INFO - Time taken for Epoch 1:2.21 - F1: 0.5996
2026-02-13 15:36:07 - INFO - Time taken for Epoch 2:2.83 - F1: 0.5918
2026-02-13 15:36:09 - INFO - Time taken for Epoch 3:2.23 - F1: 0.5792
2026-02-13 15:36:11 - INFO - Time taken for Epoch 4:2.23 - F1: 0.5742
2026-02-13 15:36:14 - INFO - Time taken for Epoch 5:2.22 - F1: 0.5424
2026-02-13 15:36:16 - INFO - Time taken for Epoch 6:2.22 - F1: 0.5014
2026-02-13 15:36:18 - INFO - Time taken for Epoch 7:2.22 - F1: 0.5329
2026-02-13 15:36:20 - INFO - Time taken for Epoch 8:2.21 - F1: 0.5698
2026-02-13 15:36:23 - INFO - Time taken for Epoch 9:2.22 - F1: 0.5780
2026-02-13 15:36:25 - INFO - Time taken for Epoch 10:2.21 - F1: 0.5837
2026-02-13 15:36:27 - INFO - Time taken for Epoch 11:2.22 - F1: 0.5938
2026-02-13 15:36:27 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 15:36:27 - INFO - Best F1:0.5996 - Best Epoch:0
2026-02-13 15:36:32 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5921, Test ECE: 0.0439
2026-02-13 15:36:32 - INFO - All results: {'f1_macro': 0.5920641087753242, 'ece': np.float64(0.04390665560369211)}
2026-02-13 15:36:32 - INFO - 
Total time taken: 646.87 seconds
2026-02-13 15:36:32 - INFO - Trial 4 finished with value: 0.5920641087753242 and parameters: {'learning_rate': 4.578374022881057e-05, 'weight_decay': 2.4109028839065426e-05, 'batch_size': 24, 'co_train_epochs': 14, 'epoch_patience': 4}. Best is trial 2 with value: 0.6250979690211119.
2026-02-13 15:36:32 - INFO - Using devices: cuda, cuda
2026-02-13 15:36:32 - INFO - Devices: cuda, cuda
2026-02-13 15:36:32 - INFO - Starting log
2026-02-13 15:36:32 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 15:36:32 - INFO - Learning Rate: 3.067003073019125e-05
Weight Decay: 0.008494354256603404
Batch Size: 24
No. Epochs: 16
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-13 15:36:33 - INFO - Generating initial weights
2026-02-13 15:36:48 - INFO - Time taken for Epoch 1:13.81 - F1: 0.1287
2026-02-13 15:37:02 - INFO - Time taken for Epoch 2:13.76 - F1: 0.0160
2026-02-13 15:37:15 - INFO - Time taken for Epoch 3:13.75 - F1: 0.0120
2026-02-13 15:37:29 - INFO - Time taken for Epoch 4:13.75 - F1: 0.0120
2026-02-13 15:37:43 - INFO - Time taken for Epoch 5:13.75 - F1: 0.0120
2026-02-13 15:37:57 - INFO - Time taken for Epoch 6:13.72 - F1: 0.0120
2026-02-13 15:38:10 - INFO - Time taken for Epoch 7:13.75 - F1: 0.0120
2026-02-13 15:38:24 - INFO - Time taken for Epoch 8:13.75 - F1: 0.0120
2026-02-13 15:38:38 - INFO - Time taken for Epoch 9:13.76 - F1: 0.0120
2026-02-13 15:38:52 - INFO - Time taken for Epoch 10:13.76 - F1: 0.0260
2026-02-13 15:39:05 - INFO - Time taken for Epoch 11:13.78 - F1: 0.0439
2026-02-13 15:39:19 - INFO - Time taken for Epoch 12:13.78 - F1: 0.0542
2026-02-13 15:39:33 - INFO - Time taken for Epoch 13:13.72 - F1: 0.0810
2026-02-13 15:39:47 - INFO - Time taken for Epoch 14:13.75 - F1: 0.1442
2026-02-13 15:40:00 - INFO - Time taken for Epoch 15:13.75 - F1: 0.1865
2026-02-13 15:40:14 - INFO - Time taken for Epoch 16:13.74 - F1: 0.2068
2026-02-13 15:40:14 - INFO - Best F1:0.2068 - Best Epoch:16
2026-02-13 15:40:15 - INFO - Starting co-training
2026-02-13 15:40:44 - INFO - Time taken for Epoch 1: 29.44s - F1: 0.40164603
2026-02-13 15:41:14 - INFO - Time taken for Epoch 2: 30.06s - F1: 0.39508273
2026-02-13 15:41:44 - INFO - Time taken for Epoch 3: 29.47s - F1: 0.50086680
2026-02-13 15:42:14 - INFO - Time taken for Epoch 4: 30.07s - F1: 0.55443392
2026-02-13 15:42:44 - INFO - Time taken for Epoch 5: 30.16s - F1: 0.53832689
2026-02-13 15:43:14 - INFO - Time taken for Epoch 6: 29.41s - F1: 0.56981219
2026-02-13 15:43:44 - INFO - Time taken for Epoch 7: 30.00s - F1: 0.56739814
2026-02-13 15:44:13 - INFO - Time taken for Epoch 8: 29.43s - F1: 0.56160426
2026-02-13 15:44:42 - INFO - Time taken for Epoch 9: 29.42s - F1: 0.58154277
2026-02-13 15:45:13 - INFO - Time taken for Epoch 10: 30.16s - F1: 0.57665045
2026-02-13 15:45:42 - INFO - Time taken for Epoch 11: 29.54s - F1: 0.59560894
2026-02-13 15:46:12 - INFO - Time taken for Epoch 12: 30.08s - F1: 0.57468853
2026-02-13 15:46:42 - INFO - Time taken for Epoch 13: 29.46s - F1: 0.59272157
2026-02-13 15:47:11 - INFO - Time taken for Epoch 14: 29.42s - F1: 0.56525316
2026-02-13 15:47:40 - INFO - Time taken for Epoch 15: 29.40s - F1: 0.59476775
2026-02-13 15:48:10 - INFO - Time taken for Epoch 16: 29.44s - F1: 0.55704074
2026-02-13 15:48:12 - INFO - Fine-tuning models
2026-02-13 15:48:14 - INFO - Time taken for Epoch 1:2.21 - F1: 0.5855
2026-02-13 15:48:17 - INFO - Time taken for Epoch 2:3.39 - F1: 0.5812
2026-02-13 15:48:20 - INFO - Time taken for Epoch 3:2.23 - F1: 0.6072
2026-02-13 15:48:23 - INFO - Time taken for Epoch 4:3.03 - F1: 0.5718
2026-02-13 15:48:25 - INFO - Time taken for Epoch 5:2.22 - F1: 0.5476
2026-02-13 15:48:27 - INFO - Time taken for Epoch 6:2.21 - F1: 0.5493
2026-02-13 15:48:29 - INFO - Time taken for Epoch 7:2.22 - F1: 0.5425
2026-02-13 15:48:31 - INFO - Time taken for Epoch 8:2.21 - F1: 0.5498
2026-02-13 15:48:34 - INFO - Time taken for Epoch 9:2.21 - F1: 0.5736
2026-02-13 15:48:36 - INFO - Time taken for Epoch 10:2.21 - F1: 0.5712
2026-02-13 15:48:38 - INFO - Time taken for Epoch 11:2.21 - F1: 0.5808
2026-02-13 15:48:40 - INFO - Time taken for Epoch 12:2.21 - F1: 0.5824
2026-02-13 15:48:43 - INFO - Time taken for Epoch 13:2.20 - F1: 0.5968
2026-02-13 15:48:43 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 15:48:43 - INFO - Best F1:0.6072 - Best Epoch:2
2026-02-13 15:48:47 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6154, Test ECE: 0.0563
2026-02-13 15:48:47 - INFO - All results: {'f1_macro': 0.6154168833249114, 'ece': np.float64(0.05631729758495668)}
2026-02-13 15:48:47 - INFO - 
Total time taken: 735.61 seconds
2026-02-13 15:48:47 - INFO - Trial 5 finished with value: 0.6154168833249114 and parameters: {'learning_rate': 3.067003073019125e-05, 'weight_decay': 0.008494354256603404, 'batch_size': 24, 'co_train_epochs': 16, 'epoch_patience': 6}. Best is trial 2 with value: 0.6250979690211119.
2026-02-13 15:48:47 - INFO - Using devices: cuda, cuda
2026-02-13 15:48:47 - INFO - Devices: cuda, cuda
2026-02-13 15:48:47 - INFO - Starting log
2026-02-13 15:48:47 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 15:48:48 - INFO - Learning Rate: 1.5832944573474566e-05
Weight Decay: 0.0010947577902889636
Batch Size: 24
No. Epochs: 8
Epoch Patience: 5
 Accumulation Steps: 2
2026-02-13 15:48:48 - INFO - Generating initial weights
2026-02-13 15:49:03 - INFO - Time taken for Epoch 1:13.81 - F1: 0.1078
2026-02-13 15:49:17 - INFO - Time taken for Epoch 2:13.78 - F1: 0.0968
2026-02-13 15:49:31 - INFO - Time taken for Epoch 3:13.74 - F1: 0.0179
2026-02-13 15:49:45 - INFO - Time taken for Epoch 4:13.74 - F1: 0.0120
2026-02-13 15:49:58 - INFO - Time taken for Epoch 5:13.75 - F1: 0.0120
2026-02-13 15:50:12 - INFO - Time taken for Epoch 6:13.76 - F1: 0.0120
2026-02-13 15:50:26 - INFO - Time taken for Epoch 7:13.75 - F1: 0.0120
2026-02-13 15:50:40 - INFO - Time taken for Epoch 8:13.71 - F1: 0.0120
2026-02-13 15:50:40 - INFO - Best F1:0.1078 - Best Epoch:1
2026-02-13 15:50:40 - INFO - Starting co-training
2026-02-13 15:51:10 - INFO - Time taken for Epoch 1: 29.53s - F1: 0.29816735
2026-02-13 15:51:40 - INFO - Time taken for Epoch 2: 30.06s - F1: 0.36069211
2026-02-13 15:52:10 - INFO - Time taken for Epoch 3: 30.14s - F1: 0.41559160
2026-02-13 15:52:40 - INFO - Time taken for Epoch 4: 30.02s - F1: 0.45887042
2026-02-13 15:53:10 - INFO - Time taken for Epoch 5: 30.09s - F1: 0.49380925
2026-02-13 15:53:40 - INFO - Time taken for Epoch 6: 30.12s - F1: 0.55422752
2026-02-13 15:54:11 - INFO - Time taken for Epoch 7: 30.13s - F1: 0.56328837
2026-02-13 15:54:41 - INFO - Time taken for Epoch 8: 30.05s - F1: 0.58186341
2026-02-13 15:54:43 - INFO - Fine-tuning models
2026-02-13 15:54:45 - INFO - Time taken for Epoch 1:2.22 - F1: 0.5853
2026-02-13 15:54:48 - INFO - Time taken for Epoch 2:2.81 - F1: 0.5664
2026-02-13 15:54:50 - INFO - Time taken for Epoch 3:2.24 - F1: 0.5686
2026-02-13 15:54:52 - INFO - Time taken for Epoch 4:2.24 - F1: 0.5571
2026-02-13 15:54:55 - INFO - Time taken for Epoch 5:2.23 - F1: 0.5459
2026-02-13 15:54:57 - INFO - Time taken for Epoch 6:2.21 - F1: 0.5495
2026-02-13 15:54:59 - INFO - Time taken for Epoch 7:2.21 - F1: 0.5667
2026-02-13 15:55:01 - INFO - Time taken for Epoch 8:2.22 - F1: 0.5747
2026-02-13 15:55:04 - INFO - Time taken for Epoch 9:2.22 - F1: 0.5777
2026-02-13 15:55:06 - INFO - Time taken for Epoch 10:2.22 - F1: 0.5799
2026-02-13 15:55:08 - INFO - Time taken for Epoch 11:2.21 - F1: 0.5773
2026-02-13 15:55:08 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 15:55:08 - INFO - Best F1:0.5853 - Best Epoch:0
2026-02-13 15:55:13 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5684, Test ECE: 0.0356
2026-02-13 15:55:13 - INFO - All results: {'f1_macro': 0.5683841787291348, 'ece': np.float64(0.03564669639586097)}
2026-02-13 15:55:13 - INFO - 
Total time taken: 385.44 seconds
2026-02-13 15:55:13 - INFO - Trial 6 finished with value: 0.5683841787291348 and parameters: {'learning_rate': 1.5832944573474566e-05, 'weight_decay': 0.0010947577902889636, 'batch_size': 24, 'co_train_epochs': 8, 'epoch_patience': 5}. Best is trial 2 with value: 0.6250979690211119.
2026-02-13 15:55:13 - INFO - Using devices: cuda, cuda
2026-02-13 15:55:13 - INFO - Devices: cuda, cuda
2026-02-13 15:55:13 - INFO - Starting log
2026-02-13 15:55:13 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 15:55:13 - INFO - Learning Rate: 7.404125072480487e-05
Weight Decay: 7.557836696798249e-05
Batch Size: 16
No. Epochs: 10
Epoch Patience: 6
 Accumulation Steps: 4
2026-02-13 15:55:14 - INFO - Generating initial weights
2026-02-13 15:55:30 - INFO - Time taken for Epoch 1:14.87 - F1: 0.0120
2026-02-13 15:55:45 - INFO - Time taken for Epoch 2:14.79 - F1: 0.0120
2026-02-13 15:56:00 - INFO - Time taken for Epoch 3:14.82 - F1: 0.0120
2026-02-13 15:56:14 - INFO - Time taken for Epoch 4:14.79 - F1: 0.0120
2026-02-13 15:56:29 - INFO - Time taken for Epoch 5:14.79 - F1: 0.0120
2026-02-13 15:56:44 - INFO - Time taken for Epoch 6:14.80 - F1: 0.0120
2026-02-13 15:56:59 - INFO - Time taken for Epoch 7:14.80 - F1: 0.0120
2026-02-13 15:57:14 - INFO - Time taken for Epoch 8:14.78 - F1: 0.0240
2026-02-13 15:57:28 - INFO - Time taken for Epoch 9:14.81 - F1: 0.0965
2026-02-13 15:57:43 - INFO - Time taken for Epoch 10:14.80 - F1: 0.1736
2026-02-13 15:57:43 - INFO - Best F1:0.1736 - Best Epoch:10
2026-02-13 15:57:44 - INFO - Starting co-training
2026-02-13 15:58:09 - INFO - Time taken for Epoch 1: 24.62s - F1: 0.34809364
2026-02-13 15:58:34 - INFO - Time taken for Epoch 2: 25.20s - F1: 0.44233473
2026-02-13 15:58:59 - INFO - Time taken for Epoch 3: 25.27s - F1: 0.47776297
2026-02-13 15:59:25 - INFO - Time taken for Epoch 4: 25.83s - F1: 0.44820000
2026-02-13 15:59:49 - INFO - Time taken for Epoch 5: 24.60s - F1: 0.51015394
2026-02-13 16:00:15 - INFO - Time taken for Epoch 6: 25.29s - F1: 0.50764736
2026-02-13 16:00:39 - INFO - Time taken for Epoch 7: 24.65s - F1: 0.54869580
2026-02-13 16:01:05 - INFO - Time taken for Epoch 8: 25.19s - F1: 0.48148797
2026-02-13 16:01:29 - INFO - Time taken for Epoch 9: 24.63s - F1: 0.58328264
2026-02-13 16:01:55 - INFO - Time taken for Epoch 10: 25.52s - F1: 0.53972662
2026-02-13 16:01:56 - INFO - Fine-tuning models
2026-02-13 16:01:59 - INFO - Time taken for Epoch 1:2.35 - F1: 0.5930
2026-02-13 16:02:02 - INFO - Time taken for Epoch 2:2.91 - F1: 0.5800
2026-02-13 16:02:04 - INFO - Time taken for Epoch 3:2.33 - F1: 0.5922
2026-02-13 16:02:06 - INFO - Time taken for Epoch 4:2.33 - F1: 0.5711
2026-02-13 16:02:09 - INFO - Time taken for Epoch 5:2.33 - F1: 0.5682
2026-02-13 16:02:11 - INFO - Time taken for Epoch 6:2.33 - F1: 0.5390
2026-02-13 16:02:13 - INFO - Time taken for Epoch 7:2.34 - F1: 0.5362
2026-02-13 16:02:16 - INFO - Time taken for Epoch 8:2.33 - F1: 0.5449
2026-02-13 16:02:18 - INFO - Time taken for Epoch 9:2.33 - F1: 0.5413
2026-02-13 16:02:20 - INFO - Time taken for Epoch 10:2.33 - F1: 0.5529
2026-02-13 16:02:23 - INFO - Time taken for Epoch 11:2.33 - F1: 0.5594
2026-02-13 16:02:23 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:02:23 - INFO - Best F1:0.5930 - Best Epoch:0
2026-02-13 16:02:28 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5968, Test ECE: 0.0719
2026-02-13 16:02:28 - INFO - All results: {'f1_macro': 0.5968283641068413, 'ece': np.float64(0.07191463996905484)}
2026-02-13 16:02:28 - INFO - 
Total time taken: 434.88 seconds
2026-02-13 16:02:28 - INFO - Trial 7 finished with value: 0.5968283641068413 and parameters: {'learning_rate': 7.404125072480487e-05, 'weight_decay': 7.557836696798249e-05, 'batch_size': 16, 'co_train_epochs': 10, 'epoch_patience': 6}. Best is trial 2 with value: 0.6250979690211119.
2026-02-13 16:02:28 - INFO - Using devices: cuda, cuda
2026-02-13 16:02:28 - INFO - Devices: cuda, cuda
2026-02-13 16:02:28 - INFO - Starting log
2026-02-13 16:02:28 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:02:28 - INFO - Learning Rate: 2.2401923867348137e-05
Weight Decay: 0.0058577904815757245
Batch Size: 8
No. Epochs: 6
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-13 16:02:29 - INFO - Generating initial weights
2026-02-13 16:02:47 - INFO - Time taken for Epoch 1:17.25 - F1: 0.0130
2026-02-13 16:03:04 - INFO - Time taken for Epoch 2:17.19 - F1: 0.0120
2026-02-13 16:03:22 - INFO - Time taken for Epoch 3:17.18 - F1: 0.0120
2026-02-13 16:03:39 - INFO - Time taken for Epoch 4:17.20 - F1: 0.0120
2026-02-13 16:03:56 - INFO - Time taken for Epoch 5:17.18 - F1: 0.0120
2026-02-13 16:04:13 - INFO - Time taken for Epoch 6:17.19 - F1: 0.0120
2026-02-13 16:04:13 - INFO - Best F1:0.0130 - Best Epoch:1
2026-02-13 16:04:14 - INFO - Starting co-training
2026-02-13 16:04:39 - INFO - Time taken for Epoch 1: 24.59s - F1: 0.28906967
2026-02-13 16:05:04 - INFO - Time taken for Epoch 2: 25.41s - F1: 0.31091615
2026-02-13 16:05:29 - INFO - Time taken for Epoch 3: 25.28s - F1: 0.35727158
2026-02-13 16:05:55 - INFO - Time taken for Epoch 4: 25.22s - F1: 0.35621126
2026-02-13 16:06:19 - INFO - Time taken for Epoch 5: 24.55s - F1: 0.40751151
2026-02-13 16:06:44 - INFO - Time taken for Epoch 6: 25.18s - F1: 0.41372714
2026-02-13 16:06:46 - INFO - Fine-tuning models
2026-02-13 16:06:49 - INFO - Time taken for Epoch 1:2.79 - F1: 0.4348
2026-02-13 16:06:52 - INFO - Time taken for Epoch 2:3.36 - F1: 0.4398
2026-02-13 16:06:56 - INFO - Time taken for Epoch 3:3.39 - F1: 0.4597
2026-02-13 16:06:59 - INFO - Time taken for Epoch 4:3.39 - F1: 0.4503
2026-02-13 16:07:02 - INFO - Time taken for Epoch 5:2.77 - F1: 0.4527
2026-02-13 16:07:05 - INFO - Time taken for Epoch 6:2.76 - F1: 0.4700
2026-02-13 16:07:08 - INFO - Time taken for Epoch 7:3.39 - F1: 0.4762
2026-02-13 16:07:12 - INFO - Time taken for Epoch 8:3.41 - F1: 0.4994
2026-02-13 16:07:15 - INFO - Time taken for Epoch 9:3.41 - F1: 0.5267
2026-02-13 16:07:18 - INFO - Time taken for Epoch 10:3.40 - F1: 0.5254
2026-02-13 16:07:21 - INFO - Time taken for Epoch 11:2.76 - F1: 0.5302
2026-02-13 16:07:34 - INFO - Time taken for Epoch 12:12.70 - F1: 0.5397
2026-02-13 16:07:37 - INFO - Time taken for Epoch 13:3.41 - F1: 0.5482
2026-02-13 16:07:41 - INFO - Time taken for Epoch 14:3.42 - F1: 0.5339
2026-02-13 16:07:43 - INFO - Time taken for Epoch 15:2.76 - F1: 0.5435
2026-02-13 16:07:46 - INFO - Time taken for Epoch 16:2.76 - F1: 0.5494
2026-02-13 16:07:50 - INFO - Time taken for Epoch 17:3.41 - F1: 0.5762
2026-02-13 16:07:53 - INFO - Time taken for Epoch 18:3.42 - F1: 0.5702
2026-02-13 16:07:56 - INFO - Time taken for Epoch 19:2.76 - F1: 0.5650
2026-02-13 16:07:59 - INFO - Time taken for Epoch 20:2.76 - F1: 0.5691
2026-02-13 16:08:01 - INFO - Time taken for Epoch 21:2.77 - F1: 0.5718
2026-02-13 16:08:04 - INFO - Time taken for Epoch 22:2.76 - F1: 0.5852
2026-02-13 16:08:15 - INFO - Time taken for Epoch 23:10.62 - F1: 0.5888
2026-02-13 16:08:18 - INFO - Time taken for Epoch 24:3.45 - F1: 0.5886
2026-02-13 16:08:21 - INFO - Time taken for Epoch 25:2.76 - F1: 0.5867
2026-02-13 16:08:24 - INFO - Time taken for Epoch 26:2.76 - F1: 0.5889
2026-02-13 16:08:27 - INFO - Time taken for Epoch 27:3.45 - F1: 0.5853
2026-02-13 16:08:30 - INFO - Time taken for Epoch 28:2.76 - F1: 0.5868
2026-02-13 16:08:33 - INFO - Time taken for Epoch 29:2.76 - F1: 0.5903
2026-02-13 16:08:36 - INFO - Time taken for Epoch 30:3.44 - F1: 0.5910
2026-02-13 16:08:40 - INFO - Time taken for Epoch 31:3.45 - F1: 0.5853
2026-02-13 16:08:42 - INFO - Time taken for Epoch 32:2.76 - F1: 0.5910
2026-02-13 16:08:45 - INFO - Time taken for Epoch 33:2.77 - F1: 0.5891
2026-02-13 16:08:48 - INFO - Time taken for Epoch 34:2.77 - F1: 0.5914
2026-02-13 16:08:56 - INFO - Time taken for Epoch 35:8.27 - F1: 0.5923
2026-02-13 16:08:59 - INFO - Time taken for Epoch 36:3.42 - F1: 0.6009
2026-02-13 16:09:03 - INFO - Time taken for Epoch 37:3.41 - F1: 0.6047
2026-02-13 16:09:06 - INFO - Time taken for Epoch 38:3.40 - F1: 0.6051
2026-02-13 16:09:10 - INFO - Time taken for Epoch 39:3.43 - F1: 0.6016
2026-02-13 16:09:12 - INFO - Time taken for Epoch 40:2.76 - F1: 0.6121
2026-02-13 16:09:16 - INFO - Time taken for Epoch 41:3.43 - F1: 0.6047
2026-02-13 16:09:19 - INFO - Time taken for Epoch 42:2.76 - F1: 0.5988
2026-02-13 16:09:21 - INFO - Time taken for Epoch 43:2.76 - F1: 0.6061
2026-02-13 16:09:24 - INFO - Time taken for Epoch 44:2.76 - F1: 0.6025
2026-02-13 16:09:27 - INFO - Time taken for Epoch 45:2.76 - F1: 0.6004
2026-02-13 16:09:30 - INFO - Time taken for Epoch 46:2.76 - F1: 0.6063
2026-02-13 16:09:34 - INFO - Time taken for Epoch 47:4.24 - F1: 0.6067
2026-02-13 16:09:37 - INFO - Time taken for Epoch 48:2.75 - F1: 0.5967
2026-02-13 16:09:39 - INFO - Time taken for Epoch 49:2.76 - F1: 0.5990
2026-02-13 16:09:42 - INFO - Time taken for Epoch 50:2.76 - F1: 0.6001
2026-02-13 16:09:42 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:09:42 - INFO - Best F1:0.6121 - Best Epoch:39
2026-02-13 16:09:48 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5918, Test ECE: 0.1063
2026-02-13 16:09:48 - INFO - All results: {'f1_macro': 0.5918478961996428, 'ece': np.float64(0.1063022417087085)}
2026-02-13 16:09:48 - INFO - 
Total time taken: 440.24 seconds
2026-02-13 16:09:48 - INFO - Trial 8 finished with value: 0.5918478961996428 and parameters: {'learning_rate': 2.2401923867348137e-05, 'weight_decay': 0.0058577904815757245, 'batch_size': 8, 'co_train_epochs': 6, 'epoch_patience': 4}. Best is trial 2 with value: 0.6250979690211119.
2026-02-13 16:09:48 - INFO - Using devices: cuda, cuda
2026-02-13 16:09:48 - INFO - Devices: cuda, cuda
2026-02-13 16:09:48 - INFO - Starting log
2026-02-13 16:09:48 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:09:48 - INFO - Learning Rate: 0.000779334318842313
Weight Decay: 9.06556248612226e-05
Batch Size: 16
No. Epochs: 19
Epoch Patience: 7
 Accumulation Steps: 4
2026-02-13 16:09:49 - INFO - Generating initial weights
2026-02-13 16:10:05 - INFO - Time taken for Epoch 1:14.82 - F1: 0.0120
2026-02-13 16:10:20 - INFO - Time taken for Epoch 2:14.78 - F1: 0.0120
2026-02-13 16:10:35 - INFO - Time taken for Epoch 3:14.74 - F1: 0.0136
2026-02-13 16:10:49 - INFO - Time taken for Epoch 4:14.77 - F1: 0.0120
2026-02-13 16:11:04 - INFO - Time taken for Epoch 5:14.77 - F1: 0.0120
2026-02-13 16:11:19 - INFO - Time taken for Epoch 6:14.75 - F1: 0.0120
2026-02-13 16:11:34 - INFO - Time taken for Epoch 7:14.75 - F1: 0.0120
2026-02-13 16:11:48 - INFO - Time taken for Epoch 8:14.78 - F1: 0.0120
2026-02-13 16:12:03 - INFO - Time taken for Epoch 9:14.76 - F1: 0.0120
2026-02-13 16:12:18 - INFO - Time taken for Epoch 10:14.68 - F1: 0.0120
2026-02-13 16:12:33 - INFO - Time taken for Epoch 11:14.66 - F1: 0.0120
2026-02-13 16:12:47 - INFO - Time taken for Epoch 12:14.67 - F1: 0.0120
2026-02-13 16:13:02 - INFO - Time taken for Epoch 13:14.66 - F1: 0.0120
2026-02-13 16:13:17 - INFO - Time taken for Epoch 14:14.69 - F1: 0.0120
2026-02-13 16:13:31 - INFO - Time taken for Epoch 15:14.66 - F1: 0.0120
2026-02-13 16:13:46 - INFO - Time taken for Epoch 16:14.65 - F1: 0.0120
2026-02-13 16:14:01 - INFO - Time taken for Epoch 17:14.66 - F1: 0.0120
2026-02-13 16:14:15 - INFO - Time taken for Epoch 18:14.67 - F1: 0.0120
2026-02-13 16:14:30 - INFO - Time taken for Epoch 19:14.69 - F1: 0.0120
2026-02-13 16:14:30 - INFO - Best F1:0.0136 - Best Epoch:3
2026-02-13 16:14:31 - INFO - Starting co-training
2026-02-13 16:14:55 - INFO - Time taken for Epoch 1: 24.47s - F1: 0.04185068
2026-02-13 16:15:20 - INFO - Time taken for Epoch 2: 25.00s - F1: 0.04185068
2026-02-13 16:15:45 - INFO - Time taken for Epoch 3: 24.47s - F1: 0.03024831
2026-02-13 16:16:09 - INFO - Time taken for Epoch 4: 24.46s - F1: 0.03024831
2026-02-13 16:16:34 - INFO - Time taken for Epoch 5: 24.41s - F1: 0.03024831
2026-02-13 16:16:58 - INFO - Time taken for Epoch 6: 24.46s - F1: 0.04185068
2026-02-13 16:17:22 - INFO - Time taken for Epoch 7: 24.45s - F1: 0.04185068
2026-02-13 16:17:47 - INFO - Time taken for Epoch 8: 24.42s - F1: 0.04185068
2026-02-13 16:17:47 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-13 16:17:49 - INFO - Fine-tuning models
2026-02-13 16:17:51 - INFO - Time taken for Epoch 1:2.31 - F1: 0.0120
2026-02-13 16:17:55 - INFO - Time taken for Epoch 2:3.79 - F1: 0.0120
2026-02-13 16:17:57 - INFO - Time taken for Epoch 3:2.30 - F1: 0.0120
2026-02-13 16:18:00 - INFO - Time taken for Epoch 4:2.29 - F1: 0.0120
2026-02-13 16:18:02 - INFO - Time taken for Epoch 5:2.29 - F1: 0.0120
2026-02-13 16:18:04 - INFO - Time taken for Epoch 6:2.29 - F1: 0.0120
2026-02-13 16:18:06 - INFO - Time taken for Epoch 7:2.30 - F1: 0.0120
2026-02-13 16:18:09 - INFO - Time taken for Epoch 8:2.30 - F1: 0.0120
2026-02-13 16:18:11 - INFO - Time taken for Epoch 9:2.30 - F1: 0.0120
2026-02-13 16:18:13 - INFO - Time taken for Epoch 10:2.30 - F1: 0.0120
2026-02-13 16:18:16 - INFO - Time taken for Epoch 11:2.30 - F1: 0.0120
2026-02-13 16:18:16 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:18:16 - INFO - Best F1:0.0120 - Best Epoch:0
2026-02-13 16:18:21 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0121, Test ECE: 0.3801
2026-02-13 16:18:21 - INFO - All results: {'f1_macro': 0.012090032154340836, 'ece': np.float64(0.3800566149524921)}
2026-02-13 16:18:21 - INFO - 
Total time taken: 512.67 seconds
2026-02-13 16:18:21 - INFO - Trial 9 finished with value: 0.012090032154340836 and parameters: {'learning_rate': 0.000779334318842313, 'weight_decay': 9.06556248612226e-05, 'batch_size': 16, 'co_train_epochs': 19, 'epoch_patience': 7}. Best is trial 2 with value: 0.6250979690211119.
2026-02-13 16:18:21 - INFO - 
[BEST TRIAL RESULTS]
2026-02-13 16:18:21 - INFO - F1 Score: 0.6251
2026-02-13 16:18:21 - INFO - Params: {'learning_rate': 7.661960619247316e-05, 'weight_decay': 8.381672112842573e-05, 'batch_size': 16, 'co_train_epochs': 20, 'epoch_patience': 10}
2026-02-13 16:18:21 - INFO -   learning_rate: 7.661960619247316e-05
2026-02-13 16:18:21 - INFO -   weight_decay: 8.381672112842573e-05
2026-02-13 16:18:21 - INFO -   batch_size: 16
2026-02-13 16:18:21 - INFO -   co_train_epochs: 20
2026-02-13 16:18:21 - INFO -   epoch_patience: 10
2026-02-13 16:18:21 - INFO - 
Total time taken: 5662.29 seconds
