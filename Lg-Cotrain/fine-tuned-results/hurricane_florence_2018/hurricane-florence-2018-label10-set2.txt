Running with 10 label/class set 2

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 10:38:38 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 10:38:38 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_florence_2018
Using devices: cuda, cuda
2026-02-13 10:38:39 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 10:38:39 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 10:38:39 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 10:38:39 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 0.0002501465807690919
Weight Decay: 1.1901270178909427e-05
Batch Size: 16
No. Epochs: 10
Epoch Patience: 8
 Accumulation Steps: 4
2026-02-13 10:38:40 - INFO - Learning Rate: 0.0002501465807690919
Weight Decay: 1.1901270178909427e-05
Batch Size: 16
No. Epochs: 10
Epoch Patience: 8
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 10:38:41 - INFO - Generating initial weights
Time taken for Epoch 1:18.28 - F1: 0.0155
2026-02-13 10:39:03 - INFO - Time taken for Epoch 1:18.28 - F1: 0.0155
Time taken for Epoch 2:17.93 - F1: 0.2049
2026-02-13 10:39:21 - INFO - Time taken for Epoch 2:17.93 - F1: 0.2049
Time taken for Epoch 3:18.01 - F1: 0.2862
2026-02-13 10:39:39 - INFO - Time taken for Epoch 3:18.01 - F1: 0.2862
Time taken for Epoch 4:18.04 - F1: 0.3620
2026-02-13 10:39:57 - INFO - Time taken for Epoch 4:18.04 - F1: 0.3620
Time taken for Epoch 5:18.11 - F1: 0.4010
2026-02-13 10:40:15 - INFO - Time taken for Epoch 5:18.11 - F1: 0.4010
Time taken for Epoch 6:18.20 - F1: 0.4104
2026-02-13 10:40:33 - INFO - Time taken for Epoch 6:18.20 - F1: 0.4104
Time taken for Epoch 7:18.22 - F1: 0.4284
2026-02-13 10:40:51 - INFO - Time taken for Epoch 7:18.22 - F1: 0.4284
Time taken for Epoch 8:18.23 - F1: 0.4133
2026-02-13 10:41:10 - INFO - Time taken for Epoch 8:18.23 - F1: 0.4133
Time taken for Epoch 9:18.28 - F1: 0.4274
2026-02-13 10:41:28 - INFO - Time taken for Epoch 9:18.28 - F1: 0.4274
Time taken for Epoch 10:18.32 - F1: 0.4270
2026-02-13 10:41:46 - INFO - Time taken for Epoch 10:18.32 - F1: 0.4270
Best F1:0.4284 - Best Epoch:7
2026-02-13 10:41:46 - INFO - Best F1:0.4284 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 10:41:47 - INFO - Starting co-training
Time taken for Epoch 1: 25.20s - F1: 0.03212851
2026-02-13 10:42:13 - INFO - Time taken for Epoch 1: 25.20s - F1: 0.03212851
Time taken for Epoch 2: 26.35s - F1: 0.03212851
2026-02-13 10:42:39 - INFO - Time taken for Epoch 2: 26.35s - F1: 0.03212851
Time taken for Epoch 3: 25.23s - F1: 0.04247539
2026-02-13 10:43:05 - INFO - Time taken for Epoch 3: 25.23s - F1: 0.04247539
Time taken for Epoch 4: 26.32s - F1: 0.04247539
2026-02-13 10:43:31 - INFO - Time taken for Epoch 4: 26.32s - F1: 0.04247539
Time taken for Epoch 5: 25.24s - F1: 0.04247539
2026-02-13 10:43:56 - INFO - Time taken for Epoch 5: 25.24s - F1: 0.04247539
Time taken for Epoch 6: 25.25s - F1: 0.04247539
2026-02-13 10:44:22 - INFO - Time taken for Epoch 6: 25.25s - F1: 0.04247539
Time taken for Epoch 7: 25.26s - F1: 0.04247539
2026-02-13 10:44:47 - INFO - Time taken for Epoch 7: 25.26s - F1: 0.04247539
Time taken for Epoch 8: 25.38s - F1: 0.04247539
2026-02-13 10:45:12 - INFO - Time taken for Epoch 8: 25.38s - F1: 0.04247539
Time taken for Epoch 9: 25.27s - F1: 0.04247539
2026-02-13 10:45:37 - INFO - Time taken for Epoch 9: 25.27s - F1: 0.04247539
Time taken for Epoch 10: 25.25s - F1: 0.04247539
2026-02-13 10:46:03 - INFO - Time taken for Epoch 10: 25.25s - F1: 0.04247539
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 10:46:05 - INFO - Fine-tuning models
Time taken for Epoch 1:2.79 - F1: 0.0425
2026-02-13 10:46:08 - INFO - Time taken for Epoch 1:2.79 - F1: 0.0425
Time taken for Epoch 2:3.94 - F1: 0.0425
2026-02-13 10:46:12 - INFO - Time taken for Epoch 2:3.94 - F1: 0.0425
Time taken for Epoch 3:2.79 - F1: 0.0205
2026-02-13 10:46:15 - INFO - Time taken for Epoch 3:2.79 - F1: 0.0205
Time taken for Epoch 4:2.78 - F1: 0.0205
2026-02-13 10:46:18 - INFO - Time taken for Epoch 4:2.78 - F1: 0.0205
Time taken for Epoch 5:2.79 - F1: 0.0017
2026-02-13 10:46:21 - INFO - Time taken for Epoch 5:2.79 - F1: 0.0017
Time taken for Epoch 6:2.79 - F1: 0.0017
2026-02-13 10:46:23 - INFO - Time taken for Epoch 6:2.79 - F1: 0.0017
Time taken for Epoch 7:2.78 - F1: 0.0017
2026-02-13 10:46:26 - INFO - Time taken for Epoch 7:2.78 - F1: 0.0017
Time taken for Epoch 8:2.78 - F1: 0.0017
2026-02-13 10:46:29 - INFO - Time taken for Epoch 8:2.78 - F1: 0.0017
Time taken for Epoch 9:2.78 - F1: 0.0155
2026-02-13 10:46:32 - INFO - Time taken for Epoch 9:2.78 - F1: 0.0155
Time taken for Epoch 10:2.78 - F1: 0.0155
2026-02-13 10:46:35 - INFO - Time taken for Epoch 10:2.78 - F1: 0.0155
Time taken for Epoch 11:2.78 - F1: 0.0155
2026-02-13 10:46:37 - INFO - Time taken for Epoch 11:2.78 - F1: 0.0155
Performance not improving for 10 consecutive epochs.
2026-02-13 10:46:37 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-13 10:46:37 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2622
2026-02-13 10:46:45 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2622
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.26222203723563964)}
2026-02-13 10:46:45 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.26222203723563964)}

Total time taken: 486.95 seconds
2026-02-13 10:46:45 - INFO - 
Total time taken: 486.95 seconds
2026-02-13 10:46:45 - INFO - Trial 0 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.0002501465807690919, 'weight_decay': 1.1901270178909427e-05, 'batch_size': 16, 'co_train_epochs': 10, 'epoch_patience': 8}. Best is trial 0 with value: 0.042445313631754314.
Using devices: cuda, cuda
2026-02-13 10:46:45 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 10:46:45 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 10:46:45 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 10:46:45 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 2.028303632700962e-05
Weight Decay: 0.004139537280534424
Batch Size: 64
No. Epochs: 9
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-13 10:46:46 - INFO - Learning Rate: 2.028303632700962e-05
Weight Decay: 0.004139537280534424
Batch Size: 64
No. Epochs: 9
Epoch Patience: 10
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 10:46:47 - INFO - Generating initial weights
Time taken for Epoch 1:17.05 - F1: 0.0670
2026-02-13 10:47:07 - INFO - Time taken for Epoch 1:17.05 - F1: 0.0670
Time taken for Epoch 2:16.95 - F1: 0.0750
2026-02-13 10:47:24 - INFO - Time taken for Epoch 2:16.95 - F1: 0.0750
Time taken for Epoch 3:16.92 - F1: 0.1005
2026-02-13 10:47:41 - INFO - Time taken for Epoch 3:16.92 - F1: 0.1005
Time taken for Epoch 4:16.94 - F1: 0.1239
2026-02-13 10:47:58 - INFO - Time taken for Epoch 4:16.94 - F1: 0.1239
Time taken for Epoch 5:16.95 - F1: 0.1505
2026-02-13 10:48:15 - INFO - Time taken for Epoch 5:16.95 - F1: 0.1505
Time taken for Epoch 6:16.94 - F1: 0.1771
2026-02-13 10:48:32 - INFO - Time taken for Epoch 6:16.94 - F1: 0.1771
Time taken for Epoch 7:16.96 - F1: 0.1831
2026-02-13 10:48:49 - INFO - Time taken for Epoch 7:16.96 - F1: 0.1831
Time taken for Epoch 8:16.94 - F1: 0.1815
2026-02-13 10:49:06 - INFO - Time taken for Epoch 8:16.94 - F1: 0.1815
Time taken for Epoch 9:16.94 - F1: 0.1954
2026-02-13 10:49:23 - INFO - Time taken for Epoch 9:16.94 - F1: 0.1954
Best F1:0.1954 - Best Epoch:9
2026-02-13 10:49:23 - INFO - Best F1:0.1954 - Best Epoch:9
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 10:49:24 - INFO - Starting co-training
Time taken for Epoch 1: 39.66s - F1: 0.56205441
2026-02-13 10:50:04 - INFO - Time taken for Epoch 1: 39.66s - F1: 0.56205441
Time taken for Epoch 2: 40.72s - F1: 0.58349465
2026-02-13 10:50:45 - INFO - Time taken for Epoch 2: 40.72s - F1: 0.58349465
Time taken for Epoch 3: 41.02s - F1: 0.59792709
2026-02-13 10:51:26 - INFO - Time taken for Epoch 3: 41.02s - F1: 0.59792709
Time taken for Epoch 4: 40.91s - F1: 0.61622177
2026-02-13 10:52:07 - INFO - Time taken for Epoch 4: 40.91s - F1: 0.61622177
Time taken for Epoch 5: 40.97s - F1: 0.62791849
2026-02-13 10:52:48 - INFO - Time taken for Epoch 5: 40.97s - F1: 0.62791849
Time taken for Epoch 6: 41.03s - F1: 0.63387053
2026-02-13 10:53:29 - INFO - Time taken for Epoch 6: 41.03s - F1: 0.63387053
Time taken for Epoch 7: 40.82s - F1: 0.65680399
2026-02-13 10:54:10 - INFO - Time taken for Epoch 7: 40.82s - F1: 0.65680399
Time taken for Epoch 8: 41.00s - F1: 0.64333475
2026-02-13 10:54:51 - INFO - Time taken for Epoch 8: 41.00s - F1: 0.64333475
Time taken for Epoch 9: 39.71s - F1: 0.63720214
2026-02-13 10:55:30 - INFO - Time taken for Epoch 9: 39.71s - F1: 0.63720214
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 10:55:33 - INFO - Fine-tuning models
Time taken for Epoch 1:2.58 - F1: 0.6388
2026-02-13 10:55:36 - INFO - Time taken for Epoch 1:2.58 - F1: 0.6388
Time taken for Epoch 2:3.64 - F1: 0.6488
2026-02-13 10:55:39 - INFO - Time taken for Epoch 2:3.64 - F1: 0.6488
Time taken for Epoch 3:3.74 - F1: 0.6524
2026-02-13 10:55:43 - INFO - Time taken for Epoch 3:3.74 - F1: 0.6524
Time taken for Epoch 4:3.75 - F1: 0.6561
2026-02-13 10:55:47 - INFO - Time taken for Epoch 4:3.75 - F1: 0.6561
Time taken for Epoch 5:4.12 - F1: 0.6553
2026-02-13 10:55:51 - INFO - Time taken for Epoch 5:4.12 - F1: 0.6553
Time taken for Epoch 6:2.56 - F1: 0.6607
2026-02-13 10:55:53 - INFO - Time taken for Epoch 6:2.56 - F1: 0.6607
Time taken for Epoch 7:3.73 - F1: 0.6606
2026-02-13 10:55:57 - INFO - Time taken for Epoch 7:3.73 - F1: 0.6606
Time taken for Epoch 8:2.56 - F1: 0.6700
2026-02-13 10:56:00 - INFO - Time taken for Epoch 8:2.56 - F1: 0.6700
Time taken for Epoch 9:3.74 - F1: 0.6659
2026-02-13 10:56:03 - INFO - Time taken for Epoch 9:3.74 - F1: 0.6659
Time taken for Epoch 10:2.56 - F1: 0.6660
2026-02-13 10:56:06 - INFO - Time taken for Epoch 10:2.56 - F1: 0.6660
Time taken for Epoch 11:2.56 - F1: 0.6689
2026-02-13 10:56:09 - INFO - Time taken for Epoch 11:2.56 - F1: 0.6689
Time taken for Epoch 12:2.56 - F1: 0.6751
2026-02-13 10:56:11 - INFO - Time taken for Epoch 12:2.56 - F1: 0.6751
Time taken for Epoch 13:3.74 - F1: 0.6835
2026-02-13 10:56:15 - INFO - Time taken for Epoch 13:3.74 - F1: 0.6835
Time taken for Epoch 14:3.74 - F1: 0.6841
2026-02-13 10:56:19 - INFO - Time taken for Epoch 14:3.74 - F1: 0.6841
Time taken for Epoch 15:3.74 - F1: 0.6802
2026-02-13 10:56:22 - INFO - Time taken for Epoch 15:3.74 - F1: 0.6802
Time taken for Epoch 16:2.56 - F1: 0.6743
2026-02-13 10:56:25 - INFO - Time taken for Epoch 16:2.56 - F1: 0.6743
Time taken for Epoch 17:2.55 - F1: 0.6714
2026-02-13 10:56:27 - INFO - Time taken for Epoch 17:2.55 - F1: 0.6714
Time taken for Epoch 18:2.56 - F1: 0.6676
2026-02-13 10:56:30 - INFO - Time taken for Epoch 18:2.56 - F1: 0.6676
Time taken for Epoch 19:2.56 - F1: 0.6675
2026-02-13 10:56:33 - INFO - Time taken for Epoch 19:2.56 - F1: 0.6675
Time taken for Epoch 20:2.56 - F1: 0.6651
2026-02-13 10:56:35 - INFO - Time taken for Epoch 20:2.56 - F1: 0.6651
Time taken for Epoch 21:2.56 - F1: 0.6645
2026-02-13 10:56:38 - INFO - Time taken for Epoch 21:2.56 - F1: 0.6645
Time taken for Epoch 22:2.56 - F1: 0.6665
2026-02-13 10:56:40 - INFO - Time taken for Epoch 22:2.56 - F1: 0.6665
Time taken for Epoch 23:2.56 - F1: 0.6644
2026-02-13 10:56:43 - INFO - Time taken for Epoch 23:2.56 - F1: 0.6644
Time taken for Epoch 24:2.56 - F1: 0.6582
2026-02-13 10:56:45 - INFO - Time taken for Epoch 24:2.56 - F1: 0.6582
Performance not improving for 10 consecutive epochs.
2026-02-13 10:56:45 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6841 - Best Epoch:13
2026-02-13 10:56:45 - INFO - Best F1:0.6841 - Best Epoch:13
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6903, Test ECE: 0.0355
2026-02-13 10:56:52 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6903, Test ECE: 0.0355
All results: {'f1_macro': 0.6903331190978219, 'ece': np.float64(0.0355141728179864)}
2026-02-13 10:56:52 - INFO - All results: {'f1_macro': 0.6903331190978219, 'ece': np.float64(0.0355141728179864)}

Total time taken: 607.19 seconds
2026-02-13 10:56:52 - INFO - 
Total time taken: 607.19 seconds
2026-02-13 10:56:53 - INFO - Trial 1 finished with value: 0.6903331190978219 and parameters: {'learning_rate': 2.028303632700962e-05, 'weight_decay': 0.004139537280534424, 'batch_size': 64, 'co_train_epochs': 9, 'epoch_patience': 10}. Best is trial 1 with value: 0.6903331190978219.
Using devices: cuda, cuda
2026-02-13 10:56:53 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 10:56:53 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 10:56:53 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 10:56:53 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 2.917186071965003e-05
Weight Decay: 1.7587717341161053e-05
Batch Size: 8
No. Epochs: 14
Epoch Patience: 9
 Accumulation Steps: 8
2026-02-13 10:56:53 - INFO - Learning Rate: 2.917186071965003e-05
Weight Decay: 1.7587717341161053e-05
Batch Size: 8
No. Epochs: 14
Epoch Patience: 9
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 10:56:54 - INFO - Generating initial weights
Time taken for Epoch 1:19.81 - F1: 0.0813
2026-02-13 10:57:17 - INFO - Time taken for Epoch 1:19.81 - F1: 0.0813
Time taken for Epoch 2:19.78 - F1: 0.0487
2026-02-13 10:57:37 - INFO - Time taken for Epoch 2:19.78 - F1: 0.0487
Time taken for Epoch 3:19.76 - F1: 0.0155
2026-02-13 10:57:57 - INFO - Time taken for Epoch 3:19.76 - F1: 0.0155
Time taken for Epoch 4:19.79 - F1: 0.0155
2026-02-13 10:58:17 - INFO - Time taken for Epoch 4:19.79 - F1: 0.0155
Time taken for Epoch 5:19.84 - F1: 0.0155
2026-02-13 10:58:37 - INFO - Time taken for Epoch 5:19.84 - F1: 0.0155
Time taken for Epoch 6:19.82 - F1: 0.0155
2026-02-13 10:58:56 - INFO - Time taken for Epoch 6:19.82 - F1: 0.0155
Time taken for Epoch 7:19.84 - F1: 0.0286
2026-02-13 10:59:16 - INFO - Time taken for Epoch 7:19.84 - F1: 0.0286
Time taken for Epoch 8:19.86 - F1: 0.0422
2026-02-13 10:59:36 - INFO - Time taken for Epoch 8:19.86 - F1: 0.0422
Time taken for Epoch 9:19.83 - F1: 0.0420
2026-02-13 10:59:56 - INFO - Time taken for Epoch 9:19.83 - F1: 0.0420
Time taken for Epoch 10:19.85 - F1: 0.0490
2026-02-13 11:00:16 - INFO - Time taken for Epoch 10:19.85 - F1: 0.0490
Time taken for Epoch 11:19.85 - F1: 0.1203
2026-02-13 11:00:36 - INFO - Time taken for Epoch 11:19.85 - F1: 0.1203
Time taken for Epoch 12:19.83 - F1: 0.1584
2026-02-13 11:00:55 - INFO - Time taken for Epoch 12:19.83 - F1: 0.1584
Time taken for Epoch 13:19.84 - F1: 0.2341
2026-02-13 11:01:15 - INFO - Time taken for Epoch 13:19.84 - F1: 0.2341
Time taken for Epoch 14:19.85 - F1: 0.3071
2026-02-13 11:01:35 - INFO - Time taken for Epoch 14:19.85 - F1: 0.3071
Best F1:0.3071 - Best Epoch:14
2026-02-13 11:01:35 - INFO - Best F1:0.3071 - Best Epoch:14
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 11:01:36 - INFO - Starting co-training
Time taken for Epoch 1: 23.69s - F1: 0.33813547
2026-02-13 11:02:00 - INFO - Time taken for Epoch 1: 23.69s - F1: 0.33813547
Time taken for Epoch 2: 24.68s - F1: 0.41078272
2026-02-13 11:02:25 - INFO - Time taken for Epoch 2: 24.68s - F1: 0.41078272
Time taken for Epoch 3: 24.86s - F1: 0.51311556
2026-02-13 11:02:50 - INFO - Time taken for Epoch 3: 24.86s - F1: 0.51311556
Time taken for Epoch 4: 24.76s - F1: 0.57022291
2026-02-13 11:03:15 - INFO - Time taken for Epoch 4: 24.76s - F1: 0.57022291
Time taken for Epoch 5: 24.83s - F1: 0.58378511
2026-02-13 11:03:40 - INFO - Time taken for Epoch 5: 24.83s - F1: 0.58378511
Time taken for Epoch 6: 24.89s - F1: 0.58338406
2026-02-13 11:04:04 - INFO - Time taken for Epoch 6: 24.89s - F1: 0.58338406
Time taken for Epoch 7: 23.69s - F1: 0.60970078
2026-02-13 11:04:28 - INFO - Time taken for Epoch 7: 23.69s - F1: 0.60970078
Time taken for Epoch 8: 24.75s - F1: 0.61180861
2026-02-13 11:04:53 - INFO - Time taken for Epoch 8: 24.75s - F1: 0.61180861
Time taken for Epoch 9: 24.79s - F1: 0.61129165
2026-02-13 11:05:18 - INFO - Time taken for Epoch 9: 24.79s - F1: 0.61129165
Time taken for Epoch 10: 23.71s - F1: 0.61192995
2026-02-13 11:05:41 - INFO - Time taken for Epoch 10: 23.71s - F1: 0.61192995
Time taken for Epoch 11: 25.08s - F1: 0.64640796
2026-02-13 11:06:06 - INFO - Time taken for Epoch 11: 25.08s - F1: 0.64640796
Time taken for Epoch 12: 24.86s - F1: 0.64594546
2026-02-13 11:06:31 - INFO - Time taken for Epoch 12: 24.86s - F1: 0.64594546
Time taken for Epoch 13: 23.63s - F1: 0.64641031
2026-02-13 11:06:55 - INFO - Time taken for Epoch 13: 23.63s - F1: 0.64641031
Time taken for Epoch 14: 24.77s - F1: 0.63269392
2026-02-13 11:07:20 - INFO - Time taken for Epoch 14: 24.77s - F1: 0.63269392
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 11:07:22 - INFO - Fine-tuning models
Time taken for Epoch 1:3.00 - F1: 0.6367
2026-02-13 11:07:25 - INFO - Time taken for Epoch 1:3.00 - F1: 0.6367
Time taken for Epoch 2:4.03 - F1: 0.6213
2026-02-13 11:07:30 - INFO - Time taken for Epoch 2:4.03 - F1: 0.6213
Time taken for Epoch 3:2.98 - F1: 0.6152
2026-02-13 11:07:32 - INFO - Time taken for Epoch 3:2.98 - F1: 0.6152
Time taken for Epoch 4:2.98 - F1: 0.6142
2026-02-13 11:07:35 - INFO - Time taken for Epoch 4:2.98 - F1: 0.6142
Time taken for Epoch 5:2.97 - F1: 0.6141
2026-02-13 11:07:38 - INFO - Time taken for Epoch 5:2.97 - F1: 0.6141
Time taken for Epoch 6:2.98 - F1: 0.6169
2026-02-13 11:07:41 - INFO - Time taken for Epoch 6:2.98 - F1: 0.6169
Time taken for Epoch 7:2.98 - F1: 0.6162
2026-02-13 11:07:44 - INFO - Time taken for Epoch 7:2.98 - F1: 0.6162
Time taken for Epoch 8:2.98 - F1: 0.6206
2026-02-13 11:07:47 - INFO - Time taken for Epoch 8:2.98 - F1: 0.6206
Time taken for Epoch 9:2.98 - F1: 0.6260
2026-02-13 11:07:50 - INFO - Time taken for Epoch 9:2.98 - F1: 0.6260
Time taken for Epoch 10:2.98 - F1: 0.6481
2026-02-13 11:07:53 - INFO - Time taken for Epoch 10:2.98 - F1: 0.6481
Time taken for Epoch 11:4.12 - F1: 0.6421
2026-02-13 11:07:57 - INFO - Time taken for Epoch 11:4.12 - F1: 0.6421
Time taken for Epoch 12:2.98 - F1: 0.6503
2026-02-13 11:08:00 - INFO - Time taken for Epoch 12:2.98 - F1: 0.6503
Time taken for Epoch 13:4.11 - F1: 0.6459
2026-02-13 11:08:05 - INFO - Time taken for Epoch 13:4.11 - F1: 0.6459
Time taken for Epoch 14:2.98 - F1: 0.6335
2026-02-13 11:08:08 - INFO - Time taken for Epoch 14:2.98 - F1: 0.6335
Time taken for Epoch 15:2.98 - F1: 0.6336
2026-02-13 11:08:10 - INFO - Time taken for Epoch 15:2.98 - F1: 0.6336
Time taken for Epoch 16:2.98 - F1: 0.6311
2026-02-13 11:08:13 - INFO - Time taken for Epoch 16:2.98 - F1: 0.6311
Time taken for Epoch 17:2.98 - F1: 0.6234
2026-02-13 11:08:16 - INFO - Time taken for Epoch 17:2.98 - F1: 0.6234
Time taken for Epoch 18:2.98 - F1: 0.6252
2026-02-13 11:08:19 - INFO - Time taken for Epoch 18:2.98 - F1: 0.6252
Time taken for Epoch 19:2.98 - F1: 0.6230
2026-02-13 11:08:22 - INFO - Time taken for Epoch 19:2.98 - F1: 0.6230
Time taken for Epoch 20:2.98 - F1: 0.6292
2026-02-13 11:08:25 - INFO - Time taken for Epoch 20:2.98 - F1: 0.6292
Time taken for Epoch 21:2.98 - F1: 0.6324
2026-02-13 11:08:28 - INFO - Time taken for Epoch 21:2.98 - F1: 0.6324
Time taken for Epoch 22:2.99 - F1: 0.6374
2026-02-13 11:08:31 - INFO - Time taken for Epoch 22:2.99 - F1: 0.6374
Performance not improving for 10 consecutive epochs.
2026-02-13 11:08:31 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6503 - Best Epoch:11
2026-02-13 11:08:31 - INFO - Best F1:0.6503 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6652, Test ECE: 0.0569
2026-02-13 11:08:40 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6652, Test ECE: 0.0569
All results: {'f1_macro': 0.6651821300662487, 'ece': np.float64(0.05687983304046028)}
2026-02-13 11:08:40 - INFO - All results: {'f1_macro': 0.6651821300662487, 'ece': np.float64(0.05687983304046028)}

Total time taken: 707.47 seconds
2026-02-13 11:08:40 - INFO - 
Total time taken: 707.47 seconds
2026-02-13 11:08:40 - INFO - Trial 2 finished with value: 0.6651821300662487 and parameters: {'learning_rate': 2.917186071965003e-05, 'weight_decay': 1.7587717341161053e-05, 'batch_size': 8, 'co_train_epochs': 14, 'epoch_patience': 9}. Best is trial 1 with value: 0.6903331190978219.
Using devices: cuda, cuda
2026-02-13 11:08:40 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 11:08:40 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 11:08:40 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 11:08:40 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 0.00012024153471268013
Weight Decay: 0.009783374091230793
Batch Size: 8
No. Epochs: 5
Epoch Patience: 9
 Accumulation Steps: 8
2026-02-13 11:08:41 - INFO - Learning Rate: 0.00012024153471268013
Weight Decay: 0.009783374091230793
Batch Size: 8
No. Epochs: 5
Epoch Patience: 9
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 11:08:41 - INFO - Generating initial weights
Time taken for Epoch 1:19.88 - F1: 0.0155
2026-02-13 11:09:05 - INFO - Time taken for Epoch 1:19.88 - F1: 0.0155
Time taken for Epoch 2:19.78 - F1: 0.0155
2026-02-13 11:09:25 - INFO - Time taken for Epoch 2:19.78 - F1: 0.0155
Time taken for Epoch 3:19.85 - F1: 0.0358
2026-02-13 11:09:45 - INFO - Time taken for Epoch 3:19.85 - F1: 0.0358
Time taken for Epoch 4:20.03 - F1: 0.2280
2026-02-13 11:10:05 - INFO - Time taken for Epoch 4:20.03 - F1: 0.2280
Time taken for Epoch 5:19.85 - F1: 0.3643
2026-02-13 11:10:25 - INFO - Time taken for Epoch 5:19.85 - F1: 0.3643
Best F1:0.3643 - Best Epoch:5
2026-02-13 11:10:25 - INFO - Best F1:0.3643 - Best Epoch:5
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 11:10:26 - INFO - Starting co-training
Time taken for Epoch 1: 23.59s - F1: 0.36404493
2026-02-13 11:10:50 - INFO - Time taken for Epoch 1: 23.59s - F1: 0.36404493
Time taken for Epoch 2: 24.64s - F1: 0.32459151
2026-02-13 11:11:14 - INFO - Time taken for Epoch 2: 24.64s - F1: 0.32459151
Time taken for Epoch 3: 23.62s - F1: 0.40165974
2026-02-13 11:11:38 - INFO - Time taken for Epoch 3: 23.62s - F1: 0.40165974
Time taken for Epoch 4: 24.69s - F1: 0.23967656
2026-02-13 11:12:03 - INFO - Time taken for Epoch 4: 24.69s - F1: 0.23967656
Time taken for Epoch 5: 23.70s - F1: 0.21078148
2026-02-13 11:12:26 - INFO - Time taken for Epoch 5: 23.70s - F1: 0.21078148
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 11:12:29 - INFO - Fine-tuning models
Time taken for Epoch 1:3.00 - F1: 0.4303
2026-02-13 11:12:32 - INFO - Time taken for Epoch 1:3.00 - F1: 0.4303
Time taken for Epoch 2:3.97 - F1: 0.4837
2026-02-13 11:12:36 - INFO - Time taken for Epoch 2:3.97 - F1: 0.4837
Time taken for Epoch 3:4.14 - F1: 0.4590
2026-02-13 11:12:40 - INFO - Time taken for Epoch 3:4.14 - F1: 0.4590
Time taken for Epoch 4:2.98 - F1: 0.4594
2026-02-13 11:12:43 - INFO - Time taken for Epoch 4:2.98 - F1: 0.4594
Time taken for Epoch 5:2.97 - F1: 0.4655
2026-02-13 11:12:46 - INFO - Time taken for Epoch 5:2.97 - F1: 0.4655
Time taken for Epoch 6:2.97 - F1: 0.4488
2026-02-13 11:12:49 - INFO - Time taken for Epoch 6:2.97 - F1: 0.4488
Time taken for Epoch 7:2.97 - F1: 0.4657
2026-02-13 11:12:52 - INFO - Time taken for Epoch 7:2.97 - F1: 0.4657
Time taken for Epoch 8:2.98 - F1: 0.4900
2026-02-13 11:12:55 - INFO - Time taken for Epoch 8:2.98 - F1: 0.4900
Time taken for Epoch 9:4.06 - F1: 0.5053
2026-02-13 11:12:59 - INFO - Time taken for Epoch 9:4.06 - F1: 0.5053
Time taken for Epoch 10:4.12 - F1: 0.5087
2026-02-13 11:13:03 - INFO - Time taken for Epoch 10:4.12 - F1: 0.5087
Time taken for Epoch 11:4.13 - F1: 0.5197
2026-02-13 11:13:07 - INFO - Time taken for Epoch 11:4.13 - F1: 0.5197
Time taken for Epoch 12:4.13 - F1: 0.5115
2026-02-13 11:13:11 - INFO - Time taken for Epoch 12:4.13 - F1: 0.5115
Time taken for Epoch 13:3.02 - F1: 0.5259
2026-02-13 11:13:14 - INFO - Time taken for Epoch 13:3.02 - F1: 0.5259
Time taken for Epoch 14:4.12 - F1: 0.5329
2026-02-13 11:13:18 - INFO - Time taken for Epoch 14:4.12 - F1: 0.5329
Time taken for Epoch 15:4.77 - F1: 0.5273
2026-02-13 11:13:23 - INFO - Time taken for Epoch 15:4.77 - F1: 0.5273
Time taken for Epoch 16:3.01 - F1: 0.5296
2026-02-13 11:13:26 - INFO - Time taken for Epoch 16:3.01 - F1: 0.5296
Time taken for Epoch 17:3.01 - F1: 0.5308
2026-02-13 11:13:29 - INFO - Time taken for Epoch 17:3.01 - F1: 0.5308
Time taken for Epoch 18:3.01 - F1: 0.5419
2026-02-13 11:13:32 - INFO - Time taken for Epoch 18:3.01 - F1: 0.5419
Time taken for Epoch 19:4.17 - F1: 0.5379
2026-02-13 11:13:36 - INFO - Time taken for Epoch 19:4.17 - F1: 0.5379
Time taken for Epoch 20:2.96 - F1: 0.5434
2026-02-13 11:13:39 - INFO - Time taken for Epoch 20:2.96 - F1: 0.5434
Time taken for Epoch 21:4.08 - F1: 0.5500
2026-02-13 11:13:43 - INFO - Time taken for Epoch 21:4.08 - F1: 0.5500
Time taken for Epoch 22:4.08 - F1: 0.5452
2026-02-13 11:13:47 - INFO - Time taken for Epoch 22:4.08 - F1: 0.5452
Time taken for Epoch 23:2.96 - F1: 0.5409
2026-02-13 11:13:50 - INFO - Time taken for Epoch 23:2.96 - F1: 0.5409
Time taken for Epoch 24:2.96 - F1: 0.5376
2026-02-13 11:13:53 - INFO - Time taken for Epoch 24:2.96 - F1: 0.5376
Time taken for Epoch 25:2.97 - F1: 0.5405
2026-02-13 11:13:56 - INFO - Time taken for Epoch 25:2.97 - F1: 0.5405
Time taken for Epoch 26:2.96 - F1: 0.5462
2026-02-13 11:13:59 - INFO - Time taken for Epoch 26:2.96 - F1: 0.5462
Time taken for Epoch 27:2.97 - F1: 0.5464
2026-02-13 11:14:02 - INFO - Time taken for Epoch 27:2.97 - F1: 0.5464
Time taken for Epoch 28:2.96 - F1: 0.5452
2026-02-13 11:14:05 - INFO - Time taken for Epoch 28:2.96 - F1: 0.5452
Time taken for Epoch 29:2.96 - F1: 0.5526
2026-02-13 11:14:08 - INFO - Time taken for Epoch 29:2.96 - F1: 0.5526
Time taken for Epoch 30:4.05 - F1: 0.5625
2026-02-13 11:14:12 - INFO - Time taken for Epoch 30:4.05 - F1: 0.5625
Time taken for Epoch 31:4.07 - F1: 0.5688
2026-02-13 11:14:16 - INFO - Time taken for Epoch 31:4.07 - F1: 0.5688
Time taken for Epoch 32:4.07 - F1: 0.5705
2026-02-13 11:14:20 - INFO - Time taken for Epoch 32:4.07 - F1: 0.5705
Time taken for Epoch 33:4.05 - F1: 0.5633
2026-02-13 11:14:24 - INFO - Time taken for Epoch 33:4.05 - F1: 0.5633
Time taken for Epoch 34:2.96 - F1: 0.5571
2026-02-13 11:14:27 - INFO - Time taken for Epoch 34:2.96 - F1: 0.5571
Time taken for Epoch 35:2.95 - F1: 0.5559
2026-02-13 11:14:30 - INFO - Time taken for Epoch 35:2.95 - F1: 0.5559
Time taken for Epoch 36:2.96 - F1: 0.5544
2026-02-13 11:14:33 - INFO - Time taken for Epoch 36:2.96 - F1: 0.5544
Time taken for Epoch 37:2.96 - F1: 0.5546
2026-02-13 11:14:36 - INFO - Time taken for Epoch 37:2.96 - F1: 0.5546
Time taken for Epoch 38:2.96 - F1: 0.5531
2026-02-13 11:14:39 - INFO - Time taken for Epoch 38:2.96 - F1: 0.5531
Time taken for Epoch 39:2.96 - F1: 0.5529
2026-02-13 11:14:42 - INFO - Time taken for Epoch 39:2.96 - F1: 0.5529
Time taken for Epoch 40:2.96 - F1: 0.5554
2026-02-13 11:14:45 - INFO - Time taken for Epoch 40:2.96 - F1: 0.5554
Time taken for Epoch 41:2.97 - F1: 0.5583
2026-02-13 11:14:48 - INFO - Time taken for Epoch 41:2.97 - F1: 0.5583
Time taken for Epoch 42:3.12 - F1: 0.5681
2026-02-13 11:14:51 - INFO - Time taken for Epoch 42:3.12 - F1: 0.5681
Performance not improving for 10 consecutive epochs.
2026-02-13 11:14:51 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.5705 - Best Epoch:31
2026-02-13 11:14:51 - INFO - Best F1:0.5705 - Best Epoch:31
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5765, Test ECE: 0.1874
2026-02-13 11:14:59 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5765, Test ECE: 0.1874
All results: {'f1_macro': 0.5764875319712451, 'ece': np.float64(0.18739797726069413)}
2026-02-13 11:14:59 - INFO - All results: {'f1_macro': 0.5764875319712451, 'ece': np.float64(0.18739797726069413)}

Total time taken: 379.41 seconds
2026-02-13 11:14:59 - INFO - 
Total time taken: 379.41 seconds
2026-02-13 11:14:59 - INFO - Trial 3 finished with value: 0.5764875319712451 and parameters: {'learning_rate': 0.00012024153471268013, 'weight_decay': 0.009783374091230793, 'batch_size': 8, 'co_train_epochs': 5, 'epoch_patience': 9}. Best is trial 1 with value: 0.6903331190978219.
Using devices: cuda, cuda
2026-02-13 11:14:59 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 11:14:59 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 11:14:59 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 11:14:59 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 1.6002330808102706e-05
Weight Decay: 0.0043391454210210696
Batch Size: 16
No. Epochs: 11
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-13 11:15:00 - INFO - Learning Rate: 1.6002330808102706e-05
Weight Decay: 0.0043391454210210696
Batch Size: 16
No. Epochs: 11
Epoch Patience: 4
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 11:15:01 - INFO - Generating initial weights
Time taken for Epoch 1:18.35 - F1: 0.0552
2026-02-13 11:15:23 - INFO - Time taken for Epoch 1:18.35 - F1: 0.0552
Time taken for Epoch 2:18.24 - F1: 0.0673
2026-02-13 11:15:41 - INFO - Time taken for Epoch 2:18.24 - F1: 0.0673
Time taken for Epoch 3:18.26 - F1: 0.1040
2026-02-13 11:15:59 - INFO - Time taken for Epoch 3:18.26 - F1: 0.1040
Time taken for Epoch 4:18.29 - F1: 0.1075
2026-02-13 11:16:18 - INFO - Time taken for Epoch 4:18.29 - F1: 0.1075
Time taken for Epoch 5:18.28 - F1: 0.1197
2026-02-13 11:16:36 - INFO - Time taken for Epoch 5:18.28 - F1: 0.1197
Time taken for Epoch 6:18.29 - F1: 0.1193
2026-02-13 11:16:54 - INFO - Time taken for Epoch 6:18.29 - F1: 0.1193
Time taken for Epoch 7:18.34 - F1: 0.1136
2026-02-13 11:17:13 - INFO - Time taken for Epoch 7:18.34 - F1: 0.1136
Time taken for Epoch 8:18.31 - F1: 0.1173
2026-02-13 11:17:31 - INFO - Time taken for Epoch 8:18.31 - F1: 0.1173
Time taken for Epoch 9:18.30 - F1: 0.1216
2026-02-13 11:17:49 - INFO - Time taken for Epoch 9:18.30 - F1: 0.1216
Time taken for Epoch 10:18.31 - F1: 0.1226
2026-02-13 11:18:08 - INFO - Time taken for Epoch 10:18.31 - F1: 0.1226
Time taken for Epoch 11:18.30 - F1: 0.1250
2026-02-13 11:18:26 - INFO - Time taken for Epoch 11:18.30 - F1: 0.1250
Best F1:0.1250 - Best Epoch:11
2026-02-13 11:18:26 - INFO - Best F1:0.1250 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 11:18:27 - INFO - Starting co-training
Time taken for Epoch 1: 25.23s - F1: 0.34382513
2026-02-13 11:18:53 - INFO - Time taken for Epoch 1: 25.23s - F1: 0.34382513
Time taken for Epoch 2: 26.24s - F1: 0.40637331
2026-02-13 11:19:19 - INFO - Time taken for Epoch 2: 26.24s - F1: 0.40637331
Time taken for Epoch 3: 26.33s - F1: 0.53133469
2026-02-13 11:19:45 - INFO - Time taken for Epoch 3: 26.33s - F1: 0.53133469
Time taken for Epoch 4: 26.33s - F1: 0.55876326
2026-02-13 11:20:12 - INFO - Time taken for Epoch 4: 26.33s - F1: 0.55876326
Time taken for Epoch 5: 26.33s - F1: 0.59575083
2026-02-13 11:20:38 - INFO - Time taken for Epoch 5: 26.33s - F1: 0.59575083
Time taken for Epoch 6: 26.54s - F1: 0.58919943
2026-02-13 11:21:04 - INFO - Time taken for Epoch 6: 26.54s - F1: 0.58919943
Time taken for Epoch 7: 25.26s - F1: 0.60745265
2026-02-13 11:21:30 - INFO - Time taken for Epoch 7: 25.26s - F1: 0.60745265
Time taken for Epoch 8: 26.31s - F1: 0.61997217
2026-02-13 11:21:56 - INFO - Time taken for Epoch 8: 26.31s - F1: 0.61997217
Time taken for Epoch 9: 26.56s - F1: 0.61681626
2026-02-13 11:22:23 - INFO - Time taken for Epoch 9: 26.56s - F1: 0.61681626
Time taken for Epoch 10: 25.27s - F1: 0.62405829
2026-02-13 11:22:48 - INFO - Time taken for Epoch 10: 25.27s - F1: 0.62405829
Time taken for Epoch 11: 26.38s - F1: 0.62439252
2026-02-13 11:23:14 - INFO - Time taken for Epoch 11: 26.38s - F1: 0.62439252
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 11:23:18 - INFO - Fine-tuning models
Time taken for Epoch 1:2.78 - F1: 0.6372
2026-02-13 11:23:21 - INFO - Time taken for Epoch 1:2.78 - F1: 0.6372
Time taken for Epoch 2:3.82 - F1: 0.6443
2026-02-13 11:23:25 - INFO - Time taken for Epoch 2:3.82 - F1: 0.6443
Time taken for Epoch 3:3.92 - F1: 0.6353
2026-02-13 11:23:29 - INFO - Time taken for Epoch 3:3.92 - F1: 0.6353
Time taken for Epoch 4:2.76 - F1: 0.6222
2026-02-13 11:23:32 - INFO - Time taken for Epoch 4:2.76 - F1: 0.6222
Time taken for Epoch 5:2.76 - F1: 0.6282
2026-02-13 11:23:34 - INFO - Time taken for Epoch 5:2.76 - F1: 0.6282
Time taken for Epoch 6:2.77 - F1: 0.6291
2026-02-13 11:23:37 - INFO - Time taken for Epoch 6:2.77 - F1: 0.6291
Time taken for Epoch 7:2.77 - F1: 0.6274
2026-02-13 11:23:40 - INFO - Time taken for Epoch 7:2.77 - F1: 0.6274
Time taken for Epoch 8:2.77 - F1: 0.6213
2026-02-13 11:23:43 - INFO - Time taken for Epoch 8:2.77 - F1: 0.6213
Time taken for Epoch 9:2.77 - F1: 0.6164
2026-02-13 11:23:45 - INFO - Time taken for Epoch 9:2.77 - F1: 0.6164
Time taken for Epoch 10:2.77 - F1: 0.6278
2026-02-13 11:23:48 - INFO - Time taken for Epoch 10:2.77 - F1: 0.6278
Time taken for Epoch 11:2.78 - F1: 0.6258
2026-02-13 11:23:51 - INFO - Time taken for Epoch 11:2.78 - F1: 0.6258
Time taken for Epoch 12:2.76 - F1: 0.6243
2026-02-13 11:23:54 - INFO - Time taken for Epoch 12:2.76 - F1: 0.6243
Performance not improving for 10 consecutive epochs.
2026-02-13 11:23:54 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6443 - Best Epoch:1
2026-02-13 11:23:54 - INFO - Best F1:0.6443 - Best Epoch:1
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6617, Test ECE: 0.0344
2026-02-13 11:24:01 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6617, Test ECE: 0.0344
All results: {'f1_macro': 0.6617413809927659, 'ece': np.float64(0.03441101951430057)}
2026-02-13 11:24:01 - INFO - All results: {'f1_macro': 0.6617413809927659, 'ece': np.float64(0.03441101951430057)}

Total time taken: 541.73 seconds
2026-02-13 11:24:01 - INFO - 
Total time taken: 541.73 seconds
2026-02-13 11:24:01 - INFO - Trial 4 finished with value: 0.6617413809927659 and parameters: {'learning_rate': 1.6002330808102706e-05, 'weight_decay': 0.0043391454210210696, 'batch_size': 16, 'co_train_epochs': 11, 'epoch_patience': 4}. Best is trial 1 with value: 0.6903331190978219.
Using devices: cuda, cuda
2026-02-13 11:24:01 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 11:24:01 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 11:24:01 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 11:24:01 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 1.2588409885845465e-05
Weight Decay: 4.35418156981855e-05
Batch Size: 32
No. Epochs: 18
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-13 11:24:02 - INFO - Learning Rate: 1.2588409885845465e-05
Weight Decay: 4.35418156981855e-05
Batch Size: 32
No. Epochs: 18
Epoch Patience: 7
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 11:24:03 - INFO - Generating initial weights
Time taken for Epoch 1:17.82 - F1: 0.0397
2026-02-13 11:24:24 - INFO - Time taken for Epoch 1:17.82 - F1: 0.0397
Time taken for Epoch 2:17.74 - F1: 0.0509
2026-02-13 11:24:42 - INFO - Time taken for Epoch 2:17.74 - F1: 0.0509
Time taken for Epoch 3:17.76 - F1: 0.0665
2026-02-13 11:25:00 - INFO - Time taken for Epoch 3:17.76 - F1: 0.0665
Time taken for Epoch 4:17.78 - F1: 0.0695
2026-02-13 11:25:17 - INFO - Time taken for Epoch 4:17.78 - F1: 0.0695
Time taken for Epoch 5:17.77 - F1: 0.0669
2026-02-13 11:25:35 - INFO - Time taken for Epoch 5:17.77 - F1: 0.0669
Time taken for Epoch 6:17.77 - F1: 0.0693
2026-02-13 11:25:53 - INFO - Time taken for Epoch 6:17.77 - F1: 0.0693
Time taken for Epoch 7:17.82 - F1: 0.0702
2026-02-13 11:26:11 - INFO - Time taken for Epoch 7:17.82 - F1: 0.0702
Time taken for Epoch 8:17.82 - F1: 0.0677
2026-02-13 11:26:28 - INFO - Time taken for Epoch 8:17.82 - F1: 0.0677
Time taken for Epoch 9:17.80 - F1: 0.0657
2026-02-13 11:26:46 - INFO - Time taken for Epoch 9:17.80 - F1: 0.0657
Time taken for Epoch 10:17.82 - F1: 0.0664
2026-02-13 11:27:04 - INFO - Time taken for Epoch 10:17.82 - F1: 0.0664
Time taken for Epoch 11:17.82 - F1: 0.0668
2026-02-13 11:27:22 - INFO - Time taken for Epoch 11:17.82 - F1: 0.0668
Time taken for Epoch 12:17.80 - F1: 0.0675
2026-02-13 11:27:40 - INFO - Time taken for Epoch 12:17.80 - F1: 0.0675
Time taken for Epoch 13:17.79 - F1: 0.0712
2026-02-13 11:27:58 - INFO - Time taken for Epoch 13:17.79 - F1: 0.0712
Time taken for Epoch 14:17.82 - F1: 0.0710
2026-02-13 11:28:15 - INFO - Time taken for Epoch 14:17.82 - F1: 0.0710
Time taken for Epoch 15:17.82 - F1: 0.0740
2026-02-13 11:28:33 - INFO - Time taken for Epoch 15:17.82 - F1: 0.0740
Time taken for Epoch 16:17.80 - F1: 0.0761
2026-02-13 11:28:51 - INFO - Time taken for Epoch 16:17.80 - F1: 0.0761
Time taken for Epoch 17:17.85 - F1: 0.0766
2026-02-13 11:29:09 - INFO - Time taken for Epoch 17:17.85 - F1: 0.0766
Time taken for Epoch 18:17.83 - F1: 0.0773
2026-02-13 11:29:27 - INFO - Time taken for Epoch 18:17.83 - F1: 0.0773
Best F1:0.0773 - Best Epoch:18
2026-02-13 11:29:27 - INFO - Best F1:0.0773 - Best Epoch:18
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 11:29:28 - INFO - Starting co-training
Time taken for Epoch 1: 30.35s - F1: 0.34991985
2026-02-13 11:29:58 - INFO - Time taken for Epoch 1: 30.35s - F1: 0.34991985
Time taken for Epoch 2: 31.40s - F1: 0.45506024
2026-02-13 11:30:30 - INFO - Time taken for Epoch 2: 31.40s - F1: 0.45506024
Time taken for Epoch 3: 31.81s - F1: 0.57757182
2026-02-13 11:31:02 - INFO - Time taken for Epoch 3: 31.81s - F1: 0.57757182
Time taken for Epoch 4: 31.53s - F1: 0.59146428
2026-02-13 11:31:33 - INFO - Time taken for Epoch 4: 31.53s - F1: 0.59146428
Time taken for Epoch 5: 31.51s - F1: 0.59654453
2026-02-13 11:32:05 - INFO - Time taken for Epoch 5: 31.51s - F1: 0.59654453
Time taken for Epoch 6: 31.52s - F1: 0.59565324
2026-02-13 11:32:36 - INFO - Time taken for Epoch 6: 31.52s - F1: 0.59565324
Time taken for Epoch 7: 30.36s - F1: 0.60362942
2026-02-13 11:33:07 - INFO - Time taken for Epoch 7: 30.36s - F1: 0.60362942
Time taken for Epoch 8: 31.50s - F1: 0.60219154
2026-02-13 11:33:38 - INFO - Time taken for Epoch 8: 31.50s - F1: 0.60219154
Time taken for Epoch 9: 30.38s - F1: 0.62542106
2026-02-13 11:34:08 - INFO - Time taken for Epoch 9: 30.38s - F1: 0.62542106
Time taken for Epoch 10: 31.49s - F1: 0.62152452
2026-02-13 11:34:40 - INFO - Time taken for Epoch 10: 31.49s - F1: 0.62152452
Time taken for Epoch 11: 30.40s - F1: 0.62503632
2026-02-13 11:35:10 - INFO - Time taken for Epoch 11: 30.40s - F1: 0.62503632
Time taken for Epoch 12: 30.40s - F1: 0.62643788
2026-02-13 11:35:41 - INFO - Time taken for Epoch 12: 30.40s - F1: 0.62643788
Time taken for Epoch 13: 31.48s - F1: 0.63502104
2026-02-13 11:36:12 - INFO - Time taken for Epoch 13: 31.48s - F1: 0.63502104
Time taken for Epoch 14: 31.49s - F1: 0.62996368
2026-02-13 11:36:44 - INFO - Time taken for Epoch 14: 31.49s - F1: 0.62996368
Time taken for Epoch 15: 30.39s - F1: 0.66483234
2026-02-13 11:37:14 - INFO - Time taken for Epoch 15: 30.39s - F1: 0.66483234
Time taken for Epoch 16: 31.47s - F1: 0.63887353
2026-02-13 11:37:46 - INFO - Time taken for Epoch 16: 31.47s - F1: 0.63887353
Time taken for Epoch 17: 30.37s - F1: 0.63844702
2026-02-13 11:38:16 - INFO - Time taken for Epoch 17: 30.37s - F1: 0.63844702
Time taken for Epoch 18: 30.37s - F1: 0.62639759
2026-02-13 11:38:46 - INFO - Time taken for Epoch 18: 30.37s - F1: 0.62639759
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 11:38:49 - INFO - Fine-tuning models
Time taken for Epoch 1:2.69 - F1: 0.6756
2026-02-13 11:38:52 - INFO - Time taken for Epoch 1:2.69 - F1: 0.6756
Time taken for Epoch 2:3.94 - F1: 0.6505
2026-02-13 11:38:56 - INFO - Time taken for Epoch 2:3.94 - F1: 0.6505
Time taken for Epoch 3:2.68 - F1: 0.6760
2026-02-13 11:38:58 - INFO - Time taken for Epoch 3:2.68 - F1: 0.6760
Time taken for Epoch 4:3.87 - F1: 0.6781
2026-02-13 11:39:02 - INFO - Time taken for Epoch 4:3.87 - F1: 0.6781
Time taken for Epoch 5:4.46 - F1: 0.6666
2026-02-13 11:39:07 - INFO - Time taken for Epoch 5:4.46 - F1: 0.6666
Time taken for Epoch 6:2.67 - F1: 0.6638
2026-02-13 11:39:09 - INFO - Time taken for Epoch 6:2.67 - F1: 0.6638
Time taken for Epoch 7:2.67 - F1: 0.6594
2026-02-13 11:39:12 - INFO - Time taken for Epoch 7:2.67 - F1: 0.6594
Time taken for Epoch 8:2.68 - F1: 0.6615
2026-02-13 11:39:15 - INFO - Time taken for Epoch 8:2.68 - F1: 0.6615
Time taken for Epoch 9:2.68 - F1: 0.6689
2026-02-13 11:39:17 - INFO - Time taken for Epoch 9:2.68 - F1: 0.6689
Time taken for Epoch 10:2.67 - F1: 0.6673
2026-02-13 11:39:20 - INFO - Time taken for Epoch 10:2.67 - F1: 0.6673
Time taken for Epoch 11:2.68 - F1: 0.6650
2026-02-13 11:39:23 - INFO - Time taken for Epoch 11:2.68 - F1: 0.6650
Time taken for Epoch 12:2.68 - F1: 0.6629
2026-02-13 11:39:25 - INFO - Time taken for Epoch 12:2.68 - F1: 0.6629
Time taken for Epoch 13:2.69 - F1: 0.6634
2026-02-13 11:39:28 - INFO - Time taken for Epoch 13:2.69 - F1: 0.6634
Time taken for Epoch 14:2.68 - F1: 0.6570
2026-02-13 11:39:31 - INFO - Time taken for Epoch 14:2.68 - F1: 0.6570
Performance not improving for 10 consecutive epochs.
2026-02-13 11:39:31 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6781 - Best Epoch:3
2026-02-13 11:39:31 - INFO - Best F1:0.6781 - Best Epoch:3
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6707, Test ECE: 0.0221
2026-02-13 11:39:38 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6707, Test ECE: 0.0221
All results: {'f1_macro': 0.6706924623463729, 'ece': np.float64(0.022094644383976868)}
2026-02-13 11:39:38 - INFO - All results: {'f1_macro': 0.6706924623463729, 'ece': np.float64(0.022094644383976868)}

Total time taken: 936.82 seconds
2026-02-13 11:39:38 - INFO - 
Total time taken: 936.82 seconds
2026-02-13 11:39:38 - INFO - Trial 5 finished with value: 0.6706924623463729 and parameters: {'learning_rate': 1.2588409885845465e-05, 'weight_decay': 4.35418156981855e-05, 'batch_size': 32, 'co_train_epochs': 18, 'epoch_patience': 7}. Best is trial 1 with value: 0.6903331190978219.
Using devices: cuda, cuda
2026-02-13 11:39:38 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 11:39:38 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 11:39:38 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 11:39:38 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 1.3105107825049032e-05
Weight Decay: 0.002423290287691337
Batch Size: 8
No. Epochs: 8
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-13 11:39:39 - INFO - Learning Rate: 1.3105107825049032e-05
Weight Decay: 0.002423290287691337
Batch Size: 8
No. Epochs: 8
Epoch Patience: 4
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 11:39:39 - INFO - Generating initial weights
Time taken for Epoch 1:19.76 - F1: 0.0600
2026-02-13 11:40:03 - INFO - Time taken for Epoch 1:19.76 - F1: 0.0600
Time taken for Epoch 2:19.66 - F1: 0.0813
2026-02-13 11:40:23 - INFO - Time taken for Epoch 2:19.66 - F1: 0.0813
Time taken for Epoch 3:19.67 - F1: 0.0764
2026-02-13 11:40:42 - INFO - Time taken for Epoch 3:19.67 - F1: 0.0764
Time taken for Epoch 4:19.69 - F1: 0.0427
2026-02-13 11:41:02 - INFO - Time taken for Epoch 4:19.69 - F1: 0.0427
Time taken for Epoch 5:19.71 - F1: 0.0235
2026-02-13 11:41:22 - INFO - Time taken for Epoch 5:19.71 - F1: 0.0235
Time taken for Epoch 6:19.74 - F1: 0.0156
2026-02-13 11:41:41 - INFO - Time taken for Epoch 6:19.74 - F1: 0.0156
Time taken for Epoch 7:19.73 - F1: 0.0155
2026-02-13 11:42:01 - INFO - Time taken for Epoch 7:19.73 - F1: 0.0155
Time taken for Epoch 8:19.72 - F1: 0.0155
2026-02-13 11:42:21 - INFO - Time taken for Epoch 8:19.72 - F1: 0.0155
Best F1:0.0813 - Best Epoch:2
2026-02-13 11:42:21 - INFO - Best F1:0.0813 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 11:42:22 - INFO - Starting co-training
Time taken for Epoch 1: 23.56s - F1: 0.22435752
2026-02-13 11:42:46 - INFO - Time taken for Epoch 1: 23.56s - F1: 0.22435752
Time taken for Epoch 2: 24.64s - F1: 0.24610358
2026-02-13 11:43:10 - INFO - Time taken for Epoch 2: 24.64s - F1: 0.24610358
Time taken for Epoch 3: 24.65s - F1: 0.37461306
2026-02-13 11:43:35 - INFO - Time taken for Epoch 3: 24.65s - F1: 0.37461306
Time taken for Epoch 4: 24.73s - F1: 0.45018966
2026-02-13 11:44:00 - INFO - Time taken for Epoch 4: 24.73s - F1: 0.45018966
Time taken for Epoch 5: 24.62s - F1: 0.47036375
2026-02-13 11:44:24 - INFO - Time taken for Epoch 5: 24.62s - F1: 0.47036375
Time taken for Epoch 6: 24.71s - F1: 0.52483015
2026-02-13 11:44:49 - INFO - Time taken for Epoch 6: 24.71s - F1: 0.52483015
Time taken for Epoch 7: 24.74s - F1: 0.53742510
2026-02-13 11:45:14 - INFO - Time taken for Epoch 7: 24.74s - F1: 0.53742510
Time taken for Epoch 8: 24.66s - F1: 0.54142924
2026-02-13 11:45:39 - INFO - Time taken for Epoch 8: 24.66s - F1: 0.54142924
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 11:45:42 - INFO - Fine-tuning models
Time taken for Epoch 1:3.03 - F1: 0.5425
2026-02-13 11:45:45 - INFO - Time taken for Epoch 1:3.03 - F1: 0.5425
Time taken for Epoch 2:4.01 - F1: 0.5408
2026-02-13 11:45:49 - INFO - Time taken for Epoch 2:4.01 - F1: 0.5408
Time taken for Epoch 3:3.01 - F1: 0.5473
2026-02-13 11:45:52 - INFO - Time taken for Epoch 3:3.01 - F1: 0.5473
Time taken for Epoch 4:4.09 - F1: 0.5533
2026-02-13 11:45:56 - INFO - Time taken for Epoch 4:4.09 - F1: 0.5533
Time taken for Epoch 5:4.10 - F1: 0.5477
2026-02-13 11:46:00 - INFO - Time taken for Epoch 5:4.10 - F1: 0.5477
Time taken for Epoch 6:3.01 - F1: 0.5515
2026-02-13 11:46:03 - INFO - Time taken for Epoch 6:3.01 - F1: 0.5515
Time taken for Epoch 7:3.00 - F1: 0.5458
2026-02-13 11:46:06 - INFO - Time taken for Epoch 7:3.00 - F1: 0.5458
Time taken for Epoch 8:3.01 - F1: 0.5559
2026-02-13 11:46:09 - INFO - Time taken for Epoch 8:3.01 - F1: 0.5559
Time taken for Epoch 9:4.09 - F1: 0.5664
2026-02-13 11:46:13 - INFO - Time taken for Epoch 9:4.09 - F1: 0.5664
Time taken for Epoch 10:4.10 - F1: 0.5712
2026-02-13 11:46:18 - INFO - Time taken for Epoch 10:4.10 - F1: 0.5712
Time taken for Epoch 11:4.08 - F1: 0.5676
2026-02-13 11:46:22 - INFO - Time taken for Epoch 11:4.08 - F1: 0.5676
Time taken for Epoch 12:3.01 - F1: 0.5743
2026-02-13 11:46:25 - INFO - Time taken for Epoch 12:3.01 - F1: 0.5743
Time taken for Epoch 13:4.11 - F1: 0.5747
2026-02-13 11:46:29 - INFO - Time taken for Epoch 13:4.11 - F1: 0.5747
Time taken for Epoch 14:4.10 - F1: 0.5729
2026-02-13 11:46:33 - INFO - Time taken for Epoch 14:4.10 - F1: 0.5729
Time taken for Epoch 15:3.00 - F1: 0.5760
2026-02-13 11:46:36 - INFO - Time taken for Epoch 15:3.00 - F1: 0.5760
Time taken for Epoch 16:4.09 - F1: 0.5817
2026-02-13 11:46:40 - INFO - Time taken for Epoch 16:4.09 - F1: 0.5817
Time taken for Epoch 17:4.09 - F1: 0.5819
2026-02-13 11:46:44 - INFO - Time taken for Epoch 17:4.09 - F1: 0.5819
Time taken for Epoch 18:4.11 - F1: 0.5848
2026-02-13 11:46:48 - INFO - Time taken for Epoch 18:4.11 - F1: 0.5848
Time taken for Epoch 19:4.11 - F1: 0.5865
2026-02-13 11:46:52 - INFO - Time taken for Epoch 19:4.11 - F1: 0.5865
Time taken for Epoch 20:4.11 - F1: 0.5944
2026-02-13 11:46:56 - INFO - Time taken for Epoch 20:4.11 - F1: 0.5944
Time taken for Epoch 21:4.11 - F1: 0.5946
2026-02-13 11:47:01 - INFO - Time taken for Epoch 21:4.11 - F1: 0.5946
Time taken for Epoch 22:4.11 - F1: 0.5939
2026-02-13 11:47:05 - INFO - Time taken for Epoch 22:4.11 - F1: 0.5939
Time taken for Epoch 23:3.00 - F1: 0.5996
2026-02-13 11:47:08 - INFO - Time taken for Epoch 23:3.00 - F1: 0.5996
Time taken for Epoch 24:4.11 - F1: 0.6074
2026-02-13 11:47:12 - INFO - Time taken for Epoch 24:4.11 - F1: 0.6074
Time taken for Epoch 25:4.11 - F1: 0.6083
2026-02-13 11:47:16 - INFO - Time taken for Epoch 25:4.11 - F1: 0.6083
Time taken for Epoch 26:4.13 - F1: 0.6100
2026-02-13 11:47:20 - INFO - Time taken for Epoch 26:4.13 - F1: 0.6100
Time taken for Epoch 27:4.10 - F1: 0.6105
2026-02-13 11:47:24 - INFO - Time taken for Epoch 27:4.10 - F1: 0.6105
Time taken for Epoch 28:4.11 - F1: 0.6073
2026-02-13 11:47:28 - INFO - Time taken for Epoch 28:4.11 - F1: 0.6073
Time taken for Epoch 29:3.00 - F1: 0.6078
2026-02-13 11:47:31 - INFO - Time taken for Epoch 29:3.00 - F1: 0.6078
Time taken for Epoch 30:3.00 - F1: 0.6093
2026-02-13 11:47:34 - INFO - Time taken for Epoch 30:3.00 - F1: 0.6093
Time taken for Epoch 31:3.00 - F1: 0.6088
2026-02-13 11:47:37 - INFO - Time taken for Epoch 31:3.00 - F1: 0.6088
Time taken for Epoch 32:3.00 - F1: 0.6105
2026-02-13 11:47:40 - INFO - Time taken for Epoch 32:3.00 - F1: 0.6105
Time taken for Epoch 33:3.01 - F1: 0.6050
2026-02-13 11:47:43 - INFO - Time taken for Epoch 33:3.01 - F1: 0.6050
Time taken for Epoch 34:3.00 - F1: 0.6074
2026-02-13 11:47:46 - INFO - Time taken for Epoch 34:3.00 - F1: 0.6074
Time taken for Epoch 35:3.00 - F1: 0.6083
2026-02-13 11:47:49 - INFO - Time taken for Epoch 35:3.00 - F1: 0.6083
Time taken for Epoch 36:3.00 - F1: 0.6070
2026-02-13 11:47:52 - INFO - Time taken for Epoch 36:3.00 - F1: 0.6070
Time taken for Epoch 37:3.00 - F1: 0.6105
2026-02-13 11:47:55 - INFO - Time taken for Epoch 37:3.00 - F1: 0.6105
Performance not improving for 10 consecutive epochs.
2026-02-13 11:47:55 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6105 - Best Epoch:26
2026-02-13 11:47:55 - INFO - Best F1:0.6105 - Best Epoch:26
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6137, Test ECE: 0.0847
2026-02-13 11:48:03 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6137, Test ECE: 0.0847
All results: {'f1_macro': 0.613746501273058, 'ece': np.float64(0.08467136413507362)}
2026-02-13 11:48:03 - INFO - All results: {'f1_macro': 0.613746501273058, 'ece': np.float64(0.08467136413507362)}

Total time taken: 505.32 seconds
2026-02-13 11:48:03 - INFO - 
Total time taken: 505.32 seconds
2026-02-13 11:48:03 - INFO - Trial 6 finished with value: 0.613746501273058 and parameters: {'learning_rate': 1.3105107825049032e-05, 'weight_decay': 0.002423290287691337, 'batch_size': 8, 'co_train_epochs': 8, 'epoch_patience': 4}. Best is trial 1 with value: 0.6903331190978219.
Using devices: cuda, cuda
2026-02-13 11:48:03 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 11:48:03 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 11:48:03 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 11:48:03 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 0.00022293507816541019
Weight Decay: 0.009049710744516975
Batch Size: 8
No. Epochs: 19
Epoch Patience: 5
 Accumulation Steps: 8
2026-02-13 11:48:04 - INFO - Learning Rate: 0.00022293507816541019
Weight Decay: 0.009049710744516975
Batch Size: 8
No. Epochs: 19
Epoch Patience: 5
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 11:48:05 - INFO - Generating initial weights
Time taken for Epoch 1:19.72 - F1: 0.0155
2026-02-13 11:48:28 - INFO - Time taken for Epoch 1:19.72 - F1: 0.0155
Time taken for Epoch 2:19.66 - F1: 0.0420
2026-02-13 11:48:48 - INFO - Time taken for Epoch 2:19.66 - F1: 0.0420
Time taken for Epoch 3:19.68 - F1: 0.3118
2026-02-13 11:49:08 - INFO - Time taken for Epoch 3:19.68 - F1: 0.3118
Time taken for Epoch 4:19.69 - F1: 0.3186
2026-02-13 11:49:27 - INFO - Time taken for Epoch 4:19.69 - F1: 0.3186
Time taken for Epoch 5:19.71 - F1: 0.3698
2026-02-13 11:49:47 - INFO - Time taken for Epoch 5:19.71 - F1: 0.3698
Time taken for Epoch 6:19.71 - F1: 0.4045
2026-02-13 11:50:07 - INFO - Time taken for Epoch 6:19.71 - F1: 0.4045
Time taken for Epoch 7:19.75 - F1: 0.4160
2026-02-13 11:50:26 - INFO - Time taken for Epoch 7:19.75 - F1: 0.4160
Time taken for Epoch 8:19.79 - F1: 0.4194
2026-02-13 11:50:46 - INFO - Time taken for Epoch 8:19.79 - F1: 0.4194
Time taken for Epoch 9:19.76 - F1: 0.4166
2026-02-13 11:51:06 - INFO - Time taken for Epoch 9:19.76 - F1: 0.4166
Time taken for Epoch 10:19.73 - F1: 0.4356
2026-02-13 11:51:26 - INFO - Time taken for Epoch 10:19.73 - F1: 0.4356
Time taken for Epoch 11:19.72 - F1: 0.4531
2026-02-13 11:51:45 - INFO - Time taken for Epoch 11:19.72 - F1: 0.4531
Time taken for Epoch 12:19.73 - F1: 0.4488
2026-02-13 11:52:05 - INFO - Time taken for Epoch 12:19.73 - F1: 0.4488
Time taken for Epoch 13:19.76 - F1: 0.4614
2026-02-13 11:52:25 - INFO - Time taken for Epoch 13:19.76 - F1: 0.4614
Time taken for Epoch 14:19.73 - F1: 0.4775
2026-02-13 11:52:45 - INFO - Time taken for Epoch 14:19.73 - F1: 0.4775
Time taken for Epoch 15:19.74 - F1: 0.4732
2026-02-13 11:53:04 - INFO - Time taken for Epoch 15:19.74 - F1: 0.4732
Time taken for Epoch 16:19.76 - F1: 0.4818
2026-02-13 11:53:24 - INFO - Time taken for Epoch 16:19.76 - F1: 0.4818
Time taken for Epoch 17:19.78 - F1: 0.4843
2026-02-13 11:53:44 - INFO - Time taken for Epoch 17:19.78 - F1: 0.4843
Time taken for Epoch 18:19.74 - F1: 0.4704
2026-02-13 11:54:04 - INFO - Time taken for Epoch 18:19.74 - F1: 0.4704
Time taken for Epoch 19:19.74 - F1: 0.4680
2026-02-13 11:54:23 - INFO - Time taken for Epoch 19:19.74 - F1: 0.4680
Best F1:0.4843 - Best Epoch:17
2026-02-13 11:54:23 - INFO - Best F1:0.4843 - Best Epoch:17
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 11:54:28 - INFO - Starting co-training
Time taken for Epoch 1: 23.62s - F1: 0.02051768
2026-02-13 11:54:52 - INFO - Time taken for Epoch 1: 23.62s - F1: 0.02051768
Time taken for Epoch 2: 24.55s - F1: 0.04247539
2026-02-13 11:55:17 - INFO - Time taken for Epoch 2: 24.55s - F1: 0.04247539
Time taken for Epoch 3: 24.62s - F1: 0.04247539
2026-02-13 11:55:41 - INFO - Time taken for Epoch 3: 24.62s - F1: 0.04247539
Time taken for Epoch 4: 23.61s - F1: 0.04247539
2026-02-13 11:56:05 - INFO - Time taken for Epoch 4: 23.61s - F1: 0.04247539
Time taken for Epoch 5: 23.67s - F1: 0.04247539
2026-02-13 11:56:29 - INFO - Time taken for Epoch 5: 23.67s - F1: 0.04247539
Time taken for Epoch 6: 23.67s - F1: 0.04247539
2026-02-13 11:56:52 - INFO - Time taken for Epoch 6: 23.67s - F1: 0.04247539
Time taken for Epoch 7: 23.69s - F1: 0.04247539
2026-02-13 11:57:16 - INFO - Time taken for Epoch 7: 23.69s - F1: 0.04247539
Performance not improving for 5 consecutive epochs.
Performance not improving for 5 consecutive epochs.
2026-02-13 11:57:16 - INFO - Performance not improving for 5 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 11:57:18 - INFO - Fine-tuning models
Time taken for Epoch 1:2.97 - F1: 0.0425
2026-02-13 11:57:22 - INFO - Time taken for Epoch 1:2.97 - F1: 0.0425
Time taken for Epoch 2:3.95 - F1: 0.0425
2026-02-13 11:57:26 - INFO - Time taken for Epoch 2:3.95 - F1: 0.0425
Time taken for Epoch 3:2.95 - F1: 0.0321
2026-02-13 11:57:29 - INFO - Time taken for Epoch 3:2.95 - F1: 0.0321
Time taken for Epoch 4:2.96 - F1: 0.0321
2026-02-13 11:57:32 - INFO - Time taken for Epoch 4:2.96 - F1: 0.0321
Time taken for Epoch 5:2.98 - F1: 0.0155
2026-02-13 11:57:34 - INFO - Time taken for Epoch 5:2.98 - F1: 0.0155
Time taken for Epoch 6:2.96 - F1: 0.0155
2026-02-13 11:57:37 - INFO - Time taken for Epoch 6:2.96 - F1: 0.0155
Time taken for Epoch 7:2.96 - F1: 0.0155
2026-02-13 11:57:40 - INFO - Time taken for Epoch 7:2.96 - F1: 0.0155
Time taken for Epoch 8:2.96 - F1: 0.0155
2026-02-13 11:57:43 - INFO - Time taken for Epoch 8:2.96 - F1: 0.0155
Time taken for Epoch 9:2.96 - F1: 0.0155
2026-02-13 11:57:46 - INFO - Time taken for Epoch 9:2.96 - F1: 0.0155
Time taken for Epoch 10:2.96 - F1: 0.0155
2026-02-13 11:57:49 - INFO - Time taken for Epoch 10:2.96 - F1: 0.0155
Time taken for Epoch 11:2.96 - F1: 0.0155
2026-02-13 11:57:52 - INFO - Time taken for Epoch 11:2.96 - F1: 0.0155
Performance not improving for 10 consecutive epochs.
2026-02-13 11:57:52 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-13 11:57:52 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2494
2026-02-13 11:58:01 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2494
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.24941421412441828)}
2026-02-13 11:58:01 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.24941421412441828)}

Total time taken: 597.12 seconds
2026-02-13 11:58:01 - INFO - 
Total time taken: 597.12 seconds
2026-02-13 11:58:01 - INFO - Trial 7 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.00022293507816541019, 'weight_decay': 0.009049710744516975, 'batch_size': 8, 'co_train_epochs': 19, 'epoch_patience': 5}. Best is trial 1 with value: 0.6903331190978219.
Using devices: cuda, cuda
2026-02-13 11:58:01 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 11:58:01 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 11:58:01 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 11:58:01 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 5.801190768400718e-05
Weight Decay: 0.003394752631217271
Batch Size: 16
No. Epochs: 11
Epoch Patience: 7
 Accumulation Steps: 4
2026-02-13 11:58:02 - INFO - Learning Rate: 5.801190768400718e-05
Weight Decay: 0.003394752631217271
Batch Size: 16
No. Epochs: 11
Epoch Patience: 7
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 11:58:03 - INFO - Generating initial weights
Time taken for Epoch 1:18.30 - F1: 0.1049
2026-02-13 11:58:24 - INFO - Time taken for Epoch 1:18.30 - F1: 0.1049
Time taken for Epoch 2:18.20 - F1: 0.1096
2026-02-13 11:58:43 - INFO - Time taken for Epoch 2:18.20 - F1: 0.1096
Time taken for Epoch 3:18.23 - F1: 0.1352
2026-02-13 11:59:01 - INFO - Time taken for Epoch 3:18.23 - F1: 0.1352
Time taken for Epoch 4:18.24 - F1: 0.1286
2026-02-13 11:59:19 - INFO - Time taken for Epoch 4:18.24 - F1: 0.1286
Time taken for Epoch 5:18.27 - F1: 0.1523
2026-02-13 11:59:37 - INFO - Time taken for Epoch 5:18.27 - F1: 0.1523
Time taken for Epoch 6:18.26 - F1: 0.2052
2026-02-13 11:59:56 - INFO - Time taken for Epoch 6:18.26 - F1: 0.2052
Time taken for Epoch 7:18.30 - F1: 0.2809
2026-02-13 12:00:14 - INFO - Time taken for Epoch 7:18.30 - F1: 0.2809
Time taken for Epoch 8:18.30 - F1: 0.3587
2026-02-13 12:00:32 - INFO - Time taken for Epoch 8:18.30 - F1: 0.3587
Time taken for Epoch 9:18.30 - F1: 0.4059
2026-02-13 12:00:51 - INFO - Time taken for Epoch 9:18.30 - F1: 0.4059
Time taken for Epoch 10:18.29 - F1: 0.4194
2026-02-13 12:01:09 - INFO - Time taken for Epoch 10:18.29 - F1: 0.4194
Time taken for Epoch 11:18.28 - F1: 0.4269
2026-02-13 12:01:27 - INFO - Time taken for Epoch 11:18.28 - F1: 0.4269
Best F1:0.4269 - Best Epoch:11
2026-02-13 12:01:27 - INFO - Best F1:0.4269 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 12:01:28 - INFO - Starting co-training
Time taken for Epoch 1: 25.23s - F1: 0.57186282
2026-02-13 12:01:54 - INFO - Time taken for Epoch 1: 25.23s - F1: 0.57186282
Time taken for Epoch 2: 26.25s - F1: 0.59216466
2026-02-13 12:02:20 - INFO - Time taken for Epoch 2: 26.25s - F1: 0.59216466
Time taken for Epoch 3: 26.29s - F1: 0.59588256
2026-02-13 12:02:46 - INFO - Time taken for Epoch 3: 26.29s - F1: 0.59588256
Time taken for Epoch 4: 26.35s - F1: 0.59685433
2026-02-13 12:03:13 - INFO - Time taken for Epoch 4: 26.35s - F1: 0.59685433
Time taken for Epoch 5: 26.29s - F1: 0.59853057
2026-02-13 12:03:39 - INFO - Time taken for Epoch 5: 26.29s - F1: 0.59853057
Time taken for Epoch 6: 27.01s - F1: 0.61093742
2026-02-13 12:04:06 - INFO - Time taken for Epoch 6: 27.01s - F1: 0.61093742
Time taken for Epoch 7: 26.33s - F1: 0.61142536
2026-02-13 12:04:32 - INFO - Time taken for Epoch 7: 26.33s - F1: 0.61142536
Time taken for Epoch 8: 26.29s - F1: 0.63004200
2026-02-13 12:04:59 - INFO - Time taken for Epoch 8: 26.29s - F1: 0.63004200
Time taken for Epoch 9: 26.34s - F1: 0.66266718
2026-02-13 12:05:25 - INFO - Time taken for Epoch 9: 26.34s - F1: 0.66266718
Time taken for Epoch 10: 26.34s - F1: 0.62639349
2026-02-13 12:05:51 - INFO - Time taken for Epoch 10: 26.34s - F1: 0.62639349
Time taken for Epoch 11: 25.22s - F1: 0.62609378
2026-02-13 12:06:17 - INFO - Time taken for Epoch 11: 25.22s - F1: 0.62609378
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 12:06:19 - INFO - Fine-tuning models
Time taken for Epoch 1:2.77 - F1: 0.6298
2026-02-13 12:06:22 - INFO - Time taken for Epoch 1:2.77 - F1: 0.6298
Time taken for Epoch 2:3.74 - F1: 0.6392
2026-02-13 12:06:26 - INFO - Time taken for Epoch 2:3.74 - F1: 0.6392
Time taken for Epoch 3:3.86 - F1: 0.6309
2026-02-13 12:06:30 - INFO - Time taken for Epoch 3:3.86 - F1: 0.6309
Time taken for Epoch 4:2.76 - F1: 0.6399
2026-02-13 12:06:32 - INFO - Time taken for Epoch 4:2.76 - F1: 0.6399
Time taken for Epoch 5:3.84 - F1: 0.6365
2026-02-13 12:06:36 - INFO - Time taken for Epoch 5:3.84 - F1: 0.6365
Time taken for Epoch 6:2.76 - F1: 0.6344
2026-02-13 12:06:39 - INFO - Time taken for Epoch 6:2.76 - F1: 0.6344
Time taken for Epoch 7:2.76 - F1: 0.6290
2026-02-13 12:06:42 - INFO - Time taken for Epoch 7:2.76 - F1: 0.6290
Time taken for Epoch 8:2.75 - F1: 0.6288
2026-02-13 12:06:44 - INFO - Time taken for Epoch 8:2.75 - F1: 0.6288
Time taken for Epoch 9:2.75 - F1: 0.6351
2026-02-13 12:06:47 - INFO - Time taken for Epoch 9:2.75 - F1: 0.6351
Time taken for Epoch 10:2.75 - F1: 0.6493
2026-02-13 12:06:50 - INFO - Time taken for Epoch 10:2.75 - F1: 0.6493
Time taken for Epoch 11:3.86 - F1: 0.6470
2026-02-13 12:06:54 - INFO - Time taken for Epoch 11:3.86 - F1: 0.6470
Time taken for Epoch 12:2.76 - F1: 0.6527
2026-02-13 12:06:57 - INFO - Time taken for Epoch 12:2.76 - F1: 0.6527
Time taken for Epoch 13:3.86 - F1: 0.6453
2026-02-13 12:07:00 - INFO - Time taken for Epoch 13:3.86 - F1: 0.6453
Time taken for Epoch 14:2.78 - F1: 0.6513
2026-02-13 12:07:03 - INFO - Time taken for Epoch 14:2.78 - F1: 0.6513
Time taken for Epoch 15:2.78 - F1: 0.6524
2026-02-13 12:07:06 - INFO - Time taken for Epoch 15:2.78 - F1: 0.6524
Time taken for Epoch 16:2.78 - F1: 0.6586
2026-02-13 12:07:09 - INFO - Time taken for Epoch 16:2.78 - F1: 0.6586
Time taken for Epoch 17:3.88 - F1: 0.6644
2026-02-13 12:07:13 - INFO - Time taken for Epoch 17:3.88 - F1: 0.6644
Time taken for Epoch 18:3.87 - F1: 0.6396
2026-02-13 12:07:17 - INFO - Time taken for Epoch 18:3.87 - F1: 0.6396
Time taken for Epoch 19:2.78 - F1: 0.6374
2026-02-13 12:07:19 - INFO - Time taken for Epoch 19:2.78 - F1: 0.6374
Time taken for Epoch 20:2.78 - F1: 0.6323
2026-02-13 12:07:22 - INFO - Time taken for Epoch 20:2.78 - F1: 0.6323
Time taken for Epoch 21:2.80 - F1: 0.6325
2026-02-13 12:07:25 - INFO - Time taken for Epoch 21:2.80 - F1: 0.6325
Time taken for Epoch 22:2.78 - F1: 0.6276
2026-02-13 12:07:28 - INFO - Time taken for Epoch 22:2.78 - F1: 0.6276
Time taken for Epoch 23:2.78 - F1: 0.6241
2026-02-13 12:07:30 - INFO - Time taken for Epoch 23:2.78 - F1: 0.6241
Time taken for Epoch 24:2.78 - F1: 0.6221
2026-02-13 12:07:33 - INFO - Time taken for Epoch 24:2.78 - F1: 0.6221
Time taken for Epoch 25:2.78 - F1: 0.6224
2026-02-13 12:07:36 - INFO - Time taken for Epoch 25:2.78 - F1: 0.6224
Time taken for Epoch 26:2.78 - F1: 0.6240
2026-02-13 12:07:39 - INFO - Time taken for Epoch 26:2.78 - F1: 0.6240
Time taken for Epoch 27:2.78 - F1: 0.6251
2026-02-13 12:07:42 - INFO - Time taken for Epoch 27:2.78 - F1: 0.6251
Performance not improving for 10 consecutive epochs.
2026-02-13 12:07:42 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6644 - Best Epoch:16
2026-02-13 12:07:42 - INFO - Best F1:0.6644 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6662, Test ECE: 0.0558
2026-02-13 12:07:49 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6662, Test ECE: 0.0558
All results: {'f1_macro': 0.6661969119812688, 'ece': np.float64(0.055835260680181)}
2026-02-13 12:07:49 - INFO - All results: {'f1_macro': 0.6661969119812688, 'ece': np.float64(0.055835260680181)}

Total time taken: 588.68 seconds
2026-02-13 12:07:49 - INFO - 
Total time taken: 588.68 seconds
2026-02-13 12:07:49 - INFO - Trial 8 finished with value: 0.6661969119812688 and parameters: {'learning_rate': 5.801190768400718e-05, 'weight_decay': 0.003394752631217271, 'batch_size': 16, 'co_train_epochs': 11, 'epoch_patience': 7}. Best is trial 1 with value: 0.6903331190978219.
Using devices: cuda, cuda
2026-02-13 12:07:49 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 12:07:49 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 12:07:49 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 12:07:49 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
Learning Rate: 1.5274921328651937e-05
Weight Decay: 0.00011523700645576346
Batch Size: 8
No. Epochs: 10
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-13 12:07:50 - INFO - Learning Rate: 1.5274921328651937e-05
Weight Decay: 0.00011523700645576346
Batch Size: 8
No. Epochs: 10
Epoch Patience: 10
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 12:07:51 - INFO - Generating initial weights
Time taken for Epoch 1:19.72 - F1: 0.0634
2026-02-13 12:08:14 - INFO - Time taken for Epoch 1:19.72 - F1: 0.0634
Time taken for Epoch 2:19.65 - F1: 0.0830
2026-02-13 12:08:34 - INFO - Time taken for Epoch 2:19.65 - F1: 0.0830
Time taken for Epoch 3:19.69 - F1: 0.0586
2026-02-13 12:08:54 - INFO - Time taken for Epoch 3:19.69 - F1: 0.0586
Time taken for Epoch 4:19.71 - F1: 0.0267
2026-02-13 12:09:13 - INFO - Time taken for Epoch 4:19.71 - F1: 0.0267
Time taken for Epoch 5:19.69 - F1: 0.0156
2026-02-13 12:09:33 - INFO - Time taken for Epoch 5:19.69 - F1: 0.0156
Time taken for Epoch 6:19.70 - F1: 0.0155
2026-02-13 12:09:53 - INFO - Time taken for Epoch 6:19.70 - F1: 0.0155
Time taken for Epoch 7:19.72 - F1: 0.0155
2026-02-13 12:10:12 - INFO - Time taken for Epoch 7:19.72 - F1: 0.0155
Time taken for Epoch 8:19.71 - F1: 0.0155
2026-02-13 12:10:32 - INFO - Time taken for Epoch 8:19.71 - F1: 0.0155
Time taken for Epoch 9:19.71 - F1: 0.0155
2026-02-13 12:10:52 - INFO - Time taken for Epoch 9:19.71 - F1: 0.0155
Time taken for Epoch 10:19.75 - F1: 0.0172
2026-02-13 12:11:12 - INFO - Time taken for Epoch 10:19.75 - F1: 0.0172
Best F1:0.0830 - Best Epoch:2
2026-02-13 12:11:12 - INFO - Best F1:0.0830 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 12:11:13 - INFO - Starting co-training
Time taken for Epoch 1: 23.58s - F1: 0.22556028
2026-02-13 12:11:37 - INFO - Time taken for Epoch 1: 23.58s - F1: 0.22556028
Time taken for Epoch 2: 24.70s - F1: 0.23965008
2026-02-13 12:12:01 - INFO - Time taken for Epoch 2: 24.70s - F1: 0.23965008
Time taken for Epoch 3: 24.84s - F1: 0.37726986
2026-02-13 12:12:26 - INFO - Time taken for Epoch 3: 24.84s - F1: 0.37726986
Time taken for Epoch 4: 24.81s - F1: 0.46071391
2026-02-13 12:12:51 - INFO - Time taken for Epoch 4: 24.81s - F1: 0.46071391
Time taken for Epoch 5: 24.82s - F1: 0.51099887
2026-02-13 12:13:16 - INFO - Time taken for Epoch 5: 24.82s - F1: 0.51099887
Time taken for Epoch 6: 24.78s - F1: 0.53484014
2026-02-13 12:13:41 - INFO - Time taken for Epoch 6: 24.78s - F1: 0.53484014
Time taken for Epoch 7: 24.72s - F1: 0.56957498
2026-02-13 12:14:05 - INFO - Time taken for Epoch 7: 24.72s - F1: 0.56957498
Time taken for Epoch 8: 24.71s - F1: 0.57944681
2026-02-13 12:14:30 - INFO - Time taken for Epoch 8: 24.71s - F1: 0.57944681
Time taken for Epoch 9: 24.73s - F1: 0.59263559
2026-02-13 12:14:55 - INFO - Time taken for Epoch 9: 24.73s - F1: 0.59263559
Time taken for Epoch 10: 24.81s - F1: 0.59700806
2026-02-13 12:15:20 - INFO - Time taken for Epoch 10: 24.81s - F1: 0.59700806
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Fine-tuning models
2026-02-13 12:15:23 - INFO - Fine-tuning models
Time taken for Epoch 1:2.98 - F1: 0.6003
2026-02-13 12:15:26 - INFO - Time taken for Epoch 1:2.98 - F1: 0.6003
Time taken for Epoch 2:4.00 - F1: 0.6038
2026-02-13 12:15:30 - INFO - Time taken for Epoch 2:4.00 - F1: 0.6038
Time taken for Epoch 3:4.09 - F1: 0.6103
2026-02-13 12:15:34 - INFO - Time taken for Epoch 3:4.09 - F1: 0.6103
Time taken for Epoch 4:4.09 - F1: 0.6137
2026-02-13 12:15:39 - INFO - Time taken for Epoch 4:4.09 - F1: 0.6137
Time taken for Epoch 5:4.09 - F1: 0.6136
2026-02-13 12:15:43 - INFO - Time taken for Epoch 5:4.09 - F1: 0.6136
Time taken for Epoch 6:2.95 - F1: 0.6073
2026-02-13 12:15:46 - INFO - Time taken for Epoch 6:2.95 - F1: 0.6073
Time taken for Epoch 7:2.95 - F1: 0.6175
2026-02-13 12:15:49 - INFO - Time taken for Epoch 7:2.95 - F1: 0.6175
Time taken for Epoch 8:4.09 - F1: 0.6108
2026-02-13 12:15:53 - INFO - Time taken for Epoch 8:4.09 - F1: 0.6108
Time taken for Epoch 9:2.95 - F1: 0.6133
2026-02-13 12:15:56 - INFO - Time taken for Epoch 9:2.95 - F1: 0.6133
Time taken for Epoch 10:2.96 - F1: 0.6055
2026-02-13 12:15:59 - INFO - Time taken for Epoch 10:2.96 - F1: 0.6055
Time taken for Epoch 11:2.96 - F1: 0.6045
2026-02-13 12:16:02 - INFO - Time taken for Epoch 11:2.96 - F1: 0.6045
Time taken for Epoch 12:2.97 - F1: 0.6044
2026-02-13 12:16:04 - INFO - Time taken for Epoch 12:2.97 - F1: 0.6044
Time taken for Epoch 13:2.96 - F1: 0.6034
2026-02-13 12:16:07 - INFO - Time taken for Epoch 13:2.96 - F1: 0.6034
Time taken for Epoch 14:2.96 - F1: 0.6069
2026-02-13 12:16:10 - INFO - Time taken for Epoch 14:2.96 - F1: 0.6069
Time taken for Epoch 15:2.96 - F1: 0.6069
2026-02-13 12:16:13 - INFO - Time taken for Epoch 15:2.96 - F1: 0.6069
Time taken for Epoch 16:2.96 - F1: 0.6022
2026-02-13 12:16:16 - INFO - Time taken for Epoch 16:2.96 - F1: 0.6022
Time taken for Epoch 17:2.96 - F1: 0.6055
2026-02-13 12:16:19 - INFO - Time taken for Epoch 17:2.96 - F1: 0.6055
Performance not improving for 10 consecutive epochs.
2026-02-13 12:16:19 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6175 - Best Epoch:6
2026-02-13 12:16:19 - INFO - Best F1:0.6175 - Best Epoch:6
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_1_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label10-set2/final_model_2_optuna-bertweet-hurricane-florence-2018-label10-set2_gpt4o_10_shot_bert-tweet_10_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6315, Test ECE: 0.0793
2026-02-13 12:16:27 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6315, Test ECE: 0.0793
All results: {'f1_macro': 0.6314693166295287, 'ece': np.float64(0.07928163183014014)}
2026-02-13 12:16:27 - INFO - All results: {'f1_macro': 0.6314693166295287, 'ece': np.float64(0.07928163183014014)}

Total time taken: 517.43 seconds
2026-02-13 12:16:27 - INFO - 
Total time taken: 517.43 seconds
2026-02-13 12:16:27 - INFO - Trial 9 finished with value: 0.6314693166295287 and parameters: {'learning_rate': 1.5274921328651937e-05, 'weight_decay': 0.00011523700645576346, 'batch_size': 8, 'co_train_epochs': 10, 'epoch_patience': 10}. Best is trial 1 with value: 0.6903331190978219.

[BEST TRIAL RESULTS]
2026-02-13 12:16:27 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6903
2026-02-13 12:16:27 - INFO - F1 Score: 0.6903
Params: {'learning_rate': 2.028303632700962e-05, 'weight_decay': 0.004139537280534424, 'batch_size': 64, 'co_train_epochs': 9, 'epoch_patience': 10}
2026-02-13 12:16:27 - INFO - Params: {'learning_rate': 2.028303632700962e-05, 'weight_decay': 0.004139537280534424, 'batch_size': 64, 'co_train_epochs': 9, 'epoch_patience': 10}
  learning_rate: 2.028303632700962e-05
2026-02-13 12:16:27 - INFO -   learning_rate: 2.028303632700962e-05
  weight_decay: 0.004139537280534424
2026-02-13 12:16:27 - INFO -   weight_decay: 0.004139537280534424
  batch_size: 64
2026-02-13 12:16:27 - INFO -   batch_size: 64
  co_train_epochs: 9
2026-02-13 12:16:27 - INFO -   co_train_epochs: 9
  epoch_patience: 10
2026-02-13 12:16:27 - INFO -   epoch_patience: 10

Total time taken: 5868.42 seconds
2026-02-13 12:16:27 - INFO - 
Total time taken: 5868.42 seconds