Running with 5 label/class set 2

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 21:52:46 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 21:52:46 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_maria_2017
Using devices: cuda, cuda
2026-02-13 21:52:46 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 21:52:46 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 21:52:46 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 21:52:46 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 1.5903213951940457e-05
Weight Decay: 2.4099157500711686e-05
Batch Size: 32
No. Epochs: 9
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-13 21:52:47 - INFO - Learning Rate: 1.5903213951940457e-05
Weight Decay: 2.4099157500711686e-05
Batch Size: 32
No. Epochs: 9
Epoch Patience: 4
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 21:52:48 - INFO - Generating initial weights
Time taken for Epoch 1:20.42 - F1: 0.0544
2026-02-13 21:53:12 - INFO - Time taken for Epoch 1:20.42 - F1: 0.0544
Time taken for Epoch 2:20.22 - F1: 0.0622
2026-02-13 21:53:33 - INFO - Time taken for Epoch 2:20.22 - F1: 0.0622
Time taken for Epoch 3:20.35 - F1: 0.0854
2026-02-13 21:53:53 - INFO - Time taken for Epoch 3:20.35 - F1: 0.0854
Time taken for Epoch 4:20.30 - F1: 0.0971
2026-02-13 21:54:13 - INFO - Time taken for Epoch 4:20.30 - F1: 0.0971
Time taken for Epoch 5:20.31 - F1: 0.1025
2026-02-13 21:54:34 - INFO - Time taken for Epoch 5:20.31 - F1: 0.1025
Time taken for Epoch 6:20.33 - F1: 0.1090
2026-02-13 21:54:54 - INFO - Time taken for Epoch 6:20.33 - F1: 0.1090
Time taken for Epoch 7:20.37 - F1: 0.1087
2026-02-13 21:55:14 - INFO - Time taken for Epoch 7:20.37 - F1: 0.1087
Time taken for Epoch 8:20.38 - F1: 0.1152
2026-02-13 21:55:35 - INFO - Time taken for Epoch 8:20.38 - F1: 0.1152
Time taken for Epoch 9:20.36 - F1: 0.1233
2026-02-13 21:55:55 - INFO - Time taken for Epoch 9:20.36 - F1: 0.1233
Best F1:0.1233 - Best Epoch:9
2026-02-13 21:55:55 - INFO - Best F1:0.1233 - Best Epoch:9
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 21:55:56 - INFO - Starting co-training
Time taken for Epoch 1: 35.71s - F1: 0.32075648
2026-02-13 21:56:33 - INFO - Time taken for Epoch 1: 35.71s - F1: 0.32075648
Time taken for Epoch 2: 36.81s - F1: 0.47178624
2026-02-13 21:57:09 - INFO - Time taken for Epoch 2: 36.81s - F1: 0.47178624
Time taken for Epoch 3: 36.94s - F1: 0.51865925
2026-02-13 21:57:46 - INFO - Time taken for Epoch 3: 36.94s - F1: 0.51865925
Time taken for Epoch 4: 36.94s - F1: 0.53318990
2026-02-13 21:58:23 - INFO - Time taken for Epoch 4: 36.94s - F1: 0.53318990
Time taken for Epoch 5: 36.95s - F1: 0.60559788
2026-02-13 21:59:00 - INFO - Time taken for Epoch 5: 36.95s - F1: 0.60559788
Time taken for Epoch 6: 36.95s - F1: 0.61222575
2026-02-13 21:59:37 - INFO - Time taken for Epoch 6: 36.95s - F1: 0.61222575
Time taken for Epoch 7: 37.15s - F1: 0.64041544
2026-02-13 22:00:14 - INFO - Time taken for Epoch 7: 37.15s - F1: 0.64041544
Time taken for Epoch 8: 36.94s - F1: 0.63497589
2026-02-13 22:00:51 - INFO - Time taken for Epoch 8: 36.94s - F1: 0.63497589
Time taken for Epoch 9: 35.73s - F1: 0.65294722
2026-02-13 22:01:27 - INFO - Time taken for Epoch 9: 35.73s - F1: 0.65294722
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 22:01:31 - INFO - Fine-tuning models
Time taken for Epoch 1:2.81 - F1: 0.6590
2026-02-13 22:01:34 - INFO - Time taken for Epoch 1:2.81 - F1: 0.6590
Time taken for Epoch 2:3.95 - F1: 0.6570
2026-02-13 22:01:38 - INFO - Time taken for Epoch 2:3.95 - F1: 0.6570
Time taken for Epoch 3:2.81 - F1: 0.6534
2026-02-13 22:01:40 - INFO - Time taken for Epoch 3:2.81 - F1: 0.6534
Time taken for Epoch 4:2.80 - F1: 0.6515
2026-02-13 22:01:43 - INFO - Time taken for Epoch 4:2.80 - F1: 0.6515
Time taken for Epoch 5:2.80 - F1: 0.6518
2026-02-13 22:01:46 - INFO - Time taken for Epoch 5:2.80 - F1: 0.6518
Time taken for Epoch 6:2.80 - F1: 0.6572
2026-02-13 22:01:49 - INFO - Time taken for Epoch 6:2.80 - F1: 0.6572
Time taken for Epoch 7:2.80 - F1: 0.6584
2026-02-13 22:01:52 - INFO - Time taken for Epoch 7:2.80 - F1: 0.6584
Time taken for Epoch 8:2.80 - F1: 0.6663
2026-02-13 22:01:54 - INFO - Time taken for Epoch 8:2.80 - F1: 0.6663
Time taken for Epoch 9:4.08 - F1: 0.6626
2026-02-13 22:01:59 - INFO - Time taken for Epoch 9:4.08 - F1: 0.6626
Time taken for Epoch 10:2.81 - F1: 0.6569
2026-02-13 22:02:01 - INFO - Time taken for Epoch 10:2.81 - F1: 0.6569
Time taken for Epoch 11:2.80 - F1: 0.6552
2026-02-13 22:02:04 - INFO - Time taken for Epoch 11:2.80 - F1: 0.6552
Time taken for Epoch 12:2.80 - F1: 0.6580
2026-02-13 22:02:07 - INFO - Time taken for Epoch 12:2.80 - F1: 0.6580
Time taken for Epoch 13:2.80 - F1: 0.6502
2026-02-13 22:02:10 - INFO - Time taken for Epoch 13:2.80 - F1: 0.6502
Time taken for Epoch 14:2.80 - F1: 0.6486
2026-02-13 22:02:13 - INFO - Time taken for Epoch 14:2.80 - F1: 0.6486
Time taken for Epoch 15:2.80 - F1: 0.6496
2026-02-13 22:02:15 - INFO - Time taken for Epoch 15:2.80 - F1: 0.6496
Time taken for Epoch 16:2.80 - F1: 0.6517
2026-02-13 22:02:18 - INFO - Time taken for Epoch 16:2.80 - F1: 0.6517
Time taken for Epoch 17:2.80 - F1: 0.6519
2026-02-13 22:02:21 - INFO - Time taken for Epoch 17:2.80 - F1: 0.6519
Time taken for Epoch 18:2.80 - F1: 0.6527
2026-02-13 22:02:24 - INFO - Time taken for Epoch 18:2.80 - F1: 0.6527
Performance not improving for 10 consecutive epochs.
2026-02-13 22:02:24 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6663 - Best Epoch:7
2026-02-13 22:02:24 - INFO - Best F1:0.6663 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6682, Test ECE: 0.0573
2026-02-13 22:02:31 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6682, Test ECE: 0.0573
All results: {'f1_macro': 0.6681542515203446, 'ece': np.float64(0.057339323129138066)}
2026-02-13 22:02:31 - INFO - All results: {'f1_macro': 0.6681542515203446, 'ece': np.float64(0.057339323129138066)}

Total time taken: 585.33 seconds
2026-02-13 22:02:31 - INFO - 
Total time taken: 585.33 seconds
2026-02-13 22:02:32 - INFO - Trial 0 finished with value: 0.6681542515203446 and parameters: {'learning_rate': 1.5903213951940457e-05, 'weight_decay': 2.4099157500711686e-05, 'batch_size': 32, 'co_train_epochs': 9, 'epoch_patience': 4}. Best is trial 0 with value: 0.6681542515203446.
Using devices: cuda, cuda
2026-02-13 22:02:32 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 22:02:32 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 22:02:32 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 22:02:32 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 2.1006697587331224e-05
Weight Decay: 0.000310345408072706
Batch Size: 8
No. Epochs: 5
Epoch Patience: 6
 Accumulation Steps: 8
2026-02-13 22:02:32 - INFO - Learning Rate: 2.1006697587331224e-05
Weight Decay: 0.000310345408072706
Batch Size: 8
No. Epochs: 5
Epoch Patience: 6
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 22:02:33 - INFO - Generating initial weights
Time taken for Epoch 1:22.49 - F1: 0.0409
2026-02-13 22:02:59 - INFO - Time taken for Epoch 1:22.49 - F1: 0.0409
Time taken for Epoch 2:22.49 - F1: 0.0460
2026-02-13 22:03:22 - INFO - Time taken for Epoch 2:22.49 - F1: 0.0460
Time taken for Epoch 3:22.48 - F1: 0.0485
2026-02-13 22:03:44 - INFO - Time taken for Epoch 3:22.48 - F1: 0.0485
Time taken for Epoch 4:22.52 - F1: 0.0657
2026-02-13 22:04:07 - INFO - Time taken for Epoch 4:22.52 - F1: 0.0657
Time taken for Epoch 5:22.58 - F1: 0.0672
2026-02-13 22:04:29 - INFO - Time taken for Epoch 5:22.58 - F1: 0.0672
Best F1:0.0672 - Best Epoch:5
2026-02-13 22:04:29 - INFO - Best F1:0.0672 - Best Epoch:5
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 22:04:30 - INFO - Starting co-training
Time taken for Epoch 1: 27.65s - F1: 0.16085714
2026-02-13 22:04:58 - INFO - Time taken for Epoch 1: 27.65s - F1: 0.16085714
Time taken for Epoch 2: 28.88s - F1: 0.29830712
2026-02-13 22:05:27 - INFO - Time taken for Epoch 2: 28.88s - F1: 0.29830712
Time taken for Epoch 3: 29.00s - F1: 0.37243581
2026-02-13 22:05:56 - INFO - Time taken for Epoch 3: 29.00s - F1: 0.37243581
Time taken for Epoch 4: 28.99s - F1: 0.44740431
2026-02-13 22:06:25 - INFO - Time taken for Epoch 4: 28.99s - F1: 0.44740431
Time taken for Epoch 5: 29.00s - F1: 0.45418201
2026-02-13 22:06:54 - INFO - Time taken for Epoch 5: 29.00s - F1: 0.45418201
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 22:06:58 - INFO - Fine-tuning models
Time taken for Epoch 1:3.15 - F1: 0.4817
2026-02-13 22:07:01 - INFO - Time taken for Epoch 1:3.15 - F1: 0.4817
Time taken for Epoch 2:4.25 - F1: 0.4714
2026-02-13 22:07:06 - INFO - Time taken for Epoch 2:4.25 - F1: 0.4714
Time taken for Epoch 3:3.09 - F1: 0.5013
2026-02-13 22:07:09 - INFO - Time taken for Epoch 3:3.09 - F1: 0.5013
Time taken for Epoch 4:4.37 - F1: 0.4949
2026-02-13 22:07:13 - INFO - Time taken for Epoch 4:4.37 - F1: 0.4949
Time taken for Epoch 5:3.11 - F1: 0.4855
2026-02-13 22:07:16 - INFO - Time taken for Epoch 5:3.11 - F1: 0.4855
Time taken for Epoch 6:3.11 - F1: 0.4882
2026-02-13 22:07:19 - INFO - Time taken for Epoch 6:3.11 - F1: 0.4882
Time taken for Epoch 7:3.11 - F1: 0.4891
2026-02-13 22:07:22 - INFO - Time taken for Epoch 7:3.11 - F1: 0.4891
Time taken for Epoch 8:3.11 - F1: 0.5092
2026-02-13 22:07:25 - INFO - Time taken for Epoch 8:3.11 - F1: 0.5092
Time taken for Epoch 9:4.37 - F1: 0.5176
2026-02-13 22:07:30 - INFO - Time taken for Epoch 9:4.37 - F1: 0.5176
Time taken for Epoch 10:4.50 - F1: 0.5099
2026-02-13 22:07:34 - INFO - Time taken for Epoch 10:4.50 - F1: 0.5099
Time taken for Epoch 11:3.08 - F1: 0.5074
2026-02-13 22:07:37 - INFO - Time taken for Epoch 11:3.08 - F1: 0.5074
Time taken for Epoch 12:3.08 - F1: 0.5521
2026-02-13 22:07:41 - INFO - Time taken for Epoch 12:3.08 - F1: 0.5521
Time taken for Epoch 13:4.34 - F1: 0.5676
2026-02-13 22:07:45 - INFO - Time taken for Epoch 13:4.34 - F1: 0.5676
Time taken for Epoch 14:4.36 - F1: 0.5873
2026-02-13 22:07:49 - INFO - Time taken for Epoch 14:4.36 - F1: 0.5873
Time taken for Epoch 15:4.34 - F1: 0.5942
2026-02-13 22:07:54 - INFO - Time taken for Epoch 15:4.34 - F1: 0.5942
Time taken for Epoch 16:4.36 - F1: 0.5999
2026-02-13 22:07:58 - INFO - Time taken for Epoch 16:4.36 - F1: 0.5999
Time taken for Epoch 17:4.33 - F1: 0.5980
2026-02-13 22:08:02 - INFO - Time taken for Epoch 17:4.33 - F1: 0.5980
Time taken for Epoch 18:3.08 - F1: 0.6161
2026-02-13 22:08:05 - INFO - Time taken for Epoch 18:3.08 - F1: 0.6161
Time taken for Epoch 19:4.34 - F1: 0.6016
2026-02-13 22:08:10 - INFO - Time taken for Epoch 19:4.34 - F1: 0.6016
Time taken for Epoch 20:3.08 - F1: 0.6056
2026-02-13 22:08:13 - INFO - Time taken for Epoch 20:3.08 - F1: 0.6056
Time taken for Epoch 21:3.08 - F1: 0.6096
2026-02-13 22:08:16 - INFO - Time taken for Epoch 21:3.08 - F1: 0.6096
Time taken for Epoch 22:3.08 - F1: 0.6045
2026-02-13 22:08:19 - INFO - Time taken for Epoch 22:3.08 - F1: 0.6045
Time taken for Epoch 23:3.08 - F1: 0.6013
2026-02-13 22:08:22 - INFO - Time taken for Epoch 23:3.08 - F1: 0.6013
Time taken for Epoch 24:3.08 - F1: 0.5968
2026-02-13 22:08:25 - INFO - Time taken for Epoch 24:3.08 - F1: 0.5968
Time taken for Epoch 25:3.08 - F1: 0.5970
2026-02-13 22:08:28 - INFO - Time taken for Epoch 25:3.08 - F1: 0.5970
Time taken for Epoch 26:3.08 - F1: 0.5969
2026-02-13 22:08:31 - INFO - Time taken for Epoch 26:3.08 - F1: 0.5969
Time taken for Epoch 27:3.08 - F1: 0.5970
2026-02-13 22:08:34 - INFO - Time taken for Epoch 27:3.08 - F1: 0.5970
Time taken for Epoch 28:3.08 - F1: 0.5956
2026-02-13 22:08:37 - INFO - Time taken for Epoch 28:3.08 - F1: 0.5956
Performance not improving for 10 consecutive epochs.
2026-02-13 22:08:37 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6161 - Best Epoch:17
2026-02-13 22:08:37 - INFO - Best F1:0.6161 - Best Epoch:17
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6231, Test ECE: 0.0892
2026-02-13 22:08:46 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6231, Test ECE: 0.0892
All results: {'f1_macro': 0.6230889813176321, 'ece': np.float64(0.08917315469618146)}
2026-02-13 22:08:46 - INFO - All results: {'f1_macro': 0.6230889813176321, 'ece': np.float64(0.08917315469618146)}

Total time taken: 373.99 seconds
2026-02-13 22:08:46 - INFO - 
Total time taken: 373.99 seconds
2026-02-13 22:08:46 - INFO - Trial 1 finished with value: 0.6230889813176321 and parameters: {'learning_rate': 2.1006697587331224e-05, 'weight_decay': 0.000310345408072706, 'batch_size': 8, 'co_train_epochs': 5, 'epoch_patience': 6}. Best is trial 0 with value: 0.6681542515203446.
Using devices: cuda, cuda
2026-02-13 22:08:46 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 22:08:46 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 22:08:46 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 22:08:46 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 1.4388782249155893e-05
Weight Decay: 0.004350928193987075
Batch Size: 32
No. Epochs: 14
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-13 22:08:46 - INFO - Learning Rate: 1.4388782249155893e-05
Weight Decay: 0.004350928193987075
Batch Size: 32
No. Epochs: 14
Epoch Patience: 9
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 22:08:47 - INFO - Generating initial weights
Time taken for Epoch 1:20.28 - F1: 0.0531
2026-02-13 22:09:11 - INFO - Time taken for Epoch 1:20.28 - F1: 0.0531
Time taken for Epoch 2:20.26 - F1: 0.0615
2026-02-13 22:09:31 - INFO - Time taken for Epoch 2:20.26 - F1: 0.0615
Time taken for Epoch 3:20.26 - F1: 0.0794
2026-02-13 22:09:51 - INFO - Time taken for Epoch 3:20.26 - F1: 0.0794
Time taken for Epoch 4:20.30 - F1: 0.0933
2026-02-13 22:10:12 - INFO - Time taken for Epoch 4:20.30 - F1: 0.0933
Time taken for Epoch 5:20.32 - F1: 0.1005
2026-02-13 22:10:32 - INFO - Time taken for Epoch 5:20.32 - F1: 0.1005
Time taken for Epoch 6:20.35 - F1: 0.1082
2026-02-13 22:10:52 - INFO - Time taken for Epoch 6:20.35 - F1: 0.1082
Time taken for Epoch 7:20.37 - F1: 0.1179
2026-02-13 22:11:13 - INFO - Time taken for Epoch 7:20.37 - F1: 0.1179
Time taken for Epoch 8:20.36 - F1: 0.1347
2026-02-13 22:11:33 - INFO - Time taken for Epoch 8:20.36 - F1: 0.1347
Time taken for Epoch 9:20.34 - F1: 0.1417
2026-02-13 22:11:53 - INFO - Time taken for Epoch 9:20.34 - F1: 0.1417
Time taken for Epoch 10:20.35 - F1: 0.1737
2026-02-13 22:12:14 - INFO - Time taken for Epoch 10:20.35 - F1: 0.1737
Time taken for Epoch 11:20.36 - F1: 0.1739
2026-02-13 22:12:34 - INFO - Time taken for Epoch 11:20.36 - F1: 0.1739
Time taken for Epoch 12:20.34 - F1: 0.1742
2026-02-13 22:12:54 - INFO - Time taken for Epoch 12:20.34 - F1: 0.1742
Time taken for Epoch 13:20.36 - F1: 0.1768
2026-02-13 22:13:15 - INFO - Time taken for Epoch 13:20.36 - F1: 0.1768
Time taken for Epoch 14:20.37 - F1: 0.1804
2026-02-13 22:13:35 - INFO - Time taken for Epoch 14:20.37 - F1: 0.1804
Best F1:0.1804 - Best Epoch:14
2026-02-13 22:13:35 - INFO - Best F1:0.1804 - Best Epoch:14
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 22:13:36 - INFO - Starting co-training
Time taken for Epoch 1: 35.69s - F1: 0.29934950
2026-02-13 22:14:12 - INFO - Time taken for Epoch 1: 35.69s - F1: 0.29934950
Time taken for Epoch 2: 36.89s - F1: 0.44902878
2026-02-13 22:14:49 - INFO - Time taken for Epoch 2: 36.89s - F1: 0.44902878
Time taken for Epoch 3: 37.00s - F1: 0.51599033
2026-02-13 22:15:26 - INFO - Time taken for Epoch 3: 37.00s - F1: 0.51599033
Time taken for Epoch 4: 37.00s - F1: 0.53312102
2026-02-13 22:16:03 - INFO - Time taken for Epoch 4: 37.00s - F1: 0.53312102
Time taken for Epoch 5: 37.02s - F1: 0.60874827
2026-02-13 22:16:40 - INFO - Time taken for Epoch 5: 37.02s - F1: 0.60874827
Time taken for Epoch 6: 37.03s - F1: 0.61891515
2026-02-13 22:17:17 - INFO - Time taken for Epoch 6: 37.03s - F1: 0.61891515
Time taken for Epoch 7: 37.04s - F1: 0.63253880
2026-02-13 22:17:54 - INFO - Time taken for Epoch 7: 37.04s - F1: 0.63253880
Time taken for Epoch 8: 37.01s - F1: 0.63576396
2026-02-13 22:18:31 - INFO - Time taken for Epoch 8: 37.01s - F1: 0.63576396
Time taken for Epoch 9: 37.01s - F1: 0.65176891
2026-02-13 22:19:08 - INFO - Time taken for Epoch 9: 37.01s - F1: 0.65176891
Time taken for Epoch 10: 36.98s - F1: 0.64588415
2026-02-13 22:19:45 - INFO - Time taken for Epoch 10: 36.98s - F1: 0.64588415
Time taken for Epoch 11: 35.75s - F1: 0.64943610
2026-02-13 22:20:21 - INFO - Time taken for Epoch 11: 35.75s - F1: 0.64943610
Time taken for Epoch 12: 35.76s - F1: 0.65439180
2026-02-13 22:20:57 - INFO - Time taken for Epoch 12: 35.76s - F1: 0.65439180
Time taken for Epoch 13: 37.12s - F1: 0.66008457
2026-02-13 22:21:34 - INFO - Time taken for Epoch 13: 37.12s - F1: 0.66008457
Time taken for Epoch 14: 36.94s - F1: 0.65283620
2026-02-13 22:22:11 - INFO - Time taken for Epoch 14: 36.94s - F1: 0.65283620
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 22:22:14 - INFO - Fine-tuning models
Time taken for Epoch 1:2.80 - F1: 0.6591
2026-02-13 22:22:17 - INFO - Time taken for Epoch 1:2.80 - F1: 0.6591
Time taken for Epoch 2:4.02 - F1: 0.6634
2026-02-13 22:22:21 - INFO - Time taken for Epoch 2:4.02 - F1: 0.6634
Time taken for Epoch 3:4.08 - F1: 0.6682
2026-02-13 22:22:25 - INFO - Time taken for Epoch 3:4.08 - F1: 0.6682
Time taken for Epoch 4:4.78 - F1: 0.6574
2026-02-13 22:22:30 - INFO - Time taken for Epoch 4:4.78 - F1: 0.6574
Time taken for Epoch 5:2.79 - F1: 0.6591
2026-02-13 22:22:32 - INFO - Time taken for Epoch 5:2.79 - F1: 0.6591
Time taken for Epoch 6:2.79 - F1: 0.6576
2026-02-13 22:22:35 - INFO - Time taken for Epoch 6:2.79 - F1: 0.6576
Time taken for Epoch 7:2.79 - F1: 0.6628
2026-02-13 22:22:38 - INFO - Time taken for Epoch 7:2.79 - F1: 0.6628
Time taken for Epoch 8:2.80 - F1: 0.6674
2026-02-13 22:22:41 - INFO - Time taken for Epoch 8:2.80 - F1: 0.6674
Time taken for Epoch 9:2.80 - F1: 0.6581
2026-02-13 22:22:44 - INFO - Time taken for Epoch 9:2.80 - F1: 0.6581
Time taken for Epoch 10:2.80 - F1: 0.6547
2026-02-13 22:22:46 - INFO - Time taken for Epoch 10:2.80 - F1: 0.6547
Time taken for Epoch 11:2.80 - F1: 0.6453
2026-02-13 22:22:49 - INFO - Time taken for Epoch 11:2.80 - F1: 0.6453
Time taken for Epoch 12:2.80 - F1: 0.6420
2026-02-13 22:22:52 - INFO - Time taken for Epoch 12:2.80 - F1: 0.6420
Time taken for Epoch 13:2.80 - F1: 0.6435
2026-02-13 22:22:55 - INFO - Time taken for Epoch 13:2.80 - F1: 0.6435
Performance not improving for 10 consecutive epochs.
2026-02-13 22:22:55 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6682 - Best Epoch:2
2026-02-13 22:22:55 - INFO - Best F1:0.6682 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6606, Test ECE: 0.0254
2026-02-13 22:23:02 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6606, Test ECE: 0.0254
All results: {'f1_macro': 0.6605739602884342, 'ece': np.float64(0.02535558871191186)}
2026-02-13 22:23:02 - INFO - All results: {'f1_macro': 0.6605739602884342, 'ece': np.float64(0.02535558871191186)}

Total time taken: 856.71 seconds
2026-02-13 22:23:02 - INFO - 
Total time taken: 856.71 seconds
2026-02-13 22:23:02 - INFO - Trial 2 finished with value: 0.6605739602884342 and parameters: {'learning_rate': 1.4388782249155893e-05, 'weight_decay': 0.004350928193987075, 'batch_size': 32, 'co_train_epochs': 14, 'epoch_patience': 9}. Best is trial 0 with value: 0.6681542515203446.
Using devices: cuda, cuda
2026-02-13 22:23:02 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 22:23:02 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 22:23:02 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 22:23:02 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 4.378503680144357e-05
Weight Decay: 4.0207673431112954e-05
Batch Size: 16
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 4
2026-02-13 22:23:03 - INFO - Learning Rate: 4.378503680144357e-05
Weight Decay: 4.0207673431112954e-05
Batch Size: 16
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 22:23:04 - INFO - Generating initial weights
Time taken for Epoch 1:20.85 - F1: 0.0421
2026-02-13 22:23:28 - INFO - Time taken for Epoch 1:20.85 - F1: 0.0421
Time taken for Epoch 2:20.84 - F1: 0.0189
2026-02-13 22:23:49 - INFO - Time taken for Epoch 2:20.84 - F1: 0.0189
Time taken for Epoch 3:20.84 - F1: 0.0189
2026-02-13 22:24:10 - INFO - Time taken for Epoch 3:20.84 - F1: 0.0189
Time taken for Epoch 4:20.86 - F1: 0.0189
2026-02-13 22:24:31 - INFO - Time taken for Epoch 4:20.86 - F1: 0.0189
Time taken for Epoch 5:20.90 - F1: 0.0189
2026-02-13 22:24:52 - INFO - Time taken for Epoch 5:20.90 - F1: 0.0189
Time taken for Epoch 6:20.94 - F1: 0.0189
2026-02-13 22:25:13 - INFO - Time taken for Epoch 6:20.94 - F1: 0.0189
Time taken for Epoch 7:20.93 - F1: 0.0189
2026-02-13 22:25:33 - INFO - Time taken for Epoch 7:20.93 - F1: 0.0189
Time taken for Epoch 8:20.93 - F1: 0.0189
2026-02-13 22:25:54 - INFO - Time taken for Epoch 8:20.93 - F1: 0.0189
Time taken for Epoch 9:20.92 - F1: 0.0189
2026-02-13 22:26:15 - INFO - Time taken for Epoch 9:20.92 - F1: 0.0189
Time taken for Epoch 10:20.94 - F1: 0.0189
2026-02-13 22:26:36 - INFO - Time taken for Epoch 10:20.94 - F1: 0.0189
Time taken for Epoch 11:20.94 - F1: 0.0189
2026-02-13 22:26:57 - INFO - Time taken for Epoch 11:20.94 - F1: 0.0189
Time taken for Epoch 12:20.95 - F1: 0.0189
2026-02-13 22:27:18 - INFO - Time taken for Epoch 12:20.95 - F1: 0.0189
Time taken for Epoch 13:20.95 - F1: 0.0220
2026-02-13 22:27:39 - INFO - Time taken for Epoch 13:20.95 - F1: 0.0220
Time taken for Epoch 14:20.92 - F1: 0.0367
2026-02-13 22:28:00 - INFO - Time taken for Epoch 14:20.92 - F1: 0.0367
Time taken for Epoch 15:20.93 - F1: 0.0734
2026-02-13 22:28:21 - INFO - Time taken for Epoch 15:20.93 - F1: 0.0734
Time taken for Epoch 16:20.91 - F1: 0.1174
2026-02-13 22:28:42 - INFO - Time taken for Epoch 16:20.91 - F1: 0.1174
Time taken for Epoch 17:20.95 - F1: 0.1701
2026-02-13 22:29:03 - INFO - Time taken for Epoch 17:20.95 - F1: 0.1701
Time taken for Epoch 18:20.91 - F1: 0.2258
2026-02-13 22:29:24 - INFO - Time taken for Epoch 18:20.91 - F1: 0.2258
Time taken for Epoch 19:20.94 - F1: 0.2444
2026-02-13 22:29:45 - INFO - Time taken for Epoch 19:20.94 - F1: 0.2444
Best F1:0.2444 - Best Epoch:19
2026-02-13 22:29:45 - INFO - Best F1:0.2444 - Best Epoch:19
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 22:29:46 - INFO - Starting co-training
Time taken for Epoch 1: 29.63s - F1: 0.31700115
2026-02-13 22:30:16 - INFO - Time taken for Epoch 1: 29.63s - F1: 0.31700115
Time taken for Epoch 2: 30.81s - F1: 0.49523433
2026-02-13 22:30:47 - INFO - Time taken for Epoch 2: 30.81s - F1: 0.49523433
Time taken for Epoch 3: 30.89s - F1: 0.49746461
2026-02-13 22:31:18 - INFO - Time taken for Epoch 3: 30.89s - F1: 0.49746461
Time taken for Epoch 4: 30.89s - F1: 0.53151387
2026-02-13 22:31:49 - INFO - Time taken for Epoch 4: 30.89s - F1: 0.53151387
Time taken for Epoch 5: 30.92s - F1: 0.61635559
2026-02-13 22:32:19 - INFO - Time taken for Epoch 5: 30.92s - F1: 0.61635559
Time taken for Epoch 6: 30.87s - F1: 0.61540254
2026-02-13 22:32:50 - INFO - Time taken for Epoch 6: 30.87s - F1: 0.61540254
Time taken for Epoch 7: 29.63s - F1: 0.62376017
2026-02-13 22:33:20 - INFO - Time taken for Epoch 7: 29.63s - F1: 0.62376017
Time taken for Epoch 8: 30.91s - F1: 0.64592101
2026-02-13 22:33:51 - INFO - Time taken for Epoch 8: 30.91s - F1: 0.64592101
Time taken for Epoch 9: 30.91s - F1: 0.63675290
2026-02-13 22:34:22 - INFO - Time taken for Epoch 9: 30.91s - F1: 0.63675290
Time taken for Epoch 10: 29.63s - F1: 0.62530719
2026-02-13 22:34:51 - INFO - Time taken for Epoch 10: 29.63s - F1: 0.62530719
Time taken for Epoch 11: 29.66s - F1: 0.62700432
2026-02-13 22:35:21 - INFO - Time taken for Epoch 11: 29.66s - F1: 0.62700432
Time taken for Epoch 12: 29.65s - F1: 0.62540143
2026-02-13 22:35:51 - INFO - Time taken for Epoch 12: 29.65s - F1: 0.62540143
Time taken for Epoch 13: 29.81s - F1: 0.62893633
2026-02-13 22:36:20 - INFO - Time taken for Epoch 13: 29.81s - F1: 0.62893633
Time taken for Epoch 14: 29.66s - F1: 0.62622649
2026-02-13 22:36:50 - INFO - Time taken for Epoch 14: 29.66s - F1: 0.62622649
Time taken for Epoch 15: 29.67s - F1: 0.61437968
2026-02-13 22:37:20 - INFO - Time taken for Epoch 15: 29.67s - F1: 0.61437968
Time taken for Epoch 16: 29.68s - F1: 0.61396596
2026-02-13 22:37:50 - INFO - Time taken for Epoch 16: 29.68s - F1: 0.61396596
Time taken for Epoch 17: 29.66s - F1: 0.60281544
2026-02-13 22:38:19 - INFO - Time taken for Epoch 17: 29.66s - F1: 0.60281544
Time taken for Epoch 18: 29.69s - F1: 0.63189487
2026-02-13 22:38:49 - INFO - Time taken for Epoch 18: 29.69s - F1: 0.63189487
Performance not improving for 10 consecutive epochs.
Performance not improving for 10 consecutive epochs.
2026-02-13 22:38:49 - INFO - Performance not improving for 10 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 22:38:52 - INFO - Fine-tuning models
Time taken for Epoch 1:3.28 - F1: 0.6455
2026-02-13 22:38:55 - INFO - Time taken for Epoch 1:3.28 - F1: 0.6455
Time taken for Epoch 2:4.02 - F1: 0.6497
2026-02-13 22:38:59 - INFO - Time taken for Epoch 2:4.02 - F1: 0.6497
Time taken for Epoch 3:4.18 - F1: 0.6474
2026-02-13 22:39:03 - INFO - Time taken for Epoch 3:4.18 - F1: 0.6474
Time taken for Epoch 4:2.90 - F1: 0.6392
2026-02-13 22:39:06 - INFO - Time taken for Epoch 4:2.90 - F1: 0.6392
Time taken for Epoch 5:2.90 - F1: 0.6543
2026-02-13 22:39:09 - INFO - Time taken for Epoch 5:2.90 - F1: 0.6543
Time taken for Epoch 6:4.18 - F1: 0.6502
2026-02-13 22:39:13 - INFO - Time taken for Epoch 6:4.18 - F1: 0.6502
Time taken for Epoch 7:2.89 - F1: 0.6366
2026-02-13 22:39:16 - INFO - Time taken for Epoch 7:2.89 - F1: 0.6366
Time taken for Epoch 8:2.90 - F1: 0.6085
2026-02-13 22:39:19 - INFO - Time taken for Epoch 8:2.90 - F1: 0.6085
Time taken for Epoch 9:2.90 - F1: 0.5769
2026-02-13 22:39:22 - INFO - Time taken for Epoch 9:2.90 - F1: 0.5769
Time taken for Epoch 10:2.90 - F1: 0.5719
2026-02-13 22:39:25 - INFO - Time taken for Epoch 10:2.90 - F1: 0.5719
Time taken for Epoch 11:2.90 - F1: 0.5807
2026-02-13 22:39:28 - INFO - Time taken for Epoch 11:2.90 - F1: 0.5807
Time taken for Epoch 12:2.90 - F1: 0.5916
2026-02-13 22:39:31 - INFO - Time taken for Epoch 12:2.90 - F1: 0.5916
Time taken for Epoch 13:2.90 - F1: 0.5945
2026-02-13 22:39:34 - INFO - Time taken for Epoch 13:2.90 - F1: 0.5945
Time taken for Epoch 14:2.90 - F1: 0.6116
2026-02-13 22:39:36 - INFO - Time taken for Epoch 14:2.90 - F1: 0.6116
Time taken for Epoch 15:2.90 - F1: 0.6209
2026-02-13 22:39:39 - INFO - Time taken for Epoch 15:2.90 - F1: 0.6209
Performance not improving for 10 consecutive epochs.
2026-02-13 22:39:39 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6543 - Best Epoch:4
2026-02-13 22:39:39 - INFO - Best F1:0.6543 - Best Epoch:4
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6559, Test ECE: 0.0783
2026-02-13 22:39:47 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6559, Test ECE: 0.0783
All results: {'f1_macro': 0.655878306907914, 'ece': np.float64(0.07833932395946964)}
2026-02-13 22:39:47 - INFO - All results: {'f1_macro': 0.655878306907914, 'ece': np.float64(0.07833932395946964)}

Total time taken: 1004.80 seconds
2026-02-13 22:39:47 - INFO - 
Total time taken: 1004.80 seconds
2026-02-13 22:39:47 - INFO - Trial 3 finished with value: 0.655878306907914 and parameters: {'learning_rate': 4.378503680144357e-05, 'weight_decay': 4.0207673431112954e-05, 'batch_size': 16, 'co_train_epochs': 19, 'epoch_patience': 10}. Best is trial 0 with value: 0.6681542515203446.
Using devices: cuda, cuda
2026-02-13 22:39:47 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 22:39:47 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 22:39:47 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 22:39:47 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0006906329059885229
Weight Decay: 4.664912426642837e-05
Batch Size: 32
No. Epochs: 19
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-13 22:39:48 - INFO - Learning Rate: 0.0006906329059885229
Weight Decay: 4.664912426642837e-05
Batch Size: 32
No. Epochs: 19
Epoch Patience: 7
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 22:39:49 - INFO - Generating initial weights
Time taken for Epoch 1:20.25 - F1: 0.1113
2026-02-13 22:40:12 - INFO - Time taken for Epoch 1:20.25 - F1: 0.1113
Time taken for Epoch 2:20.19 - F1: 0.0126
2026-02-13 22:40:33 - INFO - Time taken for Epoch 2:20.19 - F1: 0.0126
Time taken for Epoch 3:20.25 - F1: 0.1278
2026-02-13 22:40:53 - INFO - Time taken for Epoch 3:20.25 - F1: 0.1278
Time taken for Epoch 4:20.26 - F1: 0.0981
2026-02-13 22:41:13 - INFO - Time taken for Epoch 4:20.26 - F1: 0.0981
Time taken for Epoch 5:20.26 - F1: 0.0572
2026-02-13 22:41:33 - INFO - Time taken for Epoch 5:20.26 - F1: 0.0572
Time taken for Epoch 6:20.25 - F1: 0.2559
2026-02-13 22:41:54 - INFO - Time taken for Epoch 6:20.25 - F1: 0.2559
Time taken for Epoch 7:20.25 - F1: 0.2476
2026-02-13 22:42:14 - INFO - Time taken for Epoch 7:20.25 - F1: 0.2476
Time taken for Epoch 8:20.23 - F1: 0.3487
2026-02-13 22:42:34 - INFO - Time taken for Epoch 8:20.23 - F1: 0.3487
Time taken for Epoch 9:20.20 - F1: 0.3420
2026-02-13 22:42:54 - INFO - Time taken for Epoch 9:20.20 - F1: 0.3420
Time taken for Epoch 10:20.25 - F1: 0.3246
2026-02-13 22:43:14 - INFO - Time taken for Epoch 10:20.25 - F1: 0.3246
Time taken for Epoch 11:20.24 - F1: 0.3296
2026-02-13 22:43:35 - INFO - Time taken for Epoch 11:20.24 - F1: 0.3296
Time taken for Epoch 12:20.26 - F1: 0.3330
2026-02-13 22:43:55 - INFO - Time taken for Epoch 12:20.26 - F1: 0.3330
Time taken for Epoch 13:20.30 - F1: 0.3365
2026-02-13 22:44:15 - INFO - Time taken for Epoch 13:20.30 - F1: 0.3365
Time taken for Epoch 14:20.18 - F1: 0.3357
2026-02-13 22:44:35 - INFO - Time taken for Epoch 14:20.18 - F1: 0.3357
Time taken for Epoch 15:20.21 - F1: 0.3355
2026-02-13 22:44:56 - INFO - Time taken for Epoch 15:20.21 - F1: 0.3355
Time taken for Epoch 16:20.24 - F1: 0.3306
2026-02-13 22:45:16 - INFO - Time taken for Epoch 16:20.24 - F1: 0.3306
Time taken for Epoch 17:20.23 - F1: 0.3288
2026-02-13 22:45:36 - INFO - Time taken for Epoch 17:20.23 - F1: 0.3288
Time taken for Epoch 18:20.20 - F1: 0.3325
2026-02-13 22:45:56 - INFO - Time taken for Epoch 18:20.20 - F1: 0.3325
Time taken for Epoch 19:20.22 - F1: 0.3333
2026-02-13 22:46:17 - INFO - Time taken for Epoch 19:20.22 - F1: 0.3333
Best F1:0.3487 - Best Epoch:8
2026-02-13 22:46:17 - INFO - Best F1:0.3487 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 22:46:18 - INFO - Starting co-training
Time taken for Epoch 1: 35.66s - F1: 0.04755179
2026-02-13 22:46:54 - INFO - Time taken for Epoch 1: 35.66s - F1: 0.04755179
Time taken for Epoch 2: 36.78s - F1: 0.04755179
2026-02-13 22:47:31 - INFO - Time taken for Epoch 2: 36.78s - F1: 0.04755179
Time taken for Epoch 3: 35.71s - F1: 0.04755179
2026-02-13 22:48:06 - INFO - Time taken for Epoch 3: 35.71s - F1: 0.04755179
Time taken for Epoch 4: 35.71s - F1: 0.04755179
2026-02-13 22:48:42 - INFO - Time taken for Epoch 4: 35.71s - F1: 0.04755179
Time taken for Epoch 5: 35.73s - F1: 0.04755179
2026-02-13 22:49:18 - INFO - Time taken for Epoch 5: 35.73s - F1: 0.04755179
Time taken for Epoch 6: 35.73s - F1: 0.04755179
2026-02-13 22:49:53 - INFO - Time taken for Epoch 6: 35.73s - F1: 0.04755179
Time taken for Epoch 7: 35.77s - F1: 0.04755179
2026-02-13 22:50:29 - INFO - Time taken for Epoch 7: 35.77s - F1: 0.04755179
Time taken for Epoch 8: 35.74s - F1: 0.04755179
2026-02-13 22:51:05 - INFO - Time taken for Epoch 8: 35.74s - F1: 0.04755179
Performance not improving for 7 consecutive epochs.
Performance not improving for 7 consecutive epochs.
2026-02-13 22:51:05 - INFO - Performance not improving for 7 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 22:51:08 - INFO - Fine-tuning models
Time taken for Epoch 1:2.79 - F1: 0.0197
2026-02-13 22:51:11 - INFO - Time taken for Epoch 1:2.79 - F1: 0.0197
Time taken for Epoch 2:3.94 - F1: 0.0363
2026-02-13 22:51:15 - INFO - Time taken for Epoch 2:3.94 - F1: 0.0363
Time taken for Epoch 3:4.06 - F1: 0.0038
2026-02-13 22:51:19 - INFO - Time taken for Epoch 3:4.06 - F1: 0.0038
Time taken for Epoch 4:2.78 - F1: 0.0064
2026-02-13 22:51:21 - INFO - Time taken for Epoch 4:2.78 - F1: 0.0064
Time taken for Epoch 5:2.78 - F1: 0.0064
2026-02-13 22:51:24 - INFO - Time taken for Epoch 5:2.78 - F1: 0.0064
Time taken for Epoch 6:2.78 - F1: 0.0394
2026-02-13 22:51:27 - INFO - Time taken for Epoch 6:2.78 - F1: 0.0394
Time taken for Epoch 7:4.07 - F1: 0.0394
2026-02-13 22:51:31 - INFO - Time taken for Epoch 7:4.07 - F1: 0.0394
Time taken for Epoch 8:2.78 - F1: 0.0394
2026-02-13 22:51:34 - INFO - Time taken for Epoch 8:2.78 - F1: 0.0394
Time taken for Epoch 9:2.78 - F1: 0.0081
2026-02-13 22:51:37 - INFO - Time taken for Epoch 9:2.78 - F1: 0.0081
Time taken for Epoch 10:2.78 - F1: 0.0081
2026-02-13 22:51:39 - INFO - Time taken for Epoch 10:2.78 - F1: 0.0081
Time taken for Epoch 11:2.78 - F1: 0.0363
2026-02-13 22:51:42 - INFO - Time taken for Epoch 11:2.78 - F1: 0.0363
Time taken for Epoch 12:2.78 - F1: 0.0476
2026-02-13 22:51:45 - INFO - Time taken for Epoch 12:2.78 - F1: 0.0476
Time taken for Epoch 13:4.06 - F1: 0.0476
2026-02-13 22:51:49 - INFO - Time taken for Epoch 13:4.06 - F1: 0.0476
Time taken for Epoch 14:2.78 - F1: 0.0197
2026-02-13 22:51:52 - INFO - Time taken for Epoch 14:2.78 - F1: 0.0197
Time taken for Epoch 15:2.78 - F1: 0.0197
2026-02-13 22:51:55 - INFO - Time taken for Epoch 15:2.78 - F1: 0.0197
Time taken for Epoch 16:2.78 - F1: 0.0189
2026-02-13 22:51:57 - INFO - Time taken for Epoch 16:2.78 - F1: 0.0189
Time taken for Epoch 17:2.77 - F1: 0.0189
2026-02-13 22:52:00 - INFO - Time taken for Epoch 17:2.77 - F1: 0.0189
Time taken for Epoch 18:2.78 - F1: 0.0394
2026-02-13 22:52:03 - INFO - Time taken for Epoch 18:2.78 - F1: 0.0394
Time taken for Epoch 19:2.78 - F1: 0.0394
2026-02-13 22:52:06 - INFO - Time taken for Epoch 19:2.78 - F1: 0.0394
Time taken for Epoch 20:2.78 - F1: 0.0089
2026-02-13 22:52:08 - INFO - Time taken for Epoch 20:2.78 - F1: 0.0089
Time taken for Epoch 21:2.78 - F1: 0.0363
2026-02-13 22:52:11 - INFO - Time taken for Epoch 21:2.78 - F1: 0.0363
Time taken for Epoch 22:2.78 - F1: 0.0363
2026-02-13 22:52:14 - INFO - Time taken for Epoch 22:2.78 - F1: 0.0363
Performance not improving for 10 consecutive epochs.
2026-02-13 22:52:14 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:11
2026-02-13 22:52:14 - INFO - Best F1:0.0476 - Best Epoch:11
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0091
2026-02-13 22:52:22 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0091
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.009115078640215601)}
2026-02-13 22:52:22 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.009115078640215601)}

Total time taken: 754.38 seconds
2026-02-13 22:52:22 - INFO - 
Total time taken: 754.38 seconds
2026-02-13 22:52:22 - INFO - Trial 4 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0006906329059885229, 'weight_decay': 4.664912426642837e-05, 'batch_size': 32, 'co_train_epochs': 19, 'epoch_patience': 7}. Best is trial 0 with value: 0.6681542515203446.
Using devices: cuda, cuda
2026-02-13 22:52:22 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 22:52:22 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 22:52:22 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 22:52:22 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 1.7431823265298438e-05
Weight Decay: 3.64512246403886e-05
Batch Size: 64
No. Epochs: 11
Epoch Patience: 10
 Accumulation Steps: 1
2026-02-13 22:52:22 - INFO - Learning Rate: 1.7431823265298438e-05
Weight Decay: 3.64512246403886e-05
Batch Size: 64
No. Epochs: 11
Epoch Patience: 10
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 22:52:23 - INFO - Generating initial weights
Time taken for Epoch 1:19.18 - F1: 0.0562
2026-02-13 22:52:46 - INFO - Time taken for Epoch 1:19.18 - F1: 0.0562
Time taken for Epoch 2:19.12 - F1: 0.0651
2026-02-13 22:53:05 - INFO - Time taken for Epoch 2:19.12 - F1: 0.0651
Time taken for Epoch 3:19.17 - F1: 0.0929
2026-02-13 22:53:24 - INFO - Time taken for Epoch 3:19.17 - F1: 0.0929
Time taken for Epoch 4:19.15 - F1: 0.0995
2026-02-13 22:53:43 - INFO - Time taken for Epoch 4:19.15 - F1: 0.0995
Time taken for Epoch 5:19.18 - F1: 0.1179
2026-02-13 22:54:02 - INFO - Time taken for Epoch 5:19.18 - F1: 0.1179
Time taken for Epoch 6:19.19 - F1: 0.1401
2026-02-13 22:54:21 - INFO - Time taken for Epoch 6:19.19 - F1: 0.1401
Time taken for Epoch 7:19.19 - F1: 0.1616
2026-02-13 22:54:41 - INFO - Time taken for Epoch 7:19.19 - F1: 0.1616
Time taken for Epoch 8:19.19 - F1: 0.1660
2026-02-13 22:55:00 - INFO - Time taken for Epoch 8:19.19 - F1: 0.1660
Time taken for Epoch 9:19.22 - F1: 0.1786
2026-02-13 22:55:19 - INFO - Time taken for Epoch 9:19.22 - F1: 0.1786
Time taken for Epoch 10:19.20 - F1: 0.1804
2026-02-13 22:55:38 - INFO - Time taken for Epoch 10:19.20 - F1: 0.1804
Time taken for Epoch 11:19.21 - F1: 0.1836
2026-02-13 22:55:57 - INFO - Time taken for Epoch 11:19.21 - F1: 0.1836
Best F1:0.1836 - Best Epoch:11
2026-02-13 22:55:57 - INFO - Best F1:0.1836 - Best Epoch:11
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 22:55:59 - INFO - Starting co-training
Time taken for Epoch 1: 46.69s - F1: 0.45325863
2026-02-13 22:56:46 - INFO - Time taken for Epoch 1: 46.69s - F1: 0.45325863
Time taken for Epoch 2: 47.85s - F1: 0.51941934
2026-02-13 22:57:33 - INFO - Time taken for Epoch 2: 47.85s - F1: 0.51941934
Time taken for Epoch 3: 48.00s - F1: 0.59194251
2026-02-13 22:58:21 - INFO - Time taken for Epoch 3: 48.00s - F1: 0.59194251
Time taken for Epoch 4: 47.99s - F1: 0.62460442
2026-02-13 22:59:09 - INFO - Time taken for Epoch 4: 47.99s - F1: 0.62460442
Time taken for Epoch 5: 47.98s - F1: 0.62454540
2026-02-13 22:59:57 - INFO - Time taken for Epoch 5: 47.98s - F1: 0.62454540
Time taken for Epoch 6: 46.79s - F1: 0.63891946
2026-02-13 23:00:44 - INFO - Time taken for Epoch 6: 46.79s - F1: 0.63891946
Time taken for Epoch 7: 47.99s - F1: 0.65015177
2026-02-13 23:01:32 - INFO - Time taken for Epoch 7: 47.99s - F1: 0.65015177
Time taken for Epoch 8: 47.98s - F1: 0.64477508
2026-02-13 23:02:20 - INFO - Time taken for Epoch 8: 47.98s - F1: 0.64477508
Time taken for Epoch 9: 46.74s - F1: 0.64330274
2026-02-13 23:03:07 - INFO - Time taken for Epoch 9: 46.74s - F1: 0.64330274
Time taken for Epoch 10: 46.73s - F1: 0.65028509
2026-02-13 23:03:54 - INFO - Time taken for Epoch 10: 46.73s - F1: 0.65028509
Time taken for Epoch 11: 47.96s - F1: 0.64525345
2026-02-13 23:04:42 - INFO - Time taken for Epoch 11: 47.96s - F1: 0.64525345
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 23:04:44 - INFO - Fine-tuning models
Time taken for Epoch 1:2.66 - F1: 0.6451
2026-02-13 23:04:47 - INFO - Time taken for Epoch 1:2.66 - F1: 0.6451
Time taken for Epoch 2:3.77 - F1: 0.6528
2026-02-13 23:04:51 - INFO - Time taken for Epoch 2:3.77 - F1: 0.6528
Time taken for Epoch 3:3.91 - F1: 0.6529
2026-02-13 23:04:55 - INFO - Time taken for Epoch 3:3.91 - F1: 0.6529
Time taken for Epoch 4:3.90 - F1: 0.6475
2026-02-13 23:04:59 - INFO - Time taken for Epoch 4:3.90 - F1: 0.6475
Time taken for Epoch 5:2.65 - F1: 0.6588
2026-02-13 23:05:01 - INFO - Time taken for Epoch 5:2.65 - F1: 0.6588
Time taken for Epoch 6:3.89 - F1: 0.6582
2026-02-13 23:05:05 - INFO - Time taken for Epoch 6:3.89 - F1: 0.6582
Time taken for Epoch 7:2.65 - F1: 0.6684
2026-02-13 23:05:08 - INFO - Time taken for Epoch 7:2.65 - F1: 0.6684
Time taken for Epoch 8:3.89 - F1: 0.6739
2026-02-13 23:05:12 - INFO - Time taken for Epoch 8:3.89 - F1: 0.6739
Time taken for Epoch 9:3.91 - F1: 0.6763
2026-02-13 23:05:16 - INFO - Time taken for Epoch 9:3.91 - F1: 0.6763
Time taken for Epoch 10:3.88 - F1: 0.6746
2026-02-13 23:05:19 - INFO - Time taken for Epoch 10:3.88 - F1: 0.6746
Time taken for Epoch 11:2.64 - F1: 0.6703
2026-02-13 23:05:22 - INFO - Time taken for Epoch 11:2.64 - F1: 0.6703
Time taken for Epoch 12:2.64 - F1: 0.6609
2026-02-13 23:05:25 - INFO - Time taken for Epoch 12:2.64 - F1: 0.6609
Time taken for Epoch 13:2.64 - F1: 0.6526
2026-02-13 23:05:27 - INFO - Time taken for Epoch 13:2.64 - F1: 0.6526
Time taken for Epoch 14:2.64 - F1: 0.6549
2026-02-13 23:05:30 - INFO - Time taken for Epoch 14:2.64 - F1: 0.6549
Time taken for Epoch 15:2.64 - F1: 0.6443
2026-02-13 23:05:33 - INFO - Time taken for Epoch 15:2.64 - F1: 0.6443
Time taken for Epoch 16:2.65 - F1: 0.6438
2026-02-13 23:05:35 - INFO - Time taken for Epoch 16:2.65 - F1: 0.6438
Time taken for Epoch 17:2.65 - F1: 0.6357
2026-02-13 23:05:38 - INFO - Time taken for Epoch 17:2.65 - F1: 0.6357
Time taken for Epoch 18:2.65 - F1: 0.6304
2026-02-13 23:05:41 - INFO - Time taken for Epoch 18:2.65 - F1: 0.6304
Time taken for Epoch 19:2.65 - F1: 0.6385
2026-02-13 23:05:43 - INFO - Time taken for Epoch 19:2.65 - F1: 0.6385
Performance not improving for 10 consecutive epochs.
2026-02-13 23:05:43 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6763 - Best Epoch:8
2026-02-13 23:05:43 - INFO - Best F1:0.6763 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6437, Test ECE: 0.0439
2026-02-13 23:05:50 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6437, Test ECE: 0.0439
All results: {'f1_macro': 0.643667170875936, 'ece': np.float64(0.043936393421334465)}
2026-02-13 23:05:50 - INFO - All results: {'f1_macro': 0.643667170875936, 'ece': np.float64(0.043936393421334465)}

Total time taken: 808.91 seconds
2026-02-13 23:05:50 - INFO - 
Total time taken: 808.91 seconds
2026-02-13 23:05:51 - INFO - Trial 5 finished with value: 0.643667170875936 and parameters: {'learning_rate': 1.7431823265298438e-05, 'weight_decay': 3.64512246403886e-05, 'batch_size': 64, 'co_train_epochs': 11, 'epoch_patience': 10}. Best is trial 0 with value: 0.6681542515203446.
Using devices: cuda, cuda
2026-02-13 23:05:51 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 23:05:51 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 23:05:51 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 23:05:51 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 3.038193731606132e-05
Weight Decay: 0.0014299939938045962
Batch Size: 32
No. Epochs: 8
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-13 23:05:51 - INFO - Learning Rate: 3.038193731606132e-05
Weight Decay: 0.0014299939938045962
Batch Size: 32
No. Epochs: 8
Epoch Patience: 4
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 23:05:52 - INFO - Generating initial weights
Time taken for Epoch 1:20.16 - F1: 0.0631
2026-02-13 23:06:15 - INFO - Time taken for Epoch 1:20.16 - F1: 0.0631
Time taken for Epoch 2:20.14 - F1: 0.0958
2026-02-13 23:06:36 - INFO - Time taken for Epoch 2:20.14 - F1: 0.0958
Time taken for Epoch 3:20.14 - F1: 0.1426
2026-02-13 23:06:56 - INFO - Time taken for Epoch 3:20.14 - F1: 0.1426
Time taken for Epoch 4:20.23 - F1: 0.1852
2026-02-13 23:07:16 - INFO - Time taken for Epoch 4:20.23 - F1: 0.1852
Time taken for Epoch 5:20.21 - F1: 0.2090
2026-02-13 23:07:36 - INFO - Time taken for Epoch 5:20.21 - F1: 0.2090
Time taken for Epoch 6:20.26 - F1: 0.2213
2026-02-13 23:07:56 - INFO - Time taken for Epoch 6:20.26 - F1: 0.2213
Time taken for Epoch 7:20.28 - F1: 0.2234
2026-02-13 23:08:17 - INFO - Time taken for Epoch 7:20.28 - F1: 0.2234
Time taken for Epoch 8:20.28 - F1: 0.2276
2026-02-13 23:08:37 - INFO - Time taken for Epoch 8:20.28 - F1: 0.2276
Best F1:0.2276 - Best Epoch:8
2026-02-13 23:08:37 - INFO - Best F1:0.2276 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 23:08:38 - INFO - Starting co-training
Time taken for Epoch 1: 35.69s - F1: 0.48263370
2026-02-13 23:09:14 - INFO - Time taken for Epoch 1: 35.69s - F1: 0.48263370
Time taken for Epoch 2: 36.80s - F1: 0.54495170
2026-02-13 23:09:51 - INFO - Time taken for Epoch 2: 36.80s - F1: 0.54495170
Time taken for Epoch 3: 36.94s - F1: 0.60609698
2026-02-13 23:10:28 - INFO - Time taken for Epoch 3: 36.94s - F1: 0.60609698
Time taken for Epoch 4: 36.94s - F1: 0.65661714
2026-02-13 23:11:05 - INFO - Time taken for Epoch 4: 36.94s - F1: 0.65661714
Time taken for Epoch 5: 36.93s - F1: 0.64676640
2026-02-13 23:11:42 - INFO - Time taken for Epoch 5: 36.93s - F1: 0.64676640
Time taken for Epoch 6: 35.77s - F1: 0.63487659
2026-02-13 23:12:18 - INFO - Time taken for Epoch 6: 35.77s - F1: 0.63487659
Time taken for Epoch 7: 35.77s - F1: 0.64925383
2026-02-13 23:12:53 - INFO - Time taken for Epoch 7: 35.77s - F1: 0.64925383
Time taken for Epoch 8: 35.77s - F1: 0.63277454
2026-02-13 23:13:29 - INFO - Time taken for Epoch 8: 35.77s - F1: 0.63277454
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 23:13:32 - INFO - Fine-tuning models
Time taken for Epoch 1:2.81 - F1: 0.6503
2026-02-13 23:13:35 - INFO - Time taken for Epoch 1:2.81 - F1: 0.6503
Time taken for Epoch 2:3.98 - F1: 0.6588
2026-02-13 23:13:39 - INFO - Time taken for Epoch 2:3.98 - F1: 0.6588
Time taken for Epoch 3:4.10 - F1: 0.6522
2026-02-13 23:13:43 - INFO - Time taken for Epoch 3:4.10 - F1: 0.6522
Time taken for Epoch 4:2.80 - F1: 0.6544
2026-02-13 23:13:45 - INFO - Time taken for Epoch 4:2.80 - F1: 0.6544
Time taken for Epoch 5:2.81 - F1: 0.6580
2026-02-13 23:13:48 - INFO - Time taken for Epoch 5:2.81 - F1: 0.6580
Time taken for Epoch 6:2.80 - F1: 0.6403
2026-02-13 23:13:51 - INFO - Time taken for Epoch 6:2.80 - F1: 0.6403
Time taken for Epoch 7:2.80 - F1: 0.6415
2026-02-13 23:13:54 - INFO - Time taken for Epoch 7:2.80 - F1: 0.6415
Time taken for Epoch 8:2.80 - F1: 0.6510
2026-02-13 23:13:57 - INFO - Time taken for Epoch 8:2.80 - F1: 0.6510
Time taken for Epoch 9:2.80 - F1: 0.6529
2026-02-13 23:13:59 - INFO - Time taken for Epoch 9:2.80 - F1: 0.6529
Time taken for Epoch 10:2.81 - F1: 0.6530
2026-02-13 23:14:02 - INFO - Time taken for Epoch 10:2.81 - F1: 0.6530
Time taken for Epoch 11:2.80 - F1: 0.6545
2026-02-13 23:14:05 - INFO - Time taken for Epoch 11:2.80 - F1: 0.6545
Time taken for Epoch 12:2.81 - F1: 0.6561
2026-02-13 23:14:08 - INFO - Time taken for Epoch 12:2.81 - F1: 0.6561
Performance not improving for 10 consecutive epochs.
2026-02-13 23:14:08 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6588 - Best Epoch:1
2026-02-13 23:14:08 - INFO - Best F1:0.6588 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6692, Test ECE: 0.0461
2026-02-13 23:14:16 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6692, Test ECE: 0.0461
All results: {'f1_macro': 0.6691955485213505, 'ece': np.float64(0.04609750597718354)}
2026-02-13 23:14:16 - INFO - All results: {'f1_macro': 0.6691955485213505, 'ece': np.float64(0.04609750597718354)}

Total time taken: 505.47 seconds
2026-02-13 23:14:16 - INFO - 
Total time taken: 505.47 seconds
2026-02-13 23:14:16 - INFO - Trial 6 finished with value: 0.6691955485213505 and parameters: {'learning_rate': 3.038193731606132e-05, 'weight_decay': 0.0014299939938045962, 'batch_size': 32, 'co_train_epochs': 8, 'epoch_patience': 4}. Best is trial 6 with value: 0.6691955485213505.
Using devices: cuda, cuda
2026-02-13 23:14:16 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 23:14:16 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 23:14:16 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 23:14:16 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0007448938464723708
Weight Decay: 0.0006289683936533388
Batch Size: 64
No. Epochs: 5
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-13 23:14:17 - INFO - Learning Rate: 0.0007448938464723708
Weight Decay: 0.0006289683936533388
Batch Size: 64
No. Epochs: 5
Epoch Patience: 9
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 23:14:18 - INFO - Generating initial weights
Time taken for Epoch 1:19.18 - F1: 0.0953
2026-02-13 23:14:40 - INFO - Time taken for Epoch 1:19.18 - F1: 0.0953
Time taken for Epoch 2:19.15 - F1: 0.0081
2026-02-13 23:15:00 - INFO - Time taken for Epoch 2:19.15 - F1: 0.0081
Time taken for Epoch 3:19.15 - F1: 0.0643
2026-02-13 23:15:19 - INFO - Time taken for Epoch 3:19.15 - F1: 0.0643
Time taken for Epoch 4:19.16 - F1: 0.1969
2026-02-13 23:15:38 - INFO - Time taken for Epoch 4:19.16 - F1: 0.1969
Time taken for Epoch 5:19.19 - F1: 0.2133
2026-02-13 23:15:57 - INFO - Time taken for Epoch 5:19.19 - F1: 0.2133
Best F1:0.2133 - Best Epoch:5
2026-02-13 23:15:57 - INFO - Best F1:0.2133 - Best Epoch:5
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 23:15:58 - INFO - Starting co-training
Time taken for Epoch 1: 46.64s - F1: 0.04755179
2026-02-13 23:16:45 - INFO - Time taken for Epoch 1: 46.64s - F1: 0.04755179
Time taken for Epoch 2: 47.95s - F1: 0.04755179
2026-02-13 23:17:33 - INFO - Time taken for Epoch 2: 47.95s - F1: 0.04755179
Time taken for Epoch 3: 46.75s - F1: 0.04755179
2026-02-13 23:18:20 - INFO - Time taken for Epoch 3: 46.75s - F1: 0.04755179
Time taken for Epoch 4: 46.80s - F1: 0.04755179
2026-02-13 23:19:07 - INFO - Time taken for Epoch 4: 46.80s - F1: 0.04755179
Time taken for Epoch 5: 46.79s - F1: 0.04755179
2026-02-13 23:19:53 - INFO - Time taken for Epoch 5: 46.79s - F1: 0.04755179
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 23:19:56 - INFO - Fine-tuning models
Time taken for Epoch 1:2.66 - F1: 0.0189
2026-02-13 23:19:59 - INFO - Time taken for Epoch 1:2.66 - F1: 0.0189
Time taken for Epoch 2:3.76 - F1: 0.0064
2026-02-13 23:20:02 - INFO - Time taken for Epoch 2:3.76 - F1: 0.0064
Time taken for Epoch 3:2.65 - F1: 0.0089
2026-02-13 23:20:05 - INFO - Time taken for Epoch 3:2.65 - F1: 0.0089
Time taken for Epoch 4:2.64 - F1: 0.0089
2026-02-13 23:20:08 - INFO - Time taken for Epoch 4:2.64 - F1: 0.0089
Time taken for Epoch 5:2.65 - F1: 0.0089
2026-02-13 23:20:10 - INFO - Time taken for Epoch 5:2.65 - F1: 0.0089
Time taken for Epoch 6:2.65 - F1: 0.0394
2026-02-13 23:20:13 - INFO - Time taken for Epoch 6:2.65 - F1: 0.0394
Time taken for Epoch 7:3.88 - F1: 0.0363
2026-02-13 23:20:17 - INFO - Time taken for Epoch 7:3.88 - F1: 0.0363
Time taken for Epoch 8:2.65 - F1: 0.0476
2026-02-13 23:20:20 - INFO - Time taken for Epoch 8:2.65 - F1: 0.0476
Time taken for Epoch 9:3.89 - F1: 0.0476
2026-02-13 23:20:23 - INFO - Time taken for Epoch 9:3.89 - F1: 0.0476
Time taken for Epoch 10:2.64 - F1: 0.0476
2026-02-13 23:20:26 - INFO - Time taken for Epoch 10:2.64 - F1: 0.0476
Time taken for Epoch 11:2.64 - F1: 0.0189
2026-02-13 23:20:29 - INFO - Time taken for Epoch 11:2.64 - F1: 0.0189
Time taken for Epoch 12:2.64 - F1: 0.0189
2026-02-13 23:20:31 - INFO - Time taken for Epoch 12:2.64 - F1: 0.0189
Time taken for Epoch 13:2.64 - F1: 0.0189
2026-02-13 23:20:34 - INFO - Time taken for Epoch 13:2.64 - F1: 0.0189
Time taken for Epoch 14:2.65 - F1: 0.0189
2026-02-13 23:20:37 - INFO - Time taken for Epoch 14:2.65 - F1: 0.0189
Time taken for Epoch 15:2.64 - F1: 0.0189
2026-02-13 23:20:39 - INFO - Time taken for Epoch 15:2.64 - F1: 0.0189
Time taken for Epoch 16:2.64 - F1: 0.0081
2026-02-13 23:20:42 - INFO - Time taken for Epoch 16:2.64 - F1: 0.0081
Time taken for Epoch 17:2.64 - F1: 0.0081
2026-02-13 23:20:45 - INFO - Time taken for Epoch 17:2.64 - F1: 0.0081
Time taken for Epoch 18:2.64 - F1: 0.0038
2026-02-13 23:20:47 - INFO - Time taken for Epoch 18:2.64 - F1: 0.0038
Performance not improving for 10 consecutive epochs.
2026-02-13 23:20:47 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:7
2026-02-13 23:20:47 - INFO - Best F1:0.0476 - Best Epoch:7
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0062
2026-02-13 23:20:54 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0062
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.006186917527538693)}
2026-02-13 23:20:54 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.006186917527538693)}

Total time taken: 398.42 seconds
2026-02-13 23:20:54 - INFO - 
Total time taken: 398.42 seconds
2026-02-13 23:20:54 - INFO - Trial 7 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0007448938464723708, 'weight_decay': 0.0006289683936533388, 'batch_size': 64, 'co_train_epochs': 5, 'epoch_patience': 9}. Best is trial 6 with value: 0.6691955485213505.
Using devices: cuda, cuda
2026-02-13 23:20:54 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 23:20:54 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 23:20:54 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 23:20:54 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 7.514702142290255e-05
Weight Decay: 0.00031342729234580736
Batch Size: 32
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-13 23:20:55 - INFO - Learning Rate: 7.514702142290255e-05
Weight Decay: 0.00031342729234580736
Batch Size: 32
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 23:20:56 - INFO - Generating initial weights
Time taken for Epoch 1:20.22 - F1: 0.1142
2026-02-13 23:21:19 - INFO - Time taken for Epoch 1:20.22 - F1: 0.1142
Time taken for Epoch 2:20.17 - F1: 0.2129
2026-02-13 23:21:40 - INFO - Time taken for Epoch 2:20.17 - F1: 0.2129
Time taken for Epoch 3:20.19 - F1: 0.2154
2026-02-13 23:22:00 - INFO - Time taken for Epoch 3:20.19 - F1: 0.2154
Time taken for Epoch 4:20.19 - F1: 0.2192
2026-02-13 23:22:20 - INFO - Time taken for Epoch 4:20.19 - F1: 0.2192
Time taken for Epoch 5:20.21 - F1: 0.2366
2026-02-13 23:22:40 - INFO - Time taken for Epoch 5:20.21 - F1: 0.2366
Time taken for Epoch 6:20.23 - F1: 0.3035
2026-02-13 23:23:00 - INFO - Time taken for Epoch 6:20.23 - F1: 0.3035
Time taken for Epoch 7:20.25 - F1: 0.3255
2026-02-13 23:23:21 - INFO - Time taken for Epoch 7:20.25 - F1: 0.3255
Time taken for Epoch 8:20.26 - F1: 0.3274
2026-02-13 23:23:41 - INFO - Time taken for Epoch 8:20.26 - F1: 0.3274
Best F1:0.3274 - Best Epoch:8
2026-02-13 23:23:41 - INFO - Best F1:0.3274 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 23:23:42 - INFO - Starting co-training
Time taken for Epoch 1: 35.67s - F1: 0.60300379
2026-02-13 23:24:18 - INFO - Time taken for Epoch 1: 35.67s - F1: 0.60300379
Time taken for Epoch 2: 36.76s - F1: 0.61111972
2026-02-13 23:24:55 - INFO - Time taken for Epoch 2: 36.76s - F1: 0.61111972
Time taken for Epoch 3: 36.89s - F1: 0.61897335
2026-02-13 23:25:32 - INFO - Time taken for Epoch 3: 36.89s - F1: 0.61897335
Time taken for Epoch 4: 36.88s - F1: 0.60478177
2026-02-13 23:26:09 - INFO - Time taken for Epoch 4: 36.88s - F1: 0.60478177
Time taken for Epoch 5: 35.72s - F1: 0.61698838
2026-02-13 23:26:44 - INFO - Time taken for Epoch 5: 35.72s - F1: 0.61698838
Time taken for Epoch 6: 35.74s - F1: 0.65877105
2026-02-13 23:27:20 - INFO - Time taken for Epoch 6: 35.74s - F1: 0.65877105
Time taken for Epoch 7: 36.93s - F1: 0.63275710
2026-02-13 23:27:57 - INFO - Time taken for Epoch 7: 36.93s - F1: 0.63275710
Time taken for Epoch 8: 35.76s - F1: 0.64536589
2026-02-13 23:28:33 - INFO - Time taken for Epoch 8: 35.76s - F1: 0.64536589
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 23:28:35 - INFO - Fine-tuning models
Time taken for Epoch 1:2.80 - F1: 0.6540
2026-02-13 23:28:38 - INFO - Time taken for Epoch 1:2.80 - F1: 0.6540
Time taken for Epoch 2:3.90 - F1: 0.6609
2026-02-13 23:28:42 - INFO - Time taken for Epoch 2:3.90 - F1: 0.6609
Time taken for Epoch 3:4.09 - F1: 0.6639
2026-02-13 23:28:46 - INFO - Time taken for Epoch 3:4.09 - F1: 0.6639
Time taken for Epoch 4:4.05 - F1: 0.6588
2026-02-13 23:28:50 - INFO - Time taken for Epoch 4:4.05 - F1: 0.6588
Time taken for Epoch 5:2.80 - F1: 0.6583
2026-02-13 23:28:53 - INFO - Time taken for Epoch 5:2.80 - F1: 0.6583
Time taken for Epoch 6:2.79 - F1: 0.6589
2026-02-13 23:28:56 - INFO - Time taken for Epoch 6:2.79 - F1: 0.6589
Time taken for Epoch 7:2.80 - F1: 0.6591
2026-02-13 23:28:59 - INFO - Time taken for Epoch 7:2.80 - F1: 0.6591
Time taken for Epoch 8:2.80 - F1: 0.6580
2026-02-13 23:29:01 - INFO - Time taken for Epoch 8:2.80 - F1: 0.6580
Time taken for Epoch 9:2.80 - F1: 0.6485
2026-02-13 23:29:04 - INFO - Time taken for Epoch 9:2.80 - F1: 0.6485
Time taken for Epoch 10:2.80 - F1: 0.6487
2026-02-13 23:29:07 - INFO - Time taken for Epoch 10:2.80 - F1: 0.6487
Time taken for Epoch 11:2.80 - F1: 0.6402
2026-02-13 23:29:10 - INFO - Time taken for Epoch 11:2.80 - F1: 0.6402
Time taken for Epoch 12:2.80 - F1: 0.6446
2026-02-13 23:29:13 - INFO - Time taken for Epoch 12:2.80 - F1: 0.6446
Time taken for Epoch 13:2.80 - F1: 0.6478
2026-02-13 23:29:15 - INFO - Time taken for Epoch 13:2.80 - F1: 0.6478
Performance not improving for 10 consecutive epochs.
2026-02-13 23:29:15 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6639 - Best Epoch:2
2026-02-13 23:29:15 - INFO - Best F1:0.6639 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6415, Test ECE: 0.0528
2026-02-13 23:29:23 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6415, Test ECE: 0.0528
All results: {'f1_macro': 0.6414964986726924, 'ece': np.float64(0.052758123069464244)}
2026-02-13 23:29:23 - INFO - All results: {'f1_macro': 0.6414964986726924, 'ece': np.float64(0.052758123069464244)}

Total time taken: 508.35 seconds
2026-02-13 23:29:23 - INFO - 
Total time taken: 508.35 seconds
2026-02-13 23:29:23 - INFO - Trial 8 finished with value: 0.6414964986726924 and parameters: {'learning_rate': 7.514702142290255e-05, 'weight_decay': 0.00031342729234580736, 'batch_size': 32, 'co_train_epochs': 8, 'epoch_patience': 6}. Best is trial 6 with value: 0.6691955485213505.
Using devices: cuda, cuda
2026-02-13 23:29:23 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-13 23:29:23 - INFO - Devices: cuda, cuda
Starting log
2026-02-13 23:29:23 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 23:29:23 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 5.0103196234153274e-05
Weight Decay: 0.0014611933614915428
Batch Size: 32
No. Epochs: 17
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-13 23:29:23 - INFO - Learning Rate: 5.0103196234153274e-05
Weight Decay: 0.0014611933614915428
Batch Size: 32
No. Epochs: 17
Epoch Patience: 7
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-13 23:29:24 - INFO - Generating initial weights
Time taken for Epoch 1:20.21 - F1: 0.0759
2026-02-13 23:29:48 - INFO - Time taken for Epoch 1:20.21 - F1: 0.0759
Time taken for Epoch 2:20.17 - F1: 0.1838
2026-02-13 23:30:08 - INFO - Time taken for Epoch 2:20.17 - F1: 0.1838
Time taken for Epoch 3:20.17 - F1: 0.2060
2026-02-13 23:30:28 - INFO - Time taken for Epoch 3:20.17 - F1: 0.2060
Time taken for Epoch 4:20.18 - F1: 0.2094
2026-02-13 23:30:48 - INFO - Time taken for Epoch 4:20.18 - F1: 0.2094
Time taken for Epoch 5:20.21 - F1: 0.2191
2026-02-13 23:31:09 - INFO - Time taken for Epoch 5:20.21 - F1: 0.2191
Time taken for Epoch 6:20.21 - F1: 0.2527
2026-02-13 23:31:29 - INFO - Time taken for Epoch 6:20.21 - F1: 0.2527
Time taken for Epoch 7:20.21 - F1: 0.2876
2026-02-13 23:31:49 - INFO - Time taken for Epoch 7:20.21 - F1: 0.2876
Time taken for Epoch 8:20.25 - F1: 0.3297
2026-02-13 23:32:09 - INFO - Time taken for Epoch 8:20.25 - F1: 0.3297
Time taken for Epoch 9:20.25 - F1: 0.3425
2026-02-13 23:32:30 - INFO - Time taken for Epoch 9:20.25 - F1: 0.3425
Time taken for Epoch 10:20.23 - F1: 0.3476
2026-02-13 23:32:50 - INFO - Time taken for Epoch 10:20.23 - F1: 0.3476
Time taken for Epoch 11:20.32 - F1: 0.3529
2026-02-13 23:33:10 - INFO - Time taken for Epoch 11:20.32 - F1: 0.3529
Time taken for Epoch 12:20.22 - F1: 0.3582
2026-02-13 23:33:30 - INFO - Time taken for Epoch 12:20.22 - F1: 0.3582
Time taken for Epoch 13:20.25 - F1: 0.3531
2026-02-13 23:33:51 - INFO - Time taken for Epoch 13:20.25 - F1: 0.3531
Time taken for Epoch 14:20.25 - F1: 0.3590
2026-02-13 23:34:11 - INFO - Time taken for Epoch 14:20.25 - F1: 0.3590
Time taken for Epoch 15:20.29 - F1: 0.3579
2026-02-13 23:34:31 - INFO - Time taken for Epoch 15:20.29 - F1: 0.3579
Time taken for Epoch 16:20.25 - F1: 0.3611
2026-02-13 23:34:51 - INFO - Time taken for Epoch 16:20.25 - F1: 0.3611
Time taken for Epoch 17:20.30 - F1: 0.3616
2026-02-13 23:35:12 - INFO - Time taken for Epoch 17:20.30 - F1: 0.3616
Best F1:0.3616 - Best Epoch:17
2026-02-13 23:35:12 - INFO - Best F1:0.3616 - Best Epoch:17
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-13 23:35:13 - INFO - Starting co-training
Time taken for Epoch 1: 35.73s - F1: 0.58109339
2026-02-13 23:35:49 - INFO - Time taken for Epoch 1: 35.73s - F1: 0.58109339
Time taken for Epoch 2: 36.82s - F1: 0.63576474
2026-02-13 23:36:26 - INFO - Time taken for Epoch 2: 36.82s - F1: 0.63576474
Time taken for Epoch 3: 36.93s - F1: 0.61838843
2026-02-13 23:37:03 - INFO - Time taken for Epoch 3: 36.93s - F1: 0.61838843
Time taken for Epoch 4: 35.69s - F1: 0.58833159
2026-02-13 23:37:38 - INFO - Time taken for Epoch 4: 35.69s - F1: 0.58833159
Time taken for Epoch 5: 35.68s - F1: 0.63650534
2026-02-13 23:38:14 - INFO - Time taken for Epoch 5: 35.68s - F1: 0.63650534
Time taken for Epoch 6: 36.92s - F1: 0.62281275
2026-02-13 23:38:51 - INFO - Time taken for Epoch 6: 36.92s - F1: 0.62281275
Time taken for Epoch 7: 35.64s - F1: 0.63218973
2026-02-13 23:39:27 - INFO - Time taken for Epoch 7: 35.64s - F1: 0.63218973
Time taken for Epoch 8: 35.67s - F1: 0.63024732
2026-02-13 23:40:02 - INFO - Time taken for Epoch 8: 35.67s - F1: 0.63024732
Time taken for Epoch 9: 35.67s - F1: 0.66514038
2026-02-13 23:40:38 - INFO - Time taken for Epoch 9: 35.67s - F1: 0.66514038
Time taken for Epoch 10: 36.95s - F1: 0.65808148
2026-02-13 23:41:15 - INFO - Time taken for Epoch 10: 36.95s - F1: 0.65808148
Time taken for Epoch 11: 35.68s - F1: 0.66906918
2026-02-13 23:41:51 - INFO - Time taken for Epoch 11: 35.68s - F1: 0.66906918
Time taken for Epoch 12: 36.95s - F1: 0.66001750
2026-02-13 23:42:27 - INFO - Time taken for Epoch 12: 36.95s - F1: 0.66001750
Time taken for Epoch 13: 35.70s - F1: 0.65853198
2026-02-13 23:43:03 - INFO - Time taken for Epoch 13: 35.70s - F1: 0.65853198
Time taken for Epoch 14: 35.78s - F1: 0.65493529
2026-02-13 23:43:39 - INFO - Time taken for Epoch 14: 35.78s - F1: 0.65493529
Time taken for Epoch 15: 35.70s - F1: 0.66071359
2026-02-13 23:44:15 - INFO - Time taken for Epoch 15: 35.70s - F1: 0.66071359
Time taken for Epoch 16: 35.68s - F1: 0.63379734
2026-02-13 23:44:50 - INFO - Time taken for Epoch 16: 35.68s - F1: 0.63379734
Time taken for Epoch 17: 35.69s - F1: 0.64867782
2026-02-13 23:45:26 - INFO - Time taken for Epoch 17: 35.69s - F1: 0.64867782
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-13 23:45:29 - INFO - Fine-tuning models
Time taken for Epoch 1:2.80 - F1: 0.6573
2026-02-13 23:45:32 - INFO - Time taken for Epoch 1:2.80 - F1: 0.6573
Time taken for Epoch 2:3.94 - F1: 0.6694
2026-02-13 23:45:36 - INFO - Time taken for Epoch 2:3.94 - F1: 0.6694
Time taken for Epoch 3:4.08 - F1: 0.6593
2026-02-13 23:45:40 - INFO - Time taken for Epoch 3:4.08 - F1: 0.6593
Time taken for Epoch 4:2.79 - F1: 0.6645
2026-02-13 23:45:42 - INFO - Time taken for Epoch 4:2.79 - F1: 0.6645
Time taken for Epoch 5:2.80 - F1: 0.6605
2026-02-13 23:45:45 - INFO - Time taken for Epoch 5:2.80 - F1: 0.6605
Time taken for Epoch 6:2.79 - F1: 0.6650
2026-02-13 23:45:48 - INFO - Time taken for Epoch 6:2.79 - F1: 0.6650
Time taken for Epoch 7:2.80 - F1: 0.6588
2026-02-13 23:45:51 - INFO - Time taken for Epoch 7:2.80 - F1: 0.6588
Time taken for Epoch 8:2.80 - F1: 0.6557
2026-02-13 23:45:54 - INFO - Time taken for Epoch 8:2.80 - F1: 0.6557
Time taken for Epoch 9:2.80 - F1: 0.6560
2026-02-13 23:45:56 - INFO - Time taken for Epoch 9:2.80 - F1: 0.6560
Time taken for Epoch 10:2.80 - F1: 0.6548
2026-02-13 23:45:59 - INFO - Time taken for Epoch 10:2.80 - F1: 0.6548
Time taken for Epoch 11:2.80 - F1: 0.6566
2026-02-13 23:46:02 - INFO - Time taken for Epoch 11:2.80 - F1: 0.6566
Time taken for Epoch 12:2.80 - F1: 0.6629
2026-02-13 23:46:05 - INFO - Time taken for Epoch 12:2.80 - F1: 0.6629
Performance not improving for 10 consecutive epochs.
2026-02-13 23:46:05 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6694 - Best Epoch:1
2026-02-13 23:46:05 - INFO - Best F1:0.6694 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_1_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label5-set2/final_model_2_optuna-bertweet-hurricane-maria-2017-label5-set2_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6527, Test ECE: 0.0306
2026-02-13 23:46:12 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6527, Test ECE: 0.0306
All results: {'f1_macro': 0.6527428758426546, 'ece': np.float64(0.030610582947565686)}
2026-02-13 23:46:12 - INFO - All results: {'f1_macro': 0.6527428758426546, 'ece': np.float64(0.030610582947565686)}

Total time taken: 1009.52 seconds
2026-02-13 23:46:12 - INFO - 
Total time taken: 1009.52 seconds
2026-02-13 23:46:12 - INFO - Trial 9 finished with value: 0.6527428758426546 and parameters: {'learning_rate': 5.0103196234153274e-05, 'weight_decay': 0.0014611933614915428, 'batch_size': 32, 'co_train_epochs': 17, 'epoch_patience': 7}. Best is trial 6 with value: 0.6691955485213505.

[BEST TRIAL RESULTS]
2026-02-13 23:46:12 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6692
2026-02-13 23:46:12 - INFO - F1 Score: 0.6692
Params: {'learning_rate': 3.038193731606132e-05, 'weight_decay': 0.0014299939938045962, 'batch_size': 32, 'co_train_epochs': 8, 'epoch_patience': 4}
2026-02-13 23:46:12 - INFO - Params: {'learning_rate': 3.038193731606132e-05, 'weight_decay': 0.0014299939938045962, 'batch_size': 32, 'co_train_epochs': 8, 'epoch_patience': 4}
  learning_rate: 3.038193731606132e-05
2026-02-13 23:46:12 - INFO -   learning_rate: 3.038193731606132e-05
  weight_decay: 0.0014299939938045962
2026-02-13 23:46:12 - INFO -   weight_decay: 0.0014299939938045962
  batch_size: 32
2026-02-13 23:46:12 - INFO -   batch_size: 32
  co_train_epochs: 8
2026-02-13 23:46:12 - INFO -   co_train_epochs: 8
  epoch_patience: 4
2026-02-13 23:46:12 - INFO -   epoch_patience: 4

Total time taken: 6806.22 seconds
2026-02-13 23:46:12 - INFO - 
Total time taken: 6806.22 seconds