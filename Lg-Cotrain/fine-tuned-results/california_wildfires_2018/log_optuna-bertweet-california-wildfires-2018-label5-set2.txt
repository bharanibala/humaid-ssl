2026-02-13 10:41:17 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 10:41:17 - INFO - A new study created in memory with name: study_humanitarian10_california_wildfires_2018
2026-02-13 10:41:17 - INFO - Using devices: cuda, cuda
2026-02-13 10:41:17 - INFO - Devices: cuda, cuda
2026-02-13 10:41:17 - INFO - Starting log
2026-02-13 10:41:17 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 10:41:18 - INFO - Learning Rate: 0.0009167337423105695
Weight Decay: 0.0011020136880303844
Batch Size: 24
No. Epochs: 16
Epoch Patience: 5
 Accumulation Steps: 2
2026-02-13 10:41:18 - INFO - Generating initial weights
2026-02-13 10:41:34 - INFO - Time taken for Epoch 1:13.67 - F1: 0.0158
2026-02-13 10:41:47 - INFO - Time taken for Epoch 2:13.52 - F1: 0.0021
2026-02-13 10:42:01 - INFO - Time taken for Epoch 3:13.51 - F1: 0.0122
2026-02-13 10:42:14 - INFO - Time taken for Epoch 4:13.52 - F1: 0.0573
2026-02-13 10:42:28 - INFO - Time taken for Epoch 5:13.50 - F1: 0.0133
2026-02-13 10:42:41 - INFO - Time taken for Epoch 6:13.52 - F1: 0.0047
2026-02-13 10:42:55 - INFO - Time taken for Epoch 7:13.50 - F1: 0.0047
2026-02-13 10:43:08 - INFO - Time taken for Epoch 8:13.51 - F1: 0.0108
2026-02-13 10:43:22 - INFO - Time taken for Epoch 9:13.50 - F1: 0.0108
2026-02-13 10:43:35 - INFO - Time taken for Epoch 10:13.48 - F1: 0.0021
2026-02-13 10:43:49 - INFO - Time taken for Epoch 11:13.49 - F1: 0.0096
2026-02-13 10:44:02 - INFO - Time taken for Epoch 12:13.49 - F1: 0.0096
2026-02-13 10:44:16 - INFO - Time taken for Epoch 13:13.47 - F1: 0.0096
2026-02-13 10:44:29 - INFO - Time taken for Epoch 14:13.50 - F1: 0.0096
2026-02-13 10:44:43 - INFO - Time taken for Epoch 15:13.49 - F1: 0.0047
2026-02-13 10:44:56 - INFO - Time taken for Epoch 16:13.48 - F1: 0.0047
2026-02-13 10:44:56 - INFO - Best F1:0.0573 - Best Epoch:4
2026-02-13 10:44:57 - INFO - Starting co-training
2026-02-13 10:45:27 - INFO - Time taken for Epoch 1: 29.52s - F1: 0.04185068
2026-02-13 10:45:57 - INFO - Time taken for Epoch 2: 30.28s - F1: 0.04185068
2026-02-13 10:46:27 - INFO - Time taken for Epoch 3: 29.84s - F1: 0.03024831
2026-02-13 10:46:56 - INFO - Time taken for Epoch 4: 29.59s - F1: 0.03024831
2026-02-13 10:47:26 - INFO - Time taken for Epoch 5: 29.65s - F1: 0.03024831
2026-02-13 10:47:56 - INFO - Time taken for Epoch 6: 29.63s - F1: 0.03024831
2026-02-13 10:47:56 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-13 10:47:57 - INFO - Fine-tuning models
2026-02-13 10:47:59 - INFO - Time taken for Epoch 1:1.92 - F1: 0.0302
2026-02-13 10:48:02 - INFO - Time taken for Epoch 2:2.54 - F1: 0.0021
2026-02-13 10:48:04 - INFO - Time taken for Epoch 3:1.92 - F1: 0.0047
2026-02-13 10:48:06 - INFO - Time taken for Epoch 4:1.90 - F1: 0.0108
2026-02-13 10:48:08 - INFO - Time taken for Epoch 5:1.90 - F1: 0.0108
2026-02-13 10:48:09 - INFO - Time taken for Epoch 6:1.90 - F1: 0.0108
2026-02-13 10:48:11 - INFO - Time taken for Epoch 7:1.90 - F1: 0.0108
2026-02-13 10:48:13 - INFO - Time taken for Epoch 8:1.90 - F1: 0.0247
2026-02-13 10:48:15 - INFO - Time taken for Epoch 9:1.90 - F1: 0.0302
2026-02-13 10:48:17 - INFO - Time taken for Epoch 10:1.90 - F1: 0.0419
2026-02-13 10:48:20 - INFO - Time taken for Epoch 11:2.58 - F1: 0.0021
2026-02-13 10:48:22 - INFO - Time taken for Epoch 12:2.03 - F1: 0.0021
2026-02-13 10:48:24 - INFO - Time taken for Epoch 13:2.01 - F1: 0.0021
2026-02-13 10:48:26 - INFO - Time taken for Epoch 14:2.01 - F1: 0.0021
2026-02-13 10:48:28 - INFO - Time taken for Epoch 15:1.92 - F1: 0.0247
2026-02-13 10:48:30 - INFO - Time taken for Epoch 16:1.91 - F1: 0.0247
2026-02-13 10:48:31 - INFO - Time taken for Epoch 17:1.99 - F1: 0.0096
2026-02-13 10:48:34 - INFO - Time taken for Epoch 18:2.01 - F1: 0.0096
2026-02-13 10:48:36 - INFO - Time taken for Epoch 19:2.02 - F1: 0.0096
2026-02-13 10:48:38 - INFO - Time taken for Epoch 20:1.98 - F1: 0.0021
2026-02-13 10:48:38 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 10:48:38 - INFO - Best F1:0.0419 - Best Epoch:9
2026-02-13 10:48:43 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0417, Test ECE: 0.0156
2026-02-13 10:48:43 - INFO - All results: {'f1_macro': 0.04171180931744312, 'ece': np.float64(0.015585152520291384)}
2026-02-13 10:48:43 - INFO - 
Total time taken: 445.70 seconds
2026-02-13 10:48:43 - INFO - Trial 0 finished with value: 0.04171180931744312 and parameters: {'learning_rate': 0.0009167337423105695, 'weight_decay': 0.0011020136880303844, 'batch_size': 24, 'co_train_epochs': 16, 'epoch_patience': 5}. Best is trial 0 with value: 0.04171180931744312.
2026-02-13 10:48:43 - INFO - Using devices: cuda, cuda
2026-02-13 10:48:43 - INFO - Devices: cuda, cuda
2026-02-13 10:48:43 - INFO - Starting log
2026-02-13 10:48:43 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 10:48:43 - INFO - Learning Rate: 3.669390873844809e-05
Weight Decay: 0.005919065979103007
Batch Size: 16
No. Epochs: 7
Epoch Patience: 8
 Accumulation Steps: 4
2026-02-13 10:48:44 - INFO - Generating initial weights
2026-02-13 10:49:00 - INFO - Time taken for Epoch 1:14.64 - F1: 0.0264
2026-02-13 10:49:14 - INFO - Time taken for Epoch 2:14.89 - F1: 0.0120
2026-02-13 10:49:29 - INFO - Time taken for Epoch 3:14.70 - F1: 0.0120
2026-02-13 10:49:44 - INFO - Time taken for Epoch 4:14.85 - F1: 0.0120
2026-02-13 10:49:59 - INFO - Time taken for Epoch 5:14.92 - F1: 0.0120
2026-02-13 10:50:14 - INFO - Time taken for Epoch 6:15.17 - F1: 0.0120
2026-02-13 10:50:29 - INFO - Time taken for Epoch 7:14.84 - F1: 0.0134
2026-02-13 10:50:29 - INFO - Best F1:0.0264 - Best Epoch:1
2026-02-13 10:50:30 - INFO - Starting co-training
2026-02-13 10:50:55 - INFO - Time taken for Epoch 1: 25.55s - F1: 0.29805875
2026-02-13 10:51:21 - INFO - Time taken for Epoch 2: 25.50s - F1: 0.30769174
2026-02-13 10:51:47 - INFO - Time taken for Epoch 3: 26.57s - F1: 0.45678365
2026-02-13 10:52:13 - INFO - Time taken for Epoch 4: 25.95s - F1: 0.47690685
2026-02-13 10:52:39 - INFO - Time taken for Epoch 5: 25.93s - F1: 0.53510953
2026-02-13 10:53:05 - INFO - Time taken for Epoch 6: 26.14s - F1: 0.58058105
2026-02-13 10:53:32 - INFO - Time taken for Epoch 7: 26.13s - F1: 0.57686177
2026-02-13 10:53:33 - INFO - Fine-tuning models
2026-02-13 10:53:35 - INFO - Time taken for Epoch 1:2.20 - F1: 0.5853
2026-02-13 10:53:38 - INFO - Time taken for Epoch 2:2.75 - F1: 0.5720
2026-02-13 10:53:40 - INFO - Time taken for Epoch 3:2.14 - F1: 0.5641
2026-02-13 10:53:42 - INFO - Time taken for Epoch 4:2.15 - F1: 0.5722
2026-02-13 10:53:45 - INFO - Time taken for Epoch 5:2.16 - F1: 0.5680
2026-02-13 10:53:47 - INFO - Time taken for Epoch 6:2.18 - F1: 0.5690
2026-02-13 10:53:49 - INFO - Time taken for Epoch 7:2.21 - F1: 0.5514
2026-02-13 10:53:51 - INFO - Time taken for Epoch 8:2.15 - F1: 0.5433
2026-02-13 10:53:53 - INFO - Time taken for Epoch 9:2.23 - F1: 0.5347
2026-02-13 10:53:56 - INFO - Time taken for Epoch 10:2.27 - F1: 0.5311
2026-02-13 10:53:58 - INFO - Time taken for Epoch 11:2.22 - F1: 0.5313
2026-02-13 10:53:58 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 10:53:58 - INFO - Best F1:0.5853 - Best Epoch:0
2026-02-13 10:54:03 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5585, Test ECE: 0.0657
2026-02-13 10:54:03 - INFO - All results: {'f1_macro': 0.5585429787647153, 'ece': np.float64(0.06570176320235914)}
2026-02-13 10:54:03 - INFO - 
Total time taken: 320.40 seconds
2026-02-13 10:54:03 - INFO - Trial 1 finished with value: 0.5585429787647153 and parameters: {'learning_rate': 3.669390873844809e-05, 'weight_decay': 0.005919065979103007, 'batch_size': 16, 'co_train_epochs': 7, 'epoch_patience': 8}. Best is trial 1 with value: 0.5585429787647153.
2026-02-13 10:54:03 - INFO - Using devices: cuda, cuda
2026-02-13 10:54:03 - INFO - Devices: cuda, cuda
2026-02-13 10:54:03 - INFO - Starting log
2026-02-13 10:54:03 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 10:54:03 - INFO - Learning Rate: 0.00010653024125405768
Weight Decay: 0.0007797157851692308
Batch Size: 16
No. Epochs: 12
Epoch Patience: 4
 Accumulation Steps: 4
2026-02-13 10:54:04 - INFO - Generating initial weights
2026-02-13 10:54:20 - INFO - Time taken for Epoch 1:14.91 - F1: 0.0120
2026-02-13 10:54:35 - INFO - Time taken for Epoch 2:15.11 - F1: 0.0120
2026-02-13 10:54:50 - INFO - Time taken for Epoch 3:14.92 - F1: 0.0200
2026-02-13 10:55:06 - INFO - Time taken for Epoch 4:15.47 - F1: 0.0644
2026-02-13 10:55:21 - INFO - Time taken for Epoch 5:15.00 - F1: 0.0642
2026-02-13 10:55:36 - INFO - Time taken for Epoch 6:15.44 - F1: 0.0601
2026-02-13 10:55:51 - INFO - Time taken for Epoch 7:15.12 - F1: 0.0623
2026-02-13 10:56:06 - INFO - Time taken for Epoch 8:15.12 - F1: 0.1009
2026-02-13 10:56:22 - INFO - Time taken for Epoch 9:15.79 - F1: 0.1486
2026-02-13 10:56:37 - INFO - Time taken for Epoch 10:15.23 - F1: 0.2069
2026-02-13 10:56:52 - INFO - Time taken for Epoch 11:14.72 - F1: 0.2363
2026-02-13 10:57:07 - INFO - Time taken for Epoch 12:15.20 - F1: 0.2529
2026-02-13 10:57:07 - INFO - Best F1:0.2529 - Best Epoch:12
2026-02-13 10:57:08 - INFO - Starting co-training
2026-02-13 10:57:33 - INFO - Time taken for Epoch 1: 25.22s - F1: 0.34257107
2026-02-13 10:57:59 - INFO - Time taken for Epoch 2: 25.68s - F1: 0.38692493
2026-02-13 10:58:25 - INFO - Time taken for Epoch 3: 25.69s - F1: 0.43987900
2026-02-13 10:58:50 - INFO - Time taken for Epoch 4: 25.60s - F1: 0.43614855
2026-02-13 10:59:15 - INFO - Time taken for Epoch 5: 25.05s - F1: 0.45936767
2026-02-13 10:59:41 - INFO - Time taken for Epoch 6: 25.67s - F1: 0.54885406
2026-02-13 11:00:07 - INFO - Time taken for Epoch 7: 25.72s - F1: 0.57364536
2026-02-13 11:00:32 - INFO - Time taken for Epoch 8: 25.71s - F1: 0.55455053
2026-02-13 11:00:57 - INFO - Time taken for Epoch 9: 24.97s - F1: 0.56241052
2026-02-13 11:01:23 - INFO - Time taken for Epoch 10: 25.12s - F1: 0.55134951
2026-02-13 11:01:48 - INFO - Time taken for Epoch 11: 25.00s - F1: 0.55713198
2026-02-13 11:01:48 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-13 11:01:49 - INFO - Fine-tuning models
2026-02-13 11:01:51 - INFO - Time taken for Epoch 1:2.20 - F1: 0.5729
2026-02-13 11:01:54 - INFO - Time taken for Epoch 2:2.78 - F1: 0.5634
2026-02-13 11:01:56 - INFO - Time taken for Epoch 3:2.19 - F1: 0.5066
2026-02-13 11:01:59 - INFO - Time taken for Epoch 4:2.20 - F1: 0.4893
2026-02-13 11:02:01 - INFO - Time taken for Epoch 5:2.19 - F1: 0.5161
2026-02-13 11:02:03 - INFO - Time taken for Epoch 6:2.19 - F1: 0.5330
2026-02-13 11:02:05 - INFO - Time taken for Epoch 7:2.19 - F1: 0.5381
2026-02-13 11:02:07 - INFO - Time taken for Epoch 8:2.19 - F1: 0.5400
2026-02-13 11:02:09 - INFO - Time taken for Epoch 9:2.19 - F1: 0.5389
2026-02-13 11:02:12 - INFO - Time taken for Epoch 10:2.19 - F1: 0.5397
2026-02-13 11:02:14 - INFO - Time taken for Epoch 11:2.19 - F1: 0.5388
2026-02-13 11:02:14 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 11:02:14 - INFO - Best F1:0.5729 - Best Epoch:0
2026-02-13 11:02:19 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5999, Test ECE: 0.0898
2026-02-13 11:02:19 - INFO - All results: {'f1_macro': 0.599919251739942, 'ece': np.float64(0.08979138277068192)}
2026-02-13 11:02:19 - INFO - 
Total time taken: 495.83 seconds
2026-02-13 11:02:19 - INFO - Trial 2 finished with value: 0.599919251739942 and parameters: {'learning_rate': 0.00010653024125405768, 'weight_decay': 0.0007797157851692308, 'batch_size': 16, 'co_train_epochs': 12, 'epoch_patience': 4}. Best is trial 2 with value: 0.599919251739942.
2026-02-13 11:02:19 - INFO - Using devices: cuda, cuda
2026-02-13 11:02:19 - INFO - Devices: cuda, cuda
2026-02-13 11:02:19 - INFO - Starting log
2026-02-13 11:02:19 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 11:02:19 - INFO - Learning Rate: 1.5709153240979403e-05
Weight Decay: 1.881107006506445e-05
Batch Size: 8
No. Epochs: 6
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-13 11:02:20 - INFO - Generating initial weights
2026-02-13 11:02:38 - INFO - Time taken for Epoch 1:16.70 - F1: 0.0838
2026-02-13 11:02:55 - INFO - Time taken for Epoch 2:16.73 - F1: 0.0835
2026-02-13 11:03:11 - INFO - Time taken for Epoch 3:16.73 - F1: 0.0817
2026-02-13 11:03:28 - INFO - Time taken for Epoch 4:16.75 - F1: 0.0760
2026-02-13 11:03:45 - INFO - Time taken for Epoch 5:16.75 - F1: 0.0771
2026-02-13 11:04:02 - INFO - Time taken for Epoch 6:16.73 - F1: 0.0717
2026-02-13 11:04:02 - INFO - Best F1:0.0838 - Best Epoch:1
2026-02-13 11:04:02 - INFO - Starting co-training
2026-02-13 11:04:27 - INFO - Time taken for Epoch 1: 24.38s - F1: 0.21249464
2026-02-13 11:04:52 - INFO - Time taken for Epoch 2: 24.93s - F1: 0.27956975
2026-02-13 11:05:17 - INFO - Time taken for Epoch 3: 25.00s - F1: 0.32002616
2026-02-13 11:05:42 - INFO - Time taken for Epoch 4: 24.97s - F1: 0.36195659
2026-02-13 11:06:07 - INFO - Time taken for Epoch 5: 24.96s - F1: 0.38904529
2026-02-13 11:06:32 - INFO - Time taken for Epoch 6: 24.99s - F1: 0.41533309
2026-02-13 11:06:35 - INFO - Fine-tuning models
2026-02-13 11:06:38 - INFO - Time taken for Epoch 1:2.51 - F1: 0.4135
2026-02-13 11:06:41 - INFO - Time taken for Epoch 2:3.11 - F1: 0.4163
2026-02-13 11:06:44 - INFO - Time taken for Epoch 3:3.46 - F1: 0.4123
2026-02-13 11:06:47 - INFO - Time taken for Epoch 4:2.49 - F1: 0.4036
2026-02-13 11:06:49 - INFO - Time taken for Epoch 5:2.50 - F1: 0.3998
2026-02-13 11:06:52 - INFO - Time taken for Epoch 6:2.50 - F1: 0.3992
2026-02-13 11:06:54 - INFO - Time taken for Epoch 7:2.50 - F1: 0.4045
2026-02-13 11:06:57 - INFO - Time taken for Epoch 8:2.50 - F1: 0.4137
2026-02-13 11:06:59 - INFO - Time taken for Epoch 9:2.50 - F1: 0.4252
2026-02-13 11:07:02 - INFO - Time taken for Epoch 10:3.28 - F1: 0.4365
2026-02-13 11:07:06 - INFO - Time taken for Epoch 11:3.28 - F1: 0.4451
2026-02-13 11:07:09 - INFO - Time taken for Epoch 12:3.14 - F1: 0.4595
2026-02-13 11:07:16 - INFO - Time taken for Epoch 13:7.55 - F1: 0.4540
2026-02-13 11:07:19 - INFO - Time taken for Epoch 14:2.50 - F1: 0.4770
2026-02-13 11:07:22 - INFO - Time taken for Epoch 15:3.11 - F1: 0.4834
2026-02-13 11:07:25 - INFO - Time taken for Epoch 16:3.11 - F1: 0.4787
2026-02-13 11:07:28 - INFO - Time taken for Epoch 17:2.50 - F1: 0.4993
2026-02-13 11:07:31 - INFO - Time taken for Epoch 18:3.13 - F1: 0.4982
2026-02-13 11:07:33 - INFO - Time taken for Epoch 19:2.50 - F1: 0.5121
2026-02-13 11:07:36 - INFO - Time taken for Epoch 20:3.15 - F1: 0.5154
2026-02-13 11:07:40 - INFO - Time taken for Epoch 21:3.13 - F1: 0.5097
2026-02-13 11:07:42 - INFO - Time taken for Epoch 22:2.49 - F1: 0.5085
2026-02-13 11:07:45 - INFO - Time taken for Epoch 23:2.50 - F1: 0.5028
2026-02-13 11:07:47 - INFO - Time taken for Epoch 24:2.50 - F1: 0.5055
2026-02-13 11:07:50 - INFO - Time taken for Epoch 25:2.50 - F1: 0.5072
2026-02-13 11:07:52 - INFO - Time taken for Epoch 26:2.50 - F1: 0.5165
2026-02-13 11:07:58 - INFO - Time taken for Epoch 27:5.96 - F1: 0.5095
2026-02-13 11:08:01 - INFO - Time taken for Epoch 28:2.68 - F1: 0.5264
2026-02-13 11:08:04 - INFO - Time taken for Epoch 29:3.31 - F1: 0.5266
2026-02-13 11:08:07 - INFO - Time taken for Epoch 30:3.32 - F1: 0.5352
2026-02-13 11:08:11 - INFO - Time taken for Epoch 31:3.22 - F1: 0.5330
2026-02-13 11:08:13 - INFO - Time taken for Epoch 32:2.58 - F1: 0.5349
2026-02-13 11:08:16 - INFO - Time taken for Epoch 33:2.63 - F1: 0.5370
2026-02-13 11:08:19 - INFO - Time taken for Epoch 34:3.32 - F1: 0.5447
2026-02-13 11:08:22 - INFO - Time taken for Epoch 35:3.15 - F1: 0.5437
2026-02-13 11:08:25 - INFO - Time taken for Epoch 36:2.46 - F1: 0.5481
2026-02-13 11:08:28 - INFO - Time taken for Epoch 37:3.07 - F1: 0.5480
2026-02-13 11:08:30 - INFO - Time taken for Epoch 38:2.47 - F1: 0.5502
2026-02-13 11:08:38 - INFO - Time taken for Epoch 39:8.22 - F1: 0.5502
2026-02-13 11:08:41 - INFO - Time taken for Epoch 40:2.45 - F1: 0.5522
2026-02-13 11:08:44 - INFO - Time taken for Epoch 41:3.12 - F1: 0.5636
2026-02-13 11:08:47 - INFO - Time taken for Epoch 42:3.07 - F1: 0.5626
2026-02-13 11:08:50 - INFO - Time taken for Epoch 43:2.45 - F1: 0.5649
2026-02-13 11:08:53 - INFO - Time taken for Epoch 44:3.07 - F1: 0.5574
2026-02-13 11:08:55 - INFO - Time taken for Epoch 45:2.46 - F1: 0.5609
2026-02-13 11:08:58 - INFO - Time taken for Epoch 46:2.45 - F1: 0.5604
2026-02-13 11:09:00 - INFO - Time taken for Epoch 47:2.46 - F1: 0.5589
2026-02-13 11:09:02 - INFO - Time taken for Epoch 48:2.48 - F1: 0.5622
2026-02-13 11:09:05 - INFO - Time taken for Epoch 49:2.69 - F1: 0.5590
2026-02-13 11:09:08 - INFO - Time taken for Epoch 50:2.64 - F1: 0.5607
2026-02-13 11:09:10 - INFO - Time taken for Epoch 51:2.53 - F1: 0.5626
2026-02-13 11:09:13 - INFO - Time taken for Epoch 52:2.51 - F1: 0.5713
2026-02-13 11:09:19 - INFO - Time taken for Epoch 53:6.68 - F1: 0.5719
2026-02-13 11:09:23 - INFO - Time taken for Epoch 54:3.13 - F1: 0.5701
2026-02-13 11:09:25 - INFO - Time taken for Epoch 55:2.63 - F1: 0.5708
2026-02-13 11:09:28 - INFO - Time taken for Epoch 56:2.58 - F1: 0.5718
2026-02-13 11:09:30 - INFO - Time taken for Epoch 57:2.48 - F1: 0.5711
2026-02-13 11:09:33 - INFO - Time taken for Epoch 58:2.49 - F1: 0.5733
2026-02-13 11:09:36 - INFO - Time taken for Epoch 59:3.09 - F1: 0.5744
2026-02-13 11:09:39 - INFO - Time taken for Epoch 60:3.08 - F1: 0.5741
2026-02-13 11:09:41 - INFO - Time taken for Epoch 61:2.43 - F1: 0.5730
2026-02-13 11:09:44 - INFO - Time taken for Epoch 62:2.43 - F1: 0.5683
2026-02-13 11:09:46 - INFO - Time taken for Epoch 63:2.50 - F1: 0.5646
2026-02-13 11:09:49 - INFO - Time taken for Epoch 64:2.53 - F1: 0.5646
2026-02-13 11:09:51 - INFO - Time taken for Epoch 65:2.57 - F1: 0.5646
2026-02-13 11:09:54 - INFO - Time taken for Epoch 66:2.51 - F1: 0.5646
2026-02-13 11:09:56 - INFO - Time taken for Epoch 67:2.48 - F1: 0.5654
2026-02-13 11:09:59 - INFO - Time taken for Epoch 68:2.48 - F1: 0.5628
2026-02-13 11:10:01 - INFO - Time taken for Epoch 69:2.51 - F1: 0.5627
2026-02-13 11:10:01 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 11:10:01 - INFO - Best F1:0.5744 - Best Epoch:58
2026-02-13 11:10:07 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5861, Test ECE: 0.1158
2026-02-13 11:10:07 - INFO - All results: {'f1_macro': 0.5861218703619435, 'ece': np.float64(0.11580515921646237)}
2026-02-13 11:10:07 - INFO - 
Total time taken: 468.18 seconds
2026-02-13 11:10:07 - INFO - Trial 3 finished with value: 0.5861218703619435 and parameters: {'learning_rate': 1.5709153240979403e-05, 'weight_decay': 1.881107006506445e-05, 'batch_size': 8, 'co_train_epochs': 6, 'epoch_patience': 10}. Best is trial 2 with value: 0.599919251739942.
2026-02-13 11:10:07 - INFO - Using devices: cuda, cuda
2026-02-13 11:10:07 - INFO - Devices: cuda, cuda
2026-02-13 11:10:07 - INFO - Starting log
2026-02-13 11:10:07 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 11:10:08 - INFO - Learning Rate: 0.0001271531617134396
Weight Decay: 4.914013940508779e-05
Batch Size: 8
No. Epochs: 13
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-13 11:10:08 - INFO - Generating initial weights
2026-02-13 11:10:26 - INFO - Time taken for Epoch 1:16.99 - F1: 0.0402
2026-02-13 11:10:44 - INFO - Time taken for Epoch 2:17.11 - F1: 0.0591
2026-02-13 11:11:01 - INFO - Time taken for Epoch 3:17.29 - F1: 0.0878
2026-02-13 11:11:19 - INFO - Time taken for Epoch 4:17.88 - F1: 0.2001
2026-02-13 11:11:36 - INFO - Time taken for Epoch 5:16.95 - F1: 0.1718
2026-02-13 11:11:52 - INFO - Time taken for Epoch 6:16.65 - F1: 0.1770
2026-02-13 11:12:09 - INFO - Time taken for Epoch 7:16.87 - F1: 0.1962
2026-02-13 11:12:26 - INFO - Time taken for Epoch 8:17.07 - F1: 0.2033
2026-02-13 11:12:43 - INFO - Time taken for Epoch 9:16.87 - F1: 0.2175
2026-02-13 11:13:00 - INFO - Time taken for Epoch 10:17.30 - F1: 0.2218
2026-02-13 11:13:17 - INFO - Time taken for Epoch 11:16.73 - F1: 0.2342
2026-02-13 11:13:34 - INFO - Time taken for Epoch 12:16.72 - F1: 0.2373
2026-02-13 11:13:51 - INFO - Time taken for Epoch 13:16.69 - F1: 0.2247
2026-02-13 11:13:51 - INFO - Best F1:0.2373 - Best Epoch:12
2026-02-13 11:13:51 - INFO - Starting co-training
2026-02-13 11:14:16 - INFO - Time taken for Epoch 1: 24.37s - F1: 0.07283333
2026-02-13 11:14:41 - INFO - Time taken for Epoch 2: 25.34s - F1: 0.09357791
2026-02-13 11:15:06 - INFO - Time taken for Epoch 3: 25.32s - F1: 0.04185068
2026-02-13 11:15:31 - INFO - Time taken for Epoch 4: 24.33s - F1: 0.04185068
2026-02-13 11:15:55 - INFO - Time taken for Epoch 5: 24.28s - F1: 0.04185068
2026-02-13 11:16:20 - INFO - Time taken for Epoch 6: 25.38s - F1: 0.04185068
2026-02-13 11:16:20 - INFO - Performance not improving for 4 consecutive epochs.
2026-02-13 11:16:22 - INFO - Fine-tuning models
2026-02-13 11:16:24 - INFO - Time taken for Epoch 1:2.48 - F1: 0.1208
2026-02-13 11:16:27 - INFO - Time taken for Epoch 2:3.16 - F1: 0.1473
2026-02-13 11:16:31 - INFO - Time taken for Epoch 3:3.11 - F1: 0.1367
2026-02-13 11:16:33 - INFO - Time taken for Epoch 4:2.47 - F1: 0.1466
2026-02-13 11:16:36 - INFO - Time taken for Epoch 5:2.45 - F1: 0.1551
2026-02-13 11:16:39 - INFO - Time taken for Epoch 6:3.08 - F1: 0.1565
2026-02-13 11:16:42 - INFO - Time taken for Epoch 7:3.05 - F1: 0.1592
2026-02-13 11:16:45 - INFO - Time taken for Epoch 8:3.06 - F1: 0.1578
2026-02-13 11:16:47 - INFO - Time taken for Epoch 9:2.40 - F1: 0.1211
2026-02-13 11:16:50 - INFO - Time taken for Epoch 10:2.48 - F1: 0.0850
2026-02-13 11:16:52 - INFO - Time taken for Epoch 11:2.49 - F1: 0.0338
2026-02-13 11:16:55 - INFO - Time taken for Epoch 12:2.46 - F1: 0.0126
2026-02-13 11:16:57 - INFO - Time taken for Epoch 13:2.53 - F1: 0.0345
2026-02-13 11:17:00 - INFO - Time taken for Epoch 14:2.50 - F1: 0.0202
2026-02-13 11:17:02 - INFO - Time taken for Epoch 15:2.44 - F1: 0.0124
2026-02-13 11:17:04 - INFO - Time taken for Epoch 16:2.42 - F1: 0.0184
2026-02-13 11:17:07 - INFO - Time taken for Epoch 17:2.48 - F1: 0.0183
2026-02-13 11:17:07 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 11:17:07 - INFO - Best F1:0.1592 - Best Epoch:6
2026-02-13 11:17:13 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.1561, Test ECE: 0.0934
2026-02-13 11:17:13 - INFO - All results: {'f1_macro': 0.1561051603776777, 'ece': np.float64(0.09342696675284934)}
2026-02-13 11:17:13 - INFO - 
Total time taken: 425.60 seconds
2026-02-13 11:17:13 - INFO - Trial 4 finished with value: 0.1561051603776777 and parameters: {'learning_rate': 0.0001271531617134396, 'weight_decay': 4.914013940508779e-05, 'batch_size': 8, 'co_train_epochs': 13, 'epoch_patience': 4}. Best is trial 2 with value: 0.599919251739942.
2026-02-13 11:17:13 - INFO - Using devices: cuda, cuda
2026-02-13 11:17:13 - INFO - Devices: cuda, cuda
2026-02-13 11:17:13 - INFO - Starting log
2026-02-13 11:17:13 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 11:17:13 - INFO - Learning Rate: 0.0008890873495368582
Weight Decay: 0.0002912033712194302
Batch Size: 24
No. Epochs: 5
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-13 11:17:14 - INFO - Generating initial weights
2026-02-13 11:17:29 - INFO - Time taken for Epoch 1:13.52 - F1: 0.0149
2026-02-13 11:17:42 - INFO - Time taken for Epoch 2:13.55 - F1: 0.0021
2026-02-13 11:17:56 - INFO - Time taken for Epoch 3:13.54 - F1: 0.0294
2026-02-13 11:18:09 - INFO - Time taken for Epoch 4:13.55 - F1: 0.0119
2026-02-13 11:18:23 - INFO - Time taken for Epoch 5:13.78 - F1: 0.0181
2026-02-13 11:18:23 - INFO - Best F1:0.0294 - Best Epoch:3
2026-02-13 11:18:24 - INFO - Starting co-training
2026-02-13 11:18:54 - INFO - Time taken for Epoch 1: 30.27s - F1: 0.04185068
2026-02-13 11:19:25 - INFO - Time taken for Epoch 2: 31.13s - F1: 0.04185068
2026-02-13 11:19:55 - INFO - Time taken for Epoch 3: 30.09s - F1: 0.04185068
2026-02-13 11:20:25 - INFO - Time taken for Epoch 4: 30.16s - F1: 0.04185068
2026-02-13 11:20:55 - INFO - Time taken for Epoch 5: 30.06s - F1: 0.04185068
2026-02-13 11:20:57 - INFO - Fine-tuning models
2026-02-13 11:20:59 - INFO - Time taken for Epoch 1:1.96 - F1: 0.0302
2026-02-13 11:21:01 - INFO - Time taken for Epoch 2:2.52 - F1: 0.0021
2026-02-13 11:21:03 - INFO - Time taken for Epoch 3:1.93 - F1: 0.0021
2026-02-13 11:21:05 - INFO - Time taken for Epoch 4:1.94 - F1: 0.0037
2026-02-13 11:21:07 - INFO - Time taken for Epoch 5:1.95 - F1: 0.0108
2026-02-13 11:21:09 - INFO - Time taken for Epoch 6:1.96 - F1: 0.0108
2026-02-13 11:21:11 - INFO - Time taken for Epoch 7:1.98 - F1: 0.0108
2026-02-13 11:21:13 - INFO - Time taken for Epoch 8:1.95 - F1: 0.0108
2026-02-13 11:21:15 - INFO - Time taken for Epoch 9:1.98 - F1: 0.0021
2026-02-13 11:21:17 - INFO - Time taken for Epoch 10:2.00 - F1: 0.0021
2026-02-13 11:21:19 - INFO - Time taken for Epoch 11:2.01 - F1: 0.0247
2026-02-13 11:21:19 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 11:21:19 - INFO - Best F1:0.0302 - Best Epoch:0
2026-02-13 11:21:24 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0303, Test ECE: 0.1467
2026-02-13 11:21:24 - INFO - All results: {'f1_macro': 0.030313588850174218, 'ece': np.float64(0.14667953143863952)}
2026-02-13 11:21:24 - INFO - 
Total time taken: 251.32 seconds
2026-02-13 11:21:24 - INFO - Trial 5 finished with value: 0.030313588850174218 and parameters: {'learning_rate': 0.0008890873495368582, 'weight_decay': 0.0002912033712194302, 'batch_size': 24, 'co_train_epochs': 5, 'epoch_patience': 4}. Best is trial 2 with value: 0.599919251739942.
2026-02-13 11:21:24 - INFO - Using devices: cuda, cuda
2026-02-13 11:21:24 - INFO - Devices: cuda, cuda
2026-02-13 11:21:24 - INFO - Starting log
2026-02-13 11:21:24 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 11:21:24 - INFO - Learning Rate: 0.0006066412459842771
Weight Decay: 0.00020604886499337268
Batch Size: 8
No. Epochs: 20
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-13 11:21:25 - INFO - Generating initial weights
2026-02-13 11:21:44 - INFO - Time taken for Epoch 1:17.25 - F1: 0.0120
2026-02-13 11:22:01 - INFO - Time taken for Epoch 2:17.27 - F1: 0.0850
2026-02-13 11:22:18 - INFO - Time taken for Epoch 3:16.91 - F1: 0.0282
2026-02-13 11:22:35 - INFO - Time taken for Epoch 4:16.84 - F1: 0.0437
2026-02-13 11:22:51 - INFO - Time taken for Epoch 5:16.76 - F1: 0.0397
2026-02-13 11:23:08 - INFO - Time taken for Epoch 6:16.70 - F1: 0.1239
2026-02-13 11:23:25 - INFO - Time taken for Epoch 7:16.69 - F1: 0.0706
2026-02-13 11:23:42 - INFO - Time taken for Epoch 8:17.47 - F1: 0.0954
2026-02-13 11:23:59 - INFO - Time taken for Epoch 9:16.95 - F1: 0.1474
2026-02-13 11:24:16 - INFO - Time taken for Epoch 10:17.04 - F1: 0.1367
2026-02-13 11:24:34 - INFO - Time taken for Epoch 11:18.11 - F1: 0.1469
2026-02-13 11:24:52 - INFO - Time taken for Epoch 12:17.47 - F1: 0.1477
2026-02-13 11:25:10 - INFO - Time taken for Epoch 13:18.45 - F1: 0.1426
2026-02-13 11:25:28 - INFO - Time taken for Epoch 14:18.03 - F1: 0.1303
2026-02-13 11:25:46 - INFO - Time taken for Epoch 15:17.63 - F1: 0.1238
2026-02-13 11:26:04 - INFO - Time taken for Epoch 16:17.88 - F1: 0.1355
2026-02-13 11:26:21 - INFO - Time taken for Epoch 17:17.53 - F1: 0.1461
2026-02-13 11:26:38 - INFO - Time taken for Epoch 18:16.96 - F1: 0.1444
2026-02-13 11:26:56 - INFO - Time taken for Epoch 19:17.41 - F1: 0.1348
2026-02-13 11:27:12 - INFO - Time taken for Epoch 20:16.66 - F1: 0.1315
2026-02-13 11:27:12 - INFO - Best F1:0.1477 - Best Epoch:12
2026-02-13 11:27:13 - INFO - Starting co-training
2026-02-13 11:27:38 - INFO - Time taken for Epoch 1: 24.52s - F1: 0.04185068
2026-02-13 11:28:03 - INFO - Time taken for Epoch 2: 25.39s - F1: 0.04185068
2026-02-13 11:28:28 - INFO - Time taken for Epoch 3: 25.16s - F1: 0.03024831
2026-02-13 11:28:54 - INFO - Time taken for Epoch 4: 25.27s - F1: 0.04185068
2026-02-13 11:29:18 - INFO - Time taken for Epoch 5: 24.59s - F1: 0.04185068
2026-02-13 11:29:43 - INFO - Time taken for Epoch 6: 24.70s - F1: 0.04185068
2026-02-13 11:30:08 - INFO - Time taken for Epoch 7: 24.68s - F1: 0.04185068
2026-02-13 11:30:32 - INFO - Time taken for Epoch 8: 24.79s - F1: 0.04185068
2026-02-13 11:30:32 - INFO - Performance not improving for 7 consecutive epochs.
2026-02-13 11:30:34 - INFO - Fine-tuning models
2026-02-13 11:30:36 - INFO - Time taken for Epoch 1:2.52 - F1: 0.0302
2026-02-13 11:30:39 - INFO - Time taken for Epoch 2:3.02 - F1: 0.0037
2026-02-13 11:30:42 - INFO - Time taken for Epoch 3:2.45 - F1: 0.0021
2026-02-13 11:30:44 - INFO - Time taken for Epoch 4:2.49 - F1: 0.0120
2026-02-13 11:30:47 - INFO - Time taken for Epoch 5:2.43 - F1: 0.0120
2026-02-13 11:30:49 - INFO - Time taken for Epoch 6:2.43 - F1: 0.0108
2026-02-13 11:30:52 - INFO - Time taken for Epoch 7:2.45 - F1: 0.0096
2026-02-13 11:30:54 - INFO - Time taken for Epoch 8:2.44 - F1: 0.0096
2026-02-13 11:30:57 - INFO - Time taken for Epoch 9:2.52 - F1: 0.0108
2026-02-13 11:30:59 - INFO - Time taken for Epoch 10:2.47 - F1: 0.0037
2026-02-13 11:31:01 - INFO - Time taken for Epoch 11:2.47 - F1: 0.0037
2026-02-13 11:31:01 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 11:31:01 - INFO - Best F1:0.0302 - Best Epoch:0
2026-02-13 11:31:07 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0303, Test ECE: 0.1196
2026-02-13 11:31:07 - INFO - All results: {'f1_macro': 0.030313588850174218, 'ece': np.float64(0.11956244839698302)}
2026-02-13 11:31:07 - INFO - 
Total time taken: 583.13 seconds
2026-02-13 11:31:07 - INFO - Trial 6 finished with value: 0.030313588850174218 and parameters: {'learning_rate': 0.0006066412459842771, 'weight_decay': 0.00020604886499337268, 'batch_size': 8, 'co_train_epochs': 20, 'epoch_patience': 7}. Best is trial 2 with value: 0.599919251739942.
2026-02-13 11:31:07 - INFO - Using devices: cuda, cuda
2026-02-13 11:31:07 - INFO - Devices: cuda, cuda
2026-02-13 11:31:07 - INFO - Starting log
2026-02-13 11:31:07 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 11:31:08 - INFO - Learning Rate: 4.434114184331196e-05
Weight Decay: 5.947841471189674e-05
Batch Size: 24
No. Epochs: 18
Epoch Patience: 5
 Accumulation Steps: 2
2026-02-13 11:31:08 - INFO - Generating initial weights
2026-02-13 11:31:23 - INFO - Time taken for Epoch 1:13.77 - F1: 0.0965
2026-02-13 11:31:37 - INFO - Time taken for Epoch 2:13.68 - F1: 0.1019
2026-02-13 11:31:51 - INFO - Time taken for Epoch 3:13.75 - F1: 0.0842
2026-02-13 11:32:04 - INFO - Time taken for Epoch 4:13.82 - F1: 0.0740
2026-02-13 11:32:18 - INFO - Time taken for Epoch 5:13.72 - F1: 0.0767
2026-02-13 11:32:32 - INFO - Time taken for Epoch 6:13.74 - F1: 0.0744
2026-02-13 11:32:46 - INFO - Time taken for Epoch 7:13.71 - F1: 0.0945
2026-02-13 11:32:59 - INFO - Time taken for Epoch 8:13.78 - F1: 0.1256
2026-02-13 11:33:13 - INFO - Time taken for Epoch 9:14.04 - F1: 0.1473
2026-02-13 11:33:27 - INFO - Time taken for Epoch 10:13.67 - F1: 0.1715
2026-02-13 11:33:41 - INFO - Time taken for Epoch 11:13.59 - F1: 0.1720
2026-02-13 11:33:54 - INFO - Time taken for Epoch 12:13.60 - F1: 0.1623
2026-02-13 11:34:08 - INFO - Time taken for Epoch 13:13.60 - F1: 0.1611
2026-02-13 11:34:22 - INFO - Time taken for Epoch 14:13.58 - F1: 0.1633
2026-02-13 11:34:35 - INFO - Time taken for Epoch 15:13.57 - F1: 0.1633
2026-02-13 11:34:49 - INFO - Time taken for Epoch 16:13.61 - F1: 0.1618
2026-02-13 11:35:02 - INFO - Time taken for Epoch 17:13.58 - F1: 0.1595
2026-02-13 11:35:16 - INFO - Time taken for Epoch 18:13.55 - F1: 0.1607
2026-02-13 11:35:16 - INFO - Best F1:0.1720 - Best Epoch:11
2026-02-13 11:35:16 - INFO - Starting co-training
2026-02-13 11:35:46 - INFO - Time taken for Epoch 1: 29.69s - F1: 0.43665202
2026-02-13 11:36:17 - INFO - Time taken for Epoch 2: 31.03s - F1: 0.46972894
2026-02-13 11:36:47 - INFO - Time taken for Epoch 3: 30.01s - F1: 0.47121462
2026-02-13 11:37:17 - INFO - Time taken for Epoch 4: 30.11s - F1: 0.51539045
2026-02-13 11:37:48 - INFO - Time taken for Epoch 5: 30.09s - F1: 0.60246501
2026-02-13 11:38:18 - INFO - Time taken for Epoch 6: 30.00s - F1: 0.61724197
2026-02-13 11:38:48 - INFO - Time taken for Epoch 7: 30.02s - F1: 0.59499843
2026-02-13 11:39:17 - INFO - Time taken for Epoch 8: 29.45s - F1: 0.58569764
2026-02-13 11:39:47 - INFO - Time taken for Epoch 9: 29.68s - F1: 0.59724202
2026-02-13 11:40:16 - INFO - Time taken for Epoch 10: 29.73s - F1: 0.58931208
2026-02-13 11:40:47 - INFO - Time taken for Epoch 11: 30.14s - F1: 0.60309185
2026-02-13 11:40:47 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-13 11:40:48 - INFO - Fine-tuning models
2026-02-13 11:40:50 - INFO - Time taken for Epoch 1:1.96 - F1: 0.5918
2026-02-13 11:40:53 - INFO - Time taken for Epoch 2:2.59 - F1: 0.5927
2026-02-13 11:40:55 - INFO - Time taken for Epoch 3:2.65 - F1: 0.5922
2026-02-13 11:40:57 - INFO - Time taken for Epoch 4:1.94 - F1: 0.6014
2026-02-13 11:41:00 - INFO - Time taken for Epoch 5:2.60 - F1: 0.5968
2026-02-13 11:41:02 - INFO - Time taken for Epoch 6:1.94 - F1: 0.5912
2026-02-13 11:41:04 - INFO - Time taken for Epoch 7:1.95 - F1: 0.5941
2026-02-13 11:41:06 - INFO - Time taken for Epoch 8:1.94 - F1: 0.5940
2026-02-13 11:41:08 - INFO - Time taken for Epoch 9:1.94 - F1: 0.5922
2026-02-13 11:41:10 - INFO - Time taken for Epoch 10:1.95 - F1: 0.5956
2026-02-13 11:41:12 - INFO - Time taken for Epoch 11:1.96 - F1: 0.5996
2026-02-13 11:41:14 - INFO - Time taken for Epoch 12:1.96 - F1: 0.5947
2026-02-13 11:41:15 - INFO - Time taken for Epoch 13:1.94 - F1: 0.6025
2026-02-13 11:41:18 - INFO - Time taken for Epoch 14:2.62 - F1: 0.6039
2026-02-13 11:41:21 - INFO - Time taken for Epoch 15:2.59 - F1: 0.6017
2026-02-13 11:41:23 - INFO - Time taken for Epoch 16:1.93 - F1: 0.5973
2026-02-13 11:41:25 - INFO - Time taken for Epoch 17:1.94 - F1: 0.5978
2026-02-13 11:41:26 - INFO - Time taken for Epoch 18:1.94 - F1: 0.5998
2026-02-13 11:41:28 - INFO - Time taken for Epoch 19:1.94 - F1: 0.6002
2026-02-13 11:41:30 - INFO - Time taken for Epoch 20:1.94 - F1: 0.6017
2026-02-13 11:41:32 - INFO - Time taken for Epoch 21:1.94 - F1: 0.6026
2026-02-13 11:41:34 - INFO - Time taken for Epoch 22:1.93 - F1: 0.5981
2026-02-13 11:41:36 - INFO - Time taken for Epoch 23:1.94 - F1: 0.5973
2026-02-13 11:41:38 - INFO - Time taken for Epoch 24:1.93 - F1: 0.5968
2026-02-13 11:41:38 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 11:41:38 - INFO - Best F1:0.6039 - Best Epoch:13
2026-02-13 11:41:43 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6185, Test ECE: 0.0617
2026-02-13 11:41:43 - INFO - All results: {'f1_macro': 0.6184870157730874, 'ece': np.float64(0.061711224972752984)}
2026-02-13 11:41:43 - INFO - 
Total time taken: 635.68 seconds
2026-02-13 11:41:43 - INFO - Trial 7 finished with value: 0.6184870157730874 and parameters: {'learning_rate': 4.434114184331196e-05, 'weight_decay': 5.947841471189674e-05, 'batch_size': 24, 'co_train_epochs': 18, 'epoch_patience': 5}. Best is trial 7 with value: 0.6184870157730874.
2026-02-13 11:41:43 - INFO - Using devices: cuda, cuda
2026-02-13 11:41:43 - INFO - Devices: cuda, cuda
2026-02-13 11:41:43 - INFO - Starting log
2026-02-13 11:41:43 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 11:41:43 - INFO - Learning Rate: 0.0008338906025708744
Weight Decay: 1.725044376291512e-05
Batch Size: 24
No. Epochs: 13
Epoch Patience: 9
 Accumulation Steps: 2
2026-02-13 11:41:44 - INFO - Generating initial weights
2026-02-13 11:41:59 - INFO - Time taken for Epoch 1:13.64 - F1: 0.0108
2026-02-13 11:42:12 - INFO - Time taken for Epoch 2:13.64 - F1: 0.0071
2026-02-13 11:42:26 - INFO - Time taken for Epoch 3:13.61 - F1: 0.0399
2026-02-13 11:42:40 - INFO - Time taken for Epoch 4:13.60 - F1: 0.0805
2026-02-13 11:42:53 - INFO - Time taken for Epoch 5:13.62 - F1: 0.0047
2026-02-13 11:43:07 - INFO - Time taken for Epoch 6:13.59 - F1: 0.0047
2026-02-13 11:43:20 - INFO - Time taken for Epoch 7:13.59 - F1: 0.0021
2026-02-13 11:43:34 - INFO - Time taken for Epoch 8:13.58 - F1: 0.0108
2026-02-13 11:43:48 - INFO - Time taken for Epoch 9:13.59 - F1: 0.0108
2026-02-13 11:44:01 - INFO - Time taken for Epoch 10:13.59 - F1: 0.0096
2026-02-13 11:44:15 - INFO - Time taken for Epoch 11:13.57 - F1: 0.0096
2026-02-13 11:44:28 - INFO - Time taken for Epoch 12:13.59 - F1: 0.0096
2026-02-13 11:44:42 - INFO - Time taken for Epoch 13:13.57 - F1: 0.0247
2026-02-13 11:44:42 - INFO - Best F1:0.0805 - Best Epoch:4
2026-02-13 11:44:43 - INFO - Starting co-training
2026-02-13 11:45:12 - INFO - Time taken for Epoch 1: 29.51s - F1: 0.04185068
2026-02-13 11:45:42 - INFO - Time taken for Epoch 2: 30.05s - F1: 0.03024831
2026-02-13 11:46:12 - INFO - Time taken for Epoch 3: 29.60s - F1: 0.03024831
2026-02-13 11:46:41 - INFO - Time taken for Epoch 4: 29.50s - F1: 0.03024831
2026-02-13 11:47:11 - INFO - Time taken for Epoch 5: 29.54s - F1: 0.03024831
2026-02-13 11:47:40 - INFO - Time taken for Epoch 6: 29.47s - F1: 0.03024831
2026-02-13 11:48:10 - INFO - Time taken for Epoch 7: 29.58s - F1: 0.03024831
2026-02-13 11:48:39 - INFO - Time taken for Epoch 8: 29.52s - F1: 0.03024831
2026-02-13 11:49:09 - INFO - Time taken for Epoch 9: 29.52s - F1: 0.03024831
2026-02-13 11:49:39 - INFO - Time taken for Epoch 10: 29.56s - F1: 0.03024831
2026-02-13 11:49:39 - INFO - Performance not improving for 9 consecutive epochs.
2026-02-13 11:49:40 - INFO - Fine-tuning models
2026-02-13 11:49:42 - INFO - Time taken for Epoch 1:1.93 - F1: 0.0302
2026-02-13 11:49:45 - INFO - Time taken for Epoch 2:2.54 - F1: 0.0021
2026-02-13 11:49:46 - INFO - Time taken for Epoch 3:1.93 - F1: 0.0037
2026-02-13 11:49:48 - INFO - Time taken for Epoch 4:1.92 - F1: 0.0096
2026-02-13 11:49:50 - INFO - Time taken for Epoch 5:1.92 - F1: 0.0108
2026-02-13 11:49:52 - INFO - Time taken for Epoch 6:1.93 - F1: 0.0108
2026-02-13 11:49:54 - INFO - Time taken for Epoch 7:1.92 - F1: 0.0108
2026-02-13 11:49:56 - INFO - Time taken for Epoch 8:1.93 - F1: 0.0108
2026-02-13 11:49:58 - INFO - Time taken for Epoch 9:1.92 - F1: 0.0247
2026-02-13 11:50:00 - INFO - Time taken for Epoch 10:1.92 - F1: 0.0247
2026-02-13 11:50:02 - INFO - Time taken for Epoch 11:1.93 - F1: 0.0021
2026-02-13 11:50:02 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 11:50:02 - INFO - Best F1:0.0302 - Best Epoch:0
2026-02-13 11:50:07 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0303, Test ECE: 0.1365
2026-02-13 11:50:07 - INFO - All results: {'f1_macro': 0.030313588850174218, 'ece': np.float64(0.1365285564129847)}
2026-02-13 11:50:07 - INFO - 
Total time taken: 503.60 seconds
2026-02-13 11:50:07 - INFO - Trial 8 finished with value: 0.030313588850174218 and parameters: {'learning_rate': 0.0008338906025708744, 'weight_decay': 1.725044376291512e-05, 'batch_size': 24, 'co_train_epochs': 13, 'epoch_patience': 9}. Best is trial 7 with value: 0.6184870157730874.
2026-02-13 11:50:07 - INFO - Using devices: cuda, cuda
2026-02-13 11:50:07 - INFO - Devices: cuda, cuda
2026-02-13 11:50:07 - INFO - Starting log
2026-02-13 11:50:07 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-13 11:50:07 - INFO - Learning Rate: 0.0009339315372854487
Weight Decay: 9.507813365847493e-05
Batch Size: 16
No. Epochs: 5
Epoch Patience: 10
 Accumulation Steps: 4
2026-02-13 11:50:07 - INFO - Generating initial weights
2026-02-13 11:50:24 - INFO - Time taken for Epoch 1:14.75 - F1: 0.0337
2026-02-13 11:50:38 - INFO - Time taken for Epoch 2:14.70 - F1: 0.0120
2026-02-13 11:50:53 - INFO - Time taken for Epoch 3:14.68 - F1: 0.0180
2026-02-13 11:51:08 - INFO - Time taken for Epoch 4:14.71 - F1: 0.0321
2026-02-13 11:51:22 - INFO - Time taken for Epoch 5:14.67 - F1: 0.0120
2026-02-13 11:51:22 - INFO - Best F1:0.0337 - Best Epoch:1
2026-02-13 11:51:23 - INFO - Starting co-training
2026-02-13 11:51:48 - INFO - Time taken for Epoch 1: 24.73s - F1: 0.03214286
2026-02-13 11:52:14 - INFO - Time taken for Epoch 2: 25.99s - F1: 0.03214286
2026-02-13 11:52:39 - INFO - Time taken for Epoch 3: 24.75s - F1: 0.03024831
2026-02-13 11:53:03 - INFO - Time taken for Epoch 4: 24.72s - F1: 0.03024831
2026-02-13 11:53:28 - INFO - Time taken for Epoch 5: 24.69s - F1: 0.03024831
2026-02-13 11:53:29 - INFO - Fine-tuning models
2026-02-13 11:53:31 - INFO - Time taken for Epoch 1:2.12 - F1: 0.0321
2026-02-13 11:53:34 - INFO - Time taken for Epoch 2:2.68 - F1: 0.0021
2026-02-13 11:53:36 - INFO - Time taken for Epoch 3:2.10 - F1: 0.0120
2026-02-13 11:53:38 - INFO - Time taken for Epoch 4:2.11 - F1: 0.0120
2026-02-13 11:53:40 - INFO - Time taken for Epoch 5:2.11 - F1: 0.0120
2026-02-13 11:53:43 - INFO - Time taken for Epoch 6:2.11 - F1: 0.0321
2026-02-13 11:53:45 - INFO - Time taken for Epoch 7:2.10 - F1: 0.0321
2026-02-13 11:53:47 - INFO - Time taken for Epoch 8:2.10 - F1: 0.0321
2026-02-13 11:53:49 - INFO - Time taken for Epoch 9:2.12 - F1: 0.0321
2026-02-13 11:53:51 - INFO - Time taken for Epoch 10:2.11 - F1: 0.0321
2026-02-13 11:53:53 - INFO - Time taken for Epoch 11:2.11 - F1: 0.0321
2026-02-13 11:53:53 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 11:53:53 - INFO - Best F1:0.0321 - Best Epoch:0
2026-02-13 11:53:58 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0322, Test ECE: 0.1661
2026-02-13 11:53:58 - INFO - All results: {'f1_macro': 0.032165422171166, 'ece': np.float64(0.1661465793011696)}
2026-02-13 11:53:58 - INFO - 
Total time taken: 231.47 seconds
2026-02-13 11:53:58 - INFO - Trial 9 finished with value: 0.032165422171166 and parameters: {'learning_rate': 0.0009339315372854487, 'weight_decay': 9.507813365847493e-05, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 10}. Best is trial 7 with value: 0.6184870157730874.
2026-02-13 11:53:58 - INFO - 
[BEST TRIAL RESULTS]
2026-02-13 11:53:58 - INFO - F1 Score: 0.6185
2026-02-13 11:53:58 - INFO - Params: {'learning_rate': 4.434114184331196e-05, 'weight_decay': 5.947841471189674e-05, 'batch_size': 24, 'co_train_epochs': 18, 'epoch_patience': 5}
2026-02-13 11:53:58 - INFO -   learning_rate: 4.434114184331196e-05
2026-02-13 11:53:58 - INFO -   weight_decay: 5.947841471189674e-05
2026-02-13 11:53:58 - INFO -   batch_size: 24
2026-02-13 11:53:58 - INFO -   co_train_epochs: 18
2026-02-13 11:53:58 - INFO -   epoch_patience: 5
2026-02-13 11:53:58 - INFO - 
Total time taken: 4361.05 seconds
