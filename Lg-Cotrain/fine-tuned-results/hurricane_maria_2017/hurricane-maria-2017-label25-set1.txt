Running with 25 label/class set 1

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-14 08:08:38 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-14 08:08:38 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_maria_2017
Using devices: cuda, cuda
2026-02-14 08:08:38 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 08:08:38 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 08:08:38 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 08:08:38 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 2.55314706966723e-05
Weight Decay: 0.002391215161564007
Batch Size: 32
No. Epochs: 13
Epoch Patience: 4
 Accumulation Steps: 2
2026-02-14 08:08:39 - INFO - Learning Rate: 2.55314706966723e-05
Weight Decay: 0.002391215161564007
Batch Size: 32
No. Epochs: 13
Epoch Patience: 4
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 08:08:40 - INFO - Generating initial weights
Time taken for Epoch 1:20.33 - F1: 0.0247
2026-02-14 08:09:04 - INFO - Time taken for Epoch 1:20.33 - F1: 0.0247
Time taken for Epoch 2:19.95 - F1: 0.0320
2026-02-14 08:09:24 - INFO - Time taken for Epoch 2:19.95 - F1: 0.0320
Time taken for Epoch 3:20.03 - F1: 0.0700
2026-02-14 08:09:44 - INFO - Time taken for Epoch 3:20.03 - F1: 0.0700
Time taken for Epoch 4:20.12 - F1: 0.1229
2026-02-14 08:10:04 - INFO - Time taken for Epoch 4:20.12 - F1: 0.1229
Time taken for Epoch 5:20.21 - F1: 0.1172
2026-02-14 08:10:25 - INFO - Time taken for Epoch 5:20.21 - F1: 0.1172
Time taken for Epoch 6:20.28 - F1: 0.0675
2026-02-14 08:10:45 - INFO - Time taken for Epoch 6:20.28 - F1: 0.0675
Time taken for Epoch 7:20.35 - F1: 0.0732
2026-02-14 08:11:05 - INFO - Time taken for Epoch 7:20.35 - F1: 0.0732
Time taken for Epoch 8:20.41 - F1: 0.0778
2026-02-14 08:11:26 - INFO - Time taken for Epoch 8:20.41 - F1: 0.0778
Time taken for Epoch 9:20.37 - F1: 0.1101
2026-02-14 08:11:46 - INFO - Time taken for Epoch 9:20.37 - F1: 0.1101
Time taken for Epoch 10:20.40 - F1: 0.1572
2026-02-14 08:12:06 - INFO - Time taken for Epoch 10:20.40 - F1: 0.1572
Time taken for Epoch 11:20.41 - F1: 0.1925
2026-02-14 08:12:27 - INFO - Time taken for Epoch 11:20.41 - F1: 0.1925
Time taken for Epoch 12:20.40 - F1: 0.2160
2026-02-14 08:12:47 - INFO - Time taken for Epoch 12:20.40 - F1: 0.2160
Time taken for Epoch 13:20.49 - F1: 0.2210
2026-02-14 08:13:08 - INFO - Time taken for Epoch 13:20.49 - F1: 0.2210
Best F1:0.2210 - Best Epoch:13
2026-02-14 08:13:08 - INFO - Best F1:0.2210 - Best Epoch:13
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 08:13:09 - INFO - Starting co-training
Time taken for Epoch 1: 34.41s - F1: 0.47768438
2026-02-14 08:13:44 - INFO - Time taken for Epoch 1: 34.41s - F1: 0.47768438
Time taken for Epoch 2: 35.50s - F1: 0.53339324
2026-02-14 08:14:19 - INFO - Time taken for Epoch 2: 35.50s - F1: 0.53339324
Time taken for Epoch 3: 35.93s - F1: 0.53752664
2026-02-14 08:14:55 - INFO - Time taken for Epoch 3: 35.93s - F1: 0.53752664
Time taken for Epoch 4: 35.66s - F1: 0.62155921
2026-02-14 08:15:31 - INFO - Time taken for Epoch 4: 35.66s - F1: 0.62155921
Time taken for Epoch 5: 35.87s - F1: 0.64631110
2026-02-14 08:16:07 - INFO - Time taken for Epoch 5: 35.87s - F1: 0.64631110
Time taken for Epoch 6: 36.07s - F1: 0.63226815
2026-02-14 08:16:43 - INFO - Time taken for Epoch 6: 36.07s - F1: 0.63226815
Time taken for Epoch 7: 34.44s - F1: 0.65299058
2026-02-14 08:17:17 - INFO - Time taken for Epoch 7: 34.44s - F1: 0.65299058
Time taken for Epoch 8: 35.73s - F1: 0.64236838
2026-02-14 08:17:53 - INFO - Time taken for Epoch 8: 35.73s - F1: 0.64236838
Time taken for Epoch 9: 34.45s - F1: 0.63926248
2026-02-14 08:18:27 - INFO - Time taken for Epoch 9: 34.45s - F1: 0.63926248
Time taken for Epoch 10: 34.45s - F1: 0.64776005
2026-02-14 08:19:02 - INFO - Time taken for Epoch 10: 34.45s - F1: 0.64776005
Time taken for Epoch 11: 34.45s - F1: 0.65158244
2026-02-14 08:19:36 - INFO - Time taken for Epoch 11: 34.45s - F1: 0.65158244
Performance not improving for 4 consecutive epochs.
Performance not improving for 4 consecutive epochs.
2026-02-14 08:19:36 - INFO - Performance not improving for 4 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 08:19:39 - INFO - Fine-tuning models
Time taken for Epoch 1:3.66 - F1: 0.6752
2026-02-14 08:19:43 - INFO - Time taken for Epoch 1:3.66 - F1: 0.6752
Time taken for Epoch 2:4.71 - F1: 0.6627
2026-02-14 08:19:48 - INFO - Time taken for Epoch 2:4.71 - F1: 0.6627
Time taken for Epoch 3:3.65 - F1: 0.6529
2026-02-14 08:19:51 - INFO - Time taken for Epoch 3:3.65 - F1: 0.6529
Time taken for Epoch 4:3.64 - F1: 0.6504
2026-02-14 08:19:55 - INFO - Time taken for Epoch 4:3.64 - F1: 0.6504
Time taken for Epoch 5:3.64 - F1: 0.6478
2026-02-14 08:19:59 - INFO - Time taken for Epoch 5:3.64 - F1: 0.6478
Time taken for Epoch 6:3.65 - F1: 0.6530
2026-02-14 08:20:02 - INFO - Time taken for Epoch 6:3.65 - F1: 0.6530
Time taken for Epoch 7:3.65 - F1: 0.6548
2026-02-14 08:20:06 - INFO - Time taken for Epoch 7:3.65 - F1: 0.6548
Time taken for Epoch 8:3.64 - F1: 0.6571
2026-02-14 08:20:10 - INFO - Time taken for Epoch 8:3.64 - F1: 0.6571
Time taken for Epoch 9:3.64 - F1: 0.6515
2026-02-14 08:20:13 - INFO - Time taken for Epoch 9:3.64 - F1: 0.6515
Time taken for Epoch 10:3.66 - F1: 0.6685
2026-02-14 08:20:17 - INFO - Time taken for Epoch 10:3.66 - F1: 0.6685
Time taken for Epoch 11:3.64 - F1: 0.6728
2026-02-14 08:20:20 - INFO - Time taken for Epoch 11:3.64 - F1: 0.6728
Performance not improving for 10 consecutive epochs.
2026-02-14 08:20:20 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6752 - Best Epoch:0
2026-02-14 08:20:20 - INFO - Best F1:0.6752 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6775, Test ECE: 0.0299
2026-02-14 08:20:29 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6775, Test ECE: 0.0299
All results: {'f1_macro': 0.6775060123822318, 'ece': np.float64(0.02990859184780994)}
2026-02-14 08:20:29 - INFO - All results: {'f1_macro': 0.6775060123822318, 'ece': np.float64(0.02990859184780994)}

Total time taken: 710.83 seconds
2026-02-14 08:20:29 - INFO - 
Total time taken: 710.83 seconds
2026-02-14 08:20:29 - INFO - Trial 0 finished with value: 0.6775060123822318 and parameters: {'learning_rate': 2.55314706966723e-05, 'weight_decay': 0.002391215161564007, 'batch_size': 32, 'co_train_epochs': 13, 'epoch_patience': 4}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 08:20:29 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 08:20:29 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 08:20:29 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 08:20:29 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.0004092984586707356
Weight Decay: 0.0019425397465671474
Batch Size: 8
No. Epochs: 18
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-14 08:20:29 - INFO - Learning Rate: 0.0004092984586707356
Weight Decay: 0.0019425397465671474
Batch Size: 8
No. Epochs: 18
Epoch Patience: 10
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 08:20:30 - INFO - Generating initial weights
Time taken for Epoch 1:22.67 - F1: 0.0389
2026-02-14 08:20:56 - INFO - Time taken for Epoch 1:22.67 - F1: 0.0389
Time taken for Epoch 2:22.66 - F1: 0.0038
2026-02-14 08:21:19 - INFO - Time taken for Epoch 2:22.66 - F1: 0.0038
Time taken for Epoch 3:22.62 - F1: 0.0172
2026-02-14 08:21:41 - INFO - Time taken for Epoch 3:22.62 - F1: 0.0172
Time taken for Epoch 4:22.65 - F1: 0.0081
2026-02-14 08:22:04 - INFO - Time taken for Epoch 4:22.65 - F1: 0.0081
Time taken for Epoch 5:22.67 - F1: 0.0089
2026-02-14 08:22:27 - INFO - Time taken for Epoch 5:22.67 - F1: 0.0089
Time taken for Epoch 6:22.67 - F1: 0.0089
2026-02-14 08:22:49 - INFO - Time taken for Epoch 6:22.67 - F1: 0.0089
Time taken for Epoch 7:22.67 - F1: 0.0038
2026-02-14 08:23:12 - INFO - Time taken for Epoch 7:22.67 - F1: 0.0038
Time taken for Epoch 8:22.69 - F1: 0.0038
2026-02-14 08:23:35 - INFO - Time taken for Epoch 8:22.69 - F1: 0.0038
Time taken for Epoch 9:22.68 - F1: 0.0038
2026-02-14 08:23:57 - INFO - Time taken for Epoch 9:22.68 - F1: 0.0038
Time taken for Epoch 10:22.68 - F1: 0.0135
2026-02-14 08:24:20 - INFO - Time taken for Epoch 10:22.68 - F1: 0.0135
Time taken for Epoch 11:22.69 - F1: 0.0089
2026-02-14 08:24:43 - INFO - Time taken for Epoch 11:22.69 - F1: 0.0089
Time taken for Epoch 12:22.68 - F1: 0.0096
2026-02-14 08:25:06 - INFO - Time taken for Epoch 12:22.68 - F1: 0.0096
Time taken for Epoch 13:22.66 - F1: 0.0038
2026-02-14 08:25:28 - INFO - Time taken for Epoch 13:22.66 - F1: 0.0038
Time taken for Epoch 14:22.67 - F1: 0.0081
2026-02-14 08:25:51 - INFO - Time taken for Epoch 14:22.67 - F1: 0.0081
Time taken for Epoch 15:22.70 - F1: 0.0064
2026-02-14 08:26:14 - INFO - Time taken for Epoch 15:22.70 - F1: 0.0064
Time taken for Epoch 16:22.71 - F1: 0.0064
2026-02-14 08:26:36 - INFO - Time taken for Epoch 16:22.71 - F1: 0.0064
Time taken for Epoch 17:22.71 - F1: 0.0064
2026-02-14 08:26:59 - INFO - Time taken for Epoch 17:22.71 - F1: 0.0064
Time taken for Epoch 18:22.71 - F1: 0.0064
2026-02-14 08:27:22 - INFO - Time taken for Epoch 18:22.71 - F1: 0.0064
Best F1:0.0389 - Best Epoch:1
2026-02-14 08:27:22 - INFO - Best F1:0.0389 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 08:27:23 - INFO - Starting co-training
Time taken for Epoch 1: 26.82s - F1: 0.04755179
2026-02-14 08:27:50 - INFO - Time taken for Epoch 1: 26.82s - F1: 0.04755179
Time taken for Epoch 2: 27.91s - F1: 0.04755179
2026-02-14 08:28:18 - INFO - Time taken for Epoch 2: 27.91s - F1: 0.04755179
Time taken for Epoch 3: 26.83s - F1: 0.04755179
2026-02-14 08:28:45 - INFO - Time taken for Epoch 3: 26.83s - F1: 0.04755179
Time taken for Epoch 4: 26.84s - F1: 0.04755179
2026-02-14 08:29:12 - INFO - Time taken for Epoch 4: 26.84s - F1: 0.04755179
Time taken for Epoch 5: 26.80s - F1: 0.04755179
2026-02-14 08:29:38 - INFO - Time taken for Epoch 5: 26.80s - F1: 0.04755179
Time taken for Epoch 6: 26.86s - F1: 0.04755179
2026-02-14 08:30:05 - INFO - Time taken for Epoch 6: 26.86s - F1: 0.04755179
Time taken for Epoch 7: 26.87s - F1: 0.04755179
2026-02-14 08:30:32 - INFO - Time taken for Epoch 7: 26.87s - F1: 0.04755179
Time taken for Epoch 8: 26.86s - F1: 0.04755179
2026-02-14 08:30:59 - INFO - Time taken for Epoch 8: 26.86s - F1: 0.04755179
Time taken for Epoch 9: 26.87s - F1: 0.04755179
2026-02-14 08:31:26 - INFO - Time taken for Epoch 9: 26.87s - F1: 0.04755179
Time taken for Epoch 10: 27.02s - F1: 0.04755179
2026-02-14 08:31:53 - INFO - Time taken for Epoch 10: 27.02s - F1: 0.04755179
Time taken for Epoch 11: 26.85s - F1: 0.04755179
2026-02-14 08:32:20 - INFO - Time taken for Epoch 11: 26.85s - F1: 0.04755179
Performance not improving for 10 consecutive epochs.
Performance not improving for 10 consecutive epochs.
2026-02-14 08:32:20 - INFO - Performance not improving for 10 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 08:32:22 - INFO - Fine-tuning models
Time taken for Epoch 1:4.05 - F1: 0.0476
2026-02-14 08:32:27 - INFO - Time taken for Epoch 1:4.05 - F1: 0.0476
Time taken for Epoch 2:5.13 - F1: 0.0394
2026-02-14 08:32:32 - INFO - Time taken for Epoch 2:5.13 - F1: 0.0394
Time taken for Epoch 3:4.02 - F1: 0.0394
2026-02-14 08:32:36 - INFO - Time taken for Epoch 3:4.02 - F1: 0.0394
Time taken for Epoch 4:4.02 - F1: 0.0394
2026-02-14 08:32:40 - INFO - Time taken for Epoch 4:4.02 - F1: 0.0394
Time taken for Epoch 5:4.02 - F1: 0.0038
2026-02-14 08:32:44 - INFO - Time taken for Epoch 5:4.02 - F1: 0.0038
Time taken for Epoch 6:4.02 - F1: 0.0197
2026-02-14 08:32:48 - INFO - Time taken for Epoch 6:4.02 - F1: 0.0197
Time taken for Epoch 7:4.02 - F1: 0.0197
2026-02-14 08:32:52 - INFO - Time taken for Epoch 7:4.02 - F1: 0.0197
Time taken for Epoch 8:4.03 - F1: 0.0476
2026-02-14 08:32:56 - INFO - Time taken for Epoch 8:4.03 - F1: 0.0476
Time taken for Epoch 9:4.03 - F1: 0.0476
2026-02-14 08:33:00 - INFO - Time taken for Epoch 9:4.03 - F1: 0.0476
Time taken for Epoch 10:4.04 - F1: 0.0476
2026-02-14 08:33:04 - INFO - Time taken for Epoch 10:4.04 - F1: 0.0476
Time taken for Epoch 11:4.03 - F1: 0.0394
2026-02-14 08:33:08 - INFO - Time taken for Epoch 11:4.03 - F1: 0.0394
Performance not improving for 10 consecutive epochs.
2026-02-14 08:33:08 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:0
2026-02-14 08:33:08 - INFO - Best F1:0.0476 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0474, Test ECE: 0.1146
2026-02-14 08:33:16 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0474, Test ECE: 0.1146
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.11458093533486169)}
2026-02-14 08:33:16 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.11458093533486169)}

Total time taken: 767.67 seconds
2026-02-14 08:33:16 - INFO - 
Total time taken: 767.67 seconds
2026-02-14 08:33:16 - INFO - Trial 1 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0004092984586707356, 'weight_decay': 0.0019425397465671474, 'batch_size': 8, 'co_train_epochs': 18, 'epoch_patience': 10}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 08:33:16 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 08:33:16 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 08:33:16 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 08:33:16 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 4.292203772246671e-05
Weight Decay: 0.00041565336697911527
Batch Size: 8
No. Epochs: 18
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-14 08:33:17 - INFO - Learning Rate: 4.292203772246671e-05
Weight Decay: 0.00041565336697911527
Batch Size: 8
No. Epochs: 18
Epoch Patience: 10
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 08:33:18 - INFO - Generating initial weights
Time taken for Epoch 1:22.71 - F1: 0.0205
2026-02-14 08:33:44 - INFO - Time taken for Epoch 1:22.71 - F1: 0.0205
Time taken for Epoch 2:22.70 - F1: 0.0750
2026-02-14 08:34:07 - INFO - Time taken for Epoch 2:22.70 - F1: 0.0750
Time taken for Epoch 3:22.70 - F1: 0.1197
2026-02-14 08:34:29 - INFO - Time taken for Epoch 3:22.70 - F1: 0.1197
Time taken for Epoch 4:22.71 - F1: 0.1586
2026-02-14 08:34:52 - INFO - Time taken for Epoch 4:22.71 - F1: 0.1586
Time taken for Epoch 5:22.74 - F1: 0.2030
2026-02-14 08:35:15 - INFO - Time taken for Epoch 5:22.74 - F1: 0.2030
Time taken for Epoch 6:22.72 - F1: 0.3655
2026-02-14 08:35:38 - INFO - Time taken for Epoch 6:22.72 - F1: 0.3655
Time taken for Epoch 7:22.74 - F1: 0.3977
2026-02-14 08:36:00 - INFO - Time taken for Epoch 7:22.74 - F1: 0.3977
Time taken for Epoch 8:22.74 - F1: 0.4107
2026-02-14 08:36:23 - INFO - Time taken for Epoch 8:22.74 - F1: 0.4107
Time taken for Epoch 9:22.75 - F1: 0.4326
2026-02-14 08:36:46 - INFO - Time taken for Epoch 9:22.75 - F1: 0.4326
Time taken for Epoch 10:22.75 - F1: 0.4627
2026-02-14 08:37:08 - INFO - Time taken for Epoch 10:22.75 - F1: 0.4627
Time taken for Epoch 11:22.74 - F1: 0.4623
2026-02-14 08:37:31 - INFO - Time taken for Epoch 11:22.74 - F1: 0.4623
Time taken for Epoch 12:22.72 - F1: 0.4671
2026-02-14 08:37:54 - INFO - Time taken for Epoch 12:22.72 - F1: 0.4671
Time taken for Epoch 13:22.74 - F1: 0.4837
2026-02-14 08:38:17 - INFO - Time taken for Epoch 13:22.74 - F1: 0.4837
Time taken for Epoch 14:22.74 - F1: 0.4952
2026-02-14 08:38:39 - INFO - Time taken for Epoch 14:22.74 - F1: 0.4952
Time taken for Epoch 15:22.77 - F1: 0.5184
2026-02-14 08:39:02 - INFO - Time taken for Epoch 15:22.77 - F1: 0.5184
Time taken for Epoch 16:22.71 - F1: 0.5229
2026-02-14 08:39:25 - INFO - Time taken for Epoch 16:22.71 - F1: 0.5229
Time taken for Epoch 17:22.67 - F1: 0.4950
2026-02-14 08:39:48 - INFO - Time taken for Epoch 17:22.67 - F1: 0.4950
Time taken for Epoch 18:22.65 - F1: 0.5107
2026-02-14 08:40:10 - INFO - Time taken for Epoch 18:22.65 - F1: 0.5107
Best F1:0.5229 - Best Epoch:16
2026-02-14 08:40:10 - INFO - Best F1:0.5229 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 08:40:11 - INFO - Starting co-training
Time taken for Epoch 1: 26.79s - F1: 0.26742262
2026-02-14 08:40:39 - INFO - Time taken for Epoch 1: 26.79s - F1: 0.26742262
Time taken for Epoch 2: 27.86s - F1: 0.48994922
2026-02-14 08:41:06 - INFO - Time taken for Epoch 2: 27.86s - F1: 0.48994922
Time taken for Epoch 3: 27.95s - F1: 0.57948071
2026-02-14 08:41:34 - INFO - Time taken for Epoch 3: 27.95s - F1: 0.57948071
Time taken for Epoch 4: 28.33s - F1: 0.60332302
2026-02-14 08:42:03 - INFO - Time taken for Epoch 4: 28.33s - F1: 0.60332302
Time taken for Epoch 5: 27.96s - F1: 0.59357666
2026-02-14 08:42:31 - INFO - Time taken for Epoch 5: 27.96s - F1: 0.59357666
Time taken for Epoch 6: 26.78s - F1: 0.62859174
2026-02-14 08:42:57 - INFO - Time taken for Epoch 6: 26.78s - F1: 0.62859174
Time taken for Epoch 7: 27.96s - F1: 0.66333396
2026-02-14 08:43:25 - INFO - Time taken for Epoch 7: 27.96s - F1: 0.66333396
Time taken for Epoch 8: 27.95s - F1: 0.62944159
2026-02-14 08:43:53 - INFO - Time taken for Epoch 8: 27.95s - F1: 0.62944159
Time taken for Epoch 9: 26.76s - F1: 0.63928667
2026-02-14 08:44:20 - INFO - Time taken for Epoch 9: 26.76s - F1: 0.63928667
Time taken for Epoch 10: 26.98s - F1: 0.62871916
2026-02-14 08:44:47 - INFO - Time taken for Epoch 10: 26.98s - F1: 0.62871916
Time taken for Epoch 11: 26.81s - F1: 0.63340611
2026-02-14 08:45:14 - INFO - Time taken for Epoch 11: 26.81s - F1: 0.63340611
Time taken for Epoch 12: 26.87s - F1: 0.61255561
2026-02-14 08:45:41 - INFO - Time taken for Epoch 12: 26.87s - F1: 0.61255561
Time taken for Epoch 13: 26.84s - F1: 0.63107726
2026-02-14 08:46:08 - INFO - Time taken for Epoch 13: 26.84s - F1: 0.63107726
Time taken for Epoch 14: 26.85s - F1: 0.62560488
2026-02-14 08:46:35 - INFO - Time taken for Epoch 14: 26.85s - F1: 0.62560488
Time taken for Epoch 15: 26.84s - F1: 0.62772304
2026-02-14 08:47:01 - INFO - Time taken for Epoch 15: 26.84s - F1: 0.62772304
Time taken for Epoch 16: 26.83s - F1: 0.63860347
2026-02-14 08:47:28 - INFO - Time taken for Epoch 16: 26.83s - F1: 0.63860347
Time taken for Epoch 17: 26.83s - F1: 0.63532676
2026-02-14 08:47:55 - INFO - Time taken for Epoch 17: 26.83s - F1: 0.63532676
Performance not improving for 10 consecutive epochs.
Performance not improving for 10 consecutive epochs.
2026-02-14 08:47:55 - INFO - Performance not improving for 10 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 08:47:58 - INFO - Fine-tuning models
Time taken for Epoch 1:4.04 - F1: 0.6821
2026-02-14 08:48:02 - INFO - Time taken for Epoch 1:4.04 - F1: 0.6821
Time taken for Epoch 2:5.10 - F1: 0.6707
2026-02-14 08:48:07 - INFO - Time taken for Epoch 2:5.10 - F1: 0.6707
Time taken for Epoch 3:4.03 - F1: 0.6409
2026-02-14 08:48:11 - INFO - Time taken for Epoch 3:4.03 - F1: 0.6409
Time taken for Epoch 4:4.02 - F1: 0.6281
2026-02-14 08:48:15 - INFO - Time taken for Epoch 4:4.02 - F1: 0.6281
Time taken for Epoch 5:4.02 - F1: 0.6322
2026-02-14 08:48:19 - INFO - Time taken for Epoch 5:4.02 - F1: 0.6322
Time taken for Epoch 6:4.02 - F1: 0.6517
2026-02-14 08:48:23 - INFO - Time taken for Epoch 6:4.02 - F1: 0.6517
Time taken for Epoch 7:4.02 - F1: 0.6528
2026-02-14 08:48:27 - INFO - Time taken for Epoch 7:4.02 - F1: 0.6528
Time taken for Epoch 8:4.02 - F1: 0.6501
2026-02-14 08:48:31 - INFO - Time taken for Epoch 8:4.02 - F1: 0.6501
Time taken for Epoch 9:4.02 - F1: 0.6531
2026-02-14 08:48:35 - INFO - Time taken for Epoch 9:4.02 - F1: 0.6531
Time taken for Epoch 10:4.02 - F1: 0.6561
2026-02-14 08:48:39 - INFO - Time taken for Epoch 10:4.02 - F1: 0.6561
Time taken for Epoch 11:4.02 - F1: 0.6585
2026-02-14 08:48:43 - INFO - Time taken for Epoch 11:4.02 - F1: 0.6585
Performance not improving for 10 consecutive epochs.
2026-02-14 08:48:43 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6821 - Best Epoch:0
2026-02-14 08:48:43 - INFO - Best F1:0.6821 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6579, Test ECE: 0.0335
2026-02-14 08:48:52 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6579, Test ECE: 0.0335
All results: {'f1_macro': 0.6578951042298494, 'ece': np.float64(0.03351464979195562)}
2026-02-14 08:48:52 - INFO - All results: {'f1_macro': 0.6578951042298494, 'ece': np.float64(0.03351464979195562)}

Total time taken: 935.54 seconds
2026-02-14 08:48:52 - INFO - 
Total time taken: 935.54 seconds
2026-02-14 08:48:52 - INFO - Trial 2 finished with value: 0.6578951042298494 and parameters: {'learning_rate': 4.292203772246671e-05, 'weight_decay': 0.00041565336697911527, 'batch_size': 8, 'co_train_epochs': 18, 'epoch_patience': 10}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 08:48:52 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 08:48:52 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 08:48:52 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 08:48:52 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.0008093750704305598
Weight Decay: 3.0629361193018414e-05
Batch Size: 16
No. Epochs: 11
Epoch Patience: 10
 Accumulation Steps: 4
2026-02-14 08:48:52 - INFO - Learning Rate: 0.0008093750704305598
Weight Decay: 3.0629361193018414e-05
Batch Size: 16
No. Epochs: 11
Epoch Patience: 10
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 08:48:53 - INFO - Generating initial weights
Time taken for Epoch 1:21.02 - F1: 0.0104
2026-02-14 08:49:18 - INFO - Time taken for Epoch 1:21.02 - F1: 0.0104
Time taken for Epoch 2:20.96 - F1: 0.0081
2026-02-14 08:49:39 - INFO - Time taken for Epoch 2:20.96 - F1: 0.0081
Time taken for Epoch 3:20.98 - F1: 0.0064
2026-02-14 08:50:00 - INFO - Time taken for Epoch 3:20.98 - F1: 0.0064
Time taken for Epoch 4:20.99 - F1: 0.0089
2026-02-14 08:50:21 - INFO - Time taken for Epoch 4:20.99 - F1: 0.0089
Time taken for Epoch 5:20.98 - F1: 0.0089
2026-02-14 08:50:42 - INFO - Time taken for Epoch 5:20.98 - F1: 0.0089
Time taken for Epoch 6:21.01 - F1: 0.0081
2026-02-14 08:51:03 - INFO - Time taken for Epoch 6:21.01 - F1: 0.0081
Time taken for Epoch 7:21.02 - F1: 0.0038
2026-02-14 08:51:24 - INFO - Time taken for Epoch 7:21.02 - F1: 0.0038
Time taken for Epoch 8:20.97 - F1: 0.0189
2026-02-14 08:51:45 - INFO - Time taken for Epoch 8:20.97 - F1: 0.0189
Time taken for Epoch 9:21.01 - F1: 0.0189
2026-02-14 08:52:06 - INFO - Time taken for Epoch 9:21.01 - F1: 0.0189
Time taken for Epoch 10:21.00 - F1: 0.0197
2026-02-14 08:52:27 - INFO - Time taken for Epoch 10:21.00 - F1: 0.0197
Time taken for Epoch 11:20.99 - F1: 0.0197
2026-02-14 08:52:48 - INFO - Time taken for Epoch 11:20.99 - F1: 0.0197
Best F1:0.0197 - Best Epoch:10
2026-02-14 08:52:48 - INFO - Best F1:0.0197 - Best Epoch:10
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 08:52:49 - INFO - Starting co-training
Time taken for Epoch 1: 28.59s - F1: 0.04755179
2026-02-14 08:53:18 - INFO - Time taken for Epoch 1: 28.59s - F1: 0.04755179
Time taken for Epoch 2: 29.66s - F1: 0.04755179
2026-02-14 08:53:47 - INFO - Time taken for Epoch 2: 29.66s - F1: 0.04755179
Time taken for Epoch 3: 28.62s - F1: 0.04755179
2026-02-14 08:54:16 - INFO - Time taken for Epoch 3: 28.62s - F1: 0.04755179
Time taken for Epoch 4: 28.63s - F1: 0.04755179
2026-02-14 08:54:45 - INFO - Time taken for Epoch 4: 28.63s - F1: 0.04755179
Time taken for Epoch 5: 28.63s - F1: 0.04755179
2026-02-14 08:55:13 - INFO - Time taken for Epoch 5: 28.63s - F1: 0.04755179
Time taken for Epoch 6: 28.63s - F1: 0.04755179
2026-02-14 08:55:42 - INFO - Time taken for Epoch 6: 28.63s - F1: 0.04755179
Time taken for Epoch 7: 28.63s - F1: 0.04755179
2026-02-14 08:56:11 - INFO - Time taken for Epoch 7: 28.63s - F1: 0.04755179
Time taken for Epoch 8: 28.63s - F1: 0.04755179
2026-02-14 08:56:39 - INFO - Time taken for Epoch 8: 28.63s - F1: 0.04755179
Time taken for Epoch 9: 28.63s - F1: 0.04755179
2026-02-14 08:57:08 - INFO - Time taken for Epoch 9: 28.63s - F1: 0.04755179
Time taken for Epoch 10: 28.62s - F1: 0.04755179
2026-02-14 08:57:36 - INFO - Time taken for Epoch 10: 28.62s - F1: 0.04755179
Time taken for Epoch 11: 28.64s - F1: 0.04755179
2026-02-14 08:58:05 - INFO - Time taken for Epoch 11: 28.64s - F1: 0.04755179
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 08:58:08 - INFO - Fine-tuning models
Time taken for Epoch 1:3.73 - F1: 0.0064
2026-02-14 08:58:12 - INFO - Time taken for Epoch 1:3.73 - F1: 0.0064
Time taken for Epoch 2:4.80 - F1: 0.0089
2026-02-14 08:58:16 - INFO - Time taken for Epoch 2:4.80 - F1: 0.0089
Time taken for Epoch 3:5.40 - F1: 0.0089
2026-02-14 08:58:22 - INFO - Time taken for Epoch 3:5.40 - F1: 0.0089
Time taken for Epoch 4:3.73 - F1: 0.0081
2026-02-14 08:58:26 - INFO - Time taken for Epoch 4:3.73 - F1: 0.0081
Time taken for Epoch 5:3.72 - F1: 0.0363
2026-02-14 08:58:29 - INFO - Time taken for Epoch 5:3.72 - F1: 0.0363
Time taken for Epoch 6:4.90 - F1: 0.0363
2026-02-14 08:58:34 - INFO - Time taken for Epoch 6:4.90 - F1: 0.0363
Time taken for Epoch 7:3.71 - F1: 0.0363
2026-02-14 08:58:38 - INFO - Time taken for Epoch 7:3.71 - F1: 0.0363
Time taken for Epoch 8:3.72 - F1: 0.0038
2026-02-14 08:58:42 - INFO - Time taken for Epoch 8:3.72 - F1: 0.0038
Time taken for Epoch 9:3.72 - F1: 0.0089
2026-02-14 08:58:45 - INFO - Time taken for Epoch 9:3.72 - F1: 0.0089
Time taken for Epoch 10:3.72 - F1: 0.0089
2026-02-14 08:58:49 - INFO - Time taken for Epoch 10:3.72 - F1: 0.0089
Time taken for Epoch 11:3.73 - F1: 0.0089
2026-02-14 08:58:53 - INFO - Time taken for Epoch 11:3.73 - F1: 0.0089
Time taken for Epoch 12:3.87 - F1: 0.0064
2026-02-14 08:58:57 - INFO - Time taken for Epoch 12:3.87 - F1: 0.0064
Time taken for Epoch 13:3.73 - F1: 0.0064
2026-02-14 08:59:00 - INFO - Time taken for Epoch 13:3.73 - F1: 0.0064
Time taken for Epoch 14:3.73 - F1: 0.0363
2026-02-14 08:59:04 - INFO - Time taken for Epoch 14:3.73 - F1: 0.0363
Time taken for Epoch 15:3.72 - F1: 0.0089
2026-02-14 08:59:08 - INFO - Time taken for Epoch 15:3.72 - F1: 0.0089
Performance not improving for 10 consecutive epochs.
2026-02-14 08:59:08 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0363 - Best Epoch:4
2026-02-14 08:59:08 - INFO - Best F1:0.0363 - Best Epoch:4
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0365, Test ECE: 0.0729
2026-02-14 08:59:16 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0365, Test ECE: 0.0729
All results: {'f1_macro': 0.0364573268921095, 'ece': np.float64(0.07287862405036258)}
2026-02-14 08:59:16 - INFO - All results: {'f1_macro': 0.0364573268921095, 'ece': np.float64(0.07287862405036258)}

Total time taken: 623.78 seconds
2026-02-14 08:59:16 - INFO - 
Total time taken: 623.78 seconds
2026-02-14 08:59:16 - INFO - Trial 3 finished with value: 0.0364573268921095 and parameters: {'learning_rate': 0.0008093750704305598, 'weight_decay': 3.0629361193018414e-05, 'batch_size': 16, 'co_train_epochs': 11, 'epoch_patience': 10}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 08:59:16 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 08:59:16 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 08:59:16 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 08:59:16 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.0006783281130927161
Weight Decay: 0.000597295322870688
Batch Size: 8
No. Epochs: 5
Epoch Patience: 6
 Accumulation Steps: 8
2026-02-14 08:59:16 - INFO - Learning Rate: 0.0006783281130927161
Weight Decay: 0.000597295322870688
Batch Size: 8
No. Epochs: 5
Epoch Patience: 6
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 08:59:17 - INFO - Generating initial weights
Time taken for Epoch 1:22.63 - F1: 0.0038
2026-02-14 08:59:43 - INFO - Time taken for Epoch 1:22.63 - F1: 0.0038
Time taken for Epoch 2:22.56 - F1: 0.0064
2026-02-14 09:00:06 - INFO - Time taken for Epoch 2:22.56 - F1: 0.0064
Time taken for Epoch 3:22.57 - F1: 0.0394
2026-02-14 09:00:28 - INFO - Time taken for Epoch 3:22.57 - F1: 0.0394
Time taken for Epoch 4:22.55 - F1: 0.0197
2026-02-14 09:00:51 - INFO - Time taken for Epoch 4:22.55 - F1: 0.0197
Time taken for Epoch 5:22.56 - F1: 0.0197
2026-02-14 09:01:13 - INFO - Time taken for Epoch 5:22.56 - F1: 0.0197
Best F1:0.0394 - Best Epoch:3
2026-02-14 09:01:13 - INFO - Best F1:0.0394 - Best Epoch:3
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 09:01:15 - INFO - Starting co-training
Time taken for Epoch 1: 26.78s - F1: 0.04755179
2026-02-14 09:01:42 - INFO - Time taken for Epoch 1: 26.78s - F1: 0.04755179
Time taken for Epoch 2: 27.89s - F1: 0.04755179
2026-02-14 09:02:10 - INFO - Time taken for Epoch 2: 27.89s - F1: 0.04755179
Time taken for Epoch 3: 26.77s - F1: 0.04755179
2026-02-14 09:02:36 - INFO - Time taken for Epoch 3: 26.77s - F1: 0.04755179
Time taken for Epoch 4: 26.80s - F1: 0.04755179
2026-02-14 09:03:03 - INFO - Time taken for Epoch 4: 26.80s - F1: 0.04755179
Time taken for Epoch 5: 26.78s - F1: 0.04755179
2026-02-14 09:03:30 - INFO - Time taken for Epoch 5: 26.78s - F1: 0.04755179
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 09:03:32 - INFO - Fine-tuning models
Time taken for Epoch 1:4.02 - F1: 0.0476
2026-02-14 09:03:37 - INFO - Time taken for Epoch 1:4.02 - F1: 0.0476
Time taken for Epoch 2:5.11 - F1: 0.0394
2026-02-14 09:03:42 - INFO - Time taken for Epoch 2:5.11 - F1: 0.0394
Time taken for Epoch 3:4.01 - F1: 0.0394
2026-02-14 09:03:46 - INFO - Time taken for Epoch 3:4.01 - F1: 0.0394
Time taken for Epoch 4:4.01 - F1: 0.0197
2026-02-14 09:03:50 - INFO - Time taken for Epoch 4:4.01 - F1: 0.0197
Time taken for Epoch 5:4.00 - F1: 0.0197
2026-02-14 09:03:54 - INFO - Time taken for Epoch 5:4.00 - F1: 0.0197
Time taken for Epoch 6:4.00 - F1: 0.0197
2026-02-14 09:03:58 - INFO - Time taken for Epoch 6:4.00 - F1: 0.0197
Time taken for Epoch 7:4.01 - F1: 0.0476
2026-02-14 09:04:02 - INFO - Time taken for Epoch 7:4.01 - F1: 0.0476
Time taken for Epoch 8:4.00 - F1: 0.0476
2026-02-14 09:04:06 - INFO - Time taken for Epoch 8:4.00 - F1: 0.0476
Time taken for Epoch 9:4.00 - F1: 0.0476
2026-02-14 09:04:10 - INFO - Time taken for Epoch 9:4.00 - F1: 0.0476
Time taken for Epoch 10:4.00 - F1: 0.0476
2026-02-14 09:04:14 - INFO - Time taken for Epoch 10:4.00 - F1: 0.0476
Time taken for Epoch 11:4.00 - F1: 0.0394
2026-02-14 09:04:18 - INFO - Time taken for Epoch 11:4.00 - F1: 0.0394
Performance not improving for 10 consecutive epochs.
2026-02-14 09:04:18 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:0
2026-02-14 09:04:18 - INFO - Best F1:0.0476 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0825
2026-02-14 09:04:26 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0474, Test ECE: 0.0825
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.08253332862989582)}
2026-02-14 09:04:26 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.08253332862989582)}

Total time taken: 310.29 seconds
2026-02-14 09:04:26 - INFO - 
Total time taken: 310.29 seconds
2026-02-14 09:04:26 - INFO - Trial 4 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.0006783281130927161, 'weight_decay': 0.000597295322870688, 'batch_size': 8, 'co_train_epochs': 5, 'epoch_patience': 6}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 09:04:26 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 09:04:26 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 09:04:26 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 09:04:26 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 6.308543070985229e-05
Weight Decay: 5.656742148355228e-05
Batch Size: 16
No. Epochs: 5
Epoch Patience: 7
 Accumulation Steps: 4
2026-02-14 09:04:26 - INFO - Learning Rate: 6.308543070985229e-05
Weight Decay: 5.656742148355228e-05
Batch Size: 16
No. Epochs: 5
Epoch Patience: 7
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 09:04:27 - INFO - Generating initial weights
Time taken for Epoch 1:20.99 - F1: 0.0453
2026-02-14 09:04:52 - INFO - Time taken for Epoch 1:20.99 - F1: 0.0453
Time taken for Epoch 2:20.98 - F1: 0.0873
2026-02-14 09:05:13 - INFO - Time taken for Epoch 2:20.98 - F1: 0.0873
Time taken for Epoch 3:20.99 - F1: 0.1511
2026-02-14 09:05:34 - INFO - Time taken for Epoch 3:20.99 - F1: 0.1511
Time taken for Epoch 4:20.98 - F1: 0.1664
2026-02-14 09:05:55 - INFO - Time taken for Epoch 4:20.98 - F1: 0.1664
Time taken for Epoch 5:21.02 - F1: 0.2883
2026-02-14 09:06:16 - INFO - Time taken for Epoch 5:21.02 - F1: 0.2883
Best F1:0.2883 - Best Epoch:5
2026-02-14 09:06:16 - INFO - Best F1:0.2883 - Best Epoch:5
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 09:06:17 - INFO - Starting co-training
Time taken for Epoch 1: 28.58s - F1: 0.51995412
2026-02-14 09:06:46 - INFO - Time taken for Epoch 1: 28.58s - F1: 0.51995412
Time taken for Epoch 2: 29.67s - F1: 0.50843385
2026-02-14 09:07:15 - INFO - Time taken for Epoch 2: 29.67s - F1: 0.50843385
Time taken for Epoch 3: 28.62s - F1: 0.59093318
2026-02-14 09:07:44 - INFO - Time taken for Epoch 3: 28.62s - F1: 0.59093318
Time taken for Epoch 4: 29.76s - F1: 0.64472224
2026-02-14 09:08:14 - INFO - Time taken for Epoch 4: 29.76s - F1: 0.64472224
Time taken for Epoch 5: 29.75s - F1: 0.62315832
2026-02-14 09:08:44 - INFO - Time taken for Epoch 5: 29.75s - F1: 0.62315832
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 09:08:46 - INFO - Fine-tuning models
Time taken for Epoch 1:3.74 - F1: 0.6138
2026-02-14 09:08:50 - INFO - Time taken for Epoch 1:3.74 - F1: 0.6138
Time taken for Epoch 2:4.80 - F1: 0.6085
2026-02-14 09:08:55 - INFO - Time taken for Epoch 2:4.80 - F1: 0.6085
Time taken for Epoch 3:3.73 - F1: 0.6173
2026-02-14 09:08:59 - INFO - Time taken for Epoch 3:3.73 - F1: 0.6173
Time taken for Epoch 4:4.91 - F1: 0.6369
2026-02-14 09:09:04 - INFO - Time taken for Epoch 4:4.91 - F1: 0.6369
Time taken for Epoch 5:4.89 - F1: 0.6526
2026-02-14 09:09:08 - INFO - Time taken for Epoch 5:4.89 - F1: 0.6526
Time taken for Epoch 6:4.89 - F1: 0.6578
2026-02-14 09:09:13 - INFO - Time taken for Epoch 6:4.89 - F1: 0.6578
Time taken for Epoch 7:4.90 - F1: 0.6512
2026-02-14 09:09:18 - INFO - Time taken for Epoch 7:4.90 - F1: 0.6512
Time taken for Epoch 8:3.72 - F1: 0.6560
2026-02-14 09:09:22 - INFO - Time taken for Epoch 8:3.72 - F1: 0.6560
Time taken for Epoch 9:3.72 - F1: 0.6576
2026-02-14 09:09:26 - INFO - Time taken for Epoch 9:3.72 - F1: 0.6576
Time taken for Epoch 10:3.72 - F1: 0.6638
2026-02-14 09:09:29 - INFO - Time taken for Epoch 10:3.72 - F1: 0.6638
Time taken for Epoch 11:4.88 - F1: 0.6665
2026-02-14 09:09:34 - INFO - Time taken for Epoch 11:4.88 - F1: 0.6665
Time taken for Epoch 12:4.88 - F1: 0.6578
2026-02-14 09:09:39 - INFO - Time taken for Epoch 12:4.88 - F1: 0.6578
Time taken for Epoch 13:3.72 - F1: 0.6504
2026-02-14 09:09:43 - INFO - Time taken for Epoch 13:3.72 - F1: 0.6504
Time taken for Epoch 14:3.73 - F1: 0.6476
2026-02-14 09:09:47 - INFO - Time taken for Epoch 14:3.73 - F1: 0.6476
Time taken for Epoch 15:3.72 - F1: 0.6485
2026-02-14 09:09:50 - INFO - Time taken for Epoch 15:3.72 - F1: 0.6485
Time taken for Epoch 16:3.73 - F1: 0.6495
2026-02-14 09:09:54 - INFO - Time taken for Epoch 16:3.73 - F1: 0.6495
Time taken for Epoch 17:3.72 - F1: 0.6567
2026-02-14 09:09:58 - INFO - Time taken for Epoch 17:3.72 - F1: 0.6567
Time taken for Epoch 18:3.73 - F1: 0.6609
2026-02-14 09:10:01 - INFO - Time taken for Epoch 18:3.73 - F1: 0.6609
Time taken for Epoch 19:3.73 - F1: 0.6659
2026-02-14 09:10:05 - INFO - Time taken for Epoch 19:3.73 - F1: 0.6659
Time taken for Epoch 20:3.73 - F1: 0.6674
2026-02-14 09:10:09 - INFO - Time taken for Epoch 20:3.73 - F1: 0.6674
Time taken for Epoch 21:4.89 - F1: 0.6740
2026-02-14 09:10:14 - INFO - Time taken for Epoch 21:4.89 - F1: 0.6740
Time taken for Epoch 22:4.89 - F1: 0.6760
2026-02-14 09:10:19 - INFO - Time taken for Epoch 22:4.89 - F1: 0.6760
Time taken for Epoch 23:4.90 - F1: 0.6760
2026-02-14 09:10:24 - INFO - Time taken for Epoch 23:4.90 - F1: 0.6760
Time taken for Epoch 24:3.72 - F1: 0.6764
2026-02-14 09:10:27 - INFO - Time taken for Epoch 24:3.72 - F1: 0.6764
Time taken for Epoch 25:5.14 - F1: 0.6765
2026-02-14 09:10:32 - INFO - Time taken for Epoch 25:5.14 - F1: 0.6765
Time taken for Epoch 26:5.19 - F1: 0.6766
2026-02-14 09:10:38 - INFO - Time taken for Epoch 26:5.19 - F1: 0.6766
Time taken for Epoch 27:4.88 - F1: 0.6766
2026-02-14 09:10:43 - INFO - Time taken for Epoch 27:4.88 - F1: 0.6766
Time taken for Epoch 28:3.71 - F1: 0.6766
2026-02-14 09:10:46 - INFO - Time taken for Epoch 28:3.71 - F1: 0.6766
Time taken for Epoch 29:3.72 - F1: 0.6736
2026-02-14 09:10:50 - INFO - Time taken for Epoch 29:3.72 - F1: 0.6736
Time taken for Epoch 30:3.72 - F1: 0.6747
2026-02-14 09:10:54 - INFO - Time taken for Epoch 30:3.72 - F1: 0.6747
Time taken for Epoch 31:3.72 - F1: 0.6752
2026-02-14 09:10:57 - INFO - Time taken for Epoch 31:3.72 - F1: 0.6752
Time taken for Epoch 32:3.73 - F1: 0.6737
2026-02-14 09:11:01 - INFO - Time taken for Epoch 32:3.73 - F1: 0.6737
Time taken for Epoch 33:3.72 - F1: 0.6752
2026-02-14 09:11:05 - INFO - Time taken for Epoch 33:3.72 - F1: 0.6752
Time taken for Epoch 34:3.72 - F1: 0.6752
2026-02-14 09:11:09 - INFO - Time taken for Epoch 34:3.72 - F1: 0.6752
Time taken for Epoch 35:3.73 - F1: 0.6752
2026-02-14 09:11:12 - INFO - Time taken for Epoch 35:3.73 - F1: 0.6752
Time taken for Epoch 36:3.72 - F1: 0.6757
2026-02-14 09:11:16 - INFO - Time taken for Epoch 36:3.72 - F1: 0.6757
Performance not improving for 10 consecutive epochs.
2026-02-14 09:11:16 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6766 - Best Epoch:25
2026-02-14 09:11:16 - INFO - Best F1:0.6766 - Best Epoch:25
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6699, Test ECE: 0.0558
2026-02-14 09:11:24 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6699, Test ECE: 0.0558
All results: {'f1_macro': 0.6698768849889006, 'ece': np.float64(0.05576461107158793)}
2026-02-14 09:11:24 - INFO - All results: {'f1_macro': 0.6698768849889006, 'ece': np.float64(0.05576461107158793)}

Total time taken: 417.81 seconds
2026-02-14 09:11:24 - INFO - 
Total time taken: 417.81 seconds
2026-02-14 09:11:24 - INFO - Trial 5 finished with value: 0.6698768849889006 and parameters: {'learning_rate': 6.308543070985229e-05, 'weight_decay': 5.656742148355228e-05, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 7}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 09:11:24 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 09:11:24 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 09:11:24 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 09:11:24 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 1.1969351953391404e-05
Weight Decay: 9.494239266446094e-05
Batch Size: 16
No. Epochs: 16
Epoch Patience: 7
 Accumulation Steps: 4
2026-02-14 09:11:24 - INFO - Learning Rate: 1.1969351953391404e-05
Weight Decay: 9.494239266446094e-05
Batch Size: 16
No. Epochs: 16
Epoch Patience: 7
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 09:11:25 - INFO - Generating initial weights
Time taken for Epoch 1:20.97 - F1: 0.0525
2026-02-14 09:11:50 - INFO - Time taken for Epoch 1:20.97 - F1: 0.0525
Time taken for Epoch 2:20.95 - F1: 0.0415
2026-02-14 09:12:11 - INFO - Time taken for Epoch 2:20.95 - F1: 0.0415
Time taken for Epoch 3:20.97 - F1: 0.0511
2026-02-14 09:12:32 - INFO - Time taken for Epoch 3:20.97 - F1: 0.0511
Time taken for Epoch 4:20.98 - F1: 0.0769
2026-02-14 09:12:53 - INFO - Time taken for Epoch 4:20.98 - F1: 0.0769
Time taken for Epoch 5:21.01 - F1: 0.0868
2026-02-14 09:13:14 - INFO - Time taken for Epoch 5:21.01 - F1: 0.0868
Time taken for Epoch 6:21.00 - F1: 0.1111
2026-02-14 09:13:35 - INFO - Time taken for Epoch 6:21.00 - F1: 0.1111
Time taken for Epoch 7:21.01 - F1: 0.1373
2026-02-14 09:13:56 - INFO - Time taken for Epoch 7:21.01 - F1: 0.1373
Time taken for Epoch 8:21.03 - F1: 0.1613
2026-02-14 09:14:17 - INFO - Time taken for Epoch 8:21.03 - F1: 0.1613
Time taken for Epoch 9:21.03 - F1: 0.1884
2026-02-14 09:14:38 - INFO - Time taken for Epoch 9:21.03 - F1: 0.1884
Time taken for Epoch 10:21.02 - F1: 0.2121
2026-02-14 09:14:59 - INFO - Time taken for Epoch 10:21.02 - F1: 0.2121
Time taken for Epoch 11:21.04 - F1: 0.2460
2026-02-14 09:15:20 - INFO - Time taken for Epoch 11:21.04 - F1: 0.2460
Time taken for Epoch 12:21.01 - F1: 0.3040
2026-02-14 09:15:41 - INFO - Time taken for Epoch 12:21.01 - F1: 0.3040
Time taken for Epoch 13:21.00 - F1: 0.3344
2026-02-14 09:16:02 - INFO - Time taken for Epoch 13:21.00 - F1: 0.3344
Time taken for Epoch 14:21.05 - F1: 0.3672
2026-02-14 09:16:23 - INFO - Time taken for Epoch 14:21.05 - F1: 0.3672
Time taken for Epoch 15:21.02 - F1: 0.3918
2026-02-14 09:16:44 - INFO - Time taken for Epoch 15:21.02 - F1: 0.3918
Time taken for Epoch 16:21.03 - F1: 0.3978
2026-02-14 09:17:05 - INFO - Time taken for Epoch 16:21.03 - F1: 0.3978
Best F1:0.3978 - Best Epoch:16
2026-02-14 09:17:05 - INFO - Best F1:0.3978 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 09:17:06 - INFO - Starting co-training
Time taken for Epoch 1: 28.61s - F1: 0.14544327
2026-02-14 09:17:35 - INFO - Time taken for Epoch 1: 28.61s - F1: 0.14544327
Time taken for Epoch 2: 29.69s - F1: 0.30259913
2026-02-14 09:18:05 - INFO - Time taken for Epoch 2: 29.69s - F1: 0.30259913
Time taken for Epoch 3: 29.79s - F1: 0.43625833
2026-02-14 09:18:34 - INFO - Time taken for Epoch 3: 29.79s - F1: 0.43625833
Time taken for Epoch 4: 29.79s - F1: 0.48744503
2026-02-14 09:19:04 - INFO - Time taken for Epoch 4: 29.79s - F1: 0.48744503
Time taken for Epoch 5: 29.76s - F1: 0.49897649
2026-02-14 09:19:34 - INFO - Time taken for Epoch 5: 29.76s - F1: 0.49897649
Time taken for Epoch 6: 29.77s - F1: 0.50790079
2026-02-14 09:20:04 - INFO - Time taken for Epoch 6: 29.77s - F1: 0.50790079
Time taken for Epoch 7: 29.78s - F1: 0.53569742
2026-02-14 09:20:34 - INFO - Time taken for Epoch 7: 29.78s - F1: 0.53569742
Time taken for Epoch 8: 29.77s - F1: 0.53030982
2026-02-14 09:21:03 - INFO - Time taken for Epoch 8: 29.77s - F1: 0.53030982
Time taken for Epoch 9: 28.63s - F1: 0.56145892
2026-02-14 09:21:32 - INFO - Time taken for Epoch 9: 28.63s - F1: 0.56145892
Time taken for Epoch 10: 29.79s - F1: 0.58999282
2026-02-14 09:22:02 - INFO - Time taken for Epoch 10: 29.79s - F1: 0.58999282
Time taken for Epoch 11: 29.81s - F1: 0.61965300
2026-02-14 09:22:32 - INFO - Time taken for Epoch 11: 29.81s - F1: 0.61965300
Time taken for Epoch 12: 29.79s - F1: 0.64028466
2026-02-14 09:23:01 - INFO - Time taken for Epoch 12: 29.79s - F1: 0.64028466
Time taken for Epoch 13: 29.90s - F1: 0.63098331
2026-02-14 09:23:31 - INFO - Time taken for Epoch 13: 29.90s - F1: 0.63098331
Time taken for Epoch 14: 28.63s - F1: 0.64775219
2026-02-14 09:24:00 - INFO - Time taken for Epoch 14: 28.63s - F1: 0.64775219
Time taken for Epoch 15: 29.81s - F1: 0.64235764
2026-02-14 09:24:30 - INFO - Time taken for Epoch 15: 29.81s - F1: 0.64235764
Time taken for Epoch 16: 28.62s - F1: 0.63998151
2026-02-14 09:24:58 - INFO - Time taken for Epoch 16: 28.62s - F1: 0.63998151
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 09:25:01 - INFO - Fine-tuning models
Time taken for Epoch 1:3.74 - F1: 0.6573
2026-02-14 09:25:05 - INFO - Time taken for Epoch 1:3.74 - F1: 0.6573
Time taken for Epoch 2:4.79 - F1: 0.6497
2026-02-14 09:25:10 - INFO - Time taken for Epoch 2:4.79 - F1: 0.6497
Time taken for Epoch 3:3.73 - F1: 0.6503
2026-02-14 09:25:13 - INFO - Time taken for Epoch 3:3.73 - F1: 0.6503
Time taken for Epoch 4:3.73 - F1: 0.6411
2026-02-14 09:25:17 - INFO - Time taken for Epoch 4:3.73 - F1: 0.6411
Time taken for Epoch 5:3.73 - F1: 0.6496
2026-02-14 09:25:21 - INFO - Time taken for Epoch 5:3.73 - F1: 0.6496
Time taken for Epoch 6:3.73 - F1: 0.6483
2026-02-14 09:25:24 - INFO - Time taken for Epoch 6:3.73 - F1: 0.6483
Time taken for Epoch 7:3.73 - F1: 0.6523
2026-02-14 09:25:28 - INFO - Time taken for Epoch 7:3.73 - F1: 0.6523
Time taken for Epoch 8:3.73 - F1: 0.6554
2026-02-14 09:25:32 - INFO - Time taken for Epoch 8:3.73 - F1: 0.6554
Time taken for Epoch 9:3.73 - F1: 0.6557
2026-02-14 09:25:36 - INFO - Time taken for Epoch 9:3.73 - F1: 0.6557
Time taken for Epoch 10:3.74 - F1: 0.6622
2026-02-14 09:25:39 - INFO - Time taken for Epoch 10:3.74 - F1: 0.6622
Time taken for Epoch 11:4.85 - F1: 0.6496
2026-02-14 09:25:44 - INFO - Time taken for Epoch 11:4.85 - F1: 0.6496
Time taken for Epoch 12:3.73 - F1: 0.6483
2026-02-14 09:25:48 - INFO - Time taken for Epoch 12:3.73 - F1: 0.6483
Time taken for Epoch 13:3.73 - F1: 0.6555
2026-02-14 09:25:52 - INFO - Time taken for Epoch 13:3.73 - F1: 0.6555
Time taken for Epoch 14:3.73 - F1: 0.6587
2026-02-14 09:25:55 - INFO - Time taken for Epoch 14:3.73 - F1: 0.6587
Time taken for Epoch 15:3.73 - F1: 0.6635
2026-02-14 09:25:59 - INFO - Time taken for Epoch 15:3.73 - F1: 0.6635
Time taken for Epoch 16:4.92 - F1: 0.6649
2026-02-14 09:26:04 - INFO - Time taken for Epoch 16:4.92 - F1: 0.6649
Time taken for Epoch 17:4.89 - F1: 0.6665
2026-02-14 09:26:09 - INFO - Time taken for Epoch 17:4.89 - F1: 0.6665
Time taken for Epoch 18:4.88 - F1: 0.6692
2026-02-14 09:26:14 - INFO - Time taken for Epoch 18:4.88 - F1: 0.6692
Time taken for Epoch 19:4.88 - F1: 0.6741
2026-02-14 09:26:19 - INFO - Time taken for Epoch 19:4.88 - F1: 0.6741
Time taken for Epoch 20:4.89 - F1: 0.6632
2026-02-14 09:26:24 - INFO - Time taken for Epoch 20:4.89 - F1: 0.6632
Time taken for Epoch 21:3.72 - F1: 0.6620
2026-02-14 09:26:27 - INFO - Time taken for Epoch 21:3.72 - F1: 0.6620
Time taken for Epoch 22:3.72 - F1: 0.6584
2026-02-14 09:26:31 - INFO - Time taken for Epoch 22:3.72 - F1: 0.6584
Time taken for Epoch 23:3.73 - F1: 0.6603
2026-02-14 09:26:35 - INFO - Time taken for Epoch 23:3.73 - F1: 0.6603
Time taken for Epoch 24:3.73 - F1: 0.6659
2026-02-14 09:26:39 - INFO - Time taken for Epoch 24:3.73 - F1: 0.6659
Time taken for Epoch 25:3.73 - F1: 0.6650
2026-02-14 09:26:42 - INFO - Time taken for Epoch 25:3.73 - F1: 0.6650
Time taken for Epoch 26:3.73 - F1: 0.6599
2026-02-14 09:26:46 - INFO - Time taken for Epoch 26:3.73 - F1: 0.6599
Time taken for Epoch 27:3.73 - F1: 0.6574
2026-02-14 09:26:50 - INFO - Time taken for Epoch 27:3.73 - F1: 0.6574
Time taken for Epoch 28:3.73 - F1: 0.6593
2026-02-14 09:26:53 - INFO - Time taken for Epoch 28:3.73 - F1: 0.6593
Time taken for Epoch 29:3.74 - F1: 0.6603
2026-02-14 09:26:57 - INFO - Time taken for Epoch 29:3.74 - F1: 0.6603
Performance not improving for 10 consecutive epochs.
2026-02-14 09:26:57 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6741 - Best Epoch:18
2026-02-14 09:26:57 - INFO - Best F1:0.6741 - Best Epoch:18
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6764, Test ECE: 0.0437
2026-02-14 09:27:05 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6764, Test ECE: 0.0437
All results: {'f1_macro': 0.6764411201680277, 'ece': np.float64(0.04371203434616121)}
2026-02-14 09:27:05 - INFO - All results: {'f1_macro': 0.6764411201680277, 'ece': np.float64(0.04371203434616121)}

Total time taken: 941.17 seconds
2026-02-14 09:27:05 - INFO - 
Total time taken: 941.17 seconds
2026-02-14 09:27:05 - INFO - Trial 6 finished with value: 0.6764411201680277 and parameters: {'learning_rate': 1.1969351953391404e-05, 'weight_decay': 9.494239266446094e-05, 'batch_size': 16, 'co_train_epochs': 16, 'epoch_patience': 7}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 09:27:05 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 09:27:05 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 09:27:05 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 09:27:05 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.00028837521360806136
Weight Decay: 0.0011466091358176475
Batch Size: 8
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 8
2026-02-14 09:27:06 - INFO - Learning Rate: 0.00028837521360806136
Weight Decay: 0.0011466091358176475
Batch Size: 8
No. Epochs: 19
Epoch Patience: 10
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 09:27:07 - INFO - Generating initial weights
Time taken for Epoch 1:22.61 - F1: 0.0617
2026-02-14 09:27:33 - INFO - Time taken for Epoch 1:22.61 - F1: 0.0617
Time taken for Epoch 2:22.52 - F1: 0.0191
2026-02-14 09:27:55 - INFO - Time taken for Epoch 2:22.52 - F1: 0.0191
Time taken for Epoch 3:22.58 - F1: 0.0205
2026-02-14 09:28:18 - INFO - Time taken for Epoch 3:22.58 - F1: 0.0205
Time taken for Epoch 4:22.57 - F1: 0.0179
2026-02-14 09:28:40 - INFO - Time taken for Epoch 4:22.57 - F1: 0.0179
Time taken for Epoch 5:22.57 - F1: 0.0081
2026-02-14 09:29:03 - INFO - Time taken for Epoch 5:22.57 - F1: 0.0081
Time taken for Epoch 6:22.58 - F1: 0.0054
2026-02-14 09:29:25 - INFO - Time taken for Epoch 6:22.58 - F1: 0.0054
Time taken for Epoch 7:22.58 - F1: 0.0363
2026-02-14 09:29:48 - INFO - Time taken for Epoch 7:22.58 - F1: 0.0363
Time taken for Epoch 8:22.62 - F1: 0.0363
2026-02-14 09:30:11 - INFO - Time taken for Epoch 8:22.62 - F1: 0.0363
Time taken for Epoch 9:22.57 - F1: 0.0064
2026-02-14 09:30:33 - INFO - Time taken for Epoch 9:22.57 - F1: 0.0064
Time taken for Epoch 10:22.60 - F1: 0.0064
2026-02-14 09:30:56 - INFO - Time taken for Epoch 10:22.60 - F1: 0.0064
Time taken for Epoch 11:22.61 - F1: 0.0394
2026-02-14 09:31:18 - INFO - Time taken for Epoch 11:22.61 - F1: 0.0394
Time taken for Epoch 12:22.58 - F1: 0.0394
2026-02-14 09:31:41 - INFO - Time taken for Epoch 12:22.58 - F1: 0.0394
Time taken for Epoch 13:22.61 - F1: 0.0089
2026-02-14 09:32:04 - INFO - Time taken for Epoch 13:22.61 - F1: 0.0089
Time taken for Epoch 14:22.59 - F1: 0.0089
2026-02-14 09:32:26 - INFO - Time taken for Epoch 14:22.59 - F1: 0.0089
Time taken for Epoch 15:22.57 - F1: 0.0394
2026-02-14 09:32:49 - INFO - Time taken for Epoch 15:22.57 - F1: 0.0394
Time taken for Epoch 16:22.61 - F1: 0.0394
2026-02-14 09:33:11 - INFO - Time taken for Epoch 16:22.61 - F1: 0.0394
Time taken for Epoch 17:22.58 - F1: 0.0394
2026-02-14 09:33:34 - INFO - Time taken for Epoch 17:22.58 - F1: 0.0394
Time taken for Epoch 18:22.60 - F1: 0.0394
2026-02-14 09:33:57 - INFO - Time taken for Epoch 18:22.60 - F1: 0.0394
Time taken for Epoch 19:22.59 - F1: 0.0394
2026-02-14 09:34:19 - INFO - Time taken for Epoch 19:22.59 - F1: 0.0394
Best F1:0.0617 - Best Epoch:1
2026-02-14 09:34:19 - INFO - Best F1:0.0617 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 09:34:20 - INFO - Starting co-training
Time taken for Epoch 1: 26.78s - F1: 0.04755179
2026-02-14 09:34:47 - INFO - Time taken for Epoch 1: 26.78s - F1: 0.04755179
Time taken for Epoch 2: 27.82s - F1: 0.04755179
2026-02-14 09:35:15 - INFO - Time taken for Epoch 2: 27.82s - F1: 0.04755179
Time taken for Epoch 3: 26.78s - F1: 0.04755179
2026-02-14 09:35:42 - INFO - Time taken for Epoch 3: 26.78s - F1: 0.04755179
Time taken for Epoch 4: 26.79s - F1: 0.04755179
2026-02-14 09:36:09 - INFO - Time taken for Epoch 4: 26.79s - F1: 0.04755179
Time taken for Epoch 5: 26.82s - F1: 0.04755179
2026-02-14 09:36:36 - INFO - Time taken for Epoch 5: 26.82s - F1: 0.04755179
Time taken for Epoch 6: 26.81s - F1: 0.04755179
2026-02-14 09:37:02 - INFO - Time taken for Epoch 6: 26.81s - F1: 0.04755179
Time taken for Epoch 7: 26.76s - F1: 0.04755179
2026-02-14 09:37:29 - INFO - Time taken for Epoch 7: 26.76s - F1: 0.04755179
Time taken for Epoch 8: 26.81s - F1: 0.04755179
2026-02-14 09:37:56 - INFO - Time taken for Epoch 8: 26.81s - F1: 0.04755179
Time taken for Epoch 9: 26.79s - F1: 0.04755179
2026-02-14 09:38:23 - INFO - Time taken for Epoch 9: 26.79s - F1: 0.04755179
Time taken for Epoch 10: 26.93s - F1: 0.04755179
2026-02-14 09:38:50 - INFO - Time taken for Epoch 10: 26.93s - F1: 0.04755179
Time taken for Epoch 11: 26.80s - F1: 0.04755179
2026-02-14 09:39:17 - INFO - Time taken for Epoch 11: 26.80s - F1: 0.04755179
Performance not improving for 10 consecutive epochs.
Performance not improving for 10 consecutive epochs.
2026-02-14 09:39:17 - INFO - Performance not improving for 10 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 09:39:19 - INFO - Fine-tuning models
Time taken for Epoch 1:4.22 - F1: 0.0476
2026-02-14 09:39:24 - INFO - Time taken for Epoch 1:4.22 - F1: 0.0476
Time taken for Epoch 2:5.09 - F1: 0.0189
2026-02-14 09:39:29 - INFO - Time taken for Epoch 2:5.09 - F1: 0.0189
Time taken for Epoch 3:4.19 - F1: 0.0394
2026-02-14 09:39:33 - INFO - Time taken for Epoch 3:4.19 - F1: 0.0394
Time taken for Epoch 4:4.04 - F1: 0.0394
2026-02-14 09:39:37 - INFO - Time taken for Epoch 4:4.04 - F1: 0.0394
Time taken for Epoch 5:4.04 - F1: 0.0038
2026-02-14 09:39:41 - INFO - Time taken for Epoch 5:4.04 - F1: 0.0038
Time taken for Epoch 6:4.04 - F1: 0.0197
2026-02-14 09:39:45 - INFO - Time taken for Epoch 6:4.04 - F1: 0.0197
Time taken for Epoch 7:4.04 - F1: 0.0197
2026-02-14 09:39:49 - INFO - Time taken for Epoch 7:4.04 - F1: 0.0197
Time taken for Epoch 8:4.05 - F1: 0.0197
2026-02-14 09:39:53 - INFO - Time taken for Epoch 8:4.05 - F1: 0.0197
Time taken for Epoch 9:4.04 - F1: 0.0476
2026-02-14 09:39:57 - INFO - Time taken for Epoch 9:4.04 - F1: 0.0476
Time taken for Epoch 10:4.05 - F1: 0.0476
2026-02-14 09:40:01 - INFO - Time taken for Epoch 10:4.05 - F1: 0.0476
Time taken for Epoch 11:4.04 - F1: 0.0476
2026-02-14 09:40:05 - INFO - Time taken for Epoch 11:4.04 - F1: 0.0476
Performance not improving for 10 consecutive epochs.
2026-02-14 09:40:05 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0476 - Best Epoch:0
2026-02-14 09:40:05 - INFO - Best F1:0.0476 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0474, Test ECE: 0.1152
2026-02-14 09:40:14 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0474, Test ECE: 0.1152
All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.11515117706356365)}
2026-02-14 09:40:14 - INFO - All results: {'f1_macro': 0.04740255804085591, 'ece': np.float64(0.11515117706356365)}

Total time taken: 788.65 seconds
2026-02-14 09:40:14 - INFO - 
Total time taken: 788.65 seconds
2026-02-14 09:40:14 - INFO - Trial 7 finished with value: 0.04740255804085591 and parameters: {'learning_rate': 0.00028837521360806136, 'weight_decay': 0.0011466091358176475, 'batch_size': 8, 'co_train_epochs': 19, 'epoch_patience': 10}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 09:40:14 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 09:40:14 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 09:40:14 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 09:40:14 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.0008226417076523035
Weight Decay: 1.2649325541200087e-05
Batch Size: 32
No. Epochs: 7
Epoch Patience: 5
 Accumulation Steps: 2
2026-02-14 09:40:14 - INFO - Learning Rate: 0.0008226417076523035
Weight Decay: 1.2649325541200087e-05
Batch Size: 32
No. Epochs: 7
Epoch Patience: 5
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 09:40:15 - INFO - Generating initial weights
Time taken for Epoch 1:20.32 - F1: 0.0259
2026-02-14 09:40:39 - INFO - Time taken for Epoch 1:20.32 - F1: 0.0259
Time taken for Epoch 2:20.29 - F1: 0.0168
2026-02-14 09:40:59 - INFO - Time taken for Epoch 2:20.29 - F1: 0.0168
Time taken for Epoch 3:20.32 - F1: 0.0038
2026-02-14 09:41:20 - INFO - Time taken for Epoch 3:20.32 - F1: 0.0038
Time taken for Epoch 4:20.33 - F1: 0.0189
2026-02-14 09:41:40 - INFO - Time taken for Epoch 4:20.33 - F1: 0.0189
Time taken for Epoch 5:20.35 - F1: 0.0189
2026-02-14 09:42:00 - INFO - Time taken for Epoch 5:20.35 - F1: 0.0189
Time taken for Epoch 6:20.36 - F1: 0.0189
2026-02-14 09:42:21 - INFO - Time taken for Epoch 6:20.36 - F1: 0.0189
Time taken for Epoch 7:20.37 - F1: 0.0189
2026-02-14 09:42:41 - INFO - Time taken for Epoch 7:20.37 - F1: 0.0189
Best F1:0.0259 - Best Epoch:1
2026-02-14 09:42:41 - INFO - Best F1:0.0259 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 09:42:42 - INFO - Starting co-training
Time taken for Epoch 1: 34.36s - F1: 0.03632720
2026-02-14 09:43:17 - INFO - Time taken for Epoch 1: 34.36s - F1: 0.03632720
Time taken for Epoch 2: 35.44s - F1: 0.04755179
2026-02-14 09:43:52 - INFO - Time taken for Epoch 2: 35.44s - F1: 0.04755179
Time taken for Epoch 3: 35.54s - F1: 0.03632720
2026-02-14 09:44:28 - INFO - Time taken for Epoch 3: 35.54s - F1: 0.03632720
Time taken for Epoch 4: 34.44s - F1: 0.04755179
2026-02-14 09:45:02 - INFO - Time taken for Epoch 4: 34.44s - F1: 0.04755179
Time taken for Epoch 5: 34.45s - F1: 0.04755179
2026-02-14 09:45:37 - INFO - Time taken for Epoch 5: 34.45s - F1: 0.04755179
Time taken for Epoch 6: 34.44s - F1: 0.04755179
2026-02-14 09:46:11 - INFO - Time taken for Epoch 6: 34.44s - F1: 0.04755179
Time taken for Epoch 7: 34.45s - F1: 0.04755179
2026-02-14 09:46:46 - INFO - Time taken for Epoch 7: 34.45s - F1: 0.04755179
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 09:46:48 - INFO - Fine-tuning models
Time taken for Epoch 1:3.64 - F1: 0.0197
2026-02-14 09:46:52 - INFO - Time taken for Epoch 1:3.64 - F1: 0.0197
Time taken for Epoch 2:4.68 - F1: 0.0189
2026-02-14 09:46:57 - INFO - Time taken for Epoch 2:4.68 - F1: 0.0189
Time taken for Epoch 3:3.63 - F1: 0.0189
2026-02-14 09:47:00 - INFO - Time taken for Epoch 3:3.63 - F1: 0.0189
Time taken for Epoch 4:3.63 - F1: 0.0189
2026-02-14 09:47:04 - INFO - Time taken for Epoch 4:3.63 - F1: 0.0189
Time taken for Epoch 5:3.62 - F1: 0.0189
2026-02-14 09:47:08 - INFO - Time taken for Epoch 5:3.62 - F1: 0.0189
Time taken for Epoch 6:3.63 - F1: 0.0189
2026-02-14 09:47:11 - INFO - Time taken for Epoch 6:3.63 - F1: 0.0189
Time taken for Epoch 7:3.63 - F1: 0.0189
2026-02-14 09:47:15 - INFO - Time taken for Epoch 7:3.63 - F1: 0.0189
Time taken for Epoch 8:3.62 - F1: 0.0189
2026-02-14 09:47:18 - INFO - Time taken for Epoch 8:3.62 - F1: 0.0189
Time taken for Epoch 9:3.62 - F1: 0.0189
2026-02-14 09:47:22 - INFO - Time taken for Epoch 9:3.62 - F1: 0.0189
Time taken for Epoch 10:3.63 - F1: 0.0189
2026-02-14 09:47:26 - INFO - Time taken for Epoch 10:3.63 - F1: 0.0189
Time taken for Epoch 11:3.63 - F1: 0.0189
2026-02-14 09:47:29 - INFO - Time taken for Epoch 11:3.63 - F1: 0.0189
Performance not improving for 10 consecutive epochs.
2026-02-14 09:47:29 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0197 - Best Epoch:0
2026-02-14 09:47:29 - INFO - Best F1:0.0197 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0198, Test ECE: 0.3952
2026-02-14 09:47:37 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.0198, Test ECE: 0.3952
All results: {'f1_macro': 0.019793640766477154, 'ece': np.float64(0.39518501819337454)}
2026-02-14 09:47:37 - INFO - All results: {'f1_macro': 0.019793640766477154, 'ece': np.float64(0.39518501819337454)}

Total time taken: 443.41 seconds
2026-02-14 09:47:37 - INFO - 
Total time taken: 443.41 seconds
2026-02-14 09:47:37 - INFO - Trial 8 finished with value: 0.019793640766477154 and parameters: {'learning_rate': 0.0008226417076523035, 'weight_decay': 1.2649325541200087e-05, 'batch_size': 32, 'co_train_epochs': 7, 'epoch_patience': 5}. Best is trial 0 with value: 0.6775060123822318.
Using devices: cuda, cuda
2026-02-14 09:47:37 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-14 09:47:37 - INFO - Devices: cuda, cuda
Starting log
2026-02-14 09:47:37 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
2026-02-14 09:47:37 - INFO - Dataset: humanitarian9, Event: hurricane_maria_2017, N: 25, Seed: 1234, HF Model: GPT-4o, NumShots: 25, PLM: bert-tweet
Learning Rate: 0.00010611913821862913
Weight Decay: 5.085976516659413e-05
Batch Size: 32
No. Epochs: 8
Epoch Patience: 8
 Accumulation Steps: 2
2026-02-14 09:47:38 - INFO - Learning Rate: 0.00010611913821862913
Weight Decay: 5.085976516659413e-05
Batch Size: 32
No. Epochs: 8
Epoch Patience: 8
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-14 09:47:39 - INFO - Generating initial weights
Time taken for Epoch 1:20.36 - F1: 0.0720
2026-02-14 09:48:02 - INFO - Time taken for Epoch 1:20.36 - F1: 0.0720
Time taken for Epoch 2:20.32 - F1: 0.1117
2026-02-14 09:48:23 - INFO - Time taken for Epoch 2:20.32 - F1: 0.1117
Time taken for Epoch 3:20.36 - F1: 0.0356
2026-02-14 09:48:43 - INFO - Time taken for Epoch 3:20.36 - F1: 0.0356
Time taken for Epoch 4:20.40 - F1: 0.0529
2026-02-14 09:49:04 - INFO - Time taken for Epoch 4:20.40 - F1: 0.0529
Time taken for Epoch 5:20.39 - F1: 0.1543
2026-02-14 09:49:24 - INFO - Time taken for Epoch 5:20.39 - F1: 0.1543
Time taken for Epoch 6:20.38 - F1: 0.2722
2026-02-14 09:49:44 - INFO - Time taken for Epoch 6:20.38 - F1: 0.2722
Time taken for Epoch 7:20.40 - F1: 0.3171
2026-02-14 09:50:05 - INFO - Time taken for Epoch 7:20.40 - F1: 0.3171
Time taken for Epoch 8:20.38 - F1: 0.3369
2026-02-14 09:50:25 - INFO - Time taken for Epoch 8:20.38 - F1: 0.3369
Best F1:0.3369 - Best Epoch:8
2026-02-14 09:50:25 - INFO - Best F1:0.3369 - Best Epoch:8
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-14 09:50:26 - INFO - Starting co-training
Time taken for Epoch 1: 34.38s - F1: 0.50494504
2026-02-14 09:51:01 - INFO - Time taken for Epoch 1: 34.38s - F1: 0.50494504
Time taken for Epoch 2: 35.43s - F1: 0.48805344
2026-02-14 09:51:36 - INFO - Time taken for Epoch 2: 35.43s - F1: 0.48805344
Time taken for Epoch 3: 34.43s - F1: 0.59570026
2026-02-14 09:52:11 - INFO - Time taken for Epoch 3: 34.43s - F1: 0.59570026
Time taken for Epoch 4: 35.56s - F1: 0.61805341
2026-02-14 09:52:46 - INFO - Time taken for Epoch 4: 35.56s - F1: 0.61805341
Time taken for Epoch 5: 35.53s - F1: 0.62367594
2026-02-14 09:53:22 - INFO - Time taken for Epoch 5: 35.53s - F1: 0.62367594
Time taken for Epoch 6: 35.56s - F1: 0.63183879
2026-02-14 09:53:57 - INFO - Time taken for Epoch 6: 35.56s - F1: 0.63183879
Time taken for Epoch 7: 35.53s - F1: 0.65105945
2026-02-14 09:54:33 - INFO - Time taken for Epoch 7: 35.53s - F1: 0.65105945
Time taken for Epoch 8: 35.52s - F1: 0.65658470
2026-02-14 09:55:08 - INFO - Time taken for Epoch 8: 35.52s - F1: 0.65658470
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/co_trained_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Fine-tuning models
2026-02-14 09:55:12 - INFO - Fine-tuning models
Time taken for Epoch 1:3.63 - F1: 0.6693
2026-02-14 09:55:16 - INFO - Time taken for Epoch 1:3.63 - F1: 0.6693
Time taken for Epoch 2:4.70 - F1: 0.6289
2026-02-14 09:55:21 - INFO - Time taken for Epoch 2:4.70 - F1: 0.6289
Time taken for Epoch 3:3.63 - F1: 0.6318
2026-02-14 09:55:25 - INFO - Time taken for Epoch 3:3.63 - F1: 0.6318
Time taken for Epoch 4:3.63 - F1: 0.6645
2026-02-14 09:55:28 - INFO - Time taken for Epoch 4:3.63 - F1: 0.6645
Time taken for Epoch 5:3.64 - F1: 0.6661
2026-02-14 09:55:32 - INFO - Time taken for Epoch 5:3.64 - F1: 0.6661
Time taken for Epoch 6:3.63 - F1: 0.6835
2026-02-14 09:55:36 - INFO - Time taken for Epoch 6:3.63 - F1: 0.6835
Time taken for Epoch 7:4.81 - F1: 0.6809
2026-02-14 09:55:40 - INFO - Time taken for Epoch 7:4.81 - F1: 0.6809
Time taken for Epoch 8:3.63 - F1: 0.6840
2026-02-14 09:55:44 - INFO - Time taken for Epoch 8:3.63 - F1: 0.6840
Time taken for Epoch 9:4.79 - F1: 0.6778
2026-02-14 09:55:49 - INFO - Time taken for Epoch 9:4.79 - F1: 0.6778
Time taken for Epoch 10:3.63 - F1: 0.6828
2026-02-14 09:55:52 - INFO - Time taken for Epoch 10:3.63 - F1: 0.6828
Time taken for Epoch 11:3.63 - F1: 0.6827
2026-02-14 09:55:56 - INFO - Time taken for Epoch 11:3.63 - F1: 0.6827
Time taken for Epoch 12:3.64 - F1: 0.6857
2026-02-14 09:56:00 - INFO - Time taken for Epoch 12:3.64 - F1: 0.6857
Time taken for Epoch 13:4.79 - F1: 0.6878
2026-02-14 09:56:04 - INFO - Time taken for Epoch 13:4.79 - F1: 0.6878
Time taken for Epoch 14:4.80 - F1: 0.6767
2026-02-14 09:56:09 - INFO - Time taken for Epoch 14:4.80 - F1: 0.6767
Time taken for Epoch 15:3.62 - F1: 0.6781
2026-02-14 09:56:13 - INFO - Time taken for Epoch 15:3.62 - F1: 0.6781
Time taken for Epoch 16:3.63 - F1: 0.6767
2026-02-14 09:56:16 - INFO - Time taken for Epoch 16:3.63 - F1: 0.6767
Time taken for Epoch 17:3.63 - F1: 0.6808
2026-02-14 09:56:20 - INFO - Time taken for Epoch 17:3.63 - F1: 0.6808
Time taken for Epoch 18:3.63 - F1: 0.6803
2026-02-14 09:56:24 - INFO - Time taken for Epoch 18:3.63 - F1: 0.6803
Time taken for Epoch 19:3.63 - F1: 0.6831
2026-02-14 09:56:27 - INFO - Time taken for Epoch 19:3.63 - F1: 0.6831
Time taken for Epoch 20:3.64 - F1: 0.6829
2026-02-14 09:56:31 - INFO - Time taken for Epoch 20:3.64 - F1: 0.6829
Time taken for Epoch 21:3.63 - F1: 0.6843
2026-02-14 09:56:35 - INFO - Time taken for Epoch 21:3.63 - F1: 0.6843
Time taken for Epoch 22:3.64 - F1: 0.6828
2026-02-14 09:56:38 - INFO - Time taken for Epoch 22:3.64 - F1: 0.6828
Time taken for Epoch 23:3.63 - F1: 0.6817
2026-02-14 09:56:42 - INFO - Time taken for Epoch 23:3.63 - F1: 0.6817
Performance not improving for 10 consecutive epochs.
2026-02-14 09:56:42 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6878 - Best Epoch:12
2026-02-14 09:56:42 - INFO - Best F1:0.6878 - Best Epoch:12
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_1_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-maria-2017-label25-set1/final_model_2_optuna-bertweet-hurricane-maria-2017-label25-set1_gpt4o_25_shot_bert-tweet_25_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6445, Test ECE: 0.0724
2026-02-14 09:56:49 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 25, N: 25 Test SEED: 1234 F1: 0.6445, Test ECE: 0.0724
All results: {'f1_macro': 0.6444742246079667, 'ece': np.float64(0.07240959187651805)}
2026-02-14 09:56:49 - INFO - All results: {'f1_macro': 0.6444742246079667, 'ece': np.float64(0.07240959187651805)}

Total time taken: 552.20 seconds
2026-02-14 09:56:49 - INFO - 
Total time taken: 552.20 seconds
2026-02-14 09:56:49 - INFO - Trial 9 finished with value: 0.6444742246079667 and parameters: {'learning_rate': 0.00010611913821862913, 'weight_decay': 5.085976516659413e-05, 'batch_size': 32, 'co_train_epochs': 8, 'epoch_patience': 8}. Best is trial 0 with value: 0.6775060123822318.

[BEST TRIAL RESULTS]
2026-02-14 09:56:49 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6775
2026-02-14 09:56:49 - INFO - F1 Score: 0.6775
Params: {'learning_rate': 2.55314706966723e-05, 'weight_decay': 0.002391215161564007, 'batch_size': 32, 'co_train_epochs': 13, 'epoch_patience': 4}
2026-02-14 09:56:49 - INFO - Params: {'learning_rate': 2.55314706966723e-05, 'weight_decay': 0.002391215161564007, 'batch_size': 32, 'co_train_epochs': 13, 'epoch_patience': 4}
  learning_rate: 2.55314706966723e-05
2026-02-14 09:56:49 - INFO -   learning_rate: 2.55314706966723e-05
  weight_decay: 0.002391215161564007
2026-02-14 09:56:49 - INFO -   weight_decay: 0.002391215161564007
  batch_size: 32
2026-02-14 09:56:49 - INFO -   batch_size: 32
  co_train_epochs: 13
2026-02-14 09:56:49 - INFO -   co_train_epochs: 13
  epoch_patience: 4
2026-02-14 09:56:49 - INFO -   epoch_patience: 4

Total time taken: 6491.77 seconds
2026-02-14 09:56:49 - INFO - 
Total time taken: 6491.77 seconds