Running with 5 label/class set 1

[Optuna] Starting hyperparameter search with 10 trials.
2026-02-12 12:18:10 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-12 12:18:10 - INFO - A new study created in memory with name: study_humanitarian9_hurricane_florence_2018
Using devices: cuda, cuda
2026-02-12 12:18:10 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 12:18:10 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 12:18:10 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 12:18:10 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 2.114113037844942e-05
Weight Decay: 0.004720016359061808
Batch Size: 32
No. Epochs: 13
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-12 12:18:11 - INFO - Learning Rate: 2.114113037844942e-05
Weight Decay: 0.004720016359061808
Batch Size: 32
No. Epochs: 13
Epoch Patience: 7
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 12:18:12 - INFO - Generating initial weights
Time taken for Epoch 1:17.81 - F1: 0.0590
2026-02-12 12:18:34 - INFO - Time taken for Epoch 1:17.81 - F1: 0.0590
Time taken for Epoch 2:17.39 - F1: 0.0555
2026-02-12 12:18:51 - INFO - Time taken for Epoch 2:17.39 - F1: 0.0555
Time taken for Epoch 3:17.46 - F1: 0.0800
2026-02-12 12:19:08 - INFO - Time taken for Epoch 3:17.46 - F1: 0.0800
Time taken for Epoch 4:17.56 - F1: 0.0836
2026-02-12 12:19:26 - INFO - Time taken for Epoch 4:17.56 - F1: 0.0836
Time taken for Epoch 5:17.61 - F1: 0.0872
2026-02-12 12:19:44 - INFO - Time taken for Epoch 5:17.61 - F1: 0.0872
Time taken for Epoch 6:17.65 - F1: 0.0991
2026-02-12 12:20:01 - INFO - Time taken for Epoch 6:17.65 - F1: 0.0991
Time taken for Epoch 7:17.68 - F1: 0.1223
2026-02-12 12:20:19 - INFO - Time taken for Epoch 7:17.68 - F1: 0.1223
Time taken for Epoch 8:17.70 - F1: 0.1361
2026-02-12 12:20:37 - INFO - Time taken for Epoch 8:17.70 - F1: 0.1361
Time taken for Epoch 9:17.79 - F1: 0.1545
2026-02-12 12:20:54 - INFO - Time taken for Epoch 9:17.79 - F1: 0.1545
Time taken for Epoch 10:17.87 - F1: 0.1679
2026-02-12 12:21:12 - INFO - Time taken for Epoch 10:17.87 - F1: 0.1679
Time taken for Epoch 11:17.85 - F1: 0.1763
2026-02-12 12:21:30 - INFO - Time taken for Epoch 11:17.85 - F1: 0.1763
Time taken for Epoch 12:17.89 - F1: 0.1831
2026-02-12 12:21:48 - INFO - Time taken for Epoch 12:17.89 - F1: 0.1831
Time taken for Epoch 13:17.88 - F1: 0.1943
2026-02-12 12:22:06 - INFO - Time taken for Epoch 13:17.88 - F1: 0.1943
Best F1:0.1943 - Best Epoch:13
2026-02-12 12:22:06 - INFO - Best F1:0.1943 - Best Epoch:13
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 12:22:07 - INFO - Starting co-training
Time taken for Epoch 1: 30.67s - F1: 0.52101629
2026-02-12 12:22:38 - INFO - Time taken for Epoch 1: 30.67s - F1: 0.52101629
Time taken for Epoch 2: 31.76s - F1: 0.57654296
2026-02-12 12:23:10 - INFO - Time taken for Epoch 2: 31.76s - F1: 0.57654296
Time taken for Epoch 3: 32.37s - F1: 0.60449681
2026-02-12 12:23:42 - INFO - Time taken for Epoch 3: 32.37s - F1: 0.60449681
Time taken for Epoch 4: 31.85s - F1: 0.61536658
2026-02-12 12:24:14 - INFO - Time taken for Epoch 4: 31.85s - F1: 0.61536658
Time taken for Epoch 5: 32.02s - F1: 0.62267635
2026-02-12 12:24:46 - INFO - Time taken for Epoch 5: 32.02s - F1: 0.62267635
Time taken for Epoch 6: 31.88s - F1: 0.63917344
2026-02-12 12:25:18 - INFO - Time taken for Epoch 6: 31.88s - F1: 0.63917344
Time taken for Epoch 7: 32.20s - F1: 0.64870888
2026-02-12 12:25:50 - INFO - Time taken for Epoch 7: 32.20s - F1: 0.64870888
Time taken for Epoch 8: 31.86s - F1: 0.64146116
2026-02-12 12:26:22 - INFO - Time taken for Epoch 8: 31.86s - F1: 0.64146116
Time taken for Epoch 9: 30.93s - F1: 0.67315286
2026-02-12 12:26:53 - INFO - Time taken for Epoch 9: 30.93s - F1: 0.67315286
Time taken for Epoch 10: 31.87s - F1: 0.66269562
2026-02-12 12:27:25 - INFO - Time taken for Epoch 10: 31.87s - F1: 0.66269562
Time taken for Epoch 11: 30.73s - F1: 0.65667148
2026-02-12 12:27:56 - INFO - Time taken for Epoch 11: 30.73s - F1: 0.65667148
Time taken for Epoch 12: 30.76s - F1: 0.70563180
2026-02-12 12:28:27 - INFO - Time taken for Epoch 12: 30.76s - F1: 0.70563180
Time taken for Epoch 13: 31.87s - F1: 0.65490089
2026-02-12 12:28:58 - INFO - Time taken for Epoch 13: 31.87s - F1: 0.65490089
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 12:29:01 - INFO - Fine-tuning models
Time taken for Epoch 1:2.49 - F1: 0.6782
2026-02-12 12:29:04 - INFO - Time taken for Epoch 1:2.49 - F1: 0.6782
Time taken for Epoch 2:3.53 - F1: 0.6653
2026-02-12 12:29:08 - INFO - Time taken for Epoch 2:3.53 - F1: 0.6653
Time taken for Epoch 3:2.47 - F1: 0.6847
2026-02-12 12:29:10 - INFO - Time taken for Epoch 3:2.47 - F1: 0.6847
Time taken for Epoch 4:3.65 - F1: 0.6813
2026-02-12 12:29:14 - INFO - Time taken for Epoch 4:3.65 - F1: 0.6813
Time taken for Epoch 5:2.48 - F1: 0.6826
2026-02-12 12:29:16 - INFO - Time taken for Epoch 5:2.48 - F1: 0.6826
Time taken for Epoch 6:2.48 - F1: 0.6710
2026-02-12 12:29:19 - INFO - Time taken for Epoch 6:2.48 - F1: 0.6710
Time taken for Epoch 7:2.47 - F1: 0.6653
2026-02-12 12:29:21 - INFO - Time taken for Epoch 7:2.47 - F1: 0.6653
Time taken for Epoch 8:2.48 - F1: 0.6693
2026-02-12 12:29:24 - INFO - Time taken for Epoch 8:2.48 - F1: 0.6693
Time taken for Epoch 9:2.48 - F1: 0.6668
2026-02-12 12:29:26 - INFO - Time taken for Epoch 9:2.48 - F1: 0.6668
Time taken for Epoch 10:2.47 - F1: 0.6654
2026-02-12 12:29:29 - INFO - Time taken for Epoch 10:2.47 - F1: 0.6654
Time taken for Epoch 11:2.48 - F1: 0.6711
2026-02-12 12:29:31 - INFO - Time taken for Epoch 11:2.48 - F1: 0.6711
Time taken for Epoch 12:2.49 - F1: 0.6620
2026-02-12 12:29:34 - INFO - Time taken for Epoch 12:2.49 - F1: 0.6620
Time taken for Epoch 13:2.48 - F1: 0.6694
2026-02-12 12:29:36 - INFO - Time taken for Epoch 13:2.48 - F1: 0.6694
Performance not improving for 10 consecutive epochs.
2026-02-12 12:29:36 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6847 - Best Epoch:2
2026-02-12 12:29:36 - INFO - Best F1:0.6847 - Best Epoch:2
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6703, Test ECE: 0.0273
2026-02-12 12:29:44 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6703, Test ECE: 0.0273
All results: {'f1_macro': 0.6702979687742381, 'ece': np.float64(0.027336138520482847)}
2026-02-12 12:29:44 - INFO - All results: {'f1_macro': 0.6702979687742381, 'ece': np.float64(0.027336138520482847)}

Total time taken: 693.98 seconds
2026-02-12 12:29:44 - INFO - 
Total time taken: 693.98 seconds
2026-02-12 12:29:44 - INFO - Trial 0 finished with value: 0.6702979687742381 and parameters: {'learning_rate': 2.114113037844942e-05, 'weight_decay': 0.004720016359061808, 'batch_size': 32, 'co_train_epochs': 13, 'epoch_patience': 7}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 12:29:44 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 12:29:44 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 12:29:44 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 12:29:44 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0007564993825534161
Weight Decay: 0.0006922766901145254
Batch Size: 16
No. Epochs: 10
Epoch Patience: 9
 Accumulation Steps: 4
2026-02-12 12:29:44 - INFO - Learning Rate: 0.0007564993825534161
Weight Decay: 0.0006922766901145254
Batch Size: 16
No. Epochs: 10
Epoch Patience: 9
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 12:29:45 - INFO - Generating initial weights
Time taken for Epoch 1:18.34 - F1: 0.0155
2026-02-12 12:30:07 - INFO - Time taken for Epoch 1:18.34 - F1: 0.0155
Time taken for Epoch 2:18.23 - F1: 0.0155
2026-02-12 12:30:26 - INFO - Time taken for Epoch 2:18.23 - F1: 0.0155
Time taken for Epoch 3:18.29 - F1: 0.0155
2026-02-12 12:30:44 - INFO - Time taken for Epoch 3:18.29 - F1: 0.0155
Time taken for Epoch 4:18.28 - F1: 0.0155
2026-02-12 12:31:02 - INFO - Time taken for Epoch 4:18.28 - F1: 0.0155
Time taken for Epoch 5:18.31 - F1: 0.0414
2026-02-12 12:31:21 - INFO - Time taken for Epoch 5:18.31 - F1: 0.0414
Time taken for Epoch 6:18.30 - F1: 0.0226
2026-02-12 12:31:39 - INFO - Time taken for Epoch 6:18.30 - F1: 0.0226
Time taken for Epoch 7:18.32 - F1: 0.0155
2026-02-12 12:31:57 - INFO - Time taken for Epoch 7:18.32 - F1: 0.0155
Time taken for Epoch 8:18.32 - F1: 0.0155
2026-02-12 12:32:15 - INFO - Time taken for Epoch 8:18.32 - F1: 0.0155
Time taken for Epoch 9:18.32 - F1: 0.0155
2026-02-12 12:32:34 - INFO - Time taken for Epoch 9:18.32 - F1: 0.0155
Time taken for Epoch 10:18.30 - F1: 0.0155
2026-02-12 12:32:52 - INFO - Time taken for Epoch 10:18.30 - F1: 0.0155
Best F1:0.0414 - Best Epoch:5
2026-02-12 12:32:52 - INFO - Best F1:0.0414 - Best Epoch:5
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 12:32:53 - INFO - Starting co-training
Time taken for Epoch 1: 25.43s - F1: 0.03212851
2026-02-12 12:33:19 - INFO - Time taken for Epoch 1: 25.43s - F1: 0.03212851
Time taken for Epoch 2: 26.61s - F1: 0.03212851
2026-02-12 12:33:46 - INFO - Time taken for Epoch 2: 26.61s - F1: 0.03212851
Time taken for Epoch 3: 25.44s - F1: 0.04247539
2026-02-12 12:34:11 - INFO - Time taken for Epoch 3: 25.44s - F1: 0.04247539
Time taken for Epoch 4: 26.61s - F1: 0.04247539
2026-02-12 12:34:38 - INFO - Time taken for Epoch 4: 26.61s - F1: 0.04247539
Time taken for Epoch 5: 25.44s - F1: 0.04247539
2026-02-12 12:35:03 - INFO - Time taken for Epoch 5: 25.44s - F1: 0.04247539
Time taken for Epoch 6: 25.48s - F1: 0.04247539
2026-02-12 12:35:29 - INFO - Time taken for Epoch 6: 25.48s - F1: 0.04247539
Time taken for Epoch 7: 25.45s - F1: 0.04247539
2026-02-12 12:35:54 - INFO - Time taken for Epoch 7: 25.45s - F1: 0.04247539
Time taken for Epoch 8: 25.45s - F1: 0.04247539
2026-02-12 12:36:20 - INFO - Time taken for Epoch 8: 25.45s - F1: 0.04247539
Time taken for Epoch 9: 25.49s - F1: 0.04247539
2026-02-12 12:36:45 - INFO - Time taken for Epoch 9: 25.49s - F1: 0.04247539
Time taken for Epoch 10: 25.47s - F1: 0.04247539
2026-02-12 12:37:11 - INFO - Time taken for Epoch 10: 25.47s - F1: 0.04247539
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 12:37:13 - INFO - Fine-tuning models
Time taken for Epoch 1:2.57 - F1: 0.0425
2026-02-12 12:37:16 - INFO - Time taken for Epoch 1:2.57 - F1: 0.0425
Time taken for Epoch 2:3.69 - F1: 0.0425
2026-02-12 12:37:20 - INFO - Time taken for Epoch 2:3.69 - F1: 0.0425
Time taken for Epoch 3:2.55 - F1: 0.0205
2026-02-12 12:37:22 - INFO - Time taken for Epoch 3:2.55 - F1: 0.0205
Time taken for Epoch 4:2.57 - F1: 0.0155
2026-02-12 12:37:25 - INFO - Time taken for Epoch 4:2.57 - F1: 0.0155
Time taken for Epoch 5:2.57 - F1: 0.0155
2026-02-12 12:37:28 - INFO - Time taken for Epoch 5:2.57 - F1: 0.0155
Time taken for Epoch 6:2.56 - F1: 0.0155
2026-02-12 12:37:30 - INFO - Time taken for Epoch 6:2.56 - F1: 0.0155
Time taken for Epoch 7:2.56 - F1: 0.0155
2026-02-12 12:37:33 - INFO - Time taken for Epoch 7:2.56 - F1: 0.0155
Time taken for Epoch 8:2.56 - F1: 0.0155
2026-02-12 12:37:35 - INFO - Time taken for Epoch 8:2.56 - F1: 0.0155
Time taken for Epoch 9:2.56 - F1: 0.0155
2026-02-12 12:37:38 - INFO - Time taken for Epoch 9:2.56 - F1: 0.0155
Time taken for Epoch 10:2.56 - F1: 0.0155
2026-02-12 12:37:40 - INFO - Time taken for Epoch 10:2.56 - F1: 0.0155
Time taken for Epoch 11:2.56 - F1: 0.0155
2026-02-12 12:37:43 - INFO - Time taken for Epoch 11:2.56 - F1: 0.0155
Performance not improving for 10 consecutive epochs.
2026-02-12 12:37:43 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-12 12:37:43 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.3068
2026-02-12 12:37:51 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.3068
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.30681896670800274)}
2026-02-12 12:37:51 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.30681896670800274)}

Total time taken: 486.80 seconds
2026-02-12 12:37:51 - INFO - 
Total time taken: 486.80 seconds
2026-02-12 12:37:51 - INFO - Trial 1 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.0007564993825534161, 'weight_decay': 0.0006922766901145254, 'batch_size': 16, 'co_train_epochs': 10, 'epoch_patience': 9}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 12:37:51 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 12:37:51 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 12:37:51 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 12:37:51 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.00015323329896759736
Weight Decay: 0.00210824620107801
Batch Size: 32
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 2
2026-02-12 12:37:51 - INFO - Learning Rate: 0.00015323329896759736
Weight Decay: 0.00210824620107801
Batch Size: 32
No. Epochs: 8
Epoch Patience: 6
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 12:37:52 - INFO - Generating initial weights
Time taken for Epoch 1:17.79 - F1: 0.0687
2026-02-12 12:38:14 - INFO - Time taken for Epoch 1:17.79 - F1: 0.0687
Time taken for Epoch 2:17.67 - F1: 0.1905
2026-02-12 12:38:31 - INFO - Time taken for Epoch 2:17.67 - F1: 0.1905
Time taken for Epoch 3:17.74 - F1: 0.3000
2026-02-12 12:38:49 - INFO - Time taken for Epoch 3:17.74 - F1: 0.3000
Time taken for Epoch 4:17.75 - F1: 0.3442
2026-02-12 12:39:07 - INFO - Time taken for Epoch 4:17.75 - F1: 0.3442
Time taken for Epoch 5:17.75 - F1: 0.3559
2026-02-12 12:39:25 - INFO - Time taken for Epoch 5:17.75 - F1: 0.3559
Time taken for Epoch 6:17.77 - F1: 0.3614
2026-02-12 12:39:42 - INFO - Time taken for Epoch 6:17.77 - F1: 0.3614
Time taken for Epoch 7:17.73 - F1: 0.3667
2026-02-12 12:40:00 - INFO - Time taken for Epoch 7:17.73 - F1: 0.3667
Time taken for Epoch 8:17.74 - F1: 0.3655
2026-02-12 12:40:18 - INFO - Time taken for Epoch 8:17.74 - F1: 0.3655
Best F1:0.3667 - Best Epoch:7
2026-02-12 12:40:18 - INFO - Best F1:0.3667 - Best Epoch:7
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 12:40:19 - INFO - Starting co-training
Time taken for Epoch 1: 30.65s - F1: 0.61659007
2026-02-12 12:40:50 - INFO - Time taken for Epoch 1: 30.65s - F1: 0.61659007
Time taken for Epoch 2: 31.82s - F1: 0.56779437
2026-02-12 12:41:22 - INFO - Time taken for Epoch 2: 31.82s - F1: 0.56779437
Time taken for Epoch 3: 30.72s - F1: 0.60767287
2026-02-12 12:41:53 - INFO - Time taken for Epoch 3: 30.72s - F1: 0.60767287
Time taken for Epoch 4: 30.77s - F1: 0.59773352
2026-02-12 12:42:23 - INFO - Time taken for Epoch 4: 30.77s - F1: 0.59773352
Time taken for Epoch 5: 30.78s - F1: 0.60133376
2026-02-12 12:42:54 - INFO - Time taken for Epoch 5: 30.78s - F1: 0.60133376
Time taken for Epoch 6: 30.77s - F1: 0.61024272
2026-02-12 12:43:25 - INFO - Time taken for Epoch 6: 30.77s - F1: 0.61024272
Time taken for Epoch 7: 30.77s - F1: 0.61991465
2026-02-12 12:43:56 - INFO - Time taken for Epoch 7: 30.77s - F1: 0.61991465
Time taken for Epoch 8: 31.95s - F1: 0.58514418
2026-02-12 12:44:28 - INFO - Time taken for Epoch 8: 31.95s - F1: 0.58514418
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 12:44:30 - INFO - Fine-tuning models
Time taken for Epoch 1:2.49 - F1: 0.5993
2026-02-12 12:44:33 - INFO - Time taken for Epoch 1:2.49 - F1: 0.5993
Time taken for Epoch 2:3.51 - F1: 0.5381
2026-02-12 12:44:37 - INFO - Time taken for Epoch 2:3.51 - F1: 0.5381
Time taken for Epoch 3:2.48 - F1: 0.5292
2026-02-12 12:44:39 - INFO - Time taken for Epoch 3:2.48 - F1: 0.5292
Time taken for Epoch 4:2.48 - F1: 0.5485
2026-02-12 12:44:42 - INFO - Time taken for Epoch 4:2.48 - F1: 0.5485
Time taken for Epoch 5:2.48 - F1: 0.5658
2026-02-12 12:44:44 - INFO - Time taken for Epoch 5:2.48 - F1: 0.5658
Time taken for Epoch 6:2.48 - F1: 0.5698
2026-02-12 12:44:47 - INFO - Time taken for Epoch 6:2.48 - F1: 0.5698
Time taken for Epoch 7:2.48 - F1: 0.5570
2026-02-12 12:44:49 - INFO - Time taken for Epoch 7:2.48 - F1: 0.5570
Time taken for Epoch 8:2.48 - F1: 0.5500
2026-02-12 12:44:52 - INFO - Time taken for Epoch 8:2.48 - F1: 0.5500
Time taken for Epoch 9:2.48 - F1: 0.5586
2026-02-12 12:44:54 - INFO - Time taken for Epoch 9:2.48 - F1: 0.5586
Time taken for Epoch 10:2.48 - F1: 0.5554
2026-02-12 12:44:57 - INFO - Time taken for Epoch 10:2.48 - F1: 0.5554
Time taken for Epoch 11:2.48 - F1: 0.5575
2026-02-12 12:44:59 - INFO - Time taken for Epoch 11:2.48 - F1: 0.5575
Performance not improving for 10 consecutive epochs.
2026-02-12 12:44:59 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.5993 - Best Epoch:0
2026-02-12 12:44:59 - INFO - Best F1:0.5993 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6051, Test ECE: 0.0687
2026-02-12 12:45:06 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6051, Test ECE: 0.0687
All results: {'f1_macro': 0.6050851353795698, 'ece': np.float64(0.06865476159488454)}
2026-02-12 12:45:06 - INFO - All results: {'f1_macro': 0.6050851353795698, 'ece': np.float64(0.06865476159488454)}

Total time taken: 435.72 seconds
2026-02-12 12:45:06 - INFO - 
Total time taken: 435.72 seconds
2026-02-12 12:45:06 - INFO - Trial 2 finished with value: 0.6050851353795698 and parameters: {'learning_rate': 0.00015323329896759736, 'weight_decay': 0.00210824620107801, 'batch_size': 32, 'co_train_epochs': 8, 'epoch_patience': 6}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 12:45:06 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 12:45:06 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 12:45:06 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 12:45:06 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0005067346625888864
Weight Decay: 0.0003047196385904646
Batch Size: 64
No. Epochs: 19
Epoch Patience: 9
 Accumulation Steps: 1
2026-02-12 12:45:07 - INFO - Learning Rate: 0.0005067346625888864
Weight Decay: 0.0003047196385904646
Batch Size: 64
No. Epochs: 19
Epoch Patience: 9
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 12:45:08 - INFO - Generating initial weights
Time taken for Epoch 1:16.92 - F1: 0.0264
2026-02-12 12:45:28 - INFO - Time taken for Epoch 1:16.92 - F1: 0.0264
Time taken for Epoch 2:16.87 - F1: 0.0660
2026-02-12 12:45:45 - INFO - Time taken for Epoch 2:16.87 - F1: 0.0660
Time taken for Epoch 3:16.85 - F1: 0.1366
2026-02-12 12:46:02 - INFO - Time taken for Epoch 3:16.85 - F1: 0.1366
Time taken for Epoch 4:16.90 - F1: 0.2150
2026-02-12 12:46:19 - INFO - Time taken for Epoch 4:16.90 - F1: 0.2150
Time taken for Epoch 5:16.93 - F1: 0.2534
2026-02-12 12:46:36 - INFO - Time taken for Epoch 5:16.93 - F1: 0.2534
Time taken for Epoch 6:16.90 - F1: 0.2306
2026-02-12 12:46:53 - INFO - Time taken for Epoch 6:16.90 - F1: 0.2306
Time taken for Epoch 7:16.90 - F1: 0.1730
2026-02-12 12:47:10 - INFO - Time taken for Epoch 7:16.90 - F1: 0.1730
Time taken for Epoch 8:16.89 - F1: 0.0447
2026-02-12 12:47:27 - INFO - Time taken for Epoch 8:16.89 - F1: 0.0447
Time taken for Epoch 9:16.88 - F1: 0.0321
2026-02-12 12:47:44 - INFO - Time taken for Epoch 9:16.88 - F1: 0.0321
Time taken for Epoch 10:16.88 - F1: 0.0321
2026-02-12 12:48:00 - INFO - Time taken for Epoch 10:16.88 - F1: 0.0321
Time taken for Epoch 11:16.89 - F1: 0.0911
2026-02-12 12:48:17 - INFO - Time taken for Epoch 11:16.89 - F1: 0.0911
Time taken for Epoch 12:16.88 - F1: 0.0940
2026-02-12 12:48:34 - INFO - Time taken for Epoch 12:16.88 - F1: 0.0940
Time taken for Epoch 13:16.88 - F1: 0.0913
2026-02-12 12:48:51 - INFO - Time taken for Epoch 13:16.88 - F1: 0.0913
Time taken for Epoch 14:16.88 - F1: 0.0730
2026-02-12 12:49:08 - INFO - Time taken for Epoch 14:16.88 - F1: 0.0730
Time taken for Epoch 15:16.88 - F1: 0.0643
2026-02-12 12:49:25 - INFO - Time taken for Epoch 15:16.88 - F1: 0.0643
Time taken for Epoch 16:16.89 - F1: 0.0478
2026-02-12 12:49:42 - INFO - Time taken for Epoch 16:16.89 - F1: 0.0478
Time taken for Epoch 17:16.87 - F1: 0.0452
2026-02-12 12:49:59 - INFO - Time taken for Epoch 17:16.87 - F1: 0.0452
Time taken for Epoch 18:16.89 - F1: 0.0453
2026-02-12 12:50:16 - INFO - Time taken for Epoch 18:16.89 - F1: 0.0453
Time taken for Epoch 19:16.88 - F1: 0.0598
2026-02-12 12:50:32 - INFO - Time taken for Epoch 19:16.88 - F1: 0.0598
Best F1:0.2534 - Best Epoch:5
2026-02-12 12:50:32 - INFO - Best F1:0.2534 - Best Epoch:5
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 12:50:34 - INFO - Starting co-training
Time taken for Epoch 1: 40.19s - F1: 0.03212851
2026-02-12 12:51:14 - INFO - Time taken for Epoch 1: 40.19s - F1: 0.03212851
Time taken for Epoch 2: 41.39s - F1: 0.03212851
2026-02-12 12:51:56 - INFO - Time taken for Epoch 2: 41.39s - F1: 0.03212851
Time taken for Epoch 3: 40.23s - F1: 0.03212851
2026-02-12 12:52:36 - INFO - Time taken for Epoch 3: 40.23s - F1: 0.03212851
Time taken for Epoch 4: 40.25s - F1: 0.03212851
2026-02-12 12:53:16 - INFO - Time taken for Epoch 4: 40.25s - F1: 0.03212851
Time taken for Epoch 5: 40.25s - F1: 0.04247539
2026-02-12 12:53:56 - INFO - Time taken for Epoch 5: 40.25s - F1: 0.04247539
Time taken for Epoch 6: 41.41s - F1: 0.04247539
2026-02-12 12:54:38 - INFO - Time taken for Epoch 6: 41.41s - F1: 0.04247539
Time taken for Epoch 7: 40.23s - F1: 0.04247539
2026-02-12 12:55:18 - INFO - Time taken for Epoch 7: 40.23s - F1: 0.04247539
Time taken for Epoch 8: 40.26s - F1: 0.04247539
2026-02-12 12:55:58 - INFO - Time taken for Epoch 8: 40.26s - F1: 0.04247539
Time taken for Epoch 9: 40.26s - F1: 0.04247539
2026-02-12 12:56:39 - INFO - Time taken for Epoch 9: 40.26s - F1: 0.04247539
Time taken for Epoch 10: 40.25s - F1: 0.04247539
2026-02-12 12:57:19 - INFO - Time taken for Epoch 10: 40.25s - F1: 0.04247539
Time taken for Epoch 11: 40.25s - F1: 0.04247539
2026-02-12 12:57:59 - INFO - Time taken for Epoch 11: 40.25s - F1: 0.04247539
Time taken for Epoch 12: 40.26s - F1: 0.04247539
2026-02-12 12:58:39 - INFO - Time taken for Epoch 12: 40.26s - F1: 0.04247539
Time taken for Epoch 13: 40.25s - F1: 0.04247539
2026-02-12 12:59:20 - INFO - Time taken for Epoch 13: 40.25s - F1: 0.04247539
Time taken for Epoch 14: 40.26s - F1: 0.04247539
2026-02-12 13:00:00 - INFO - Time taken for Epoch 14: 40.26s - F1: 0.04247539
Performance not improving for 9 consecutive epochs.
Performance not improving for 9 consecutive epochs.
2026-02-12 13:00:00 - INFO - Performance not improving for 9 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 13:00:03 - INFO - Fine-tuning models
Time taken for Epoch 1:2.36 - F1: 0.0425
2026-02-12 13:00:06 - INFO - Time taken for Epoch 1:2.36 - F1: 0.0425
Time taken for Epoch 2:3.51 - F1: 0.0425
2026-02-12 13:00:09 - INFO - Time taken for Epoch 2:3.51 - F1: 0.0425
Time taken for Epoch 3:2.35 - F1: 0.0017
2026-02-12 13:00:11 - INFO - Time taken for Epoch 3:2.35 - F1: 0.0017
Time taken for Epoch 4:2.35 - F1: 0.0425
2026-02-12 13:00:14 - INFO - Time taken for Epoch 4:2.35 - F1: 0.0425
Time taken for Epoch 5:2.35 - F1: 0.0017
2026-02-12 13:00:16 - INFO - Time taken for Epoch 5:2.35 - F1: 0.0017
Time taken for Epoch 6:2.35 - F1: 0.0017
2026-02-12 13:00:18 - INFO - Time taken for Epoch 6:2.35 - F1: 0.0017
Time taken for Epoch 7:2.35 - F1: 0.0100
2026-02-12 13:00:21 - INFO - Time taken for Epoch 7:2.35 - F1: 0.0100
Time taken for Epoch 8:2.35 - F1: 0.0017
2026-02-12 13:00:23 - INFO - Time taken for Epoch 8:2.35 - F1: 0.0017
Time taken for Epoch 9:2.35 - F1: 0.0017
2026-02-12 13:00:25 - INFO - Time taken for Epoch 9:2.35 - F1: 0.0017
Time taken for Epoch 10:2.35 - F1: 0.0425
2026-02-12 13:00:28 - INFO - Time taken for Epoch 10:2.35 - F1: 0.0425
Time taken for Epoch 11:2.35 - F1: 0.0425
2026-02-12 13:00:30 - INFO - Time taken for Epoch 11:2.35 - F1: 0.0425
Performance not improving for 10 consecutive epochs.
2026-02-12 13:00:30 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-12 13:00:30 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2829
2026-02-12 13:00:37 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2829
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.2829391349716402)}
2026-02-12 13:00:37 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.2829391349716402)}

Total time taken: 930.92 seconds
2026-02-12 13:00:37 - INFO - 
Total time taken: 930.92 seconds
2026-02-12 13:00:37 - INFO - Trial 3 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.0005067346625888864, 'weight_decay': 0.0003047196385904646, 'batch_size': 64, 'co_train_epochs': 19, 'epoch_patience': 9}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 13:00:37 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 13:00:37 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 13:00:37 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:00:37 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.00016040573537615574
Weight Decay: 4.795613573537417e-05
Batch Size: 16
No. Epochs: 20
Epoch Patience: 9
 Accumulation Steps: 4
2026-02-12 13:00:38 - INFO - Learning Rate: 0.00016040573537615574
Weight Decay: 4.795613573537417e-05
Batch Size: 16
No. Epochs: 20
Epoch Patience: 9
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 13:00:39 - INFO - Generating initial weights
Time taken for Epoch 1:18.27 - F1: 0.0155
2026-02-12 13:01:01 - INFO - Time taken for Epoch 1:18.27 - F1: 0.0155
Time taken for Epoch 2:18.20 - F1: 0.0155
2026-02-12 13:01:19 - INFO - Time taken for Epoch 2:18.20 - F1: 0.0155
Time taken for Epoch 3:18.25 - F1: 0.0155
2026-02-12 13:01:37 - INFO - Time taken for Epoch 3:18.25 - F1: 0.0155
Time taken for Epoch 4:18.28 - F1: 0.0155
2026-02-12 13:01:56 - INFO - Time taken for Epoch 4:18.28 - F1: 0.0155
Time taken for Epoch 5:18.29 - F1: 0.0404
2026-02-12 13:02:14 - INFO - Time taken for Epoch 5:18.29 - F1: 0.0404
Time taken for Epoch 6:18.30 - F1: 0.1639
2026-02-12 13:02:32 - INFO - Time taken for Epoch 6:18.30 - F1: 0.1639
Time taken for Epoch 7:18.29 - F1: 0.2753
2026-02-12 13:02:51 - INFO - Time taken for Epoch 7:18.29 - F1: 0.2753
Time taken for Epoch 8:18.31 - F1: 0.3384
2026-02-12 13:03:09 - INFO - Time taken for Epoch 8:18.31 - F1: 0.3384
Time taken for Epoch 9:18.34 - F1: 0.3427
2026-02-12 13:03:27 - INFO - Time taken for Epoch 9:18.34 - F1: 0.3427
Time taken for Epoch 10:18.32 - F1: 0.3231
2026-02-12 13:03:46 - INFO - Time taken for Epoch 10:18.32 - F1: 0.3231
Time taken for Epoch 11:18.35 - F1: 0.3273
2026-02-12 13:04:04 - INFO - Time taken for Epoch 11:18.35 - F1: 0.3273
Time taken for Epoch 12:18.29 - F1: 0.3427
2026-02-12 13:04:22 - INFO - Time taken for Epoch 12:18.29 - F1: 0.3427
Time taken for Epoch 13:18.29 - F1: 0.3525
2026-02-12 13:04:40 - INFO - Time taken for Epoch 13:18.29 - F1: 0.3525
Time taken for Epoch 14:18.27 - F1: 0.3743
2026-02-12 13:04:59 - INFO - Time taken for Epoch 14:18.27 - F1: 0.3743
Time taken for Epoch 15:18.28 - F1: 0.3883
2026-02-12 13:05:17 - INFO - Time taken for Epoch 15:18.28 - F1: 0.3883
Time taken for Epoch 16:18.27 - F1: 0.3928
2026-02-12 13:05:35 - INFO - Time taken for Epoch 16:18.27 - F1: 0.3928
Time taken for Epoch 17:18.34 - F1: 0.3963
2026-02-12 13:05:54 - INFO - Time taken for Epoch 17:18.34 - F1: 0.3963
Time taken for Epoch 18:18.36 - F1: 0.3996
2026-02-12 13:06:12 - INFO - Time taken for Epoch 18:18.36 - F1: 0.3996
Time taken for Epoch 19:18.37 - F1: 0.4079
2026-02-12 13:06:30 - INFO - Time taken for Epoch 19:18.37 - F1: 0.4079
Time taken for Epoch 20:18.33 - F1: 0.4034
2026-02-12 13:06:49 - INFO - Time taken for Epoch 20:18.33 - F1: 0.4034
Best F1:0.4079 - Best Epoch:19
2026-02-12 13:06:49 - INFO - Best F1:0.4079 - Best Epoch:19
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 13:06:50 - INFO - Starting co-training
Time taken for Epoch 1: 25.39s - F1: 0.40138129
2026-02-12 13:07:16 - INFO - Time taken for Epoch 1: 25.39s - F1: 0.40138129
Time taken for Epoch 2: 26.56s - F1: 0.48041182
2026-02-12 13:07:42 - INFO - Time taken for Epoch 2: 26.56s - F1: 0.48041182
Time taken for Epoch 3: 26.63s - F1: 0.43632714
2026-02-12 13:08:09 - INFO - Time taken for Epoch 3: 26.63s - F1: 0.43632714
Time taken for Epoch 4: 25.48s - F1: 0.50386441
2026-02-12 13:08:34 - INFO - Time taken for Epoch 4: 25.48s - F1: 0.50386441
Time taken for Epoch 5: 26.64s - F1: 0.58770772
2026-02-12 13:09:01 - INFO - Time taken for Epoch 5: 26.64s - F1: 0.58770772
Time taken for Epoch 6: 26.61s - F1: 0.55019943
2026-02-12 13:09:28 - INFO - Time taken for Epoch 6: 26.61s - F1: 0.55019943
Time taken for Epoch 7: 25.49s - F1: 0.56269319
2026-02-12 13:09:53 - INFO - Time taken for Epoch 7: 25.49s - F1: 0.56269319
Time taken for Epoch 8: 25.48s - F1: 0.55736543
2026-02-12 13:10:19 - INFO - Time taken for Epoch 8: 25.48s - F1: 0.55736543
Time taken for Epoch 9: 25.51s - F1: 0.52451230
2026-02-12 13:10:44 - INFO - Time taken for Epoch 9: 25.51s - F1: 0.52451230
Time taken for Epoch 10: 25.52s - F1: 0.58984413
2026-02-12 13:11:10 - INFO - Time taken for Epoch 10: 25.52s - F1: 0.58984413
Time taken for Epoch 11: 26.65s - F1: 0.53995739
2026-02-12 13:11:36 - INFO - Time taken for Epoch 11: 26.65s - F1: 0.53995739
Time taken for Epoch 12: 25.48s - F1: 0.40808185
2026-02-12 13:12:02 - INFO - Time taken for Epoch 12: 25.48s - F1: 0.40808185
Time taken for Epoch 13: 25.49s - F1: 0.54148473
2026-02-12 13:12:27 - INFO - Time taken for Epoch 13: 25.49s - F1: 0.54148473
Time taken for Epoch 14: 25.50s - F1: 0.56156458
2026-02-12 13:12:53 - INFO - Time taken for Epoch 14: 25.50s - F1: 0.56156458
Time taken for Epoch 15: 25.49s - F1: 0.56354713
2026-02-12 13:13:18 - INFO - Time taken for Epoch 15: 25.49s - F1: 0.56354713
Time taken for Epoch 16: 25.51s - F1: 0.58438338
2026-02-12 13:13:44 - INFO - Time taken for Epoch 16: 25.51s - F1: 0.58438338
Time taken for Epoch 17: 25.51s - F1: 0.50755224
2026-02-12 13:14:09 - INFO - Time taken for Epoch 17: 25.51s - F1: 0.50755224
Time taken for Epoch 18: 25.49s - F1: 0.46298050
2026-02-12 13:14:35 - INFO - Time taken for Epoch 18: 25.49s - F1: 0.46298050
Time taken for Epoch 19: 25.62s - F1: 0.53828680
2026-02-12 13:15:00 - INFO - Time taken for Epoch 19: 25.62s - F1: 0.53828680
Performance not improving for 9 consecutive epochs.
Performance not improving for 9 consecutive epochs.
2026-02-12 13:15:00 - INFO - Performance not improving for 9 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 13:15:03 - INFO - Fine-tuning models
Time taken for Epoch 1:2.59 - F1: 0.5855
2026-02-12 13:15:06 - INFO - Time taken for Epoch 1:2.59 - F1: 0.5855
Time taken for Epoch 2:3.69 - F1: 0.5670
2026-02-12 13:15:10 - INFO - Time taken for Epoch 2:3.69 - F1: 0.5670
Time taken for Epoch 3:2.58 - F1: 0.5316
2026-02-12 13:15:12 - INFO - Time taken for Epoch 3:2.58 - F1: 0.5316
Time taken for Epoch 4:2.58 - F1: 0.4767
2026-02-12 13:15:15 - INFO - Time taken for Epoch 4:2.58 - F1: 0.4767
Time taken for Epoch 5:2.57 - F1: 0.4842
2026-02-12 13:15:17 - INFO - Time taken for Epoch 5:2.57 - F1: 0.4842
Time taken for Epoch 6:2.58 - F1: 0.5489
2026-02-12 13:15:20 - INFO - Time taken for Epoch 6:2.58 - F1: 0.5489
Time taken for Epoch 7:2.59 - F1: 0.5498
2026-02-12 13:15:23 - INFO - Time taken for Epoch 7:2.59 - F1: 0.5498
Time taken for Epoch 8:2.58 - F1: 0.5572
2026-02-12 13:15:25 - INFO - Time taken for Epoch 8:2.58 - F1: 0.5572
Time taken for Epoch 9:2.58 - F1: 0.5589
2026-02-12 13:15:28 - INFO - Time taken for Epoch 9:2.58 - F1: 0.5589
Time taken for Epoch 10:2.58 - F1: 0.5774
2026-02-12 13:15:30 - INFO - Time taken for Epoch 10:2.58 - F1: 0.5774
Time taken for Epoch 11:2.58 - F1: 0.5679
2026-02-12 13:15:33 - INFO - Time taken for Epoch 11:2.58 - F1: 0.5679
Performance not improving for 10 consecutive epochs.
2026-02-12 13:15:33 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.5855 - Best Epoch:0
2026-02-12 13:15:33 - INFO - Best F1:0.5855 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5874, Test ECE: 0.0586
2026-02-12 13:15:40 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.5874, Test ECE: 0.0586
All results: {'f1_macro': 0.5873581284954524, 'ece': np.float64(0.05860239800477009)}
2026-02-12 13:15:40 - INFO - All results: {'f1_macro': 0.5873581284954524, 'ece': np.float64(0.05860239800477009)}

Total time taken: 903.04 seconds
2026-02-12 13:15:40 - INFO - 
Total time taken: 903.04 seconds
2026-02-12 13:15:40 - INFO - Trial 4 finished with value: 0.5873581284954524 and parameters: {'learning_rate': 0.00016040573537615574, 'weight_decay': 4.795613573537417e-05, 'batch_size': 16, 'co_train_epochs': 20, 'epoch_patience': 9}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 13:15:41 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 13:15:41 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 13:15:41 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:15:41 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 3.13917743457377e-05
Weight Decay: 0.004830243589389774
Batch Size: 64
No. Epochs: 15
Epoch Patience: 6
 Accumulation Steps: 1
2026-02-12 13:15:41 - INFO - Learning Rate: 3.13917743457377e-05
Weight Decay: 0.004830243589389774
Batch Size: 64
No. Epochs: 15
Epoch Patience: 6
 Accumulation Steps: 1
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 13:15:42 - INFO - Generating initial weights
Time taken for Epoch 1:17.00 - F1: 0.0575
2026-02-12 13:16:03 - INFO - Time taken for Epoch 1:17.00 - F1: 0.0575
Time taken for Epoch 2:16.90 - F1: 0.0803
2026-02-12 13:16:20 - INFO - Time taken for Epoch 2:16.90 - F1: 0.0803
Time taken for Epoch 3:16.91 - F1: 0.0842
2026-02-12 13:16:36 - INFO - Time taken for Epoch 3:16.91 - F1: 0.0842
Time taken for Epoch 4:16.93 - F1: 0.1065
2026-02-12 13:16:53 - INFO - Time taken for Epoch 4:16.93 - F1: 0.1065
Time taken for Epoch 5:16.94 - F1: 0.1281
2026-02-12 13:17:10 - INFO - Time taken for Epoch 5:16.94 - F1: 0.1281
Time taken for Epoch 6:16.94 - F1: 0.1773
2026-02-12 13:17:27 - INFO - Time taken for Epoch 6:16.94 - F1: 0.1773
Time taken for Epoch 7:16.92 - F1: 0.2105
2026-02-12 13:17:44 - INFO - Time taken for Epoch 7:16.92 - F1: 0.2105
Time taken for Epoch 8:16.94 - F1: 0.2355
2026-02-12 13:18:01 - INFO - Time taken for Epoch 8:16.94 - F1: 0.2355
Time taken for Epoch 9:16.93 - F1: 0.2459
2026-02-12 13:18:18 - INFO - Time taken for Epoch 9:16.93 - F1: 0.2459
Time taken for Epoch 10:16.92 - F1: 0.2745
2026-02-12 13:18:35 - INFO - Time taken for Epoch 10:16.92 - F1: 0.2745
Time taken for Epoch 11:16.96 - F1: 0.2875
2026-02-12 13:18:52 - INFO - Time taken for Epoch 11:16.96 - F1: 0.2875
Time taken for Epoch 12:16.93 - F1: 0.2942
2026-02-12 13:19:09 - INFO - Time taken for Epoch 12:16.93 - F1: 0.2942
Time taken for Epoch 13:16.92 - F1: 0.2919
2026-02-12 13:19:26 - INFO - Time taken for Epoch 13:16.92 - F1: 0.2919
Time taken for Epoch 14:16.93 - F1: 0.2969
2026-02-12 13:19:43 - INFO - Time taken for Epoch 14:16.93 - F1: 0.2969
Time taken for Epoch 15:16.92 - F1: 0.3007
2026-02-12 13:20:00 - INFO - Time taken for Epoch 15:16.92 - F1: 0.3007
Best F1:0.3007 - Best Epoch:15
2026-02-12 13:20:00 - INFO - Best F1:0.3007 - Best Epoch:15
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 13:20:01 - INFO - Starting co-training
Time taken for Epoch 1: 40.19s - F1: 0.58615879
2026-02-12 13:20:42 - INFO - Time taken for Epoch 1: 40.19s - F1: 0.58615879
Time taken for Epoch 2: 41.28s - F1: 0.61678271
2026-02-12 13:21:23 - INFO - Time taken for Epoch 2: 41.28s - F1: 0.61678271
Time taken for Epoch 3: 41.37s - F1: 0.60872589
2026-02-12 13:22:04 - INFO - Time taken for Epoch 3: 41.37s - F1: 0.60872589
Time taken for Epoch 4: 40.26s - F1: 0.61716529
2026-02-12 13:22:44 - INFO - Time taken for Epoch 4: 40.26s - F1: 0.61716529
Time taken for Epoch 5: 41.39s - F1: 0.62574151
2026-02-12 13:23:26 - INFO - Time taken for Epoch 5: 41.39s - F1: 0.62574151
Time taken for Epoch 6: 41.33s - F1: 0.61779055
2026-02-12 13:24:07 - INFO - Time taken for Epoch 6: 41.33s - F1: 0.61779055
Time taken for Epoch 7: 40.26s - F1: 0.63131852
2026-02-12 13:24:47 - INFO - Time taken for Epoch 7: 40.26s - F1: 0.63131852
Time taken for Epoch 8: 41.38s - F1: 0.63364839
2026-02-12 13:25:29 - INFO - Time taken for Epoch 8: 41.38s - F1: 0.63364839
Time taken for Epoch 9: 41.40s - F1: 0.63365209
2026-02-12 13:26:10 - INFO - Time taken for Epoch 9: 41.40s - F1: 0.63365209
Time taken for Epoch 10: 41.39s - F1: 0.64404552
2026-02-12 13:26:52 - INFO - Time taken for Epoch 10: 41.39s - F1: 0.64404552
Time taken for Epoch 11: 41.38s - F1: 0.68488854
2026-02-12 13:27:33 - INFO - Time taken for Epoch 11: 41.38s - F1: 0.68488854
Time taken for Epoch 12: 41.39s - F1: 0.65208978
2026-02-12 13:28:14 - INFO - Time taken for Epoch 12: 41.39s - F1: 0.65208978
Time taken for Epoch 13: 40.26s - F1: 0.65185204
2026-02-12 13:28:55 - INFO - Time taken for Epoch 13: 40.26s - F1: 0.65185204
Time taken for Epoch 14: 40.28s - F1: 0.70385183
2026-02-12 13:29:35 - INFO - Time taken for Epoch 14: 40.28s - F1: 0.70385183
Time taken for Epoch 15: 41.40s - F1: 0.68101520
2026-02-12 13:30:16 - INFO - Time taken for Epoch 15: 41.40s - F1: 0.68101520
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 13:30:19 - INFO - Fine-tuning models
Time taken for Epoch 1:2.36 - F1: 0.7139
2026-02-12 13:30:22 - INFO - Time taken for Epoch 1:2.36 - F1: 0.7139
Time taken for Epoch 2:3.41 - F1: 0.6948
2026-02-12 13:30:25 - INFO - Time taken for Epoch 2:3.41 - F1: 0.6948
Time taken for Epoch 3:2.36 - F1: 0.6839
2026-02-12 13:30:27 - INFO - Time taken for Epoch 3:2.36 - F1: 0.6839
Time taken for Epoch 4:2.36 - F1: 0.6737
2026-02-12 13:30:30 - INFO - Time taken for Epoch 4:2.36 - F1: 0.6737
Time taken for Epoch 5:2.36 - F1: 0.6585
2026-02-12 13:30:32 - INFO - Time taken for Epoch 5:2.36 - F1: 0.6585
Time taken for Epoch 6:2.36 - F1: 0.6548
2026-02-12 13:30:34 - INFO - Time taken for Epoch 6:2.36 - F1: 0.6548
Time taken for Epoch 7:2.36 - F1: 0.6569
2026-02-12 13:30:37 - INFO - Time taken for Epoch 7:2.36 - F1: 0.6569
Time taken for Epoch 8:2.36 - F1: 0.6494
2026-02-12 13:30:39 - INFO - Time taken for Epoch 8:2.36 - F1: 0.6494
Time taken for Epoch 9:2.36 - F1: 0.6529
2026-02-12 13:30:41 - INFO - Time taken for Epoch 9:2.36 - F1: 0.6529
Time taken for Epoch 10:2.36 - F1: 0.6520
2026-02-12 13:30:44 - INFO - Time taken for Epoch 10:2.36 - F1: 0.6520
Time taken for Epoch 11:2.36 - F1: 0.6478
2026-02-12 13:30:46 - INFO - Time taken for Epoch 11:2.36 - F1: 0.6478
Performance not improving for 10 consecutive epochs.
2026-02-12 13:30:46 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.7139 - Best Epoch:0
2026-02-12 13:30:46 - INFO - Best F1:0.7139 - Best Epoch:0
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6633, Test ECE: 0.0195
2026-02-12 13:30:53 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6633, Test ECE: 0.0195
All results: {'f1_macro': 0.6633453306741051, 'ece': np.float64(0.019487211004944213)}
2026-02-12 13:30:53 - INFO - All results: {'f1_macro': 0.6633453306741051, 'ece': np.float64(0.019487211004944213)}

Total time taken: 912.40 seconds
2026-02-12 13:30:53 - INFO - 
Total time taken: 912.40 seconds
2026-02-12 13:30:53 - INFO - Trial 5 finished with value: 0.6633453306741051 and parameters: {'learning_rate': 3.13917743457377e-05, 'weight_decay': 0.004830243589389774, 'batch_size': 64, 'co_train_epochs': 15, 'epoch_patience': 6}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 13:30:53 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 13:30:53 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 13:30:53 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:30:53 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.000360749987664525
Weight Decay: 4.453199484683686e-05
Batch Size: 32
No. Epochs: 17
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-12 13:30:54 - INFO - Learning Rate: 0.000360749987664525
Weight Decay: 4.453199484683686e-05
Batch Size: 32
No. Epochs: 17
Epoch Patience: 7
 Accumulation Steps: 2
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 13:30:55 - INFO - Generating initial weights
Time taken for Epoch 1:17.73 - F1: 0.0411
2026-02-12 13:31:16 - INFO - Time taken for Epoch 1:17.73 - F1: 0.0411
Time taken for Epoch 2:17.66 - F1: 0.1840
2026-02-12 13:31:34 - INFO - Time taken for Epoch 2:17.66 - F1: 0.1840
Time taken for Epoch 3:17.72 - F1: 0.1678
2026-02-12 13:31:51 - INFO - Time taken for Epoch 3:17.72 - F1: 0.1678
Time taken for Epoch 4:17.77 - F1: 0.2674
2026-02-12 13:32:09 - INFO - Time taken for Epoch 4:17.77 - F1: 0.2674
Time taken for Epoch 5:17.75 - F1: 0.2967
2026-02-12 13:32:27 - INFO - Time taken for Epoch 5:17.75 - F1: 0.2967
Time taken for Epoch 6:17.77 - F1: 0.3457
2026-02-12 13:32:45 - INFO - Time taken for Epoch 6:17.77 - F1: 0.3457
Time taken for Epoch 7:17.77 - F1: 0.3453
2026-02-12 13:33:02 - INFO - Time taken for Epoch 7:17.77 - F1: 0.3453
Time taken for Epoch 8:17.75 - F1: 0.3518
2026-02-12 13:33:20 - INFO - Time taken for Epoch 8:17.75 - F1: 0.3518
Time taken for Epoch 9:17.76 - F1: 0.3791
2026-02-12 13:33:38 - INFO - Time taken for Epoch 9:17.76 - F1: 0.3791
Time taken for Epoch 10:17.80 - F1: 0.3960
2026-02-12 13:33:56 - INFO - Time taken for Epoch 10:17.80 - F1: 0.3960
Time taken for Epoch 11:17.79 - F1: 0.3929
2026-02-12 13:34:13 - INFO - Time taken for Epoch 11:17.79 - F1: 0.3929
Time taken for Epoch 12:17.82 - F1: 0.4090
2026-02-12 13:34:31 - INFO - Time taken for Epoch 12:17.82 - F1: 0.4090
Time taken for Epoch 13:17.80 - F1: 0.4151
2026-02-12 13:34:49 - INFO - Time taken for Epoch 13:17.80 - F1: 0.4151
Time taken for Epoch 14:17.82 - F1: 0.4021
2026-02-12 13:35:07 - INFO - Time taken for Epoch 14:17.82 - F1: 0.4021
Time taken for Epoch 15:17.79 - F1: 0.4033
2026-02-12 13:35:25 - INFO - Time taken for Epoch 15:17.79 - F1: 0.4033
Time taken for Epoch 16:17.83 - F1: 0.4020
2026-02-12 13:35:43 - INFO - Time taken for Epoch 16:17.83 - F1: 0.4020
Time taken for Epoch 17:17.79 - F1: 0.3989
2026-02-12 13:36:00 - INFO - Time taken for Epoch 17:17.79 - F1: 0.3989
Best F1:0.4151 - Best Epoch:13
2026-02-12 13:36:00 - INFO - Best F1:0.4151 - Best Epoch:13
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 13:36:02 - INFO - Starting co-training
Time taken for Epoch 1: 30.68s - F1: 0.03212851
2026-02-12 13:36:33 - INFO - Time taken for Epoch 1: 30.68s - F1: 0.03212851
Time taken for Epoch 2: 31.73s - F1: 0.03212851
2026-02-12 13:37:04 - INFO - Time taken for Epoch 2: 31.73s - F1: 0.03212851
Time taken for Epoch 3: 30.73s - F1: 0.04247539
2026-02-12 13:37:35 - INFO - Time taken for Epoch 3: 30.73s - F1: 0.04247539
Time taken for Epoch 4: 31.87s - F1: 0.04247539
2026-02-12 13:38:07 - INFO - Time taken for Epoch 4: 31.87s - F1: 0.04247539
Time taken for Epoch 5: 30.74s - F1: 0.04247539
2026-02-12 13:38:38 - INFO - Time taken for Epoch 5: 30.74s - F1: 0.04247539
Time taken for Epoch 6: 30.77s - F1: 0.04247539
2026-02-12 13:39:08 - INFO - Time taken for Epoch 6: 30.77s - F1: 0.04247539
Time taken for Epoch 7: 30.77s - F1: 0.04247539
2026-02-12 13:39:39 - INFO - Time taken for Epoch 7: 30.77s - F1: 0.04247539
Time taken for Epoch 8: 30.75s - F1: 0.04247539
2026-02-12 13:40:10 - INFO - Time taken for Epoch 8: 30.75s - F1: 0.04247539
Time taken for Epoch 9: 30.77s - F1: 0.04247539
2026-02-12 13:40:41 - INFO - Time taken for Epoch 9: 30.77s - F1: 0.04247539
Time taken for Epoch 10: 30.79s - F1: 0.04247539
2026-02-12 13:41:12 - INFO - Time taken for Epoch 10: 30.79s - F1: 0.04247539
Performance not improving for 7 consecutive epochs.
Performance not improving for 7 consecutive epochs.
2026-02-12 13:41:12 - INFO - Performance not improving for 7 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 13:41:14 - INFO - Fine-tuning models
Time taken for Epoch 1:2.49 - F1: 0.0425
2026-02-12 13:41:17 - INFO - Time taken for Epoch 1:2.49 - F1: 0.0425
Time taken for Epoch 2:3.56 - F1: 0.0425
2026-02-12 13:41:21 - INFO - Time taken for Epoch 2:3.56 - F1: 0.0425
Time taken for Epoch 3:2.47 - F1: 0.0425
2026-02-12 13:41:23 - INFO - Time taken for Epoch 3:2.47 - F1: 0.0425
Time taken for Epoch 4:2.48 - F1: 0.0205
2026-02-12 13:41:25 - INFO - Time taken for Epoch 4:2.48 - F1: 0.0205
Time taken for Epoch 5:2.48 - F1: 0.0017
2026-02-12 13:41:28 - INFO - Time taken for Epoch 5:2.48 - F1: 0.0017
Time taken for Epoch 6:2.48 - F1: 0.0017
2026-02-12 13:41:30 - INFO - Time taken for Epoch 6:2.48 - F1: 0.0017
Time taken for Epoch 7:2.47 - F1: 0.0017
2026-02-12 13:41:33 - INFO - Time taken for Epoch 7:2.47 - F1: 0.0017
Time taken for Epoch 8:2.48 - F1: 0.0017
2026-02-12 13:41:35 - INFO - Time taken for Epoch 8:2.48 - F1: 0.0017
Time taken for Epoch 9:2.47 - F1: 0.0017
2026-02-12 13:41:38 - INFO - Time taken for Epoch 9:2.47 - F1: 0.0017
Time taken for Epoch 10:2.48 - F1: 0.0205
2026-02-12 13:41:40 - INFO - Time taken for Epoch 10:2.48 - F1: 0.0205
Time taken for Epoch 11:2.48 - F1: 0.0385
2026-02-12 13:41:43 - INFO - Time taken for Epoch 11:2.48 - F1: 0.0385
Performance not improving for 10 consecutive epochs.
2026-02-12 13:41:43 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-12 13:41:43 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2515
2026-02-12 13:41:50 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2515
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.2515292548537735)}
2026-02-12 13:41:50 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.2515292548537735)}

Total time taken: 657.07 seconds
2026-02-12 13:41:50 - INFO - 
Total time taken: 657.07 seconds
2026-02-12 13:41:50 - INFO - Trial 6 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.000360749987664525, 'weight_decay': 4.453199484683686e-05, 'batch_size': 32, 'co_train_epochs': 17, 'epoch_patience': 7}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 13:41:50 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 13:41:50 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 13:41:50 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:41:50 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0007225056467074643
Weight Decay: 4.1141622813763966e-05
Batch Size: 8
No. Epochs: 6
Epoch Patience: 4
 Accumulation Steps: 8
2026-02-12 13:41:51 - INFO - Learning Rate: 0.0007225056467074643
Weight Decay: 4.1141622813763966e-05
Batch Size: 8
No. Epochs: 6
Epoch Patience: 4
 Accumulation Steps: 8
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 13:41:51 - INFO - Generating initial weights
Time taken for Epoch 1:19.67 - F1: 0.0155
2026-02-12 13:42:15 - INFO - Time taken for Epoch 1:19.67 - F1: 0.0155
Time taken for Epoch 2:19.62 - F1: 0.0425
2026-02-12 13:42:34 - INFO - Time taken for Epoch 2:19.62 - F1: 0.0425
Time taken for Epoch 3:19.61 - F1: 0.0155
2026-02-12 13:42:54 - INFO - Time taken for Epoch 3:19.61 - F1: 0.0155
Time taken for Epoch 4:19.67 - F1: 0.0150
2026-02-12 13:43:14 - INFO - Time taken for Epoch 4:19.67 - F1: 0.0150
Time taken for Epoch 5:19.61 - F1: 0.0483
2026-02-12 13:43:33 - INFO - Time taken for Epoch 5:19.61 - F1: 0.0483
Time taken for Epoch 6:19.59 - F1: 0.0385
2026-02-12 13:43:53 - INFO - Time taken for Epoch 6:19.59 - F1: 0.0385
Best F1:0.0483 - Best Epoch:5
2026-02-12 13:43:53 - INFO - Best F1:0.0483 - Best Epoch:5
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 13:43:54 - INFO - Starting co-training
Time taken for Epoch 1: 23.71s - F1: 0.03852235
2026-02-12 13:44:18 - INFO - Time taken for Epoch 1: 23.71s - F1: 0.03852235
Time taken for Epoch 2: 24.96s - F1: 0.01091270
2026-02-12 13:44:43 - INFO - Time taken for Epoch 2: 24.96s - F1: 0.01091270
Time taken for Epoch 3: 24.04s - F1: 0.03852235
2026-02-12 13:45:07 - INFO - Time taken for Epoch 3: 24.04s - F1: 0.03852235
Time taken for Epoch 4: 24.03s - F1: 0.03212851
2026-02-12 13:45:31 - INFO - Time taken for Epoch 4: 24.03s - F1: 0.03212851
Time taken for Epoch 5: 23.90s - F1: 0.03212851
2026-02-12 13:45:55 - INFO - Time taken for Epoch 5: 23.90s - F1: 0.03212851
Performance not improving for 4 consecutive epochs.
Performance not improving for 4 consecutive epochs.
2026-02-12 13:45:55 - INFO - Performance not improving for 4 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 13:45:58 - INFO - Fine-tuning models
Time taken for Epoch 1:2.74 - F1: 0.0155
2026-02-12 13:46:01 - INFO - Time taken for Epoch 1:2.74 - F1: 0.0155
Time taken for Epoch 2:3.80 - F1: 0.0155
2026-02-12 13:46:05 - INFO - Time taken for Epoch 2:3.80 - F1: 0.0155
Time taken for Epoch 3:2.72 - F1: 0.0017
2026-02-12 13:46:07 - INFO - Time taken for Epoch 3:2.72 - F1: 0.0017
Time taken for Epoch 4:2.73 - F1: 0.0017
2026-02-12 13:46:10 - INFO - Time taken for Epoch 4:2.73 - F1: 0.0017
Time taken for Epoch 5:2.72 - F1: 0.0155
2026-02-12 13:46:13 - INFO - Time taken for Epoch 5:2.72 - F1: 0.0155
Time taken for Epoch 6:2.71 - F1: 0.0155
2026-02-12 13:46:15 - INFO - Time taken for Epoch 6:2.71 - F1: 0.0155
Time taken for Epoch 7:2.72 - F1: 0.0155
2026-02-12 13:46:18 - INFO - Time taken for Epoch 7:2.72 - F1: 0.0155
Time taken for Epoch 8:2.72 - F1: 0.0155
2026-02-12 13:46:21 - INFO - Time taken for Epoch 8:2.72 - F1: 0.0155
Time taken for Epoch 9:2.72 - F1: 0.0155
2026-02-12 13:46:24 - INFO - Time taken for Epoch 9:2.72 - F1: 0.0155
Time taken for Epoch 10:2.72 - F1: 0.0155
2026-02-12 13:46:26 - INFO - Time taken for Epoch 10:2.72 - F1: 0.0155
Time taken for Epoch 11:2.71 - F1: 0.0155
2026-02-12 13:46:29 - INFO - Time taken for Epoch 11:2.71 - F1: 0.0155
Performance not improving for 10 consecutive epochs.
2026-02-12 13:46:29 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0155 - Best Epoch:0
2026-02-12 13:46:29 - INFO - Best F1:0.0155 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0156, Test ECE: 0.3579
2026-02-12 13:46:37 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0156, Test ECE: 0.3579
All results: {'f1_macro': 0.015647107781939243, 'ece': np.float64(0.35791391518498317)}
2026-02-12 13:46:37 - INFO - All results: {'f1_macro': 0.015647107781939243, 'ece': np.float64(0.35791391518498317)}

Total time taken: 286.79 seconds
2026-02-12 13:46:37 - INFO - 
Total time taken: 286.79 seconds
2026-02-12 13:46:37 - INFO - Trial 7 finished with value: 0.015647107781939243 and parameters: {'learning_rate': 0.0007225056467074643, 'weight_decay': 4.1141622813763966e-05, 'batch_size': 8, 'co_train_epochs': 6, 'epoch_patience': 4}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 13:46:37 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 13:46:37 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 13:46:37 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:46:37 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.0008343717951991379
Weight Decay: 0.0012538539411360895
Batch Size: 16
No. Epochs: 19
Epoch Patience: 9
 Accumulation Steps: 4
2026-02-12 13:46:37 - INFO - Learning Rate: 0.0008343717951991379
Weight Decay: 0.0012538539411360895
Batch Size: 16
No. Epochs: 19
Epoch Patience: 9
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 13:46:38 - INFO - Generating initial weights
Time taken for Epoch 1:18.30 - F1: 0.0155
2026-02-12 13:47:00 - INFO - Time taken for Epoch 1:18.30 - F1: 0.0155
Time taken for Epoch 2:18.22 - F1: 0.0155
2026-02-12 13:47:19 - INFO - Time taken for Epoch 2:18.22 - F1: 0.0155
Time taken for Epoch 3:18.24 - F1: 0.0100
2026-02-12 13:47:37 - INFO - Time taken for Epoch 3:18.24 - F1: 0.0100
Time taken for Epoch 4:18.25 - F1: 0.0155
2026-02-12 13:47:55 - INFO - Time taken for Epoch 4:18.25 - F1: 0.0155
Time taken for Epoch 5:18.23 - F1: 0.0155
2026-02-12 13:48:13 - INFO - Time taken for Epoch 5:18.23 - F1: 0.0155
Time taken for Epoch 6:18.26 - F1: 0.0155
2026-02-12 13:48:32 - INFO - Time taken for Epoch 6:18.26 - F1: 0.0155
Time taken for Epoch 7:18.23 - F1: 0.0155
2026-02-12 13:48:50 - INFO - Time taken for Epoch 7:18.23 - F1: 0.0155
Time taken for Epoch 8:18.22 - F1: 0.0155
2026-02-12 13:49:08 - INFO - Time taken for Epoch 8:18.22 - F1: 0.0155
Time taken for Epoch 9:18.23 - F1: 0.0155
2026-02-12 13:49:26 - INFO - Time taken for Epoch 9:18.23 - F1: 0.0155
Time taken for Epoch 10:18.19 - F1: 0.0155
2026-02-12 13:49:44 - INFO - Time taken for Epoch 10:18.19 - F1: 0.0155
Time taken for Epoch 11:18.20 - F1: 0.0155
2026-02-12 13:50:03 - INFO - Time taken for Epoch 11:18.20 - F1: 0.0155
Time taken for Epoch 12:18.21 - F1: 0.0155
2026-02-12 13:50:21 - INFO - Time taken for Epoch 12:18.21 - F1: 0.0155
Time taken for Epoch 13:18.20 - F1: 0.0155
2026-02-12 13:50:39 - INFO - Time taken for Epoch 13:18.20 - F1: 0.0155
Time taken for Epoch 14:18.23 - F1: 0.0155
2026-02-12 13:50:57 - INFO - Time taken for Epoch 14:18.23 - F1: 0.0155
Time taken for Epoch 15:18.19 - F1: 0.0155
2026-02-12 13:51:16 - INFO - Time taken for Epoch 15:18.19 - F1: 0.0155
Time taken for Epoch 16:18.22 - F1: 0.0155
2026-02-12 13:51:34 - INFO - Time taken for Epoch 16:18.22 - F1: 0.0155
Time taken for Epoch 17:18.21 - F1: 0.0155
2026-02-12 13:51:52 - INFO - Time taken for Epoch 17:18.21 - F1: 0.0155
Time taken for Epoch 18:18.20 - F1: 0.0155
2026-02-12 13:52:10 - INFO - Time taken for Epoch 18:18.20 - F1: 0.0155
Time taken for Epoch 19:18.20 - F1: 0.0155
2026-02-12 13:52:28 - INFO - Time taken for Epoch 19:18.20 - F1: 0.0155
Best F1:0.0155 - Best Epoch:1
2026-02-12 13:52:28 - INFO - Best F1:0.0155 - Best Epoch:1
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 13:52:30 - INFO - Starting co-training
Time taken for Epoch 1: 25.59s - F1: 0.03212851
2026-02-12 13:52:56 - INFO - Time taken for Epoch 1: 25.59s - F1: 0.03212851
Time taken for Epoch 2: 26.48s - F1: 0.03852235
2026-02-12 13:53:22 - INFO - Time taken for Epoch 2: 26.48s - F1: 0.03852235
Time taken for Epoch 3: 26.52s - F1: 0.04247539
2026-02-12 13:53:49 - INFO - Time taken for Epoch 3: 26.52s - F1: 0.04247539
Time taken for Epoch 4: 26.54s - F1: 0.04247539
2026-02-12 13:54:15 - INFO - Time taken for Epoch 4: 26.54s - F1: 0.04247539
Time taken for Epoch 5: 25.46s - F1: 0.04247539
2026-02-12 13:54:41 - INFO - Time taken for Epoch 5: 25.46s - F1: 0.04247539
Time taken for Epoch 6: 25.44s - F1: 0.04247539
2026-02-12 13:55:06 - INFO - Time taken for Epoch 6: 25.44s - F1: 0.04247539
Time taken for Epoch 7: 25.46s - F1: 0.04247539
2026-02-12 13:55:31 - INFO - Time taken for Epoch 7: 25.46s - F1: 0.04247539
Time taken for Epoch 8: 25.45s - F1: 0.04247539
2026-02-12 13:55:57 - INFO - Time taken for Epoch 8: 25.45s - F1: 0.04247539
Time taken for Epoch 9: 25.49s - F1: 0.04247539
2026-02-12 13:56:22 - INFO - Time taken for Epoch 9: 25.49s - F1: 0.04247539
Time taken for Epoch 10: 25.51s - F1: 0.04247539
2026-02-12 13:56:48 - INFO - Time taken for Epoch 10: 25.51s - F1: 0.04247539
Time taken for Epoch 11: 25.46s - F1: 0.04247539
2026-02-12 13:57:13 - INFO - Time taken for Epoch 11: 25.46s - F1: 0.04247539
Time taken for Epoch 12: 25.49s - F1: 0.04247539
2026-02-12 13:57:39 - INFO - Time taken for Epoch 12: 25.49s - F1: 0.04247539
Performance not improving for 9 consecutive epochs.
Performance not improving for 9 consecutive epochs.
2026-02-12 13:57:39 - INFO - Performance not improving for 9 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 13:57:41 - INFO - Fine-tuning models
Time taken for Epoch 1:2.57 - F1: 0.0425
2026-02-12 13:57:44 - INFO - Time taken for Epoch 1:2.57 - F1: 0.0425
Time taken for Epoch 2:3.65 - F1: 0.0385
2026-02-12 13:57:48 - INFO - Time taken for Epoch 2:3.65 - F1: 0.0385
Time taken for Epoch 3:2.57 - F1: 0.0017
2026-02-12 13:57:51 - INFO - Time taken for Epoch 3:2.57 - F1: 0.0017
Time taken for Epoch 4:2.56 - F1: 0.0017
2026-02-12 13:57:53 - INFO - Time taken for Epoch 4:2.56 - F1: 0.0017
Time taken for Epoch 5:2.56 - F1: 0.0017
2026-02-12 13:57:56 - INFO - Time taken for Epoch 5:2.56 - F1: 0.0017
Time taken for Epoch 6:2.57 - F1: 0.0017
2026-02-12 13:57:58 - INFO - Time taken for Epoch 6:2.57 - F1: 0.0017
Time taken for Epoch 7:2.56 - F1: 0.0155
2026-02-12 13:58:01 - INFO - Time taken for Epoch 7:2.56 - F1: 0.0155
Time taken for Epoch 8:2.56 - F1: 0.0155
2026-02-12 13:58:03 - INFO - Time taken for Epoch 8:2.56 - F1: 0.0155
Time taken for Epoch 9:2.56 - F1: 0.0155
2026-02-12 13:58:06 - INFO - Time taken for Epoch 9:2.56 - F1: 0.0155
Time taken for Epoch 10:2.56 - F1: 0.0155
2026-02-12 13:58:08 - INFO - Time taken for Epoch 10:2.56 - F1: 0.0155
Time taken for Epoch 11:2.56 - F1: 0.0155
2026-02-12 13:58:11 - INFO - Time taken for Epoch 11:2.56 - F1: 0.0155
Performance not improving for 10 consecutive epochs.
2026-02-12 13:58:11 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.0425 - Best Epoch:0
2026-02-12 13:58:11 - INFO - Best F1:0.0425 - Best Epoch:0
/homes/bharanibala/optuna/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2559
2026-02-12 13:58:19 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.0424, Test ECE: 0.2559
All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.25591764468804756)}
2026-02-12 13:58:19 - INFO - All results: {'f1_macro': 0.042445313631754314, 'ece': np.float64(0.25591764468804756)}

Total time taken: 701.79 seconds
2026-02-12 13:58:19 - INFO - 
Total time taken: 701.79 seconds
2026-02-12 13:58:19 - INFO - Trial 8 finished with value: 0.042445313631754314 and parameters: {'learning_rate': 0.0008343717951991379, 'weight_decay': 0.0012538539411360895, 'batch_size': 16, 'co_train_epochs': 19, 'epoch_patience': 9}. Best is trial 0 with value: 0.6702979687742381.
Using devices: cuda, cuda
2026-02-12 13:58:19 - INFO - Using devices: cuda, cuda
Devices: cuda, cuda
2026-02-12 13:58:19 - INFO - Devices: cuda, cuda
Starting log
2026-02-12 13:58:19 - INFO - Starting log
Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
2026-02-12 13:58:19 - INFO - Dataset: humanitarian9, Event: hurricane_florence_2018, N: 5, Seed: 1234, HF Model: GPT-4o, NumShots: 5, PLM: bert-tweet
Learning Rate: 0.00011666983036609838
Weight Decay: 0.00395893025387279
Batch Size: 16
No. Epochs: 19
Epoch Patience: 7
 Accumulation Steps: 4
2026-02-12 13:58:19 - INFO - Learning Rate: 0.00011666983036609838
Weight Decay: 0.00395893025387279
Batch Size: 16
No. Epochs: 19
Epoch Patience: 7
 Accumulation Steps: 4
Using Bert Tweet model: bert-tweet
Generating initial weights
2026-02-12 13:58:20 - INFO - Generating initial weights
Time taken for Epoch 1:18.28 - F1: 0.0155
2026-02-12 13:58:42 - INFO - Time taken for Epoch 1:18.28 - F1: 0.0155
Time taken for Epoch 2:18.23 - F1: 0.0155
2026-02-12 13:59:00 - INFO - Time taken for Epoch 2:18.23 - F1: 0.0155
Time taken for Epoch 3:18.28 - F1: 0.0155
2026-02-12 13:59:19 - INFO - Time taken for Epoch 3:18.28 - F1: 0.0155
Time taken for Epoch 4:18.29 - F1: 0.0155
2026-02-12 13:59:37 - INFO - Time taken for Epoch 4:18.29 - F1: 0.0155
Time taken for Epoch 5:18.29 - F1: 0.0155
2026-02-12 13:59:55 - INFO - Time taken for Epoch 5:18.29 - F1: 0.0155
Time taken for Epoch 6:18.30 - F1: 0.0348
2026-02-12 14:00:14 - INFO - Time taken for Epoch 6:18.30 - F1: 0.0348
Time taken for Epoch 7:18.31 - F1: 0.0592
2026-02-12 14:00:32 - INFO - Time taken for Epoch 7:18.31 - F1: 0.0592
Time taken for Epoch 8:18.30 - F1: 0.2117
2026-02-12 14:00:50 - INFO - Time taken for Epoch 8:18.30 - F1: 0.2117
Time taken for Epoch 9:18.31 - F1: 0.2925
2026-02-12 14:01:08 - INFO - Time taken for Epoch 9:18.31 - F1: 0.2925
Time taken for Epoch 10:18.31 - F1: 0.3370
2026-02-12 14:01:27 - INFO - Time taken for Epoch 10:18.31 - F1: 0.3370
Time taken for Epoch 11:18.32 - F1: 0.3404
2026-02-12 14:01:45 - INFO - Time taken for Epoch 11:18.32 - F1: 0.3404
Time taken for Epoch 12:18.33 - F1: 0.3247
2026-02-12 14:02:03 - INFO - Time taken for Epoch 12:18.33 - F1: 0.3247
Time taken for Epoch 13:18.30 - F1: 0.3151
2026-02-12 14:02:22 - INFO - Time taken for Epoch 13:18.30 - F1: 0.3151
Time taken for Epoch 14:18.32 - F1: 0.3293
2026-02-12 14:02:40 - INFO - Time taken for Epoch 14:18.32 - F1: 0.3293
Time taken for Epoch 15:18.32 - F1: 0.3567
2026-02-12 14:02:58 - INFO - Time taken for Epoch 15:18.32 - F1: 0.3567
Time taken for Epoch 16:18.33 - F1: 0.3711
2026-02-12 14:03:17 - INFO - Time taken for Epoch 16:18.33 - F1: 0.3711
Time taken for Epoch 17:18.31 - F1: 0.3788
2026-02-12 14:03:35 - INFO - Time taken for Epoch 17:18.31 - F1: 0.3788
Time taken for Epoch 18:18.36 - F1: 0.3771
2026-02-12 14:03:53 - INFO - Time taken for Epoch 18:18.36 - F1: 0.3771
Time taken for Epoch 19:18.34 - F1: 0.3730
2026-02-12 14:04:12 - INFO - Time taken for Epoch 19:18.34 - F1: 0.3730
Best F1:0.3788 - Best Epoch:17
2026-02-12 14:04:12 - INFO - Best F1:0.3788 - Best Epoch:17
Using Bert Tweet model: bert-tweet
Starting co-training
2026-02-12 14:04:13 - INFO - Starting co-training
Time taken for Epoch 1: 25.44s - F1: 0.57174590
2026-02-12 14:04:39 - INFO - Time taken for Epoch 1: 25.44s - F1: 0.57174590
Time taken for Epoch 2: 26.55s - F1: 0.53878355
2026-02-12 14:05:05 - INFO - Time taken for Epoch 2: 26.55s - F1: 0.53878355
Time taken for Epoch 3: 25.48s - F1: 0.59744183
2026-02-12 14:05:31 - INFO - Time taken for Epoch 3: 25.48s - F1: 0.59744183
Time taken for Epoch 4: 26.71s - F1: 0.58796910
2026-02-12 14:05:58 - INFO - Time taken for Epoch 4: 26.71s - F1: 0.58796910
Time taken for Epoch 5: 25.49s - F1: 0.58873219
2026-02-12 14:06:23 - INFO - Time taken for Epoch 5: 25.49s - F1: 0.58873219
Time taken for Epoch 6: 25.49s - F1: 0.58826851
2026-02-12 14:06:49 - INFO - Time taken for Epoch 6: 25.49s - F1: 0.58826851
Time taken for Epoch 7: 25.50s - F1: 0.58376721
2026-02-12 14:07:14 - INFO - Time taken for Epoch 7: 25.50s - F1: 0.58376721
Time taken for Epoch 8: 25.50s - F1: 0.60722517
2026-02-12 14:07:40 - INFO - Time taken for Epoch 8: 25.50s - F1: 0.60722517
Time taken for Epoch 9: 26.78s - F1: 0.62303310
2026-02-12 14:08:06 - INFO - Time taken for Epoch 9: 26.78s - F1: 0.62303310
Time taken for Epoch 10: 26.74s - F1: 0.58316356
2026-02-12 14:08:33 - INFO - Time taken for Epoch 10: 26.74s - F1: 0.58316356
Time taken for Epoch 11: 25.51s - F1: 0.56046639
2026-02-12 14:08:59 - INFO - Time taken for Epoch 11: 25.51s - F1: 0.56046639
Time taken for Epoch 12: 25.54s - F1: 0.59273752
2026-02-12 14:09:24 - INFO - Time taken for Epoch 12: 25.54s - F1: 0.59273752
Time taken for Epoch 13: 25.52s - F1: 0.59578366
2026-02-12 14:09:50 - INFO - Time taken for Epoch 13: 25.52s - F1: 0.59578366
Time taken for Epoch 14: 25.51s - F1: 0.61144050
2026-02-12 14:10:15 - INFO - Time taken for Epoch 14: 25.51s - F1: 0.61144050
Time taken for Epoch 15: 25.50s - F1: 0.58867583
2026-02-12 14:10:41 - INFO - Time taken for Epoch 15: 25.50s - F1: 0.58867583
Time taken for Epoch 16: 25.52s - F1: 0.59617455
2026-02-12 14:11:06 - INFO - Time taken for Epoch 16: 25.52s - F1: 0.59617455
Performance not improving for 7 consecutive epochs.
Performance not improving for 7 consecutive epochs.
2026-02-12 14:11:06 - INFO - Performance not improving for 7 consecutive epochs.
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/co_trained_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Fine-tuning models
2026-02-12 14:11:09 - INFO - Fine-tuning models
Time taken for Epoch 1:2.59 - F1: 0.6154
2026-02-12 14:11:12 - INFO - Time taken for Epoch 1:2.59 - F1: 0.6154
Time taken for Epoch 2:3.63 - F1: 0.6165
2026-02-12 14:11:16 - INFO - Time taken for Epoch 2:3.63 - F1: 0.6165
Time taken for Epoch 3:3.77 - F1: 0.6126
2026-02-12 14:11:19 - INFO - Time taken for Epoch 3:3.77 - F1: 0.6126
Time taken for Epoch 4:2.57 - F1: 0.6121
2026-02-12 14:11:22 - INFO - Time taken for Epoch 4:2.57 - F1: 0.6121
Time taken for Epoch 5:2.57 - F1: 0.6541
2026-02-12 14:11:24 - INFO - Time taken for Epoch 5:2.57 - F1: 0.6541
Time taken for Epoch 6:3.75 - F1: 0.6255
2026-02-12 14:11:28 - INFO - Time taken for Epoch 6:3.75 - F1: 0.6255
Time taken for Epoch 7:2.56 - F1: 0.6224
2026-02-12 14:11:31 - INFO - Time taken for Epoch 7:2.56 - F1: 0.6224
Time taken for Epoch 8:2.57 - F1: 0.6248
2026-02-12 14:11:33 - INFO - Time taken for Epoch 8:2.57 - F1: 0.6248
Time taken for Epoch 9:2.57 - F1: 0.6269
2026-02-12 14:11:36 - INFO - Time taken for Epoch 9:2.57 - F1: 0.6269
Time taken for Epoch 10:2.57 - F1: 0.6569
2026-02-12 14:11:39 - INFO - Time taken for Epoch 10:2.57 - F1: 0.6569
Time taken for Epoch 11:3.76 - F1: 0.6475
2026-02-12 14:11:42 - INFO - Time taken for Epoch 11:3.76 - F1: 0.6475
Time taken for Epoch 12:2.57 - F1: 0.6506
2026-02-12 14:11:45 - INFO - Time taken for Epoch 12:2.57 - F1: 0.6506
Time taken for Epoch 13:2.57 - F1: 0.6404
2026-02-12 14:11:47 - INFO - Time taken for Epoch 13:2.57 - F1: 0.6404
Time taken for Epoch 14:2.57 - F1: 0.6492
2026-02-12 14:11:50 - INFO - Time taken for Epoch 14:2.57 - F1: 0.6492
Time taken for Epoch 15:2.57 - F1: 0.6451
2026-02-12 14:11:53 - INFO - Time taken for Epoch 15:2.57 - F1: 0.6451
Time taken for Epoch 16:2.57 - F1: 0.6419
2026-02-12 14:11:55 - INFO - Time taken for Epoch 16:2.57 - F1: 0.6419
Time taken for Epoch 17:2.57 - F1: 0.6612
2026-02-12 14:11:58 - INFO - Time taken for Epoch 17:2.57 - F1: 0.6612
Time taken for Epoch 18:3.74 - F1: 0.6610
2026-02-12 14:12:01 - INFO - Time taken for Epoch 18:3.74 - F1: 0.6610
Time taken for Epoch 19:2.57 - F1: 0.6597
2026-02-12 14:12:04 - INFO - Time taken for Epoch 19:2.57 - F1: 0.6597
Time taken for Epoch 20:2.57 - F1: 0.6554
2026-02-12 14:12:07 - INFO - Time taken for Epoch 20:2.57 - F1: 0.6554
Time taken for Epoch 21:2.57 - F1: 0.6445
2026-02-12 14:12:09 - INFO - Time taken for Epoch 21:2.57 - F1: 0.6445
Time taken for Epoch 22:2.57 - F1: 0.6427
2026-02-12 14:12:12 - INFO - Time taken for Epoch 22:2.57 - F1: 0.6427
Time taken for Epoch 23:2.57 - F1: 0.6248
2026-02-12 14:12:14 - INFO - Time taken for Epoch 23:2.57 - F1: 0.6248
Time taken for Epoch 24:2.58 - F1: 0.6240
2026-02-12 14:12:17 - INFO - Time taken for Epoch 24:2.58 - F1: 0.6240
Time taken for Epoch 25:2.58 - F1: 0.6185
2026-02-12 14:12:19 - INFO - Time taken for Epoch 25:2.58 - F1: 0.6185
Time taken for Epoch 26:2.58 - F1: 0.6133
2026-02-12 14:12:22 - INFO - Time taken for Epoch 26:2.58 - F1: 0.6133
Time taken for Epoch 27:2.57 - F1: 0.6055
2026-02-12 14:12:25 - INFO - Time taken for Epoch 27:2.57 - F1: 0.6055
Performance not improving for 10 consecutive epochs.
2026-02-12 14:12:25 - INFO - Performance not improving for 10 consecutive epochs.
Best F1:0.6612 - Best Epoch:16
2026-02-12 14:12:25 - INFO - Best F1:0.6612 - Best Epoch:16
Using Bert Tweet model: bert-tweet
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_1_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt
Successfully deleted /homes/bharanibala/llmcot/saved_models/humanitarian9/optuna-bertweet-hurricane-florence-2018-label5-set1/final_model_2_optuna-bertweet-hurricane-florence-2018-label5-set1_gpt4o_5_shot_bert-tweet_5_seed_1234.pt


Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6472, Test ECE: 0.0517
2026-02-12 14:12:32 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian9, NumShots: 5, N: 5 Test SEED: 1234 F1: 0.6472, Test ECE: 0.0517
All results: {'f1_macro': 0.6471993568077172, 'ece': np.float64(0.05166600977192955)}
2026-02-12 14:12:32 - INFO - All results: {'f1_macro': 0.6471993568077172, 'ece': np.float64(0.05166600977192955)}

Total time taken: 853.45 seconds
2026-02-12 14:12:32 - INFO - 
Total time taken: 853.45 seconds
2026-02-12 14:12:32 - INFO - Trial 9 finished with value: 0.6471993568077172 and parameters: {'learning_rate': 0.00011666983036609838, 'weight_decay': 0.00395893025387279, 'batch_size': 16, 'co_train_epochs': 19, 'epoch_patience': 7}. Best is trial 0 with value: 0.6702979687742381.

[BEST TRIAL RESULTS]
2026-02-12 14:12:32 - INFO - 
[BEST TRIAL RESULTS]
F1 Score: 0.6703
2026-02-12 14:12:32 - INFO - F1 Score: 0.6703
Params: {'learning_rate': 2.114113037844942e-05, 'weight_decay': 0.004720016359061808, 'batch_size': 32, 'co_train_epochs': 13, 'epoch_patience': 7}
2026-02-12 14:12:32 - INFO - Params: {'learning_rate': 2.114113037844942e-05, 'weight_decay': 0.004720016359061808, 'batch_size': 32, 'co_train_epochs': 13, 'epoch_patience': 7}
  learning_rate: 2.114113037844942e-05
2026-02-12 14:12:32 - INFO -   learning_rate: 2.114113037844942e-05
  weight_decay: 0.004720016359061808
2026-02-12 14:12:32 - INFO -   weight_decay: 0.004720016359061808
  batch_size: 32
2026-02-12 14:12:32 - INFO -   batch_size: 32
  co_train_epochs: 13
2026-02-12 14:12:32 - INFO -   co_train_epochs: 13
  epoch_patience: 7
2026-02-12 14:12:32 - INFO -   epoch_patience: 7

Total time taken: 6862.48 seconds
2026-02-12 14:12:32 - INFO - 
Total time taken: 6862.48 seconds