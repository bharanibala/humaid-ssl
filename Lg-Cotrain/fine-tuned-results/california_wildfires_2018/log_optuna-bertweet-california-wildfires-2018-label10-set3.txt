2026-02-13 16:18:24 - INFO - 
[Optuna] Starting hyperparameter search with 10 trials.
2026-02-13 16:18:24 - INFO - A new study created in memory with name: study_humanitarian10_california_wildfires_2018
2026-02-13 16:18:24 - INFO - Using devices: cuda, cuda
2026-02-13 16:18:24 - INFO - Devices: cuda, cuda
2026-02-13 16:18:24 - INFO - Starting log
2026-02-13 16:18:24 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:18:27 - INFO - Learning Rate: 2.9905290031458952e-05
Weight Decay: 0.0049314306855903454
Batch Size: 8
No. Epochs: 10
Epoch Patience: 9
 Accumulation Steps: 8
2026-02-13 16:18:27 - INFO - Generating initial weights
2026-02-13 16:18:46 - INFO - Time taken for Epoch 1:16.95 - F1: 0.0120
2026-02-13 16:19:03 - INFO - Time taken for Epoch 2:16.80 - F1: 0.0120
2026-02-13 16:19:19 - INFO - Time taken for Epoch 3:16.80 - F1: 0.0120
2026-02-13 16:19:36 - INFO - Time taken for Epoch 4:16.79 - F1: 0.0120
2026-02-13 16:19:53 - INFO - Time taken for Epoch 5:16.81 - F1: 0.0120
2026-02-13 16:20:10 - INFO - Time taken for Epoch 6:16.80 - F1: 0.0120
2026-02-13 16:20:27 - INFO - Time taken for Epoch 7:16.80 - F1: 0.0120
2026-02-13 16:20:44 - INFO - Time taken for Epoch 8:16.83 - F1: 0.0120
2026-02-13 16:21:00 - INFO - Time taken for Epoch 9:16.80 - F1: 0.0120
2026-02-13 16:21:17 - INFO - Time taken for Epoch 10:16.82 - F1: 0.0120
2026-02-13 16:21:17 - INFO - Best F1:0.0120 - Best Epoch:1
2026-02-13 16:21:18 - INFO - Starting co-training
2026-02-13 16:21:42 - INFO - Time taken for Epoch 1: 24.17s - F1: 0.29881579
2026-02-13 16:22:07 - INFO - Time taken for Epoch 2: 24.77s - F1: 0.30193345
2026-02-13 16:22:32 - INFO - Time taken for Epoch 3: 24.90s - F1: 0.38364645
2026-02-13 16:22:57 - INFO - Time taken for Epoch 4: 24.78s - F1: 0.46449760
2026-02-13 16:23:22 - INFO - Time taken for Epoch 5: 24.82s - F1: 0.46759071
2026-02-13 16:23:47 - INFO - Time taken for Epoch 6: 25.69s - F1: 0.46896006
2026-02-13 16:24:13 - INFO - Time taken for Epoch 7: 25.95s - F1: 0.49460525
2026-02-13 16:24:38 - INFO - Time taken for Epoch 8: 24.80s - F1: 0.47848758
2026-02-13 16:25:02 - INFO - Time taken for Epoch 9: 24.21s - F1: 0.51477895
2026-02-13 16:25:27 - INFO - Time taken for Epoch 10: 24.78s - F1: 0.50485352
2026-02-13 16:25:29 - INFO - Fine-tuning models
2026-02-13 16:25:31 - INFO - Time taken for Epoch 1:2.71 - F1: 0.5304
2026-02-13 16:25:35 - INFO - Time taken for Epoch 2:3.42 - F1: 0.5357
2026-02-13 16:25:38 - INFO - Time taken for Epoch 3:3.40 - F1: 0.5580
2026-02-13 16:25:42 - INFO - Time taken for Epoch 4:3.39 - F1: 0.5568
2026-02-13 16:25:44 - INFO - Time taken for Epoch 5:2.69 - F1: 0.5459
2026-02-13 16:25:47 - INFO - Time taken for Epoch 6:2.69 - F1: 0.5458
2026-02-13 16:25:50 - INFO - Time taken for Epoch 7:2.69 - F1: 0.5535
2026-02-13 16:25:52 - INFO - Time taken for Epoch 8:2.69 - F1: 0.5508
2026-02-13 16:25:55 - INFO - Time taken for Epoch 9:2.69 - F1: 0.5574
2026-02-13 16:25:58 - INFO - Time taken for Epoch 10:2.69 - F1: 0.5532
2026-02-13 16:26:00 - INFO - Time taken for Epoch 11:2.69 - F1: 0.5641
2026-02-13 16:26:04 - INFO - Time taken for Epoch 12:3.32 - F1: 0.5656
2026-02-13 16:26:15 - INFO - Time taken for Epoch 13:11.17 - F1: 0.5509
2026-02-13 16:26:18 - INFO - Time taken for Epoch 14:2.70 - F1: 0.5424
2026-02-13 16:26:20 - INFO - Time taken for Epoch 15:2.69 - F1: 0.5412
2026-02-13 16:26:23 - INFO - Time taken for Epoch 16:2.70 - F1: 0.5383
2026-02-13 16:26:26 - INFO - Time taken for Epoch 17:2.69 - F1: 0.5428
2026-02-13 16:26:28 - INFO - Time taken for Epoch 18:2.69 - F1: 0.5390
2026-02-13 16:26:31 - INFO - Time taken for Epoch 19:2.69 - F1: 0.5426
2026-02-13 16:26:34 - INFO - Time taken for Epoch 20:2.69 - F1: 0.5499
2026-02-13 16:26:36 - INFO - Time taken for Epoch 21:2.69 - F1: 0.5487
2026-02-13 16:26:39 - INFO - Time taken for Epoch 22:2.69 - F1: 0.5482
2026-02-13 16:26:39 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:26:39 - INFO - Best F1:0.5656 - Best Epoch:11
2026-02-13 16:26:45 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5818, Test ECE: 0.0939
2026-02-13 16:26:45 - INFO - All results: {'f1_macro': 0.5818489223990179, 'ece': np.float64(0.09392752119484704)}
2026-02-13 16:26:45 - INFO - 
Total time taken: 501.15 seconds
2026-02-13 16:26:45 - INFO - Trial 0 finished with value: 0.5818489223990179 and parameters: {'learning_rate': 2.9905290031458952e-05, 'weight_decay': 0.0049314306855903454, 'batch_size': 8, 'co_train_epochs': 10, 'epoch_patience': 9}. Best is trial 0 with value: 0.5818489223990179.
2026-02-13 16:26:45 - INFO - Using devices: cuda, cuda
2026-02-13 16:26:45 - INFO - Devices: cuda, cuda
2026-02-13 16:26:45 - INFO - Starting log
2026-02-13 16:26:45 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:26:45 - INFO - Learning Rate: 1.922182735361119e-05
Weight Decay: 1.819007002111511e-05
Batch Size: 16
No. Epochs: 5
Epoch Patience: 10
 Accumulation Steps: 4
2026-02-13 16:26:46 - INFO - Generating initial weights
2026-02-13 16:27:02 - INFO - Time taken for Epoch 1:14.74 - F1: 0.0120
2026-02-13 16:27:17 - INFO - Time taken for Epoch 2:14.72 - F1: 0.0120
2026-02-13 16:27:31 - INFO - Time taken for Epoch 3:14.71 - F1: 0.0120
2026-02-13 16:27:46 - INFO - Time taken for Epoch 4:14.70 - F1: 0.0120
2026-02-13 16:28:01 - INFO - Time taken for Epoch 5:14.70 - F1: 0.0120
2026-02-13 16:28:01 - INFO - Best F1:0.0120 - Best Epoch:1
2026-02-13 16:28:01 - INFO - Starting co-training
2026-02-13 16:28:26 - INFO - Time taken for Epoch 1: 24.47s - F1: 0.29139894
2026-02-13 16:28:51 - INFO - Time taken for Epoch 2: 25.06s - F1: 0.30048717
2026-02-13 16:29:16 - INFO - Time taken for Epoch 3: 25.16s - F1: 0.35958188
2026-02-13 16:29:41 - INFO - Time taken for Epoch 4: 25.09s - F1: 0.46308640
2026-02-13 16:30:06 - INFO - Time taken for Epoch 5: 25.17s - F1: 0.45540440
2026-02-13 16:30:08 - INFO - Fine-tuning models
2026-02-13 16:30:10 - INFO - Time taken for Epoch 1:2.33 - F1: 0.4493
2026-02-13 16:30:13 - INFO - Time taken for Epoch 2:2.93 - F1: 0.4370
2026-02-13 16:30:15 - INFO - Time taken for Epoch 3:2.33 - F1: 0.4539
2026-02-13 16:30:18 - INFO - Time taken for Epoch 4:2.95 - F1: 0.4544
2026-02-13 16:30:21 - INFO - Time taken for Epoch 5:2.95 - F1: 0.4647
2026-02-13 16:30:24 - INFO - Time taken for Epoch 6:2.96 - F1: 0.4735
2026-02-13 16:30:27 - INFO - Time taken for Epoch 7:2.95 - F1: 0.4850
2026-02-13 16:30:30 - INFO - Time taken for Epoch 8:2.95 - F1: 0.4889
2026-02-13 16:30:33 - INFO - Time taken for Epoch 9:2.96 - F1: 0.5426
2026-02-13 16:30:36 - INFO - Time taken for Epoch 10:2.96 - F1: 0.5452
2026-02-13 16:30:39 - INFO - Time taken for Epoch 11:2.98 - F1: 0.5634
2026-02-13 16:30:42 - INFO - Time taken for Epoch 12:2.98 - F1: 0.5606
2026-02-13 16:30:44 - INFO - Time taken for Epoch 13:2.32 - F1: 0.5502
2026-02-13 16:30:47 - INFO - Time taken for Epoch 14:2.33 - F1: 0.5568
2026-02-13 16:30:49 - INFO - Time taken for Epoch 15:2.32 - F1: 0.5598
2026-02-13 16:30:51 - INFO - Time taken for Epoch 16:2.32 - F1: 0.5632
2026-02-13 16:30:54 - INFO - Time taken for Epoch 17:2.32 - F1: 0.5637
2026-02-13 16:30:57 - INFO - Time taken for Epoch 18:2.97 - F1: 0.5622
2026-02-13 16:30:59 - INFO - Time taken for Epoch 19:2.32 - F1: 0.5648
2026-02-13 16:31:02 - INFO - Time taken for Epoch 20:3.00 - F1: 0.5672
2026-02-13 16:31:05 - INFO - Time taken for Epoch 21:3.00 - F1: 0.5658
2026-02-13 16:31:07 - INFO - Time taken for Epoch 22:2.32 - F1: 0.5665
2026-02-13 16:31:10 - INFO - Time taken for Epoch 23:2.32 - F1: 0.5656
2026-02-13 16:31:12 - INFO - Time taken for Epoch 24:2.32 - F1: 0.5718
2026-02-13 16:31:15 - INFO - Time taken for Epoch 25:3.00 - F1: 0.5750
2026-02-13 16:31:18 - INFO - Time taken for Epoch 26:2.97 - F1: 0.5784
2026-02-13 16:31:21 - INFO - Time taken for Epoch 27:3.05 - F1: 0.5820
2026-02-13 16:31:24 - INFO - Time taken for Epoch 28:3.02 - F1: 0.5860
2026-02-13 16:31:27 - INFO - Time taken for Epoch 29:3.21 - F1: 0.5892
2026-02-13 16:31:37 - INFO - Time taken for Epoch 30:9.85 - F1: 0.5900
2026-02-13 16:31:40 - INFO - Time taken for Epoch 31:3.08 - F1: 0.5842
2026-02-13 16:31:42 - INFO - Time taken for Epoch 32:2.31 - F1: 0.5850
2026-02-13 16:31:45 - INFO - Time taken for Epoch 33:2.32 - F1: 0.5824
2026-02-13 16:31:47 - INFO - Time taken for Epoch 34:2.32 - F1: 0.5894
2026-02-13 16:31:49 - INFO - Time taken for Epoch 35:2.32 - F1: 0.5872
2026-02-13 16:31:52 - INFO - Time taken for Epoch 36:2.32 - F1: 0.5903
2026-02-13 16:31:55 - INFO - Time taken for Epoch 37:2.99 - F1: 0.5994
2026-02-13 16:31:58 - INFO - Time taken for Epoch 38:2.96 - F1: 0.6057
2026-02-13 16:32:01 - INFO - Time taken for Epoch 39:2.94 - F1: 0.6057
2026-02-13 16:32:03 - INFO - Time taken for Epoch 40:2.32 - F1: 0.5985
2026-02-13 16:32:05 - INFO - Time taken for Epoch 41:2.32 - F1: 0.5922
2026-02-13 16:32:08 - INFO - Time taken for Epoch 42:2.32 - F1: 0.5913
2026-02-13 16:32:10 - INFO - Time taken for Epoch 43:2.33 - F1: 0.5949
2026-02-13 16:32:12 - INFO - Time taken for Epoch 44:2.33 - F1: 0.5942
2026-02-13 16:32:14 - INFO - Time taken for Epoch 45:2.33 - F1: 0.5926
2026-02-13 16:32:17 - INFO - Time taken for Epoch 46:2.33 - F1: 0.5890
2026-02-13 16:32:19 - INFO - Time taken for Epoch 47:2.32 - F1: 0.5895
2026-02-13 16:32:21 - INFO - Time taken for Epoch 48:2.32 - F1: 0.5913
2026-02-13 16:32:21 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:32:21 - INFO - Best F1:0.6057 - Best Epoch:37
2026-02-13 16:32:27 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5869, Test ECE: 0.1073
2026-02-13 16:32:27 - INFO - All results: {'f1_macro': 0.5869324987058845, 'ece': np.float64(0.10730440546436558)}
2026-02-13 16:32:27 - INFO - 
Total time taken: 341.64 seconds
2026-02-13 16:32:27 - INFO - Trial 1 finished with value: 0.5869324987058845 and parameters: {'learning_rate': 1.922182735361119e-05, 'weight_decay': 1.819007002111511e-05, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 10}. Best is trial 1 with value: 0.5869324987058845.
2026-02-13 16:32:27 - INFO - Using devices: cuda, cuda
2026-02-13 16:32:27 - INFO - Devices: cuda, cuda
2026-02-13 16:32:27 - INFO - Starting log
2026-02-13 16:32:27 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:32:27 - INFO - Learning Rate: 2.1352408798432105e-05
Weight Decay: 0.0005411660233026563
Batch Size: 24
No. Epochs: 6
Epoch Patience: 5
 Accumulation Steps: 2
2026-02-13 16:32:28 - INFO - Generating initial weights
2026-02-13 16:32:43 - INFO - Time taken for Epoch 1:13.73 - F1: 0.1270
2026-02-13 16:32:56 - INFO - Time taken for Epoch 2:13.70 - F1: 0.0500
2026-02-13 16:33:10 - INFO - Time taken for Epoch 3:13.72 - F1: 0.0130
2026-02-13 16:33:24 - INFO - Time taken for Epoch 4:13.71 - F1: 0.0120
2026-02-13 16:33:37 - INFO - Time taken for Epoch 5:13.71 - F1: 0.0120
2026-02-13 16:33:51 - INFO - Time taken for Epoch 6:13.71 - F1: 0.0120
2026-02-13 16:33:51 - INFO - Best F1:0.1270 - Best Epoch:1
2026-02-13 16:33:52 - INFO - Starting co-training
2026-02-13 16:34:21 - INFO - Time taken for Epoch 1: 29.22s - F1: 0.33599938
2026-02-13 16:34:51 - INFO - Time taken for Epoch 2: 29.77s - F1: 0.38849608
2026-02-13 16:35:21 - INFO - Time taken for Epoch 3: 29.82s - F1: 0.43579876
2026-02-13 16:35:50 - INFO - Time taken for Epoch 4: 29.82s - F1: 0.54429738
2026-02-13 16:36:20 - INFO - Time taken for Epoch 5: 29.89s - F1: 0.59717071
2026-02-13 16:36:50 - INFO - Time taken for Epoch 6: 29.77s - F1: 0.60718743
2026-02-13 16:36:52 - INFO - Fine-tuning models
2026-02-13 16:36:54 - INFO - Time taken for Epoch 1:2.20 - F1: 0.5797
2026-02-13 16:36:57 - INFO - Time taken for Epoch 2:2.81 - F1: 0.5819
2026-02-13 16:37:00 - INFO - Time taken for Epoch 3:2.83 - F1: 0.5765
2026-02-13 16:37:02 - INFO - Time taken for Epoch 4:2.23 - F1: 0.5691
2026-02-13 16:37:04 - INFO - Time taken for Epoch 5:2.22 - F1: 0.5694
2026-02-13 16:37:07 - INFO - Time taken for Epoch 6:2.21 - F1: 0.5628
2026-02-13 16:37:09 - INFO - Time taken for Epoch 7:2.21 - F1: 0.5654
2026-02-13 16:37:11 - INFO - Time taken for Epoch 8:2.20 - F1: 0.5762
2026-02-13 16:37:13 - INFO - Time taken for Epoch 9:2.21 - F1: 0.5823
2026-02-13 16:37:16 - INFO - Time taken for Epoch 10:2.84 - F1: 0.5897
2026-02-13 16:37:19 - INFO - Time taken for Epoch 11:2.85 - F1: 0.5867
2026-02-13 16:37:21 - INFO - Time taken for Epoch 12:2.19 - F1: 0.5969
2026-02-13 16:37:24 - INFO - Time taken for Epoch 13:2.83 - F1: 0.5963
2026-02-13 16:37:26 - INFO - Time taken for Epoch 14:2.19 - F1: 0.5999
2026-02-13 16:37:29 - INFO - Time taken for Epoch 15:2.89 - F1: 0.5840
2026-02-13 16:37:31 - INFO - Time taken for Epoch 16:2.23 - F1: 0.5865
2026-02-13 16:37:34 - INFO - Time taken for Epoch 17:2.29 - F1: 0.5919
2026-02-13 16:37:36 - INFO - Time taken for Epoch 18:2.24 - F1: 0.5911
2026-02-13 16:37:38 - INFO - Time taken for Epoch 19:2.29 - F1: 0.5945
2026-02-13 16:37:40 - INFO - Time taken for Epoch 20:2.23 - F1: 0.6012
2026-02-13 16:37:43 - INFO - Time taken for Epoch 21:2.91 - F1: 0.5997
2026-02-13 16:37:45 - INFO - Time taken for Epoch 22:2.23 - F1: 0.5997
2026-02-13 16:37:48 - INFO - Time taken for Epoch 23:2.28 - F1: 0.5968
2026-02-13 16:37:50 - INFO - Time taken for Epoch 24:2.23 - F1: 0.5970
2026-02-13 16:37:52 - INFO - Time taken for Epoch 25:2.28 - F1: 0.5952
2026-02-13 16:37:54 - INFO - Time taken for Epoch 26:2.24 - F1: 0.5920
2026-02-13 16:37:57 - INFO - Time taken for Epoch 27:2.28 - F1: 0.5971
2026-02-13 16:37:59 - INFO - Time taken for Epoch 28:2.23 - F1: 0.5970
2026-02-13 16:38:01 - INFO - Time taken for Epoch 29:2.28 - F1: 0.5968
2026-02-13 16:38:03 - INFO - Time taken for Epoch 30:2.24 - F1: 0.5971
2026-02-13 16:38:03 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:38:03 - INFO - Best F1:0.6012 - Best Epoch:19
2026-02-13 16:38:08 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6397, Test ECE: 0.0553
2026-02-13 16:38:08 - INFO - All results: {'f1_macro': 0.6397259092532659, 'ece': np.float64(0.05532946974589839)}
2026-02-13 16:38:08 - INFO - 
Total time taken: 341.72 seconds
2026-02-13 16:38:08 - INFO - Trial 2 finished with value: 0.6397259092532659 and parameters: {'learning_rate': 2.1352408798432105e-05, 'weight_decay': 0.0005411660233026563, 'batch_size': 24, 'co_train_epochs': 6, 'epoch_patience': 5}. Best is trial 2 with value: 0.6397259092532659.
2026-02-13 16:38:08 - INFO - Using devices: cuda, cuda
2026-02-13 16:38:08 - INFO - Devices: cuda, cuda
2026-02-13 16:38:08 - INFO - Starting log
2026-02-13 16:38:08 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:38:09 - INFO - Learning Rate: 9.182215624098082e-05
Weight Decay: 0.00019874552376444355
Batch Size: 16
No. Epochs: 5
Epoch Patience: 9
 Accumulation Steps: 4
2026-02-13 16:38:09 - INFO - Generating initial weights
2026-02-13 16:38:25 - INFO - Time taken for Epoch 1:14.80 - F1: 0.0120
2026-02-13 16:38:40 - INFO - Time taken for Epoch 2:14.74 - F1: 0.0120
2026-02-13 16:38:55 - INFO - Time taken for Epoch 3:14.72 - F1: 0.0120
2026-02-13 16:39:10 - INFO - Time taken for Epoch 4:14.71 - F1: 0.0120
2026-02-13 16:39:24 - INFO - Time taken for Epoch 5:14.75 - F1: 0.0120
2026-02-13 16:39:24 - INFO - Best F1:0.0120 - Best Epoch:1
2026-02-13 16:39:25 - INFO - Starting co-training
2026-02-13 16:39:50 - INFO - Time taken for Epoch 1: 24.46s - F1: 0.36906251
2026-02-13 16:40:15 - INFO - Time taken for Epoch 2: 25.03s - F1: 0.34852046
2026-02-13 16:40:39 - INFO - Time taken for Epoch 3: 24.45s - F1: 0.43326578
2026-02-13 16:41:04 - INFO - Time taken for Epoch 4: 25.06s - F1: 0.45743061
2026-02-13 16:41:29 - INFO - Time taken for Epoch 5: 25.14s - F1: 0.42397443
2026-02-13 16:41:30 - INFO - Fine-tuning models
2026-02-13 16:41:33 - INFO - Time taken for Epoch 1:2.34 - F1: 0.4681
2026-02-13 16:41:36 - INFO - Time taken for Epoch 2:3.00 - F1: 0.5126
2026-02-13 16:41:39 - INFO - Time taken for Epoch 3:2.97 - F1: 0.5240
2026-02-13 16:41:42 - INFO - Time taken for Epoch 4:2.96 - F1: 0.5312
2026-02-13 16:41:45 - INFO - Time taken for Epoch 5:2.97 - F1: 0.5330
2026-02-13 16:41:48 - INFO - Time taken for Epoch 6:2.98 - F1: 0.5438
2026-02-13 16:41:51 - INFO - Time taken for Epoch 7:2.96 - F1: 0.5453
2026-02-13 16:41:54 - INFO - Time taken for Epoch 8:2.99 - F1: 0.5435
2026-02-13 16:41:56 - INFO - Time taken for Epoch 9:2.31 - F1: 0.5408
2026-02-13 16:41:58 - INFO - Time taken for Epoch 10:2.32 - F1: 0.5390
2026-02-13 16:42:01 - INFO - Time taken for Epoch 11:2.32 - F1: 0.5335
2026-02-13 16:42:03 - INFO - Time taken for Epoch 12:2.32 - F1: 0.5478
2026-02-13 16:42:06 - INFO - Time taken for Epoch 13:2.93 - F1: 0.5711
2026-02-13 16:42:17 - INFO - Time taken for Epoch 14:11.19 - F1: 0.5625
2026-02-13 16:42:19 - INFO - Time taken for Epoch 15:2.31 - F1: 0.5619
2026-02-13 16:42:22 - INFO - Time taken for Epoch 16:2.31 - F1: 0.5567
2026-02-13 16:42:24 - INFO - Time taken for Epoch 17:2.32 - F1: 0.5549
2026-02-13 16:42:26 - INFO - Time taken for Epoch 18:2.31 - F1: 0.5508
2026-02-13 16:42:29 - INFO - Time taken for Epoch 19:2.32 - F1: 0.5467
2026-02-13 16:42:31 - INFO - Time taken for Epoch 20:2.31 - F1: 0.5482
2026-02-13 16:42:33 - INFO - Time taken for Epoch 21:2.31 - F1: 0.5472
2026-02-13 16:42:36 - INFO - Time taken for Epoch 22:2.32 - F1: 0.5499
2026-02-13 16:42:38 - INFO - Time taken for Epoch 23:2.32 - F1: 0.5515
2026-02-13 16:42:38 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:42:38 - INFO - Best F1:0.5711 - Best Epoch:12
2026-02-13 16:42:43 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5570, Test ECE: 0.1152
2026-02-13 16:42:43 - INFO - All results: {'f1_macro': 0.5569932480262043, 'ece': np.float64(0.11519393080069054)}
2026-02-13 16:42:43 - INFO - 
Total time taken: 274.64 seconds
2026-02-13 16:42:43 - INFO - Trial 3 finished with value: 0.5569932480262043 and parameters: {'learning_rate': 9.182215624098082e-05, 'weight_decay': 0.00019874552376444355, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 9}. Best is trial 2 with value: 0.6397259092532659.
2026-02-13 16:42:43 - INFO - Using devices: cuda, cuda
2026-02-13 16:42:43 - INFO - Devices: cuda, cuda
2026-02-13 16:42:43 - INFO - Starting log
2026-02-13 16:42:43 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:42:43 - INFO - Learning Rate: 2.083425561767457e-05
Weight Decay: 9.382390164786078e-05
Batch Size: 8
No. Epochs: 15
Epoch Patience: 5
 Accumulation Steps: 8
2026-02-13 16:42:44 - INFO - Generating initial weights
2026-02-13 16:43:02 - INFO - Time taken for Epoch 1:16.90 - F1: 0.0130
2026-02-13 16:43:19 - INFO - Time taken for Epoch 2:16.83 - F1: 0.0120
2026-02-13 16:43:36 - INFO - Time taken for Epoch 3:16.84 - F1: 0.0120
2026-02-13 16:43:53 - INFO - Time taken for Epoch 4:16.85 - F1: 0.0120
2026-02-13 16:44:09 - INFO - Time taken for Epoch 5:16.83 - F1: 0.0120
2026-02-13 16:44:26 - INFO - Time taken for Epoch 6:16.84 - F1: 0.0120
2026-02-13 16:44:43 - INFO - Time taken for Epoch 7:16.84 - F1: 0.0120
2026-02-13 16:45:00 - INFO - Time taken for Epoch 8:16.86 - F1: 0.0120
2026-02-13 16:45:17 - INFO - Time taken for Epoch 9:16.90 - F1: 0.0120
2026-02-13 16:45:34 - INFO - Time taken for Epoch 10:16.87 - F1: 0.0120
2026-02-13 16:45:51 - INFO - Time taken for Epoch 11:16.84 - F1: 0.0120
2026-02-13 16:46:07 - INFO - Time taken for Epoch 12:16.84 - F1: 0.0120
2026-02-13 16:46:24 - INFO - Time taken for Epoch 13:16.84 - F1: 0.0120
2026-02-13 16:46:41 - INFO - Time taken for Epoch 14:16.85 - F1: 0.0253
2026-02-13 16:46:58 - INFO - Time taken for Epoch 15:16.84 - F1: 0.0253
2026-02-13 16:46:58 - INFO - Best F1:0.0253 - Best Epoch:14
2026-02-13 16:46:59 - INFO - Starting co-training
2026-02-13 16:47:24 - INFO - Time taken for Epoch 1: 24.76s - F1: 0.29284268
2026-02-13 16:47:48 - INFO - Time taken for Epoch 2: 24.85s - F1: 0.29761511
2026-02-13 16:48:14 - INFO - Time taken for Epoch 3: 25.43s - F1: 0.30040663
2026-02-13 16:48:39 - INFO - Time taken for Epoch 4: 25.00s - F1: 0.40146685
2026-02-13 16:49:04 - INFO - Time taken for Epoch 5: 25.28s - F1: 0.46194080
2026-02-13 16:49:29 - INFO - Time taken for Epoch 6: 24.93s - F1: 0.45553668
2026-02-13 16:49:53 - INFO - Time taken for Epoch 7: 24.32s - F1: 0.46947230
2026-02-13 16:50:19 - INFO - Time taken for Epoch 8: 25.23s - F1: 0.51314275
2026-02-13 16:50:44 - INFO - Time taken for Epoch 9: 25.67s - F1: 0.49340948
2026-02-13 16:51:09 - INFO - Time taken for Epoch 10: 24.52s - F1: 0.56470062
2026-02-13 16:51:34 - INFO - Time taken for Epoch 11: 25.30s - F1: 0.53103539
2026-02-13 16:51:59 - INFO - Time taken for Epoch 12: 24.79s - F1: 0.56233963
2026-02-13 16:52:25 - INFO - Time taken for Epoch 13: 25.82s - F1: 0.56030266
2026-02-13 16:52:50 - INFO - Time taken for Epoch 14: 24.80s - F1: 0.57293734
2026-02-13 16:53:14 - INFO - Time taken for Epoch 15: 24.97s - F1: 0.60828493
2026-02-13 16:53:17 - INFO - Fine-tuning models
2026-02-13 16:53:20 - INFO - Time taken for Epoch 1:2.89 - F1: 0.6069
2026-02-13 16:53:23 - INFO - Time taken for Epoch 2:3.35 - F1: 0.5847
2026-02-13 16:53:26 - INFO - Time taken for Epoch 3:2.70 - F1: 0.5826
2026-02-13 16:53:28 - INFO - Time taken for Epoch 4:2.70 - F1: 0.5796
2026-02-13 16:53:31 - INFO - Time taken for Epoch 5:2.70 - F1: 0.5737
2026-02-13 16:53:34 - INFO - Time taken for Epoch 6:2.71 - F1: 0.5652
2026-02-13 16:53:36 - INFO - Time taken for Epoch 7:2.70 - F1: 0.5638
2026-02-13 16:53:39 - INFO - Time taken for Epoch 8:2.69 - F1: 0.5623
2026-02-13 16:53:42 - INFO - Time taken for Epoch 9:2.69 - F1: 0.5653
2026-02-13 16:53:44 - INFO - Time taken for Epoch 10:2.70 - F1: 0.5659
2026-02-13 16:53:47 - INFO - Time taken for Epoch 11:2.71 - F1: 0.5730
2026-02-13 16:53:47 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:53:47 - INFO - Best F1:0.6069 - Best Epoch:0
2026-02-13 16:53:53 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5845, Test ECE: 0.0359
2026-02-13 16:53:53 - INFO - All results: {'f1_macro': 0.5845258707688205, 'ece': np.float64(0.03594864854904008)}
2026-02-13 16:53:53 - INFO - 
Total time taken: 669.98 seconds
2026-02-13 16:53:53 - INFO - Trial 4 finished with value: 0.5845258707688205 and parameters: {'learning_rate': 2.083425561767457e-05, 'weight_decay': 9.382390164786078e-05, 'batch_size': 8, 'co_train_epochs': 15, 'epoch_patience': 5}. Best is trial 2 with value: 0.6397259092532659.
2026-02-13 16:53:53 - INFO - Using devices: cuda, cuda
2026-02-13 16:53:53 - INFO - Devices: cuda, cuda
2026-02-13 16:53:53 - INFO - Starting log
2026-02-13 16:53:53 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:53:53 - INFO - Learning Rate: 0.0007808906845515562
Weight Decay: 0.0007283123695660817
Batch Size: 16
No. Epochs: 5
Epoch Patience: 6
 Accumulation Steps: 4
2026-02-13 16:53:54 - INFO - Generating initial weights
2026-02-13 16:54:10 - INFO - Time taken for Epoch 1:14.83 - F1: 0.0120
2026-02-13 16:54:25 - INFO - Time taken for Epoch 2:14.77 - F1: 0.0119
2026-02-13 16:54:40 - INFO - Time taken for Epoch 3:14.73 - F1: 0.0120
2026-02-13 16:54:54 - INFO - Time taken for Epoch 4:14.72 - F1: 0.0120
2026-02-13 16:55:09 - INFO - Time taken for Epoch 5:14.99 - F1: 0.0120
2026-02-13 16:55:09 - INFO - Best F1:0.0120 - Best Epoch:1
2026-02-13 16:55:10 - INFO - Starting co-training
2026-02-13 16:55:35 - INFO - Time taken for Epoch 1: 24.72s - F1: 0.01200000
2026-02-13 16:56:00 - INFO - Time taken for Epoch 2: 25.16s - F1: 0.03024831
2026-02-13 16:56:25 - INFO - Time taken for Epoch 3: 25.27s - F1: 0.03024831
2026-02-13 16:56:50 - INFO - Time taken for Epoch 4: 24.78s - F1: 0.03024831
2026-02-13 16:57:15 - INFO - Time taken for Epoch 5: 25.36s - F1: 0.03024831
2026-02-13 16:57:16 - INFO - Fine-tuning models
2026-02-13 16:57:19 - INFO - Time taken for Epoch 1:2.33 - F1: 0.0302
2026-02-13 16:57:22 - INFO - Time taken for Epoch 2:2.90 - F1: 0.0302
2026-02-13 16:57:24 - INFO - Time taken for Epoch 3:2.31 - F1: 0.0037
2026-02-13 16:57:26 - INFO - Time taken for Epoch 4:2.31 - F1: 0.0120
2026-02-13 16:57:29 - INFO - Time taken for Epoch 5:2.31 - F1: 0.0120
2026-02-13 16:57:31 - INFO - Time taken for Epoch 6:2.31 - F1: 0.0120
2026-02-13 16:57:33 - INFO - Time taken for Epoch 7:2.31 - F1: 0.0120
2026-02-13 16:57:36 - INFO - Time taken for Epoch 8:2.31 - F1: 0.0120
2026-02-13 16:57:38 - INFO - Time taken for Epoch 9:2.31 - F1: 0.0120
2026-02-13 16:57:40 - INFO - Time taken for Epoch 10:2.31 - F1: 0.0120
2026-02-13 16:57:43 - INFO - Time taken for Epoch 11:2.31 - F1: 0.0120
2026-02-13 16:57:43 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 16:57:43 - INFO - Best F1:0.0302 - Best Epoch:0
2026-02-13 16:57:48 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0303, Test ECE: 0.4182
2026-02-13 16:57:48 - INFO - All results: {'f1_macro': 0.030313588850174218, 'ece': np.float64(0.41819461819082804)}
2026-02-13 16:57:48 - INFO - 
Total time taken: 234.55 seconds
2026-02-13 16:57:48 - INFO - Trial 5 finished with value: 0.030313588850174218 and parameters: {'learning_rate': 0.0007808906845515562, 'weight_decay': 0.0007283123695660817, 'batch_size': 16, 'co_train_epochs': 5, 'epoch_patience': 6}. Best is trial 2 with value: 0.6397259092532659.
2026-02-13 16:57:48 - INFO - Using devices: cuda, cuda
2026-02-13 16:57:48 - INFO - Devices: cuda, cuda
2026-02-13 16:57:48 - INFO - Starting log
2026-02-13 16:57:48 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 16:57:48 - INFO - Learning Rate: 0.00023002532418304298
Weight Decay: 9.702953865598856e-05
Batch Size: 24
No. Epochs: 8
Epoch Patience: 10
 Accumulation Steps: 2
2026-02-13 16:57:48 - INFO - Generating initial weights
2026-02-13 16:58:04 - INFO - Time taken for Epoch 1:13.81 - F1: 0.0247
2026-02-13 16:58:17 - INFO - Time taken for Epoch 2:13.78 - F1: 0.0120
2026-02-13 16:58:31 - INFO - Time taken for Epoch 3:14.02 - F1: 0.0702
2026-02-13 16:58:45 - INFO - Time taken for Epoch 4:13.96 - F1: 0.1781
2026-02-13 16:58:59 - INFO - Time taken for Epoch 5:13.90 - F1: 0.2374
2026-02-13 16:59:13 - INFO - Time taken for Epoch 6:13.98 - F1: 0.3572
2026-02-13 16:59:27 - INFO - Time taken for Epoch 7:14.14 - F1: 0.3723
2026-02-13 16:59:42 - INFO - Time taken for Epoch 8:14.31 - F1: 0.4162
2026-02-13 16:59:42 - INFO - Best F1:0.4162 - Best Epoch:8
2026-02-13 16:59:42 - INFO - Starting co-training
2026-02-13 17:00:13 - INFO - Time taken for Epoch 1: 30.52s - F1: 0.06766950
2026-02-13 17:00:44 - INFO - Time taken for Epoch 2: 30.92s - F1: 0.04185068
2026-02-13 17:01:14 - INFO - Time taken for Epoch 3: 29.75s - F1: 0.04185068
2026-02-13 17:01:43 - INFO - Time taken for Epoch 4: 29.56s - F1: 0.04185068
2026-02-13 17:02:13 - INFO - Time taken for Epoch 5: 29.74s - F1: 0.04185068
2026-02-13 17:02:42 - INFO - Time taken for Epoch 6: 29.60s - F1: 0.04185068
2026-02-13 17:03:12 - INFO - Time taken for Epoch 7: 29.33s - F1: 0.04185068
2026-02-13 17:03:41 - INFO - Time taken for Epoch 8: 29.26s - F1: 0.04185068
2026-02-13 17:03:42 - INFO - Fine-tuning models
2026-02-13 17:03:45 - INFO - Time taken for Epoch 1:2.20 - F1: 0.0302
2026-02-13 17:03:47 - INFO - Time taken for Epoch 2:2.79 - F1: 0.0120
2026-02-13 17:03:50 - INFO - Time taken for Epoch 3:2.23 - F1: 0.0120
2026-02-13 17:03:52 - INFO - Time taken for Epoch 4:2.22 - F1: 0.0120
2026-02-13 17:03:54 - INFO - Time taken for Epoch 5:2.22 - F1: 0.0120
2026-02-13 17:03:56 - INFO - Time taken for Epoch 6:2.21 - F1: 0.0120
2026-02-13 17:03:58 - INFO - Time taken for Epoch 7:2.21 - F1: 0.0120
2026-02-13 17:04:01 - INFO - Time taken for Epoch 8:2.22 - F1: 0.0120
2026-02-13 17:04:03 - INFO - Time taken for Epoch 9:2.28 - F1: 0.0120
2026-02-13 17:04:05 - INFO - Time taken for Epoch 10:2.24 - F1: 0.0120
2026-02-13 17:04:07 - INFO - Time taken for Epoch 11:2.23 - F1: 0.0120
2026-02-13 17:04:07 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 17:04:07 - INFO - Best F1:0.0302 - Best Epoch:0
2026-02-13 17:04:12 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0303, Test ECE: 0.3012
2026-02-13 17:04:12 - INFO - All results: {'f1_macro': 0.030313588850174218, 'ece': np.float64(0.3011974136483416)}
2026-02-13 17:04:12 - INFO - 
Total time taken: 384.63 seconds
2026-02-13 17:04:12 - INFO - Trial 6 finished with value: 0.030313588850174218 and parameters: {'learning_rate': 0.00023002532418304298, 'weight_decay': 9.702953865598856e-05, 'batch_size': 24, 'co_train_epochs': 8, 'epoch_patience': 10}. Best is trial 2 with value: 0.6397259092532659.
2026-02-13 17:04:12 - INFO - Using devices: cuda, cuda
2026-02-13 17:04:12 - INFO - Devices: cuda, cuda
2026-02-13 17:04:12 - INFO - Starting log
2026-02-13 17:04:12 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 17:04:13 - INFO - Learning Rate: 0.000413937471762184
Weight Decay: 0.0008693770865076302
Batch Size: 16
No. Epochs: 17
Epoch Patience: 5
 Accumulation Steps: 4
2026-02-13 17:04:13 - INFO - Generating initial weights
2026-02-13 17:04:30 - INFO - Time taken for Epoch 1:15.30 - F1: 0.0120
2026-02-13 17:04:45 - INFO - Time taken for Epoch 2:15.53 - F1: 0.0120
2026-02-13 17:05:00 - INFO - Time taken for Epoch 3:15.15 - F1: 0.0120
2026-02-13 17:05:16 - INFO - Time taken for Epoch 4:15.18 - F1: 0.0801
2026-02-13 17:05:31 - INFO - Time taken for Epoch 5:14.99 - F1: 0.1355
2026-02-13 17:05:46 - INFO - Time taken for Epoch 6:15.01 - F1: 0.1135
2026-02-13 17:06:01 - INFO - Time taken for Epoch 7:15.25 - F1: 0.2198
2026-02-13 17:06:16 - INFO - Time taken for Epoch 8:15.02 - F1: 0.3536
2026-02-13 17:06:31 - INFO - Time taken for Epoch 9:15.16 - F1: 0.3640
2026-02-13 17:06:46 - INFO - Time taken for Epoch 10:15.23 - F1: 0.3532
2026-02-13 17:07:01 - INFO - Time taken for Epoch 11:15.09 - F1: 0.3679
2026-02-13 17:07:16 - INFO - Time taken for Epoch 12:15.17 - F1: 0.4158
2026-02-13 17:07:31 - INFO - Time taken for Epoch 13:14.83 - F1: 0.4122
2026-02-13 17:07:46 - INFO - Time taken for Epoch 14:14.89 - F1: 0.4217
2026-02-13 17:08:02 - INFO - Time taken for Epoch 15:15.32 - F1: 0.4198
2026-02-13 17:08:17 - INFO - Time taken for Epoch 16:15.34 - F1: 0.4298
2026-02-13 17:08:32 - INFO - Time taken for Epoch 17:15.16 - F1: 0.4352
2026-02-13 17:08:32 - INFO - Best F1:0.4352 - Best Epoch:17
2026-02-13 17:08:33 - INFO - Starting co-training
2026-02-13 17:08:58 - INFO - Time taken for Epoch 1: 25.03s - F1: 0.04185068
2026-02-13 17:09:24 - INFO - Time taken for Epoch 2: 25.69s - F1: 0.04185068
2026-02-13 17:09:49 - INFO - Time taken for Epoch 3: 24.96s - F1: 0.04185068
2026-02-13 17:10:13 - INFO - Time taken for Epoch 4: 24.84s - F1: 0.04185068
2026-02-13 17:10:38 - INFO - Time taken for Epoch 5: 25.02s - F1: 0.04185068
2026-02-13 17:11:03 - INFO - Time taken for Epoch 6: 24.89s - F1: 0.04185068
2026-02-13 17:11:03 - INFO - Performance not improving for 5 consecutive epochs.
2026-02-13 17:11:05 - INFO - Fine-tuning models
2026-02-13 17:11:07 - INFO - Time taken for Epoch 1:2.35 - F1: 0.0419
2026-02-13 17:11:10 - INFO - Time taken for Epoch 2:3.05 - F1: 0.0120
2026-02-13 17:11:13 - INFO - Time taken for Epoch 3:2.32 - F1: 0.0120
2026-02-13 17:11:15 - INFO - Time taken for Epoch 4:2.32 - F1: 0.0120
2026-02-13 17:11:17 - INFO - Time taken for Epoch 5:2.33 - F1: 0.0120
2026-02-13 17:11:20 - INFO - Time taken for Epoch 6:2.45 - F1: 0.0120
2026-02-13 17:11:22 - INFO - Time taken for Epoch 7:2.46 - F1: 0.0120
2026-02-13 17:11:25 - INFO - Time taken for Epoch 8:2.44 - F1: 0.0120
2026-02-13 17:11:27 - INFO - Time taken for Epoch 9:2.34 - F1: 0.0120
2026-02-13 17:11:29 - INFO - Time taken for Epoch 10:2.42 - F1: 0.0120
2026-02-13 17:11:32 - INFO - Time taken for Epoch 11:2.33 - F1: 0.0120
2026-02-13 17:11:32 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 17:11:32 - INFO - Best F1:0.0419 - Best Epoch:0
2026-02-13 17:11:37 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.0417, Test ECE: 0.2669
2026-02-13 17:11:37 - INFO - All results: {'f1_macro': 0.04171180931744312, 'ece': np.float64(0.2669246663874262)}
2026-02-13 17:11:37 - INFO - 
Total time taken: 444.53 seconds
2026-02-13 17:11:37 - INFO - Trial 7 finished with value: 0.04171180931744312 and parameters: {'learning_rate': 0.000413937471762184, 'weight_decay': 0.0008693770865076302, 'batch_size': 16, 'co_train_epochs': 17, 'epoch_patience': 5}. Best is trial 2 with value: 0.6397259092532659.
2026-02-13 17:11:37 - INFO - Using devices: cuda, cuda
2026-02-13 17:11:37 - INFO - Devices: cuda, cuda
2026-02-13 17:11:37 - INFO - Starting log
2026-02-13 17:11:37 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 17:11:37 - INFO - Learning Rate: 1.4050746432730447e-05
Weight Decay: 0.0002979594939722573
Batch Size: 24
No. Epochs: 7
Epoch Patience: 7
 Accumulation Steps: 2
2026-02-13 17:11:38 - INFO - Generating initial weights
2026-02-13 17:11:53 - INFO - Time taken for Epoch 1:14.19 - F1: 0.1173
2026-02-13 17:12:07 - INFO - Time taken for Epoch 2:14.13 - F1: 0.0956
2026-02-13 17:12:21 - INFO - Time taken for Epoch 3:13.95 - F1: 0.0340
2026-02-13 17:12:35 - INFO - Time taken for Epoch 4:13.93 - F1: 0.0130
2026-02-13 17:12:49 - INFO - Time taken for Epoch 5:13.89 - F1: 0.0120
2026-02-13 17:13:03 - INFO - Time taken for Epoch 6:13.92 - F1: 0.0120
2026-02-13 17:13:17 - INFO - Time taken for Epoch 7:14.42 - F1: 0.0120
2026-02-13 17:13:17 - INFO - Best F1:0.1173 - Best Epoch:1
2026-02-13 17:13:18 - INFO - Starting co-training
2026-02-13 17:13:48 - INFO - Time taken for Epoch 1: 29.49s - F1: 0.29500597
2026-02-13 17:14:18 - INFO - Time taken for Epoch 2: 29.94s - F1: 0.32720343
2026-02-13 17:14:48 - INFO - Time taken for Epoch 3: 30.15s - F1: 0.39943765
2026-02-13 17:15:18 - INFO - Time taken for Epoch 4: 29.82s - F1: 0.45206773
2026-02-13 17:15:48 - INFO - Time taken for Epoch 5: 30.74s - F1: 0.44889237
2026-02-13 17:16:18 - INFO - Time taken for Epoch 6: 29.33s - F1: 0.52785893
2026-02-13 17:16:48 - INFO - Time taken for Epoch 7: 30.27s - F1: 0.54519511
2026-02-13 17:16:50 - INFO - Fine-tuning models
2026-02-13 17:16:52 - INFO - Time taken for Epoch 1:2.29 - F1: 0.5570
2026-02-13 17:16:55 - INFO - Time taken for Epoch 2:2.92 - F1: 0.5333
2026-02-13 17:16:57 - INFO - Time taken for Epoch 3:2.26 - F1: 0.5048
2026-02-13 17:17:00 - INFO - Time taken for Epoch 4:2.43 - F1: 0.5120
2026-02-13 17:17:02 - INFO - Time taken for Epoch 5:2.28 - F1: 0.5288
2026-02-13 17:17:04 - INFO - Time taken for Epoch 6:2.28 - F1: 0.5344
2026-02-13 17:17:07 - INFO - Time taken for Epoch 7:2.33 - F1: 0.5499
2026-02-13 17:17:09 - INFO - Time taken for Epoch 8:2.31 - F1: 0.5756
2026-02-13 17:17:12 - INFO - Time taken for Epoch 9:2.92 - F1: 0.5727
2026-02-13 17:17:14 - INFO - Time taken for Epoch 10:2.29 - F1: 0.5683
2026-02-13 17:17:16 - INFO - Time taken for Epoch 11:2.32 - F1: 0.5706
2026-02-13 17:17:19 - INFO - Time taken for Epoch 12:2.27 - F1: 0.5763
2026-02-13 17:17:22 - INFO - Time taken for Epoch 13:2.89 - F1: 0.5828
2026-02-13 17:17:24 - INFO - Time taken for Epoch 14:2.84 - F1: 0.5785
2026-02-13 17:17:27 - INFO - Time taken for Epoch 15:2.32 - F1: 0.5817
2026-02-13 17:17:29 - INFO - Time taken for Epoch 16:2.26 - F1: 0.5862
2026-02-13 17:17:37 - INFO - Time taken for Epoch 17:7.99 - F1: 0.5828
2026-02-13 17:17:39 - INFO - Time taken for Epoch 18:2.25 - F1: 0.5898
2026-02-13 17:17:42 - INFO - Time taken for Epoch 19:2.91 - F1: 0.5907
2026-02-13 17:17:45 - INFO - Time taken for Epoch 20:2.88 - F1: 0.6029
2026-02-13 17:17:48 - INFO - Time taken for Epoch 21:2.87 - F1: 0.6080
2026-02-13 17:17:51 - INFO - Time taken for Epoch 22:2.88 - F1: 0.6032
2026-02-13 17:17:53 - INFO - Time taken for Epoch 23:2.26 - F1: 0.6141
2026-02-13 17:17:56 - INFO - Time taken for Epoch 24:2.87 - F1: 0.6141
2026-02-13 17:17:58 - INFO - Time taken for Epoch 25:2.30 - F1: 0.6230
2026-02-13 17:18:01 - INFO - Time taken for Epoch 26:2.92 - F1: 0.6268
2026-02-13 17:18:04 - INFO - Time taken for Epoch 27:2.87 - F1: 0.6210
2026-02-13 17:18:06 - INFO - Time taken for Epoch 28:2.28 - F1: 0.6237
2026-02-13 17:18:09 - INFO - Time taken for Epoch 29:2.30 - F1: 0.6199
2026-02-13 17:18:11 - INFO - Time taken for Epoch 30:2.29 - F1: 0.6207
2026-02-13 17:18:13 - INFO - Time taken for Epoch 31:2.37 - F1: 0.6159
2026-02-13 17:18:15 - INFO - Time taken for Epoch 32:2.31 - F1: 0.6119
2026-02-13 17:18:18 - INFO - Time taken for Epoch 33:2.33 - F1: 0.6107
2026-02-13 17:18:20 - INFO - Time taken for Epoch 34:2.32 - F1: 0.6107
2026-02-13 17:18:22 - INFO - Time taken for Epoch 35:2.34 - F1: 0.6127
2026-02-13 17:18:25 - INFO - Time taken for Epoch 36:2.34 - F1: 0.6133
2026-02-13 17:18:25 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 17:18:25 - INFO - Best F1:0.6268 - Best Epoch:25
2026-02-13 17:18:30 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.6197, Test ECE: 0.0679
2026-02-13 17:18:30 - INFO - All results: {'f1_macro': 0.6196760564428415, 'ece': np.float64(0.06786804469295923)}
2026-02-13 17:18:30 - INFO - 
Total time taken: 412.80 seconds
2026-02-13 17:18:30 - INFO - Trial 8 finished with value: 0.6196760564428415 and parameters: {'learning_rate': 1.4050746432730447e-05, 'weight_decay': 0.0002979594939722573, 'batch_size': 24, 'co_train_epochs': 7, 'epoch_patience': 7}. Best is trial 2 with value: 0.6397259092532659.
2026-02-13 17:18:30 - INFO - Using devices: cuda, cuda
2026-02-13 17:18:30 - INFO - Devices: cuda, cuda
2026-02-13 17:18:30 - INFO - Starting log
2026-02-13 17:18:30 - INFO - Dataset: humanitarian10, Event: california_wildfires_2018, N: 10, Seed: 1234, HF Model: GPT-4o, NumShots: 10, PLM: bert-tweet
2026-02-13 17:18:30 - INFO - Learning Rate: 8.80242263565883e-05
Weight Decay: 0.0005930622232038862
Batch Size: 8
No. Epochs: 8
Epoch Patience: 7
 Accumulation Steps: 8
2026-02-13 17:18:30 - INFO - Generating initial weights
2026-02-13 17:18:49 - INFO - Time taken for Epoch 1:17.30 - F1: 0.0120
2026-02-13 17:19:06 - INFO - Time taken for Epoch 2:17.19 - F1: 0.0120
2026-02-13 17:19:23 - INFO - Time taken for Epoch 3:17.19 - F1: 0.0120
2026-02-13 17:19:41 - INFO - Time taken for Epoch 4:17.29 - F1: 0.0120
2026-02-13 17:19:58 - INFO - Time taken for Epoch 5:17.03 - F1: 0.0861
2026-02-13 17:20:15 - INFO - Time taken for Epoch 6:16.86 - F1: 0.1709
2026-02-13 17:20:32 - INFO - Time taken for Epoch 7:16.85 - F1: 0.2399
2026-02-13 17:20:49 - INFO - Time taken for Epoch 8:17.02 - F1: 0.2625
2026-02-13 17:20:49 - INFO - Best F1:0.2625 - Best Epoch:8
2026-02-13 17:20:49 - INFO - Starting co-training
2026-02-13 17:21:14 - INFO - Time taken for Epoch 1: 24.30s - F1: 0.29318514
2026-02-13 17:21:38 - INFO - Time taken for Epoch 2: 24.82s - F1: 0.21909741
2026-02-13 17:22:03 - INFO - Time taken for Epoch 3: 24.31s - F1: 0.28449081
2026-02-13 17:22:27 - INFO - Time taken for Epoch 4: 24.25s - F1: 0.28451266
2026-02-13 17:22:51 - INFO - Time taken for Epoch 5: 24.24s - F1: 0.25519701
2026-02-13 17:23:15 - INFO - Time taken for Epoch 6: 24.23s - F1: 0.27017743
2026-02-13 17:23:40 - INFO - Time taken for Epoch 7: 24.26s - F1: 0.25101004
2026-02-13 17:24:04 - INFO - Time taken for Epoch 8: 24.27s - F1: 0.27234414
2026-02-13 17:24:05 - INFO - Fine-tuning models
2026-02-13 17:24:08 - INFO - Time taken for Epoch 1:2.72 - F1: 0.2904
2026-02-13 17:24:11 - INFO - Time taken for Epoch 2:3.30 - F1: 0.2836
2026-02-13 17:24:14 - INFO - Time taken for Epoch 3:2.70 - F1: 0.3370
2026-02-13 17:24:17 - INFO - Time taken for Epoch 4:3.37 - F1: 0.3700
2026-02-13 17:24:21 - INFO - Time taken for Epoch 5:3.38 - F1: 0.3688
2026-02-13 17:24:23 - INFO - Time taken for Epoch 6:2.70 - F1: 0.3757
2026-02-13 17:24:28 - INFO - Time taken for Epoch 7:4.37 - F1: 0.3826
2026-02-13 17:24:31 - INFO - Time taken for Epoch 8:3.37 - F1: 0.3805
2026-02-13 17:24:34 - INFO - Time taken for Epoch 9:2.69 - F1: 0.3880
2026-02-13 17:24:37 - INFO - Time taken for Epoch 10:3.39 - F1: 0.4132
2026-02-13 17:24:41 - INFO - Time taken for Epoch 11:3.37 - F1: 0.4208
2026-02-13 17:24:53 - INFO - Time taken for Epoch 12:11.89 - F1: 0.4152
2026-02-13 17:24:55 - INFO - Time taken for Epoch 13:2.69 - F1: 0.4105
2026-02-13 17:24:58 - INFO - Time taken for Epoch 14:2.70 - F1: 0.4170
2026-02-13 17:25:01 - INFO - Time taken for Epoch 15:2.70 - F1: 0.4366
2026-02-13 17:25:04 - INFO - Time taken for Epoch 16:3.41 - F1: 0.4648
2026-02-13 17:25:07 - INFO - Time taken for Epoch 17:3.39 - F1: 0.4848
2026-02-13 17:25:11 - INFO - Time taken for Epoch 18:3.37 - F1: 0.5075
2026-02-13 17:25:14 - INFO - Time taken for Epoch 19:3.33 - F1: 0.5014
2026-02-13 17:25:17 - INFO - Time taken for Epoch 20:2.70 - F1: 0.5134
2026-02-13 17:25:20 - INFO - Time taken for Epoch 21:3.42 - F1: 0.5189
2026-02-13 17:25:26 - INFO - Time taken for Epoch 22:6.09 - F1: 0.5159
2026-02-13 17:25:29 - INFO - Time taken for Epoch 23:2.69 - F1: 0.5037
2026-02-13 17:25:32 - INFO - Time taken for Epoch 24:2.69 - F1: 0.5001
2026-02-13 17:25:34 - INFO - Time taken for Epoch 25:2.69 - F1: 0.4961
2026-02-13 17:25:37 - INFO - Time taken for Epoch 26:2.69 - F1: 0.5029
2026-02-13 17:25:40 - INFO - Time taken for Epoch 27:2.69 - F1: 0.5063
2026-02-13 17:25:42 - INFO - Time taken for Epoch 28:2.69 - F1: 0.5237
2026-02-13 17:25:46 - INFO - Time taken for Epoch 29:3.34 - F1: 0.5112
2026-02-13 17:25:48 - INFO - Time taken for Epoch 30:2.70 - F1: 0.5156
2026-02-13 17:25:51 - INFO - Time taken for Epoch 31:2.70 - F1: 0.5204
2026-02-13 17:25:54 - INFO - Time taken for Epoch 32:2.70 - F1: 0.5179
2026-02-13 17:25:57 - INFO - Time taken for Epoch 33:2.70 - F1: 0.5204
2026-02-13 17:25:59 - INFO - Time taken for Epoch 34:2.74 - F1: 0.5187
2026-02-13 17:26:02 - INFO - Time taken for Epoch 35:2.70 - F1: 0.5198
2026-02-13 17:26:05 - INFO - Time taken for Epoch 36:2.70 - F1: 0.5227
2026-02-13 17:26:07 - INFO - Time taken for Epoch 37:2.70 - F1: 0.5202
2026-02-13 17:26:10 - INFO - Time taken for Epoch 38:2.70 - F1: 0.5193
2026-02-13 17:26:10 - INFO - Performance not improving for 10 consecutive epochs.
2026-02-13 17:26:10 - INFO - Best F1:0.5237 - Best Epoch:27
2026-02-13 17:26:16 - INFO - 

Hf Model: GPT-4o PLM: bert-tweet Dataset: humanitarian10, NumShots: 10, N: 10 Test SEED: 1234 F1: 0.5080, Test ECE: 0.1665
2026-02-13 17:26:16 - INFO - All results: {'f1_macro': 0.50800930500272, 'ece': np.float64(0.1665288360864638)}
2026-02-13 17:26:16 - INFO - 
Total time taken: 466.19 seconds
2026-02-13 17:26:16 - INFO - Trial 9 finished with value: 0.50800930500272 and parameters: {'learning_rate': 8.80242263565883e-05, 'weight_decay': 0.0005930622232038862, 'batch_size': 8, 'co_train_epochs': 8, 'epoch_patience': 7}. Best is trial 2 with value: 0.6397259092532659.
2026-02-13 17:26:16 - INFO - 
[BEST TRIAL RESULTS]
2026-02-13 17:26:16 - INFO - F1 Score: 0.6397
2026-02-13 17:26:16 - INFO - Params: {'learning_rate': 2.1352408798432105e-05, 'weight_decay': 0.0005411660233026563, 'batch_size': 24, 'co_train_epochs': 6, 'epoch_patience': 5}
2026-02-13 17:26:16 - INFO -   learning_rate: 2.1352408798432105e-05
2026-02-13 17:26:16 - INFO -   weight_decay: 0.0005411660233026563
2026-02-13 17:26:16 - INFO -   batch_size: 24
2026-02-13 17:26:16 - INFO -   co_train_epochs: 6
2026-02-13 17:26:16 - INFO -   epoch_patience: 5
2026-02-13 17:26:16 - INFO - 
Total time taken: 4072.16 seconds
